
#include <kernel/kernel_includes.hpp>
static constexpr void *__stream = &sc::runtime::default_stream;

extern int8_t rn50_backbone_bs4_data[222400];
static constexpr int8_t* __module_data = rn50_backbone_bs4_data;
alignas(64) static int8_t __uninitialized_data[23657592UL];

static void __init_const_globals(int8_t* __restrict__ backbone_output, int8_t* __restrict__ backbone_input, float* __restrict__ res2a_weight_b, float* __restrict__ res2a_bias_b, float* __restrict__ res2a_weight_0, float* __restrict__ res2a_bias_0, float* __restrict__ res2a_weight_1, float* __restrict__ res2a_bias_1, float* __restrict__ res2a_weight_2, float* __restrict__ res2a_bias_2, float* __restrict__ res2b_weight_0, float* __restrict__ res2b_bias_0, float* __restrict__ res2b_weight_1, float* __restrict__ res2b_bias_1, float* __restrict__ res2b_weight_2, float* __restrict__ res2b_bias_2, float* __restrict__ res2c_weight_0, float* __restrict__ res2c_bias_0, float* __restrict__ res2c_weight_1, float* __restrict__ res2c_bias_1, float* __restrict__ res2c_weight_2, float* __restrict__ res2c_bias_2, float* __restrict__ res3a_weight_b, float* __restrict__ res3a_bias_b, float* __restrict__ res3a_weight_0, float* __restrict__ res3a_bias_0, float* __restrict__ res3a_weight_1, float* __restrict__ res3a_bias_1, float* __restrict__ res3a_weight_2, float* __restrict__ res3a_bias_2, float* __restrict__ res3b_weight_0, float* __restrict__ res3b_bias_0, float* __restrict__ res3b_weight_1, float* __restrict__ res3b_bias_1, float* __restrict__ res3b_weight_2, float* __restrict__ res3b_bias_2, float* __restrict__ res3c_weight_0, float* __restrict__ res3c_bias_0, float* __restrict__ res3c_weight_1, float* __restrict__ res3c_bias_1, float* __restrict__ res3c_weight_2, float* __restrict__ res3c_bias_2, float* __restrict__ res3d_weight_0, float* __restrict__ res3d_bias_0, float* __restrict__ res3d_weight_1, float* __restrict__ res3d_bias_1, float* __restrict__ res3d_weight_2, float* __restrict__ res3d_bias_2, float* __restrict__ res4a_weight_b, float* __restrict__ res4a_bias_b, float* __restrict__ res4a_weight_0, float* __restrict__ res4a_bias_0, float* __restrict__ res4a_weight_1, float* __restrict__ res4a_bias_1, float* __restrict__ res4a_weight_2, float* __restrict__ res4a_bias_2, float* __restrict__ res4b_weight_0, float* __restrict__ res4b_bias_0, float* __restrict__ res4b_weight_1, float* __restrict__ res4b_bias_1, float* __restrict__ res4b_weight_2, float* __restrict__ res4b_bias_2, float* __restrict__ res4c_weight_0, float* __restrict__ res4c_bias_0, float* __restrict__ res4c_weight_1, float* __restrict__ res4c_bias_1, float* __restrict__ res4c_weight_2, float* __restrict__ res4c_bias_2, float* __restrict__ res4d_weight_0, float* __restrict__ res4d_bias_0, float* __restrict__ res4d_weight_1, float* __restrict__ res4d_bias_1, float* __restrict__ res4d_weight_2, float* __restrict__ res4d_bias_2, float* __restrict__ res4e_weight_0, float* __restrict__ res4e_bias_0, float* __restrict__ res4e_weight_1, float* __restrict__ res4e_bias_1, float* __restrict__ res4e_weight_2, float* __restrict__ res4e_bias_2, float* __restrict__ res4f_weight_0, float* __restrict__ res4f_bias_0, float* __restrict__ res4f_weight_1, float* __restrict__ res4f_bias_1, float* __restrict__ res4f_weight_2, float* __restrict__ res4f_bias_2, float* __restrict__ res5a_weight_b, float* __restrict__ res5a_bias_b, float* __restrict__ res5a_weight_0, float* __restrict__ res5a_bias_0, float* __restrict__ res5a_weight_1, float* __restrict__ res5a_bias_1, float* __restrict__ res5a_weight_2, float* __restrict__ res5a_bias_2, float* __restrict__ res5b_weight_0, float* __restrict__ res5b_bias_0, float* __restrict__ res5b_weight_1, float* __restrict__ res5b_bias_1, float* __restrict__ res5b_weight_2, float* __restrict__ res5b_bias_2, float* __restrict__ res5c_weight_0, float* __restrict__ res5c_bias_0, float* __restrict__ res5c_weight_1, float* __restrict__ res5c_bias_1, float* __restrict__ res5c_weight_2, float* __restrict__ res5c_bias_2) noexcept __attribute__((nonnull (1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25,26,27,28,29,30,31,32,33,34,35,36,37,38,39,40,41,42,43,44,45,46,47,48,49,50,51,52,53,54,55,56,57,58,59,60,61,62,63,64,65,66,67,68,69,70,71,72,73,74,75,76,77,78,79,80,81,82,83,84,85,86,87,88,89,90,91,92,93,94,95,96,97,98,99,100,101,102,103,104,105,106)));
static bool batchwise_4_fused_res2a_conv_b_cast_mul_add_cast_reorder_res2a_conv_0_cast_mul_add_relu_cast_res2a_conv_1_cast_mul_add_relu_cast_res2a_conv_2_cast_mul_add_cast_add_cast_reorder_res2b_conv_0_cast_mul_add_relu_cast_res2b_conv_1_cast_mul_add_relu_cast_reorder_res2b_conv_2_cast_mul_add_cast_add_cast_reorder_res2c_conv_0_cast_mul_add_relu_cast_reorder_res2c_conv_1_cast_mul_add_relu_cast_reorder_res2c_conv_2_cast_mul_add_cast_add_cast_reorder_res3a_conv_b_cast_mul_add_cast_res3a_conv_0_cast_mul_add_relu_cast_reorder_res3a_conv_1_cast_mul_add_relu_cast_res3a_conv_2_cast_mul_add_cast_add_cast_reorder_res3b_conv_0_cast_mul_add_relu_cast_reorder_res3b_conv_1_cast_mul_add_relu_cast_reorder_res3b_conv_2_cast_mul_add_cast_add_cast_reorder_res3c_conv_0_cast_mul_add_relu_cast_reorder_res3c_conv_1_cast_mul_add_relu_cast_reorder_res3c_conv_2_cast_mul_add_cast_add_cast_reorder_res3d_conv_0_cast_mul_add_relu_cast_reorder_res3d_conv_1_cast_mul_add_relu_cast_res3d_conv_2_cast_mul_add_cast_add_cast_res4a_conv_b_cast_mul_add_cast_reorder_res4a_conv_0_cast_mul_add_relu_cast_res4a_conv_1_cast_mul_add_relu_cast_reorder_res4a_conv_2_cast_mul_add_cast_add_cast_reorder_res4b_conv_0_cast_mul_add_relu_cast_reorder_res4b_conv_1_cast_mul_add_relu_cast_reorder_res4b_conv_2_cast_mul_add_cast_add_cast_reorder_res4c_conv_0_cast_mul_add_relu_cast_reorder_res4c_conv_1_cast_mul_add_relu_cast_reorder_res4c_conv_2_cast_mul_add_cast_add_cast_res4d_conv_0_cast_mul_add_relu_cast_res4d_conv_1_cast_mul_add_relu_cast_reorder_res4d_conv_2_cast_mul_add_cast_add_cast_res4e_conv_0_cast_mul_add_relu_cast_reorder_res4e_conv_1_cast_mul_add_relu_cast_reorder_res4e_conv_2_cast_mul_add_cast_add_cast_reorder_reorder_res4f_conv_0_cast_mul_add_relu_cast_reorder_res4f_conv_1_cast_mul_add_relu_cast_reorder_res4f_conv_2_cast_mul_add_cast_add_cast__683(uint8_t* __restrict__ __outs_0, int8_t* __restrict__ __ins_0, int8_t* __restrict__ __ins_1, float* __restrict__ __ins_2, float* __restrict__ __ins_3, int8_t* __restrict__ __ins_4, float* __restrict__ __ins_5, float* __restrict__ __ins_6, int8_t* __restrict__ __ins_7, float* __restrict__ __ins_8, float* __restrict__ __ins_9, int8_t* __restrict__ __ins_10, float* __restrict__ __ins_11, float* __restrict__ __ins_12, int8_t* __restrict__ __ins_13, float* __restrict__ __ins_14, float* __restrict__ __ins_15, int8_t* __restrict__ __ins_16, float* __restrict__ __ins_17, float* __restrict__ __ins_18, int8_t* __restrict__ __ins_19, float* __restrict__ __ins_20, float* __restrict__ __ins_21, int8_t* __restrict__ __ins_22, float* __restrict__ __ins_23, float* __restrict__ __ins_24, int8_t* __restrict__ __ins_25, float* __restrict__ __ins_26, float* __restrict__ __ins_27, int8_t* __restrict__ __ins_28, float* __restrict__ __ins_29, float* __restrict__ __ins_30, int8_t* __restrict__ __ins_31, float* __restrict__ __ins_32, float* __restrict__ __ins_33, int8_t* __restrict__ __ins_34, float* __restrict__ __ins_35, float* __restrict__ __ins_36, int8_t* __restrict__ __ins_37, float* __restrict__ __ins_38, float* __restrict__ __ins_39, int8_t* __restrict__ __ins_40, float* __restrict__ __ins_41, float* __restrict__ __ins_42, int8_t* __restrict__ __ins_43, float* __restrict__ __ins_44, float* __restrict__ __ins_45, int8_t* __restrict__ __ins_46, float* __restrict__ __ins_47, float* __restrict__ __ins_48, int8_t* __restrict__ __ins_49, float* __restrict__ __ins_50, float* __restrict__ __ins_51, int8_t* __restrict__ __ins_52, float* __restrict__ __ins_53, float* __restrict__ __ins_54, int8_t* __restrict__ __ins_55, float* __restrict__ __ins_56, float* __restrict__ __ins_57, int8_t* __restrict__ __ins_58, float* __restrict__ __ins_59, float* __restrict__ __ins_60, int8_t* __restrict__ __ins_61, float* __restrict__ __ins_62, float* __restrict__ __ins_63, int8_t* __restrict__ __ins_64, float* __restrict__ __ins_65, float* __restrict__ __ins_66, int8_t* __restrict__ __ins_67, float* __restrict__ __ins_68, float* __restrict__ __ins_69, int8_t* __restrict__ __ins_70, float* __restrict__ __ins_71, float* __restrict__ __ins_72, int8_t* __restrict__ __ins_73, float* __restrict__ __ins_74, float* __restrict__ __ins_75, int8_t* __restrict__ __ins_76, float* __restrict__ __ins_77, float* __restrict__ __ins_78, int8_t* __restrict__ __ins_79, float* __restrict__ __ins_80, float* __restrict__ __ins_81, int8_t* __restrict__ __ins_82, float* __restrict__ __ins_83, float* __restrict__ __ins_84, int8_t* __restrict__ __ins_85, float* __restrict__ __ins_86, float* __restrict__ __ins_87, int8_t* __restrict__ __ins_88, float* __restrict__ __ins_89, float* __restrict__ __ins_90, int8_t* __restrict__ __ins_91, float* __restrict__ __ins_92, float* __restrict__ __ins_93, int8_t* __restrict__ __ins_94, float* __restrict__ __ins_95, float* __restrict__ __ins_96, int8_t* __restrict__ __ins_97, float* __restrict__ __ins_98, float* __restrict__ __ins_99, int8_t* __restrict__ __ins_100, float* __restrict__ __ins_101, float* __restrict__ __ins_102, int8_t* __restrict__ __ins_103, float* __restrict__ __ins_104, float* __restrict__ __ins_105, int8_t* __restrict__ __ins_106, float* __restrict__ __ins_107, float* __restrict__ __ins_108, int8_t* __restrict__ __ins_109, float* __restrict__ __ins_110, float* __restrict__ __ins_111, int8_t* __restrict__ __ins_112, float* __restrict__ __ins_113, float* __restrict__ __ins_114, int8_t* __restrict__ __ins_115, float* __restrict__ __ins_116, float* __restrict__ __ins_117, int8_t* __restrict__ __ins_118, float* __restrict__ __ins_119, float* __restrict__ __ins_120, int8_t* __restrict__ __ins_121, float* __restrict__ __ins_122, float* __restrict__ __ins_123, int8_t* __restrict__ __ins_124, float* __restrict__ __ins_125, float* __restrict__ __ins_126) noexcept __attribute__((nonnull (1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25,26,27,28,29,30,31,32,33,34,35,36,37,38,39,40,41,42,43,44,45,46,47,48,49,50,51,52,53,54,55,56,57,58,59,60,61,62,63,64,65,66,67,68,69,70,71,72,73,74,75,76,77,78,79,80,81,82,83,84,85,86,87,88,89,90,91,92,93,94,95,96,97,98,99,100,101,102,103,104,105,106,107,108,109,110,111,112,113,114,115,116,117,118,119,120,121,122,123,124,125,126,127,128)));
static bool res5a_conv_0_cast_mul_add_relu_cast_reorder__681(int8_t* __restrict__ __outs_0, uint8_t* __restrict__ __ins_0, int8_t* __restrict__ __ins_1, float* __restrict__ __ins_2, float* __restrict__ __ins_3) noexcept __attribute__((nonnull (1,2,3,4,5)));
static bool res5a_conv_1_cast_mul_add_relu_cast_reorder__680(int8_t* __restrict__ __outs_0, int8_t* __restrict__ __ins_0, int8_t* __restrict__ __ins_1, float* __restrict__ __ins_2, float* __restrict__ __ins_3) noexcept __attribute__((nonnull (1,2,3,4,5)));
static bool res5a_conv_b_cast_mul_add_cast_reorder__682(int8_t* __restrict__ __outs_0, uint8_t* __restrict__ __ins_0, int8_t* __restrict__ __ins_1, float* __restrict__ __ins_2, float* __restrict__ __ins_3) noexcept __attribute__((nonnull (1,2,3,4,5)));
static bool res5a_conv_2_cast_mul_add_cast_add_cast__679(uint8_t* __restrict__ __outs_0, int8_t* __restrict__ __ins_0, int8_t* __restrict__ __ins_1, float* __restrict__ __ins_2, float* __restrict__ __ins_3, int8_t* __restrict__ __ins_4) noexcept __attribute__((nonnull (1,2,3,4,5,6)));
static bool res5b_conv_0_cast_mul_add_relu_cast_reorder__678(int8_t* __restrict__ __outs_0, uint8_t* __restrict__ __ins_0, int8_t* __restrict__ __ins_1, float* __restrict__ __ins_2, float* __restrict__ __ins_3) noexcept __attribute__((nonnull (1,2,3,4,5)));
static bool res5b_conv_1_cast_mul_add_relu_cast_reorder__677(int8_t* __restrict__ __outs_0, int8_t* __restrict__ __ins_0, int8_t* __restrict__ __ins_1, float* __restrict__ __ins_2, float* __restrict__ __ins_3) noexcept __attribute__((nonnull (1,2,3,4,5)));
static bool res5b_conv_2_cast_mul_add_cast_add_cast_reorder__676(uint8_t* __restrict__ __outs_0, int8_t* __restrict__ __ins_0, int8_t* __restrict__ __ins_1, float* __restrict__ __ins_2, float* __restrict__ __ins_3, uint8_t* __restrict__ __ins_4) noexcept __attribute__((nonnull (1,2,3,4,5,6)));
static bool res5c_conv_0_cast_mul_add_relu_cast_reorder__675(int8_t* __restrict__ __outs_0, uint8_t* __restrict__ __ins_0, int8_t* __restrict__ __ins_1, float* __restrict__ __ins_2, float* __restrict__ __ins_3) noexcept __attribute__((nonnull (1,2,3,4,5)));
static bool res5c_conv_1_cast_mul_add_relu_cast_reorder__674(int8_t* __restrict__ __outs_0, int8_t* __restrict__ __ins_0, int8_t* __restrict__ __ins_1, float* __restrict__ __ins_2, float* __restrict__ __ins_3) noexcept __attribute__((nonnull (1,2,3,4,5)));
static bool res5c_conv_2_cast_mul_add_cast_add_cast_cast__673(int8_t* __restrict__ __outs_0, int8_t* __restrict__ __ins_0, int8_t* __restrict__ __ins_1, float* __restrict__ __ins_2, float* __restrict__ __ins_3, uint8_t* __restrict__ __ins_4) noexcept __attribute__((nonnull (1,2,3,4,5,6)));
static bool reorder__105(int8_t* __restrict__ __outs_0, int8_t* __restrict__ __ins_0) noexcept __attribute__((nonnull (1,2)));
static void reorder__4810_closure_0_0wrapper(void* __stream, int8_t* __restrict__ __module_data, uint64_t i, generic_val* __restrict__ args) noexcept __attribute__((nonnull (2,4)));
static void reorder__4880_closure_1_0wrapper(void* __stream, int8_t* __restrict__ __module_data, uint64_t i, generic_val* __restrict__ args) noexcept __attribute__((nonnull (2,4)));
static void reorder__5040_closure_2_0wrapper(void* __stream, int8_t* __restrict__ __module_data, uint64_t i, generic_val* __restrict__ args) noexcept __attribute__((nonnull (2,4)));
static void reorder__5130_closure_3_0wrapper(void* __stream, int8_t* __restrict__ __module_data, uint64_t i, generic_val* __restrict__ args) noexcept __attribute__((nonnull (2,4)));
static void reorder__5200_closure_4_0wrapper(void* __stream, int8_t* __restrict__ __module_data, uint64_t i, generic_val* __restrict__ args) noexcept __attribute__((nonnull (2,4)));
static void reorder__5290_closure_5_0wrapper(void* __stream, int8_t* __restrict__ __module_data, uint64_t i, generic_val* __restrict__ args) noexcept __attribute__((nonnull (2,4)));
static void reorder__4200_closure_6_0wrapper(void* __stream, int8_t* __restrict__ __module_data, uint64_t i, generic_val* __restrict__ args) noexcept __attribute__((nonnull (2,4)));
static void reorder__4240_closure_7_0wrapper(void* __stream, int8_t* __restrict__ __module_data, uint64_t i, generic_val* __restrict__ args) noexcept __attribute__((nonnull (2,4)));
static void reorder__4270_closure_8_0wrapper(void* __stream, int8_t* __restrict__ __module_data, uint64_t i, generic_val* __restrict__ args) noexcept __attribute__((nonnull (2,4)));
static void reorder__4310_closure_9_0wrapper(void* __stream, int8_t* __restrict__ __module_data, uint64_t i, generic_val* __restrict__ args) noexcept __attribute__((nonnull (2,4)));
static void reorder__4340_closure_10_0wrapper(void* __stream, int8_t* __restrict__ __module_data, uint64_t i, generic_val* __restrict__ args) noexcept __attribute__((nonnull (2,4)));
static void reorder__4370_closure_11_0wrapper(void* __stream, int8_t* __restrict__ __module_data, uint64_t i, generic_val* __restrict__ args) noexcept __attribute__((nonnull (2,4)));
static void reorder__4400_closure_12_0wrapper(void* __stream, int8_t* __restrict__ __module_data, uint64_t i, generic_val* __restrict__ args) noexcept __attribute__((nonnull (2,4)));
static void reorder__4430_closure_13_0wrapper(void* __stream, int8_t* __restrict__ __module_data, uint64_t i, generic_val* __restrict__ args) noexcept __attribute__((nonnull (2,4)));
static void reorder__4460_closure_14_0wrapper(void* __stream, int8_t* __restrict__ __module_data, uint64_t i, generic_val* __restrict__ args) noexcept __attribute__((nonnull (2,4)));
static void reorder__4490_closure_15_0wrapper(void* __stream, int8_t* __restrict__ __module_data, uint64_t i, generic_val* __restrict__ args) noexcept __attribute__((nonnull (2,4)));
static void reorder__4520_closure_16_0wrapper(void* __stream, int8_t* __restrict__ __module_data, uint64_t i, generic_val* __restrict__ args) noexcept __attribute__((nonnull (2,4)));
static void reorder__4550_closure_17_0wrapper(void* __stream, int8_t* __restrict__ __module_data, uint64_t i, generic_val* __restrict__ args) noexcept __attribute__((nonnull (2,4)));
static void reorder__4580_closure_18_0wrapper(void* __stream, int8_t* __restrict__ __module_data, uint64_t i, generic_val* __restrict__ args) noexcept __attribute__((nonnull (2,4)));
static void reorder__4620_closure_19_0wrapper(void* __stream, int8_t* __restrict__ __module_data, uint64_t i, generic_val* __restrict__ args) noexcept __attribute__((nonnull (2,4)));
static void reorder__4660_closure_20_0wrapper(void* __stream, int8_t* __restrict__ __module_data, uint64_t i, generic_val* __restrict__ args) noexcept __attribute__((nonnull (2,4)));
static void reorder__4690_closure_21_0wrapper(void* __stream, int8_t* __restrict__ __module_data, uint64_t i, generic_val* __restrict__ args) noexcept __attribute__((nonnull (2,4)));
static void reorder__4720_closure_22_0wrapper(void* __stream, int8_t* __restrict__ __module_data, uint64_t i, generic_val* __restrict__ args) noexcept __attribute__((nonnull (2,4)));
static void reorder__4750_closure_23_0wrapper(void* __stream, int8_t* __restrict__ __module_data, uint64_t i, generic_val* __restrict__ args) noexcept __attribute__((nonnull (2,4)));
static void reorder__4780_closure_24_0wrapper(void* __stream, int8_t* __restrict__ __module_data, uint64_t i, generic_val* __restrict__ args) noexcept __attribute__((nonnull (2,4)));
static void reorder__4850_closure_25_0wrapper(void* __stream, int8_t* __restrict__ __module_data, uint64_t i, generic_val* __restrict__ args) noexcept __attribute__((nonnull (2,4)));
static void reorder__4910_closure_26_0wrapper(void* __stream, int8_t* __restrict__ __module_data, uint64_t i, generic_val* __restrict__ args) noexcept __attribute__((nonnull (2,4)));
static void reorder__4940_closure_27_0wrapper(void* __stream, int8_t* __restrict__ __module_data, uint64_t i, generic_val* __restrict__ args) noexcept __attribute__((nonnull (2,4)));
static void reorder__4980_closure_28_0wrapper(void* __stream, int8_t* __restrict__ __module_data, uint64_t i, generic_val* __restrict__ args) noexcept __attribute__((nonnull (2,4)));
static void reorder__5010_closure_29_0wrapper(void* __stream, int8_t* __restrict__ __module_data, uint64_t i, generic_val* __restrict__ args) noexcept __attribute__((nonnull (2,4)));
static void reorder__5070_closure_30_0wrapper(void* __stream, int8_t* __restrict__ __module_data, uint64_t i, generic_val* __restrict__ args) noexcept __attribute__((nonnull (2,4)));
static void reorder__5100_closure_31_0wrapper(void* __stream, int8_t* __restrict__ __module_data, uint64_t i, generic_val* __restrict__ args) noexcept __attribute__((nonnull (2,4)));
static void reorder__5170_closure_32_0wrapper(void* __stream, int8_t* __restrict__ __module_data, uint64_t i, generic_val* __restrict__ args) noexcept __attribute__((nonnull (2,4)));
static void reorder__5230_closure_33_0wrapper(void* __stream, int8_t* __restrict__ __module_data, uint64_t i, generic_val* __restrict__ args) noexcept __attribute__((nonnull (2,4)));
static void reorder__5260_closure_34_0wrapper(void* __stream, int8_t* __restrict__ __module_data, uint64_t i, generic_val* __restrict__ args) noexcept __attribute__((nonnull (2,4)));
static void reorder__5320_closure_35_0wrapper(void* __stream, int8_t* __restrict__ __module_data, uint64_t i, generic_val* __restrict__ args) noexcept __attribute__((nonnull (2,4)));
static void reorder__5350_closure_36_0wrapper(void* __stream, int8_t* __restrict__ __module_data, uint64_t i, generic_val* __restrict__ args) noexcept __attribute__((nonnull (2,4)));
static void reorder__5380_closure_37_0wrapper(void* __stream, int8_t* __restrict__ __module_data, uint64_t i, generic_val* __restrict__ args) noexcept __attribute__((nonnull (2,4)));
static void reorder__5410_closure_38_0wrapper(void* __stream, int8_t* __restrict__ __module_data, uint64_t i, generic_val* __restrict__ args) noexcept __attribute__((nonnull (2,4)));
static void reorder__5440_closure_39_0wrapper(void* __stream, int8_t* __restrict__ __module_data, uint64_t i, generic_val* __restrict__ args) noexcept __attribute__((nonnull (2,4)));
static void reorder__5470_closure_40_0wrapper(void* __stream, int8_t* __restrict__ __module_data, uint64_t i, generic_val* __restrict__ args) noexcept __attribute__((nonnull (2,4)));
static void reorder__5500_closure_41_0wrapper(void* __stream, int8_t* __restrict__ __module_data, uint64_t i, generic_val* __restrict__ args) noexcept __attribute__((nonnull (2,4)));
static void reorder__5530_closure_42_0wrapper(void* __stream, int8_t* __restrict__ __module_data, uint64_t i, generic_val* __restrict__ args) noexcept __attribute__((nonnull (2,4)));
static void reorder__5560_closure_43_0wrapper(void* __stream, int8_t* __restrict__ __module_data, uint64_t i, generic_val* __restrict__ args) noexcept __attribute__((nonnull (2,4)));
static void reorder__5590_closure_44_0wrapper(void* __stream, int8_t* __restrict__ __module_data, uint64_t i, generic_val* __restrict__ args) noexcept __attribute__((nonnull (2,4)));
static void reorder__4250_closure_45_0wrapper(void* __stream, int8_t* __restrict__ __module_data, uint64_t i, generic_val* __restrict__ args) noexcept __attribute__((nonnull (2,4)));
static void reorder__4320_closure_46_0wrapper(void* __stream, int8_t* __restrict__ __module_data, uint64_t i, generic_val* __restrict__ args) noexcept __attribute__((nonnull (2,4)));
static void reorder__4380_closure_47_0wrapper(void* __stream, int8_t* __restrict__ __module_data, uint64_t i, generic_val* __restrict__ args) noexcept __attribute__((nonnull (2,4)));
static void reorder__4410_closure_48_0wrapper(void* __stream, int8_t* __restrict__ __module_data, uint64_t i, generic_val* __restrict__ args) noexcept __attribute__((nonnull (2,4)));
static void reorder__4500_closure_49_0wrapper(void* __stream, int8_t* __restrict__ __module_data, uint64_t i, generic_val* __restrict__ args) noexcept __attribute__((nonnull (2,4)));
static void reorder__4530_closure_50_0wrapper(void* __stream, int8_t* __restrict__ __module_data, uint64_t i, generic_val* __restrict__ args) noexcept __attribute__((nonnull (2,4)));
static void reorder__4590_closure_51_0wrapper(void* __stream, int8_t* __restrict__ __module_data, uint64_t i, generic_val* __restrict__ args) noexcept __attribute__((nonnull (2,4)));
static void reorder__4670_closure_52_0wrapper(void* __stream, int8_t* __restrict__ __module_data, uint64_t i, generic_val* __restrict__ args) noexcept __attribute__((nonnull (2,4)));
static void reorder__4730_closure_53_0wrapper(void* __stream, int8_t* __restrict__ __module_data, uint64_t i, generic_val* __restrict__ args) noexcept __attribute__((nonnull (2,4)));
static void reorder__4760_closure_54_0wrapper(void* __stream, int8_t* __restrict__ __module_data, uint64_t i, generic_val* __restrict__ args) noexcept __attribute__((nonnull (2,4)));
static void reorder__4210_closure_55_0wrapper(void* __stream, int8_t* __restrict__ __module_data, uint64_t i, generic_val* __restrict__ args) noexcept __attribute__((nonnull (2,4)));
static void reorder__4280_closure_56_0wrapper(void* __stream, int8_t* __restrict__ __module_data, uint64_t i, generic_val* __restrict__ args) noexcept __attribute__((nonnull (2,4)));
static void reorder__4350_closure_57_0wrapper(void* __stream, int8_t* __restrict__ __module_data, uint64_t i, generic_val* __restrict__ args) noexcept __attribute__((nonnull (2,4)));
static void reorder__4440_closure_58_0wrapper(void* __stream, int8_t* __restrict__ __module_data, uint64_t i, generic_val* __restrict__ args) noexcept __attribute__((nonnull (2,4)));
static void reorder__4860_closure_59_0wrapper(void* __stream, int8_t* __restrict__ __module_data, uint64_t i, generic_val* __restrict__ args) noexcept __attribute__((nonnull (2,4)));
static void reorder__4920_closure_60_0wrapper(void* __stream, int8_t* __restrict__ __module_data, uint64_t i, generic_val* __restrict__ args) noexcept __attribute__((nonnull (2,4)));
static void reorder__4950_closure_61_0wrapper(void* __stream, int8_t* __restrict__ __module_data, uint64_t i, generic_val* __restrict__ args) noexcept __attribute__((nonnull (2,4)));
static void reorder__4990_closure_62_0wrapper(void* __stream, int8_t* __restrict__ __module_data, uint64_t i, generic_val* __restrict__ args) noexcept __attribute__((nonnull (2,4)));
static void reorder__5020_closure_63_0wrapper(void* __stream, int8_t* __restrict__ __module_data, uint64_t i, generic_val* __restrict__ args) noexcept __attribute__((nonnull (2,4)));
static void reorder__5080_closure_64_0wrapper(void* __stream, int8_t* __restrict__ __module_data, uint64_t i, generic_val* __restrict__ args) noexcept __attribute__((nonnull (2,4)));
static void reorder__5110_closure_65_0wrapper(void* __stream, int8_t* __restrict__ __module_data, uint64_t i, generic_val* __restrict__ args) noexcept __attribute__((nonnull (2,4)));
static void reorder__5180_closure_66_0wrapper(void* __stream, int8_t* __restrict__ __module_data, uint64_t i, generic_val* __restrict__ args) noexcept __attribute__((nonnull (2,4)));
static void reorder__5240_closure_67_0wrapper(void* __stream, int8_t* __restrict__ __module_data, uint64_t i, generic_val* __restrict__ args) noexcept __attribute__((nonnull (2,4)));
static void reorder__5270_closure_68_0wrapper(void* __stream, int8_t* __restrict__ __module_data, uint64_t i, generic_val* __restrict__ args) noexcept __attribute__((nonnull (2,4)));
static void reorder__4470_closure_69_0wrapper(void* __stream, int8_t* __restrict__ __module_data, uint64_t i, generic_val* __restrict__ args) noexcept __attribute__((nonnull (2,4)));
static void reorder__4560_closure_70_0wrapper(void* __stream, int8_t* __restrict__ __module_data, uint64_t i, generic_val* __restrict__ args) noexcept __attribute__((nonnull (2,4)));
static void reorder__4630_closure_71_0wrapper(void* __stream, int8_t* __restrict__ __module_data, uint64_t i, generic_val* __restrict__ args) noexcept __attribute__((nonnull (2,4)));
static void reorder__4700_closure_72_0wrapper(void* __stream, int8_t* __restrict__ __module_data, uint64_t i, generic_val* __restrict__ args) noexcept __attribute__((nonnull (2,4)));
static void reorder__4790_closure_73_0wrapper(void* __stream, int8_t* __restrict__ __module_data, uint64_t i, generic_val* __restrict__ args) noexcept __attribute__((nonnull (2,4)));
static void reorder__5360_closure_74_0wrapper(void* __stream, int8_t* __restrict__ __module_data, uint64_t i, generic_val* __restrict__ args) noexcept __attribute__((nonnull (2,4)));
static void reorder__5390_closure_75_0wrapper(void* __stream, int8_t* __restrict__ __module_data, uint64_t i, generic_val* __restrict__ args) noexcept __attribute__((nonnull (2,4)));
static void reorder__5450_closure_76_0wrapper(void* __stream, int8_t* __restrict__ __module_data, uint64_t i, generic_val* __restrict__ args) noexcept __attribute__((nonnull (2,4)));
static void reorder__5480_closure_77_0wrapper(void* __stream, int8_t* __restrict__ __module_data, uint64_t i, generic_val* __restrict__ args) noexcept __attribute__((nonnull (2,4)));
static void reorder__5540_closure_78_0wrapper(void* __stream, int8_t* __restrict__ __module_data, uint64_t i, generic_val* __restrict__ args) noexcept __attribute__((nonnull (2,4)));
static void reorder__5570_closure_79_0wrapper(void* __stream, int8_t* __restrict__ __module_data, uint64_t i, generic_val* __restrict__ args) noexcept __attribute__((nonnull (2,4)));
static void reorder__4820_closure_80_0wrapper(void* __stream, int8_t* __restrict__ __module_data, uint64_t i, generic_val* __restrict__ args) noexcept __attribute__((nonnull (2,4)));
static void reorder__4890_closure_81_0wrapper(void* __stream, int8_t* __restrict__ __module_data, uint64_t i, generic_val* __restrict__ args) noexcept __attribute__((nonnull (2,4)));
static void reorder__5050_closure_82_0wrapper(void* __stream, int8_t* __restrict__ __module_data, uint64_t i, generic_val* __restrict__ args) noexcept __attribute__((nonnull (2,4)));
static void reorder__5140_closure_83_0wrapper(void* __stream, int8_t* __restrict__ __module_data, uint64_t i, generic_val* __restrict__ args) noexcept __attribute__((nonnull (2,4)));
static void reorder__5210_closure_84_0wrapper(void* __stream, int8_t* __restrict__ __module_data, uint64_t i, generic_val* __restrict__ args) noexcept __attribute__((nonnull (2,4)));
static void reorder__5300_closure_85_0wrapper(void* __stream, int8_t* __restrict__ __module_data, uint64_t i, generic_val* __restrict__ args) noexcept __attribute__((nonnull (2,4)));
static void reorder__5330_closure_86_0wrapper(void* __stream, int8_t* __restrict__ __module_data, uint64_t i, generic_val* __restrict__ args) noexcept __attribute__((nonnull (2,4)));
static void reorder__5420_closure_87_0wrapper(void* __stream, int8_t* __restrict__ __module_data, uint64_t i, generic_val* __restrict__ args) noexcept __attribute__((nonnull (2,4)));
static void reorder__5510_closure_88_0wrapper(void* __stream, int8_t* __restrict__ __module_data, uint64_t i, generic_val* __restrict__ args) noexcept __attribute__((nonnull (2,4)));
static void reorder__5600_closure_89_0wrapper(void* __stream, int8_t* __restrict__ __module_data, uint64_t i, generic_val* __restrict__ args) noexcept __attribute__((nonnull (2,4)));
static void mul__1110_closure_90_0wrapper(void* __stream, int8_t* __restrict__ __module_data, uint64_t i, generic_val* __restrict__ args) noexcept __attribute__((nonnull (2,4)));
static void cast__1120_closure_91_0wrapper(void* __stream, int8_t* __restrict__ __module_data, uint64_t i, generic_val* __restrict__ args) noexcept __attribute__((nonnull (2,4)));
static void mul__1080_closure_92_0wrapper(void* __stream, int8_t* __restrict__ __module_data, uint64_t i, generic_val* __restrict__ args) noexcept __attribute__((nonnull (2,4)));
static void cast__1090_closure_93_0wrapper(void* __stream, int8_t* __restrict__ __module_data, uint64_t i, generic_val* __restrict__ args) noexcept __attribute__((nonnull (2,4)));
static void mul__1170_closure_94_0wrapper(void* __stream, int8_t* __restrict__ __module_data, uint64_t i, generic_val* __restrict__ args) noexcept __attribute__((nonnull (2,4)));
static void cast__1180_closure_95_0wrapper(void* __stream, int8_t* __restrict__ __module_data, uint64_t i, generic_val* __restrict__ args) noexcept __attribute__((nonnull (2,4)));
static void mul__1260_closure_96_0wrapper(void* __stream, int8_t* __restrict__ __module_data, uint64_t i, generic_val* __restrict__ args) noexcept __attribute__((nonnull (2,4)));
static void cast__1270_closure_97_0wrapper(void* __stream, int8_t* __restrict__ __module_data, uint64_t i, generic_val* __restrict__ args) noexcept __attribute__((nonnull (2,4)));
static void mul__1350_closure_98_0wrapper(void* __stream, int8_t* __restrict__ __module_data, uint64_t i, generic_val* __restrict__ args) noexcept __attribute__((nonnull (2,4)));
static void cast__1360_closure_99_0wrapper(void* __stream, int8_t* __restrict__ __module_data, uint64_t i, generic_val* __restrict__ args) noexcept __attribute__((nonnull (2,4)));
static void mul__1200_closure_100_0wrapper(void* __stream, int8_t* __restrict__ __module_data, uint64_t i, generic_val* __restrict__ args) noexcept __attribute__((nonnull (2,4)));
static void cast__1210_closure_101_0wrapper(void* __stream, int8_t* __restrict__ __module_data, uint64_t i, generic_val* __restrict__ args) noexcept __attribute__((nonnull (2,4)));
static void mul__1290_closure_102_0wrapper(void* __stream, int8_t* __restrict__ __module_data, uint64_t i, generic_val* __restrict__ args) noexcept __attribute__((nonnull (2,4)));
static void cast__1300_closure_103_0wrapper(void* __stream, int8_t* __restrict__ __module_data, uint64_t i, generic_val* __restrict__ args) noexcept __attribute__((nonnull (2,4)));
static void mul__1410_closure_104_0wrapper(void* __stream, int8_t* __restrict__ __module_data, uint64_t i, generic_val* __restrict__ args) noexcept __attribute__((nonnull (2,4)));
static void cast__1420_closure_105_0wrapper(void* __stream, int8_t* __restrict__ __module_data, uint64_t i, generic_val* __restrict__ args) noexcept __attribute__((nonnull (2,4)));
static void mul__1140_closure_106_0wrapper(void* __stream, int8_t* __restrict__ __module_data, uint64_t i, generic_val* __restrict__ args) noexcept __attribute__((nonnull (2,4)));
static void cast__1150_closure_107_0wrapper(void* __stream, int8_t* __restrict__ __module_data, uint64_t i, generic_val* __restrict__ args) noexcept __attribute__((nonnull (2,4)));
static void mul__1230_closure_108_0wrapper(void* __stream, int8_t* __restrict__ __module_data, uint64_t i, generic_val* __restrict__ args) noexcept __attribute__((nonnull (2,4)));
static void cast__1240_closure_109_0wrapper(void* __stream, int8_t* __restrict__ __module_data, uint64_t i, generic_val* __restrict__ args) noexcept __attribute__((nonnull (2,4)));
static void mul__1320_closure_110_0wrapper(void* __stream, int8_t* __restrict__ __module_data, uint64_t i, generic_val* __restrict__ args) noexcept __attribute__((nonnull (2,4)));
static void cast__1330_closure_111_0wrapper(void* __stream, int8_t* __restrict__ __module_data, uint64_t i, generic_val* __restrict__ args) noexcept __attribute__((nonnull (2,4)));
static void mul__1470_closure_112_0wrapper(void* __stream, int8_t* __restrict__ __module_data, uint64_t i, generic_val* __restrict__ args) noexcept __attribute__((nonnull (2,4)));
static void cast__1480_closure_113_0wrapper(void* __stream, int8_t* __restrict__ __module_data, uint64_t i, generic_val* __restrict__ args) noexcept __attribute__((nonnull (2,4)));
static void mul__1560_closure_114_0wrapper(void* __stream, int8_t* __restrict__ __module_data, uint64_t i, generic_val* __restrict__ args) noexcept __attribute__((nonnull (2,4)));
static void cast__1570_closure_115_0wrapper(void* __stream, int8_t* __restrict__ __module_data, uint64_t i, generic_val* __restrict__ args) noexcept __attribute__((nonnull (2,4)));
static void mul__1650_closure_116_0wrapper(void* __stream, int8_t* __restrict__ __module_data, uint64_t i, generic_val* __restrict__ args) noexcept __attribute__((nonnull (2,4)));
static void cast__1660_closure_117_0wrapper(void* __stream, int8_t* __restrict__ __module_data, uint64_t i, generic_val* __restrict__ args) noexcept __attribute__((nonnull (2,4)));
static void mul__1740_closure_118_0wrapper(void* __stream, int8_t* __restrict__ __module_data, uint64_t i, generic_val* __restrict__ args) noexcept __attribute__((nonnull (2,4)));
static void cast__1750_closure_119_0wrapper(void* __stream, int8_t* __restrict__ __module_data, uint64_t i, generic_val* __restrict__ args) noexcept __attribute__((nonnull (2,4)));
static void mul__1500_closure_120_0wrapper(void* __stream, int8_t* __restrict__ __module_data, uint64_t i, generic_val* __restrict__ args) noexcept __attribute__((nonnull (2,4)));
static void cast__1510_closure_121_0wrapper(void* __stream, int8_t* __restrict__ __module_data, uint64_t i, generic_val* __restrict__ args) noexcept __attribute__((nonnull (2,4)));
static void mul__1590_closure_122_0wrapper(void* __stream, int8_t* __restrict__ __module_data, uint64_t i, generic_val* __restrict__ args) noexcept __attribute__((nonnull (2,4)));
static void cast__1600_closure_123_0wrapper(void* __stream, int8_t* __restrict__ __module_data, uint64_t i, generic_val* __restrict__ args) noexcept __attribute__((nonnull (2,4)));
static void mul__1680_closure_124_0wrapper(void* __stream, int8_t* __restrict__ __module_data, uint64_t i, generic_val* __restrict__ args) noexcept __attribute__((nonnull (2,4)));
static void cast__1690_closure_125_0wrapper(void* __stream, int8_t* __restrict__ __module_data, uint64_t i, generic_val* __restrict__ args) noexcept __attribute__((nonnull (2,4)));
static void mul__1380_closure_126_0wrapper(void* __stream, int8_t* __restrict__ __module_data, uint64_t i, generic_val* __restrict__ args) noexcept __attribute__((nonnull (2,4)));
static void cast__1390_closure_127_0wrapper(void* __stream, int8_t* __restrict__ __module_data, uint64_t i, generic_val* __restrict__ args) noexcept __attribute__((nonnull (2,4)));
static void mul__1800_closure_128_0wrapper(void* __stream, int8_t* __restrict__ __module_data, uint64_t i, generic_val* __restrict__ args) noexcept __attribute__((nonnull (2,4)));
static void cast__1810_closure_129_0wrapper(void* __stream, int8_t* __restrict__ __module_data, uint64_t i, generic_val* __restrict__ args) noexcept __attribute__((nonnull (2,4)));
static void mul__1440_closure_130_0wrapper(void* __stream, int8_t* __restrict__ __module_data, uint64_t i, generic_val* __restrict__ args) noexcept __attribute__((nonnull (2,4)));
static void cast__1450_closure_131_0wrapper(void* __stream, int8_t* __restrict__ __module_data, uint64_t i, generic_val* __restrict__ args) noexcept __attribute__((nonnull (2,4)));
static void mul__1530_closure_132_0wrapper(void* __stream, int8_t* __restrict__ __module_data, uint64_t i, generic_val* __restrict__ args) noexcept __attribute__((nonnull (2,4)));
static void cast__1540_closure_133_0wrapper(void* __stream, int8_t* __restrict__ __module_data, uint64_t i, generic_val* __restrict__ args) noexcept __attribute__((nonnull (2,4)));
static void mul__1620_closure_134_0wrapper(void* __stream, int8_t* __restrict__ __module_data, uint64_t i, generic_val* __restrict__ args) noexcept __attribute__((nonnull (2,4)));
static void cast__1630_closure_135_0wrapper(void* __stream, int8_t* __restrict__ __module_data, uint64_t i, generic_val* __restrict__ args) noexcept __attribute__((nonnull (2,4)));
static void mul__1710_closure_136_0wrapper(void* __stream, int8_t* __restrict__ __module_data, uint64_t i, generic_val* __restrict__ args) noexcept __attribute__((nonnull (2,4)));
static void cast__1720_closure_137_0wrapper(void* __stream, int8_t* __restrict__ __module_data, uint64_t i, generic_val* __restrict__ args) noexcept __attribute__((nonnull (2,4)));
static void mul__1860_closure_138_0wrapper(void* __stream, int8_t* __restrict__ __module_data, uint64_t i, generic_val* __restrict__ args) noexcept __attribute__((nonnull (2,4)));
static void cast__1870_closure_139_0wrapper(void* __stream, int8_t* __restrict__ __module_data, uint64_t i, generic_val* __restrict__ args) noexcept __attribute__((nonnull (2,4)));
static void mul__1950_closure_140_0wrapper(void* __stream, int8_t* __restrict__ __module_data, uint64_t i, generic_val* __restrict__ args) noexcept __attribute__((nonnull (2,4)));
static void cast__1960_closure_141_0wrapper(void* __stream, int8_t* __restrict__ __module_data, uint64_t i, generic_val* __restrict__ args) noexcept __attribute__((nonnull (2,4)));
static void mul__2040_closure_142_0wrapper(void* __stream, int8_t* __restrict__ __module_data, uint64_t i, generic_val* __restrict__ args) noexcept __attribute__((nonnull (2,4)));
static void cast__2050_closure_143_0wrapper(void* __stream, int8_t* __restrict__ __module_data, uint64_t i, generic_val* __restrict__ args) noexcept __attribute__((nonnull (2,4)));
static void mul__2130_closure_144_0wrapper(void* __stream, int8_t* __restrict__ __module_data, uint64_t i, generic_val* __restrict__ args) noexcept __attribute__((nonnull (2,4)));
static void cast__2140_closure_145_0wrapper(void* __stream, int8_t* __restrict__ __module_data, uint64_t i, generic_val* __restrict__ args) noexcept __attribute__((nonnull (2,4)));
static void mul__2220_closure_146_0wrapper(void* __stream, int8_t* __restrict__ __module_data, uint64_t i, generic_val* __restrict__ args) noexcept __attribute__((nonnull (2,4)));
static void cast__2230_closure_147_0wrapper(void* __stream, int8_t* __restrict__ __module_data, uint64_t i, generic_val* __restrict__ args) noexcept __attribute__((nonnull (2,4)));
static void mul__2310_closure_148_0wrapper(void* __stream, int8_t* __restrict__ __module_data, uint64_t i, generic_val* __restrict__ args) noexcept __attribute__((nonnull (2,4)));
static void cast__2320_closure_149_0wrapper(void* __stream, int8_t* __restrict__ __module_data, uint64_t i, generic_val* __restrict__ args) noexcept __attribute__((nonnull (2,4)));
static void mul__1890_closure_150_0wrapper(void* __stream, int8_t* __restrict__ __module_data, uint64_t i, generic_val* __restrict__ args) noexcept __attribute__((nonnull (2,4)));
static void cast__1900_closure_151_0wrapper(void* __stream, int8_t* __restrict__ __module_data, uint64_t i, generic_val* __restrict__ args) noexcept __attribute__((nonnull (2,4)));
static void mul__1980_closure_152_0wrapper(void* __stream, int8_t* __restrict__ __module_data, uint64_t i, generic_val* __restrict__ args) noexcept __attribute__((nonnull (2,4)));
static void cast__1990_closure_153_0wrapper(void* __stream, int8_t* __restrict__ __module_data, uint64_t i, generic_val* __restrict__ args) noexcept __attribute__((nonnull (2,4)));
static void mul__2070_closure_154_0wrapper(void* __stream, int8_t* __restrict__ __module_data, uint64_t i, generic_val* __restrict__ args) noexcept __attribute__((nonnull (2,4)));
static void cast__2080_closure_155_0wrapper(void* __stream, int8_t* __restrict__ __module_data, uint64_t i, generic_val* __restrict__ args) noexcept __attribute__((nonnull (2,4)));
static void mul__2160_closure_156_0wrapper(void* __stream, int8_t* __restrict__ __module_data, uint64_t i, generic_val* __restrict__ args) noexcept __attribute__((nonnull (2,4)));
static void cast__2170_closure_157_0wrapper(void* __stream, int8_t* __restrict__ __module_data, uint64_t i, generic_val* __restrict__ args) noexcept __attribute__((nonnull (2,4)));
static void mul__2250_closure_158_0wrapper(void* __stream, int8_t* __restrict__ __module_data, uint64_t i, generic_val* __restrict__ args) noexcept __attribute__((nonnull (2,4)));
static void cast__2260_closure_159_0wrapper(void* __stream, int8_t* __restrict__ __module_data, uint64_t i, generic_val* __restrict__ args) noexcept __attribute__((nonnull (2,4)));
static void mul__1770_closure_160_0wrapper(void* __stream, int8_t* __restrict__ __module_data, uint64_t i, generic_val* __restrict__ args) noexcept __attribute__((nonnull (2,4)));
static void cast__1780_closure_161_0wrapper(void* __stream, int8_t* __restrict__ __module_data, uint64_t i, generic_val* __restrict__ args) noexcept __attribute__((nonnull (2,4)));
static void mul__2370_closure_162_0wrapper(void* __stream, int8_t* __restrict__ __module_data, uint64_t i, generic_val* __restrict__ args) noexcept __attribute__((nonnull (2,4)));
static void cast__2380_closure_163_0wrapper(void* __stream, int8_t* __restrict__ __module_data, uint64_t i, generic_val* __restrict__ args) noexcept __attribute__((nonnull (2,4)));
static void mul__1830_closure_164_0wrapper(void* __stream, int8_t* __restrict__ __module_data, uint64_t i, generic_val* __restrict__ args) noexcept __attribute__((nonnull (2,4)));
static void cast__1840_closure_165_0wrapper(void* __stream, int8_t* __restrict__ __module_data, uint64_t i, generic_val* __restrict__ args) noexcept __attribute__((nonnull (2,4)));
static void mul__1920_closure_166_0wrapper(void* __stream, int8_t* __restrict__ __module_data, uint64_t i, generic_val* __restrict__ args) noexcept __attribute__((nonnull (2,4)));
static void cast__1930_closure_167_0wrapper(void* __stream, int8_t* __restrict__ __module_data, uint64_t i, generic_val* __restrict__ args) noexcept __attribute__((nonnull (2,4)));
static void mul__2010_closure_168_0wrapper(void* __stream, int8_t* __restrict__ __module_data, uint64_t i, generic_val* __restrict__ args) noexcept __attribute__((nonnull (2,4)));
static void cast__2020_closure_169_0wrapper(void* __stream, int8_t* __restrict__ __module_data, uint64_t i, generic_val* __restrict__ args) noexcept __attribute__((nonnull (2,4)));
static void mul__2100_closure_170_0wrapper(void* __stream, int8_t* __restrict__ __module_data, uint64_t i, generic_val* __restrict__ args) noexcept __attribute__((nonnull (2,4)));
static void cast__2110_closure_171_0wrapper(void* __stream, int8_t* __restrict__ __module_data, uint64_t i, generic_val* __restrict__ args) noexcept __attribute__((nonnull (2,4)));
static void mul__2190_closure_172_0wrapper(void* __stream, int8_t* __restrict__ __module_data, uint64_t i, generic_val* __restrict__ args) noexcept __attribute__((nonnull (2,4)));
static void cast__2200_closure_173_0wrapper(void* __stream, int8_t* __restrict__ __module_data, uint64_t i, generic_val* __restrict__ args) noexcept __attribute__((nonnull (2,4)));
static void mul__2280_closure_174_0wrapper(void* __stream, int8_t* __restrict__ __module_data, uint64_t i, generic_val* __restrict__ args) noexcept __attribute__((nonnull (2,4)));
static void cast__2290_closure_175_0wrapper(void* __stream, int8_t* __restrict__ __module_data, uint64_t i, generic_val* __restrict__ args) noexcept __attribute__((nonnull (2,4)));
static void reorder__5250_closure_176_0wrapper(void* __stream, int8_t* __restrict__ __module_data, uint64_t i, generic_val* __restrict__ args) noexcept __attribute__((nonnull (2,4)));
static void mul__6520_closure_177_0wrapper(void* __stream, int8_t* __restrict__ __module_data, uint64_t i, generic_val* __restrict__ args) noexcept __attribute__((nonnull (2,4)));
static void mul__6510_closure_178_0wrapper(void* __stream, int8_t* __restrict__ __module_data, uint64_t i, generic_val* __restrict__ args) noexcept __attribute__((nonnull (2,4)));
static void mul__6460_closure_179_0wrapper(void* __stream, int8_t* __restrict__ __module_data, uint64_t i, generic_val* __restrict__ args) noexcept __attribute__((nonnull (2,4)));
static void mul__6450_closure_180_0wrapper(void* __stream, int8_t* __restrict__ __module_data, uint64_t i, generic_val* __restrict__ args) noexcept __attribute__((nonnull (2,4)));
static void mul__6400_closure_181_0wrapper(void* __stream, int8_t* __restrict__ __module_data, uint64_t i, generic_val* __restrict__ args) noexcept __attribute__((nonnull (2,4)));
static void mul__6390_closure_182_0wrapper(void* __stream, int8_t* __restrict__ __module_data, uint64_t i, generic_val* __restrict__ args) noexcept __attribute__((nonnull (2,4)));
static void mul__6340_closure_183_0wrapper(void* __stream, int8_t* __restrict__ __module_data, uint64_t i, generic_val* __restrict__ args) noexcept __attribute__((nonnull (2,4)));
static void mul__6330_closure_184_0wrapper(void* __stream, int8_t* __restrict__ __module_data, uint64_t i, generic_val* __restrict__ args) noexcept __attribute__((nonnull (2,4)));
static void mul__6220_closure_185_0wrapper(void* __stream, int8_t* __restrict__ __module_data, uint64_t i, generic_val* __restrict__ args) noexcept __attribute__((nonnull (2,4)));
static void mul__6210_closure_186_0wrapper(void* __stream, int8_t* __restrict__ __module_data, uint64_t i, generic_val* __restrict__ args) noexcept __attribute__((nonnull (2,4)));
static void mul__6160_closure_187_0wrapper(void* __stream, int8_t* __restrict__ __module_data, uint64_t i, generic_val* __restrict__ args) noexcept __attribute__((nonnull (2,4)));
static void mul__6150_closure_188_0wrapper(void* __stream, int8_t* __restrict__ __module_data, uint64_t i, generic_val* __restrict__ args) noexcept __attribute__((nonnull (2,4)));
static void mul__6140_closure_189_0wrapper(void* __stream, int8_t* __restrict__ __module_data, uint64_t i, generic_val* __restrict__ args) noexcept __attribute__((nonnull (2,4)));
static void mul__6130_closure_190_0wrapper(void* __stream, int8_t* __restrict__ __module_data, uint64_t i, generic_val* __restrict__ args) noexcept __attribute__((nonnull (2,4)));
static void mul__6080_closure_191_0wrapper(void* __stream, int8_t* __restrict__ __module_data, uint64_t i, generic_val* __restrict__ args) noexcept __attribute__((nonnull (2,4)));
static void mul__6070_closure_192_0wrapper(void* __stream, int8_t* __restrict__ __module_data, uint64_t i, generic_val* __restrict__ args) noexcept __attribute__((nonnull (2,4)));
static void mul__6020_closure_193_0wrapper(void* __stream, int8_t* __restrict__ __module_data, uint64_t i, generic_val* __restrict__ args) noexcept __attribute__((nonnull (2,4)));
static void mul__6010_closure_194_0wrapper(void* __stream, int8_t* __restrict__ __module_data, uint64_t i, generic_val* __restrict__ args) noexcept __attribute__((nonnull (2,4)));
static void mul__5960_closure_195_0wrapper(void* __stream, int8_t* __restrict__ __module_data, uint64_t i, generic_val* __restrict__ args) noexcept __attribute__((nonnull (2,4)));
static void mul__5950_closure_196_0wrapper(void* __stream, int8_t* __restrict__ __module_data, uint64_t i, generic_val* __restrict__ args) noexcept __attribute__((nonnull (2,4)));
static void mul__5900_closure_197_0wrapper(void* __stream, int8_t* __restrict__ __module_data, uint64_t i, generic_val* __restrict__ args) noexcept __attribute__((nonnull (2,4)));
static void mul__5890_closure_198_0wrapper(void* __stream, int8_t* __restrict__ __module_data, uint64_t i, generic_val* __restrict__ args) noexcept __attribute__((nonnull (2,4)));
static void mul__6500_closure_199_0wrapper(void* __stream, int8_t* __restrict__ __module_data, uint64_t i, generic_val* __restrict__ args) noexcept __attribute__((nonnull (2,4)));
static void mul__6490_closure_200_0wrapper(void* __stream, int8_t* __restrict__ __module_data, uint64_t i, generic_val* __restrict__ args) noexcept __attribute__((nonnull (2,4)));
static void mul__6480_closure_201_0wrapper(void* __stream, int8_t* __restrict__ __module_data, uint64_t i, generic_val* __restrict__ args) noexcept __attribute__((nonnull (2,4)));
static void mul__6470_closure_202_0wrapper(void* __stream, int8_t* __restrict__ __module_data, uint64_t i, generic_val* __restrict__ args) noexcept __attribute__((nonnull (2,4)));
static void mul__6440_closure_203_0wrapper(void* __stream, int8_t* __restrict__ __module_data, uint64_t i, generic_val* __restrict__ args) noexcept __attribute__((nonnull (2,4)));
static void mul__6430_closure_204_0wrapper(void* __stream, int8_t* __restrict__ __module_data, uint64_t i, generic_val* __restrict__ args) noexcept __attribute__((nonnull (2,4)));
static void mul__6380_closure_205_0wrapper(void* __stream, int8_t* __restrict__ __module_data, uint64_t i, generic_val* __restrict__ args) noexcept __attribute__((nonnull (2,4)));
static void mul__6370_closure_206_0wrapper(void* __stream, int8_t* __restrict__ __module_data, uint64_t i, generic_val* __restrict__ args) noexcept __attribute__((nonnull (2,4)));
static void mul__6360_closure_207_0wrapper(void* __stream, int8_t* __restrict__ __module_data, uint64_t i, generic_val* __restrict__ args) noexcept __attribute__((nonnull (2,4)));
static void mul__6350_closure_208_0wrapper(void* __stream, int8_t* __restrict__ __module_data, uint64_t i, generic_val* __restrict__ args) noexcept __attribute__((nonnull (2,4)));
static void mul__6320_closure_209_0wrapper(void* __stream, int8_t* __restrict__ __module_data, uint64_t i, generic_val* __restrict__ args) noexcept __attribute__((nonnull (2,4)));
static void mul__6310_closure_210_0wrapper(void* __stream, int8_t* __restrict__ __module_data, uint64_t i, generic_val* __restrict__ args) noexcept __attribute__((nonnull (2,4)));
static void mul__6300_closure_211_0wrapper(void* __stream, int8_t* __restrict__ __module_data, uint64_t i, generic_val* __restrict__ args) noexcept __attribute__((nonnull (2,4)));
static void mul__6290_closure_212_0wrapper(void* __stream, int8_t* __restrict__ __module_data, uint64_t i, generic_val* __restrict__ args) noexcept __attribute__((nonnull (2,4)));
static void mul__6260_closure_213_0wrapper(void* __stream, int8_t* __restrict__ __module_data, uint64_t i, generic_val* __restrict__ args) noexcept __attribute__((nonnull (2,4)));
static void mul__6250_closure_214_0wrapper(void* __stream, int8_t* __restrict__ __module_data, uint64_t i, generic_val* __restrict__ args) noexcept __attribute__((nonnull (2,4)));
static void mul__6240_closure_215_0wrapper(void* __stream, int8_t* __restrict__ __module_data, uint64_t i, generic_val* __restrict__ args) noexcept __attribute__((nonnull (2,4)));
static void mul__6230_closure_216_0wrapper(void* __stream, int8_t* __restrict__ __module_data, uint64_t i, generic_val* __restrict__ args) noexcept __attribute__((nonnull (2,4)));
static void mul__6200_closure_217_0wrapper(void* __stream, int8_t* __restrict__ __module_data, uint64_t i, generic_val* __restrict__ args) noexcept __attribute__((nonnull (2,4)));
static void mul__6190_closure_218_0wrapper(void* __stream, int8_t* __restrict__ __module_data, uint64_t i, generic_val* __restrict__ args) noexcept __attribute__((nonnull (2,4)));
static void mul__5880_closure_219_0wrapper(void* __stream, int8_t* __restrict__ __module_data, uint64_t i, generic_val* __restrict__ args) noexcept __attribute__((nonnull (2,4)));
static void mul__5870_closure_220_0wrapper(void* __stream, int8_t* __restrict__ __module_data, uint64_t i, generic_val* __restrict__ args) noexcept __attribute__((nonnull (2,4)));
static void mul__5820_closure_221_0wrapper(void* __stream, int8_t* __restrict__ __module_data, uint64_t i, generic_val* __restrict__ args) noexcept __attribute__((nonnull (2,4)));
static void mul__5810_closure_222_0wrapper(void* __stream, int8_t* __restrict__ __module_data, uint64_t i, generic_val* __restrict__ args) noexcept __attribute__((nonnull (2,4)));
static void mul__5760_closure_223_0wrapper(void* __stream, int8_t* __restrict__ __module_data, uint64_t i, generic_val* __restrict__ args) noexcept __attribute__((nonnull (2,4)));
static void mul__5750_closure_224_0wrapper(void* __stream, int8_t* __restrict__ __module_data, uint64_t i, generic_val* __restrict__ args) noexcept __attribute__((nonnull (2,4)));
static void mul__5700_closure_225_0wrapper(void* __stream, int8_t* __restrict__ __module_data, uint64_t i, generic_val* __restrict__ args) noexcept __attribute__((nonnull (2,4)));
static void mul__5690_closure_226_0wrapper(void* __stream, int8_t* __restrict__ __module_data, uint64_t i, generic_val* __restrict__ args) noexcept __attribute__((nonnull (2,4)));
static void reorder__5220_closure_227_0wrapper(void* __stream, int8_t* __restrict__ __module_data, uint64_t i, generic_val* __restrict__ args) noexcept __attribute__((nonnull (2,4)));
static void reorder__5150_closure_228_0wrapper(void* __stream, int8_t* __restrict__ __module_data, uint64_t i, generic_val* __restrict__ args) noexcept __attribute__((nonnull (2,4)));
static void reorder__5060_closure_229_0wrapper(void* __stream, int8_t* __restrict__ __module_data, uint64_t i, generic_val* __restrict__ args) noexcept __attribute__((nonnull (2,4)));
static void reorder__4970_closure_230_0wrapper(void* __stream, int8_t* __restrict__ __module_data, uint64_t i, generic_val* __restrict__ args) noexcept __attribute__((nonnull (2,4)));
static void reorder__4900_closure_231_0wrapper(void* __stream, int8_t* __restrict__ __module_data, uint64_t i, generic_val* __restrict__ args) noexcept __attribute__((nonnull (2,4)));
static void reorder__5280_closure_232_0wrapper(void* __stream, int8_t* __restrict__ __module_data, uint64_t i, generic_val* __restrict__ args) noexcept __attribute__((nonnull (2,4)));
static void reorder__5190_closure_233_0wrapper(void* __stream, int8_t* __restrict__ __module_data, uint64_t i, generic_val* __restrict__ args) noexcept __attribute__((nonnull (2,4)));
static void reorder__5120_closure_234_0wrapper(void* __stream, int8_t* __restrict__ __module_data, uint64_t i, generic_val* __restrict__ args) noexcept __attribute__((nonnull (2,4)));
static void reorder__5030_closure_235_0wrapper(void* __stream, int8_t* __restrict__ __module_data, uint64_t i, generic_val* __restrict__ args) noexcept __attribute__((nonnull (2,4)));
static void reorder__4960_closure_236_0wrapper(void* __stream, int8_t* __restrict__ __module_data, uint64_t i, generic_val* __restrict__ args) noexcept __attribute__((nonnull (2,4)));
static void reorder__4870_closure_237_0wrapper(void* __stream, int8_t* __restrict__ __module_data, uint64_t i, generic_val* __restrict__ args) noexcept __attribute__((nonnull (2,4)));
static void reorder__4220_closure_238_0wrapper(void* __stream, int8_t* __restrict__ __module_data, uint64_t i, generic_val* __restrict__ args) noexcept __attribute__((nonnull (2,4)));
static void mul__6120_closure_239_0wrapper(void* __stream, int8_t* __restrict__ __module_data, uint64_t i, generic_val* __restrict__ args) noexcept __attribute__((nonnull (2,4)));
static void mul__6110_closure_240_0wrapper(void* __stream, int8_t* __restrict__ __module_data, uint64_t i, generic_val* __restrict__ args) noexcept __attribute__((nonnull (2,4)));
static void mul__6100_closure_241_0wrapper(void* __stream, int8_t* __restrict__ __module_data, uint64_t i, generic_val* __restrict__ args) noexcept __attribute__((nonnull (2,4)));
static void mul__6090_closure_242_0wrapper(void* __stream, int8_t* __restrict__ __module_data, uint64_t i, generic_val* __restrict__ args) noexcept __attribute__((nonnull (2,4)));
static void mul__6060_closure_243_0wrapper(void* __stream, int8_t* __restrict__ __module_data, uint64_t i, generic_val* __restrict__ args) noexcept __attribute__((nonnull (2,4)));
static void mul__6050_closure_244_0wrapper(void* __stream, int8_t* __restrict__ __module_data, uint64_t i, generic_val* __restrict__ args) noexcept __attribute__((nonnull (2,4)));
static void mul__5980_closure_245_0wrapper(void* __stream, int8_t* __restrict__ __module_data, uint64_t i, generic_val* __restrict__ args) noexcept __attribute__((nonnull (2,4)));
static void mul__5970_closure_246_0wrapper(void* __stream, int8_t* __restrict__ __module_data, uint64_t i, generic_val* __restrict__ args) noexcept __attribute__((nonnull (2,4)));
static void mul__5940_closure_247_0wrapper(void* __stream, int8_t* __restrict__ __module_data, uint64_t i, generic_val* __restrict__ args) noexcept __attribute__((nonnull (2,4)));
static void mul__5930_closure_248_0wrapper(void* __stream, int8_t* __restrict__ __module_data, uint64_t i, generic_val* __restrict__ args) noexcept __attribute__((nonnull (2,4)));
static void mul__5920_closure_249_0wrapper(void* __stream, int8_t* __restrict__ __module_data, uint64_t i, generic_val* __restrict__ args) noexcept __attribute__((nonnull (2,4)));
static void mul__5910_closure_250_0wrapper(void* __stream, int8_t* __restrict__ __module_data, uint64_t i, generic_val* __restrict__ args) noexcept __attribute__((nonnull (2,4)));
static void mul__5860_closure_251_0wrapper(void* __stream, int8_t* __restrict__ __module_data, uint64_t i, generic_val* __restrict__ args) noexcept __attribute__((nonnull (2,4)));
static void mul__5850_closure_252_0wrapper(void* __stream, int8_t* __restrict__ __module_data, uint64_t i, generic_val* __restrict__ args) noexcept __attribute__((nonnull (2,4)));
static void mul__5840_closure_253_0wrapper(void* __stream, int8_t* __restrict__ __module_data, uint64_t i, generic_val* __restrict__ args) noexcept __attribute__((nonnull (2,4)));
static void mul__5830_closure_254_0wrapper(void* __stream, int8_t* __restrict__ __module_data, uint64_t i, generic_val* __restrict__ args) noexcept __attribute__((nonnull (2,4)));
static void mul__5800_closure_255_0wrapper(void* __stream, int8_t* __restrict__ __module_data, uint64_t i, generic_val* __restrict__ args) noexcept __attribute__((nonnull (2,4)));
static void mul__5790_closure_256_0wrapper(void* __stream, int8_t* __restrict__ __module_data, uint64_t i, generic_val* __restrict__ args) noexcept __attribute__((nonnull (2,4)));
static void mul__5740_closure_257_0wrapper(void* __stream, int8_t* __restrict__ __module_data, uint64_t i, generic_val* __restrict__ args) noexcept __attribute__((nonnull (2,4)));
static void mul__5730_closure_258_0wrapper(void* __stream, int8_t* __restrict__ __module_data, uint64_t i, generic_val* __restrict__ args) noexcept __attribute__((nonnull (2,4)));
static void reorder__5160_closure_259_0wrapper(void* __stream, int8_t* __restrict__ __module_data, uint64_t i, generic_val* __restrict__ args) noexcept __attribute__((nonnull (2,4)));
static void reorder__5090_closure_260_0wrapper(void* __stream, int8_t* __restrict__ __module_data, uint64_t i, generic_val* __restrict__ args) noexcept __attribute__((nonnull (2,4)));
static void reorder__5000_closure_261_0wrapper(void* __stream, int8_t* __restrict__ __module_data, uint64_t i, generic_val* __restrict__ args) noexcept __attribute__((nonnull (2,4)));
static void reorder__4930_closure_262_0wrapper(void* __stream, int8_t* __restrict__ __module_data, uint64_t i, generic_val* __restrict__ args) noexcept __attribute__((nonnull (2,4)));
static void reorder__4840_closure_263_0wrapper(void* __stream, int8_t* __restrict__ __module_data, uint64_t i, generic_val* __restrict__ args) noexcept __attribute__((nonnull (2,4)));
static void reorder__4800_closure_264_0wrapper(void* __stream, int8_t* __restrict__ __module_data, uint64_t i, generic_val* __restrict__ args) noexcept __attribute__((nonnull (2,4)));
static void reorder__4740_closure_265_0wrapper(void* __stream, int8_t* __restrict__ __module_data, uint64_t i, generic_val* __restrict__ args) noexcept __attribute__((nonnull (2,4)));
static void reorder__4650_closure_266_0wrapper(void* __stream, int8_t* __restrict__ __module_data, uint64_t i, generic_val* __restrict__ args) noexcept __attribute__((nonnull (2,4)));
static void reorder__4600_closure_267_0wrapper(void* __stream, int8_t* __restrict__ __module_data, uint64_t i, generic_val* __restrict__ args) noexcept __attribute__((nonnull (2,4)));
static void reorder__4510_closure_268_0wrapper(void* __stream, int8_t* __restrict__ __module_data, uint64_t i, generic_val* __restrict__ args) noexcept __attribute__((nonnull (2,4)));
static void reorder__4830_closure_269_0wrapper(void* __stream, int8_t* __restrict__ __module_data, uint64_t i, generic_val* __restrict__ args) noexcept __attribute__((nonnull (2,4)));
static void reorder__4450_closure_270_0wrapper(void* __stream, int8_t* __restrict__ __module_data, uint64_t i, generic_val* __restrict__ args) noexcept __attribute__((nonnull (2,4)));
static void reorder__4710_closure_271_0wrapper(void* __stream, int8_t* __restrict__ __module_data, uint64_t i, generic_val* __restrict__ args) noexcept __attribute__((nonnull (2,4)));
static void reorder__4640_closure_272_0wrapper(void* __stream, int8_t* __restrict__ __module_data, uint64_t i, generic_val* __restrict__ args) noexcept __attribute__((nonnull (2,4)));
static void reorder__4570_closure_273_0wrapper(void* __stream, int8_t* __restrict__ __module_data, uint64_t i, generic_val* __restrict__ args) noexcept __attribute__((nonnull (2,4)));
static void reorder__4770_closure_274_0wrapper(void* __stream, int8_t* __restrict__ __module_data, uint64_t i, generic_val* __restrict__ args) noexcept __attribute__((nonnull (2,4)));
static void reorder__4680_closure_275_0wrapper(void* __stream, int8_t* __restrict__ __module_data, uint64_t i, generic_val* __restrict__ args) noexcept __attribute__((nonnull (2,4)));
static void reorder__4610_closure_276_0wrapper(void* __stream, int8_t* __restrict__ __module_data, uint64_t i, generic_val* __restrict__ args) noexcept __attribute__((nonnull (2,4)));
static void reorder__4540_closure_277_0wrapper(void* __stream, int8_t* __restrict__ __module_data, uint64_t i, generic_val* __restrict__ args) noexcept __attribute__((nonnull (2,4)));
static void reorder__4390_closure_278_0wrapper(void* __stream, int8_t* __restrict__ __module_data, uint64_t i, generic_val* __restrict__ args) noexcept __attribute__((nonnull (2,4)));
static void reorder__4300_closure_279_0wrapper(void* __stream, int8_t* __restrict__ __module_data, uint64_t i, generic_val* __restrict__ args) noexcept __attribute__((nonnull (2,4)));
static void reorder__4230_closure_280_0wrapper(void* __stream, int8_t* __restrict__ __module_data, uint64_t i, generic_val* __restrict__ args) noexcept __attribute__((nonnull (2,4)));
static void reorder__4480_closure_281_0wrapper(void* __stream, int8_t* __restrict__ __module_data, uint64_t i, generic_val* __restrict__ args) noexcept __attribute__((nonnull (2,4)));
static void reorder__4360_closure_282_0wrapper(void* __stream, int8_t* __restrict__ __module_data, uint64_t i, generic_val* __restrict__ args) noexcept __attribute__((nonnull (2,4)));
static void reorder__4290_closure_283_0wrapper(void* __stream, int8_t* __restrict__ __module_data, uint64_t i, generic_val* __restrict__ args) noexcept __attribute__((nonnull (2,4)));
static void reorder__4420_closure_284_0wrapper(void* __stream, int8_t* __restrict__ __module_data, uint64_t i, generic_val* __restrict__ args) noexcept __attribute__((nonnull (2,4)));
static void reorder__4330_closure_285_0wrapper(void* __stream, int8_t* __restrict__ __module_data, uint64_t i, generic_val* __restrict__ args) noexcept __attribute__((nonnull (2,4)));
static void reorder__4260_closure_286_0wrapper(void* __stream, int8_t* __restrict__ __module_data, uint64_t i, generic_val* __restrict__ args) noexcept __attribute__((nonnull (2,4)));
static void reorder__4190_closure_287_0wrapper(void* __stream, int8_t* __restrict__ __module_data, uint64_t i, generic_val* __restrict__ args) noexcept __attribute__((nonnull (2,4)));
static void mul__6560_closure_288_0wrapper(void* __stream, int8_t* __restrict__ __module_data, uint64_t i, generic_val* __restrict__ args) noexcept __attribute__((nonnull (2,4)));
static void mul__6550_closure_289_0wrapper(void* __stream, int8_t* __restrict__ __module_data, uint64_t i, generic_val* __restrict__ args) noexcept __attribute__((nonnull (2,4)));
static void reorder__5340_closure_290_0wrapper(void* __stream, int8_t* __restrict__ __module_data, uint64_t i, generic_val* __restrict__ args) noexcept __attribute__((nonnull (2,4)));
static void mul__2430_closure_291_0wrapper(void* __stream, int8_t* __restrict__ __module_data, uint64_t i, generic_val* __restrict__ args) noexcept __attribute__((nonnull (2,4)));
static void cast__2440_closure_292_0wrapper(void* __stream, int8_t* __restrict__ __module_data, uint64_t i, generic_val* __restrict__ args) noexcept __attribute__((nonnull (2,4)));
static void mul__2520_closure_293_0wrapper(void* __stream, int8_t* __restrict__ __module_data, uint64_t i, generic_val* __restrict__ args) noexcept __attribute__((nonnull (2,4)));
static void cast__2530_closure_294_0wrapper(void* __stream, int8_t* __restrict__ __module_data, uint64_t i, generic_val* __restrict__ args) noexcept __attribute__((nonnull (2,4)));
static void mul__2610_closure_295_0wrapper(void* __stream, int8_t* __restrict__ __module_data, uint64_t i, generic_val* __restrict__ args) noexcept __attribute__((nonnull (2,4)));
static void cast__2620_closure_296_0wrapper(void* __stream, int8_t* __restrict__ __module_data, uint64_t i, generic_val* __restrict__ args) noexcept __attribute__((nonnull (2,4)));
static void mul__2460_closure_297_0wrapper(void* __stream, int8_t* __restrict__ __module_data, uint64_t i, generic_val* __restrict__ args) noexcept __attribute__((nonnull (2,4)));
static void cast__2470_closure_298_0wrapper(void* __stream, int8_t* __restrict__ __module_data, uint64_t i, generic_val* __restrict__ args) noexcept __attribute__((nonnull (2,4)));
static void mul__2550_closure_299_0wrapper(void* __stream, int8_t* __restrict__ __module_data, uint64_t i, generic_val* __restrict__ args) noexcept __attribute__((nonnull (2,4)));
static void cast__2560_closure_300_0wrapper(void* __stream, int8_t* __restrict__ __module_data, uint64_t i, generic_val* __restrict__ args) noexcept __attribute__((nonnull (2,4)));
static void mul__2340_closure_301_0wrapper(void* __stream, int8_t* __restrict__ __module_data, uint64_t i, generic_val* __restrict__ args) noexcept __attribute__((nonnull (2,4)));
static void cast__2350_closure_302_0wrapper(void* __stream, int8_t* __restrict__ __module_data, uint64_t i, generic_val* __restrict__ args) noexcept __attribute__((nonnull (2,4)));
static void batchwise_4_fused_res2a_conv_b_cast_mul_add_cast_reorder_res2a_conv_0_cast_mul_add_relu_cast_res2a_conv_1_cast_mul_add_relu_cast_res2a_conv_2_cast_mul_add_cast_add_cast_reorder_res2b_conv_0_cast_mul_add_relu_cast_res2b_conv_1_cast_mul_add_relu_cast_reorder_res2b_conv_2_cast_mul_add_cast_add_cast_reorder_res2c_conv_0_cast_mul_add_relu_cast_reorder_res2c_conv_1_cast_mul_add_relu_cast_reorder_res2c_conv_2_cast_mul_add_cast_add_cast_reorder_res3a_conv_b_cast_mul_add_cast_res3a_conv_0_cast_mul_add_relu_cast_reorder_res3a_conv_1_cast_mul_add_relu_cast_res3a_conv_2_cast_mul_add_cast_add_cast_reorder_res3b_conv_0_cast_mul_add_relu_cast_reorder_res3b_conv_1_cast_mul_add_relu_cast_reorder_res3b_conv_2_cast_mul_add_cast_add_cast_reorder_res3c_conv_0_cast_mul_add_relu_cast_reorder_res3c_conv_1_cast_mul_add_relu_cast_reorder_res3c_conv_2_cast_mul_add_cast_add_cast_reorder_res3d_conv_0_cast_mul_add_relu_cast_reorder_res3d_conv_1_cast_mul_add_relu_cast_res3d_conv_2_cast_mul_add_cast_add_cast_res4a_conv_b_cast_mul_add_cast_reorder_res4a_conv_0_cast_mul_add_relu_cast_res4a_conv_1_cast_mul_add_relu_cast_reorder_res4a_conv_2_cast_mul_add_cast_add_cast_reorder_res4b_conv_0_cast_mul_add_relu_cast_reorder_res4b_conv_1_cast_mul_add_relu_cast_reorder_res4b_conv_2_cast_mul_add_cast_add_cast_reorder_res4c_conv_0_cast_mul_add_relu_cast_reorder_res4c_conv_1_cast_mul_add_relu_cast_reorder_res4c_conv_2_cast_mul_add_cast_add_cast_res4d_conv_0_cast_mul_add_relu_cast_res4d_conv_1_cast_mul_add_relu_cast_reorder_res4d_conv_2_cast_mul_add_cast_add_cast_res4e_conv_0_cast_mul_add_relu_cast_reorder_res4e_conv_1_cast_mul_add_relu_cast_reorder_res4e_conv_2_cast_mul_add_cast_add_cast_reorder_reorder_res4f_conv_0_cast_mul_add_relu_cast_reorder_res4f_conv_1_cast_mul_add_relu_cast_reorder_res4f_conv_2_cast_mul_add_cast_add_cast__6830_closure_303_0wrapper(void* __stream, int8_t* __restrict__ __module_data, uint64_t i, generic_val* __restrict__ args) noexcept __attribute__((nonnull (2,4)));
static bool res2a_conv_0_cast_mul_add_relu_cast__8(int8_t* __restrict__ __outs_0, int8_t* __restrict__ __ins_0, int8_t* __restrict__ __ins_1, float* __restrict__ __ins_2, float* __restrict__ __ins_3) noexcept __attribute__((nonnull (1,2,3,4,5)));
static bool res2a_conv_1_cast_mul_add_relu_cast__12(int8_t* __restrict__ __outs_0, int8_t* __restrict__ __ins_0, int8_t* __restrict__ __ins_1, float* __restrict__ __ins_2, float* __restrict__ __ins_3) noexcept __attribute__((nonnull (1,2,3,4,5)));
static bool res2a_conv_b_cast_mul_add_cast_reorder__4(int8_t* __restrict__ __outs_0, int8_t* __restrict__ __ins_0, int8_t* __restrict__ __ins_1, float* __restrict__ __ins_2, float* __restrict__ __ins_3) noexcept __attribute__((nonnull (1,2,3,4,5)));
static bool res2a_conv_2_cast_mul_add_cast_add_cast_reorder__16(uint8_t* __restrict__ __outs_0, int8_t* __restrict__ __ins_0, int8_t* __restrict__ __ins_1, float* __restrict__ __ins_2, float* __restrict__ __ins_3, int8_t* __restrict__ __ins_4) noexcept __attribute__((nonnull (1,2,3,4,5,6)));
static bool res2b_conv_0_cast_mul_add_relu_cast__20(int8_t* __restrict__ __outs_0, uint8_t* __restrict__ __ins_0, int8_t* __restrict__ __ins_1, float* __restrict__ __ins_2, float* __restrict__ __ins_3) noexcept __attribute__((nonnull (1,2,3,4,5)));
static bool res2b_conv_1_cast_mul_add_relu_cast_reorder__24(int8_t* __restrict__ __outs_0, int8_t* __restrict__ __ins_0, int8_t* __restrict__ __ins_1, float* __restrict__ __ins_2, float* __restrict__ __ins_3) noexcept __attribute__((nonnull (1,2,3,4,5)));
static bool res2b_conv_2_cast_mul_add_cast_add_cast_reorder__28(uint8_t* __restrict__ __outs_0, int8_t* __restrict__ __ins_0, int8_t* __restrict__ __ins_1, float* __restrict__ __ins_2, float* __restrict__ __ins_3, uint8_t* __restrict__ __ins_4) noexcept __attribute__((nonnull (1,2,3,4,5,6)));
static bool res2c_conv_0_cast_mul_add_relu_cast_reorder__32(int8_t* __restrict__ __outs_0, uint8_t* __restrict__ __ins_0, int8_t* __restrict__ __ins_1, float* __restrict__ __ins_2, float* __restrict__ __ins_3) noexcept __attribute__((nonnull (1,2,3,4,5)));
static bool res2c_conv_1_cast_mul_add_relu_cast_reorder__36(int8_t* __restrict__ __outs_0, int8_t* __restrict__ __ins_0, int8_t* __restrict__ __ins_1, float* __restrict__ __ins_2, float* __restrict__ __ins_3) noexcept __attribute__((nonnull (1,2,3,4,5)));
static bool res2c_conv_2_cast_mul_add_cast_add_cast_reorder__40(uint8_t* __restrict__ __outs_0, int8_t* __restrict__ __ins_0, int8_t* __restrict__ __ins_1, float* __restrict__ __ins_2, float* __restrict__ __ins_3, uint8_t* __restrict__ __ins_4) noexcept __attribute__((nonnull (1,2,3,4,5,6)));
static bool res3a_conv_0_cast_mul_add_relu_cast_reorder__48(int8_t* __restrict__ __outs_0, uint8_t* __restrict__ __ins_0, int8_t* __restrict__ __ins_1, float* __restrict__ __ins_2, float* __restrict__ __ins_3) noexcept __attribute__((nonnull (1,2,3,4,5)));
static bool res3a_conv_1_cast_mul_add_relu_cast__52(int8_t* __restrict__ __outs_0, int8_t* __restrict__ __ins_0, int8_t* __restrict__ __ins_1, float* __restrict__ __ins_2, float* __restrict__ __ins_3) noexcept __attribute__((nonnull (1,2,3,4,5)));
static bool res3a_conv_b_cast_mul_add_cast__44(int8_t* __restrict__ __outs_0, uint8_t* __restrict__ __ins_0, int8_t* __restrict__ __ins_1, float* __restrict__ __ins_2, float* __restrict__ __ins_3) noexcept __attribute__((nonnull (1,2,3,4,5)));
static bool res3a_conv_2_cast_mul_add_cast_add_cast_reorder__56(uint8_t* __restrict__ __outs_0, int8_t* __restrict__ __ins_0, int8_t* __restrict__ __ins_1, float* __restrict__ __ins_2, float* __restrict__ __ins_3, int8_t* __restrict__ __ins_4) noexcept __attribute__((nonnull (1,2,3,4,5,6)));
static bool res3b_conv_0_cast_mul_add_relu_cast_reorder__60(int8_t* __restrict__ __outs_0, uint8_t* __restrict__ __ins_0, int8_t* __restrict__ __ins_1, float* __restrict__ __ins_2, float* __restrict__ __ins_3) noexcept __attribute__((nonnull (1,2,3,4,5)));
static bool res3b_conv_1_cast_mul_add_relu_cast_reorder__64(int8_t* __restrict__ __outs_0, int8_t* __restrict__ __ins_0, int8_t* __restrict__ __ins_1, float* __restrict__ __ins_2, float* __restrict__ __ins_3) noexcept __attribute__((nonnull (1,2,3,4,5)));
static bool res3b_conv_2_cast_mul_add_cast_add_cast_reorder__68(uint8_t* __restrict__ __outs_0, int8_t* __restrict__ __ins_0, int8_t* __restrict__ __ins_1, float* __restrict__ __ins_2, float* __restrict__ __ins_3, uint8_t* __restrict__ __ins_4) noexcept __attribute__((nonnull (1,2,3,4,5,6)));
static bool res3c_conv_0_cast_mul_add_relu_cast_reorder__72(int8_t* __restrict__ __outs_0, uint8_t* __restrict__ __ins_0, int8_t* __restrict__ __ins_1, float* __restrict__ __ins_2, float* __restrict__ __ins_3) noexcept __attribute__((nonnull (1,2,3,4,5)));
static bool res3c_conv_1_cast_mul_add_relu_cast_reorder__76(int8_t* __restrict__ __outs_0, int8_t* __restrict__ __ins_0, int8_t* __restrict__ __ins_1, float* __restrict__ __ins_2, float* __restrict__ __ins_3) noexcept __attribute__((nonnull (1,2,3,4,5)));
static bool res3c_conv_2_cast_mul_add_cast_add_cast_reorder__80(uint8_t* __restrict__ __outs_0, int8_t* __restrict__ __ins_0, int8_t* __restrict__ __ins_1, float* __restrict__ __ins_2, float* __restrict__ __ins_3, uint8_t* __restrict__ __ins_4) noexcept __attribute__((nonnull (1,2,3,4,5,6)));
static bool res3d_conv_0_cast_mul_add_relu_cast_reorder__84(int8_t* __restrict__ __outs_0, uint8_t* __restrict__ __ins_0, int8_t* __restrict__ __ins_1, float* __restrict__ __ins_2, float* __restrict__ __ins_3) noexcept __attribute__((nonnull (1,2,3,4,5)));
static bool res3d_conv_1_cast_mul_add_relu_cast__88(int8_t* __restrict__ __outs_0, int8_t* __restrict__ __ins_0, int8_t* __restrict__ __ins_1, float* __restrict__ __ins_2, float* __restrict__ __ins_3) noexcept __attribute__((nonnull (1,2,3,4,5)));
static bool res3d_conv_2_cast_mul_add_cast_add_cast__92(uint8_t* __restrict__ __outs_0, int8_t* __restrict__ __ins_0, int8_t* __restrict__ __ins_1, float* __restrict__ __ins_2, float* __restrict__ __ins_3, uint8_t* __restrict__ __ins_4) noexcept __attribute__((nonnull (1,2,3,4,5,6)));
static bool res4a_conv_0_cast_mul_add_relu_cast__100(int8_t* __restrict__ __outs_0, uint8_t* __restrict__ __ins_0, int8_t* __restrict__ __ins_1, float* __restrict__ __ins_2, float* __restrict__ __ins_3) noexcept __attribute__((nonnull (1,2,3,4,5)));
static bool res4a_conv_1_cast_mul_add_relu_cast_reorder__104(int8_t* __restrict__ __outs_0, int8_t* __restrict__ __ins_0, int8_t* __restrict__ __ins_1, float* __restrict__ __ins_2, float* __restrict__ __ins_3) noexcept __attribute__((nonnull (1,2,3,4,5)));
static bool res4a_conv_b_cast_mul_add_cast_reorder__96(int8_t* __restrict__ __outs_0, uint8_t* __restrict__ __ins_0, int8_t* __restrict__ __ins_1, float* __restrict__ __ins_2, float* __restrict__ __ins_3) noexcept __attribute__((nonnull (1,2,3,4,5)));
static bool res4a_conv_2_cast_mul_add_cast_add_cast_reorder__108(uint8_t* __restrict__ __outs_0, int8_t* __restrict__ __ins_0, int8_t* __restrict__ __ins_1, float* __restrict__ __ins_2, float* __restrict__ __ins_3, int8_t* __restrict__ __ins_4) noexcept __attribute__((nonnull (1,2,3,4,5,6)));
static bool res4b_conv_0_cast_mul_add_relu_cast_reorder__112(int8_t* __restrict__ __outs_0, uint8_t* __restrict__ __ins_0, int8_t* __restrict__ __ins_1, float* __restrict__ __ins_2, float* __restrict__ __ins_3) noexcept __attribute__((nonnull (1,2,3,4,5)));
static bool res4b_conv_1_cast_mul_add_relu_cast_reorder__116(int8_t* __restrict__ __outs_0, int8_t* __restrict__ __ins_0, int8_t* __restrict__ __ins_1, float* __restrict__ __ins_2, float* __restrict__ __ins_3) noexcept __attribute__((nonnull (1,2,3,4,5)));
static bool res4b_conv_2_cast_mul_add_cast_add_cast_reorder__120(uint8_t* __restrict__ __outs_0, int8_t* __restrict__ __ins_0, int8_t* __restrict__ __ins_1, float* __restrict__ __ins_2, float* __restrict__ __ins_3, uint8_t* __restrict__ __ins_4) noexcept __attribute__((nonnull (1,2,3,4,5,6)));
static bool res4c_conv_0_cast_mul_add_relu_cast_reorder__124(int8_t* __restrict__ __outs_0, uint8_t* __restrict__ __ins_0, int8_t* __restrict__ __ins_1, float* __restrict__ __ins_2, float* __restrict__ __ins_3) noexcept __attribute__((nonnull (1,2,3,4,5)));
static bool res4c_conv_1_cast_mul_add_relu_cast_reorder__128(int8_t* __restrict__ __outs_0, int8_t* __restrict__ __ins_0, int8_t* __restrict__ __ins_1, float* __restrict__ __ins_2, float* __restrict__ __ins_3) noexcept __attribute__((nonnull (1,2,3,4,5)));
static bool res4c_conv_2_cast_mul_add_cast_add_cast__132(uint8_t* __restrict__ __outs_0, int8_t* __restrict__ __ins_0, int8_t* __restrict__ __ins_1, float* __restrict__ __ins_2, float* __restrict__ __ins_3, uint8_t* __restrict__ __ins_4) noexcept __attribute__((nonnull (1,2,3,4,5,6)));
static bool res4d_conv_0_cast_mul_add_relu_cast__136(int8_t* __restrict__ __outs_0, uint8_t* __restrict__ __ins_0, int8_t* __restrict__ __ins_1, float* __restrict__ __ins_2, float* __restrict__ __ins_3) noexcept __attribute__((nonnull (1,2,3,4,5)));
static bool res4d_conv_1_cast_mul_add_relu_cast_reorder__140(int8_t* __restrict__ __outs_0, int8_t* __restrict__ __ins_0, int8_t* __restrict__ __ins_1, float* __restrict__ __ins_2, float* __restrict__ __ins_3) noexcept __attribute__((nonnull (1,2,3,4,5)));
static bool res4d_conv_2_cast_mul_add_cast_add_cast__144(uint8_t* __restrict__ __outs_0, int8_t* __restrict__ __ins_0, int8_t* __restrict__ __ins_1, float* __restrict__ __ins_2, float* __restrict__ __ins_3, uint8_t* __restrict__ __ins_4) noexcept __attribute__((nonnull (1,2,3,4,5,6)));
static bool res4e_conv_0_cast_mul_add_relu_cast_reorder__148(int8_t* __restrict__ __outs_0, uint8_t* __restrict__ __ins_0, int8_t* __restrict__ __ins_1, float* __restrict__ __ins_2, float* __restrict__ __ins_3) noexcept __attribute__((nonnull (1,2,3,4,5)));
static bool res4e_conv_1_cast_mul_add_relu_cast_reorder__152(int8_t* __restrict__ __outs_0, int8_t* __restrict__ __ins_0, int8_t* __restrict__ __ins_1, float* __restrict__ __ins_2, float* __restrict__ __ins_3) noexcept __attribute__((nonnull (1,2,3,4,5)));
static bool res4e_conv_2_cast_mul_add_cast_add_cast_reorder__156(uint8_t* __restrict__ __outs_0, uint8_t* __restrict__ __outs_1, int8_t* __restrict__ __ins_0, int8_t* __restrict__ __ins_1, float* __restrict__ __ins_2, float* __restrict__ __ins_3, uint8_t* __restrict__ __ins_4) noexcept __attribute__((nonnull (1,2,3,4,5,6,7)));
static bool reorder__157(uint8_t* __restrict__ __outs_0, uint8_t* __restrict__ __ins_0) noexcept __attribute__((nonnull (1,2)));
static bool res4f_conv_0_cast_mul_add_relu_cast_reorder__161(int8_t* __restrict__ __outs_0, uint8_t* __restrict__ __ins_0, int8_t* __restrict__ __ins_1, float* __restrict__ __ins_2, float* __restrict__ __ins_3) noexcept __attribute__((nonnull (1,2,3,4,5)));
static bool res4f_conv_1_cast_mul_add_relu_cast_reorder__165(int8_t* __restrict__ __outs_0, int8_t* __restrict__ __ins_0, int8_t* __restrict__ __ins_1, float* __restrict__ __ins_2, float* __restrict__ __ins_3) noexcept __attribute__((nonnull (1,2,3,4,5)));
static bool res4f_conv_2_cast_mul_add_cast_add_cast__170(uint8_t* __restrict__ __outs_0, int8_t* __restrict__ __ins_0, int8_t* __restrict__ __ins_1, float* __restrict__ __ins_2, float* __restrict__ __ins_3, uint8_t* __restrict__ __ins_4) noexcept __attribute__((nonnull (1,2,3,4,5,6)));
extern "C" void* memset(void* ptr, int32_t v, uint64_t len) noexcept;
static void reorder__5310_closure_304_0wrapper(void* __stream, int8_t* __restrict__ __module_data, uint64_t i, generic_val* __restrict__ args) noexcept __attribute__((nonnull (2,4)));
static void mul__6540_closure_305_0wrapper(void* __stream, int8_t* __restrict__ __module_data, uint64_t i, generic_val* __restrict__ args) noexcept __attribute__((nonnull (2,4)));
static void mul__6530_closure_306_0wrapper(void* __stream, int8_t* __restrict__ __module_data, uint64_t i, generic_val* __restrict__ args) noexcept __attribute__((nonnull (2,4)));
static void mul__2400_closure_307_0wrapper(void* __stream, int8_t* __restrict__ __module_data, uint64_t i, generic_val* __restrict__ args) noexcept __attribute__((nonnull (2,4)));
static void cast__2410_closure_308_0wrapper(void* __stream, int8_t* __restrict__ __module_data, uint64_t i, generic_val* __restrict__ args) noexcept __attribute__((nonnull (2,4)));
static void reorder__5370_closure_309_0wrapper(void* __stream, int8_t* __restrict__ __module_data, uint64_t i, generic_val* __restrict__ args) noexcept __attribute__((nonnull (2,4)));
static void res5a_conv_0_cast_mul_add_relu_cast_reorder__6810_closure_310_0wrapper(void* __stream, int8_t* __restrict__ __module_data, uint64_t i, generic_val* __restrict__ args) noexcept __attribute__((nonnull (2,4)));
static void res5a_conv_0_cast_mul_add_relu_cast_reorder__6810_closure_311_0wrapper(void* __stream, int8_t* __restrict__ __module_data, uint64_t i, generic_val* __restrict__ args) noexcept __attribute__((nonnull (2,4)));
static void mul__6580_closure_312_0wrapper(void* __stream, int8_t* __restrict__ __module_data, uint64_t i, generic_val* __restrict__ args) noexcept __attribute__((nonnull (2,4)));
static void mul__6570_closure_313_0wrapper(void* __stream, int8_t* __restrict__ __module_data, uint64_t i, generic_val* __restrict__ args) noexcept __attribute__((nonnull (2,4)));
static void res5a_conv_1_cast_mul_add_relu_cast_reorder__6800_closure_314_0wrapper(void* __stream, int8_t* __restrict__ __module_data, uint64_t i, generic_val* __restrict__ args) noexcept __attribute__((nonnull (2,4)));
static void res5a_conv_b_cast_mul_add_cast_reorder__6820_closure_315_0wrapper(void* __stream, int8_t* __restrict__ __module_data, uint64_t i, generic_val* __restrict__ args) noexcept __attribute__((nonnull (2,4)));
static void res5a_conv_b_cast_mul_add_cast_reorder__6820_closure_316_0wrapper(void* __stream, int8_t* __restrict__ __module_data, uint64_t i, generic_val* __restrict__ args) noexcept __attribute__((nonnull (2,4)));
static void mul__6600_closure_317_0wrapper(void* __stream, int8_t* __restrict__ __module_data, uint64_t i, generic_val* __restrict__ args) noexcept __attribute__((nonnull (2,4)));
static void mul__6590_closure_318_0wrapper(void* __stream, int8_t* __restrict__ __module_data, uint64_t i, generic_val* __restrict__ args) noexcept __attribute__((nonnull (2,4)));
static void reorder__5400_closure_319_0wrapper(void* __stream, int8_t* __restrict__ __module_data, uint64_t i, generic_val* __restrict__ args) noexcept __attribute__((nonnull (2,4)));
static void mul__6620_closure_320_0wrapper(void* __stream, int8_t* __restrict__ __module_data, uint64_t i, generic_val* __restrict__ args) noexcept __attribute__((nonnull (2,4)));
static void mul__6610_closure_321_0wrapper(void* __stream, int8_t* __restrict__ __module_data, uint64_t i, generic_val* __restrict__ args) noexcept __attribute__((nonnull (2,4)));
static void reorder__5430_closure_322_0wrapper(void* __stream, int8_t* __restrict__ __module_data, uint64_t i, generic_val* __restrict__ args) noexcept __attribute__((nonnull (2,4)));
static void mul__2490_closure_323_0wrapper(void* __stream, int8_t* __restrict__ __module_data, uint64_t i, generic_val* __restrict__ args) noexcept __attribute__((nonnull (2,4)));
static void cast__2500_closure_324_0wrapper(void* __stream, int8_t* __restrict__ __module_data, uint64_t i, generic_val* __restrict__ args) noexcept __attribute__((nonnull (2,4)));
static void mul__2580_closure_325_0wrapper(void* __stream, int8_t* __restrict__ __module_data, uint64_t i, generic_val* __restrict__ args) noexcept __attribute__((nonnull (2,4)));
static void cast__2590_closure_326_0wrapper(void* __stream, int8_t* __restrict__ __module_data, uint64_t i, generic_val* __restrict__ args) noexcept __attribute__((nonnull (2,4)));
static void mul__6640_closure_327_0wrapper(void* __stream, int8_t* __restrict__ __module_data, uint64_t i, generic_val* __restrict__ args) noexcept __attribute__((nonnull (2,4)));
static void mul__6630_closure_328_0wrapper(void* __stream, int8_t* __restrict__ __module_data, uint64_t i, generic_val* __restrict__ args) noexcept __attribute__((nonnull (2,4)));
static void reorder__5460_closure_329_0wrapper(void* __stream, int8_t* __restrict__ __module_data, uint64_t i, generic_val* __restrict__ args) noexcept __attribute__((nonnull (2,4)));
static void res5a_conv_2_cast_mul_add_cast_add_cast__6790_closure_330_0wrapper(void* __stream, int8_t* __restrict__ __module_data, uint64_t i, generic_val* __restrict__ args) noexcept __attribute__((nonnull (2,4)));
static void res5b_conv_0_cast_mul_add_relu_cast_reorder__6780_closure_331_0wrapper(void* __stream, int8_t* __restrict__ __module_data, uint64_t i, generic_val* __restrict__ args) noexcept __attribute__((nonnull (2,4)));
static void res5b_conv_0_cast_mul_add_relu_cast_reorder__6780_closure_332_0wrapper(void* __stream, int8_t* __restrict__ __module_data, uint64_t i, generic_val* __restrict__ args) noexcept __attribute__((nonnull (2,4)));
static void res5b_conv_1_cast_mul_add_relu_cast_reorder__6770_closure_333_0wrapper(void* __stream, int8_t* __restrict__ __module_data, uint64_t i, generic_val* __restrict__ args) noexcept __attribute__((nonnull (2,4)));
static void mul__6660_closure_334_0wrapper(void* __stream, int8_t* __restrict__ __module_data, uint64_t i, generic_val* __restrict__ args) noexcept __attribute__((nonnull (2,4)));
static void mul__6650_closure_335_0wrapper(void* __stream, int8_t* __restrict__ __module_data, uint64_t i, generic_val* __restrict__ args) noexcept __attribute__((nonnull (2,4)));
static void reorder__5490_closure_336_0wrapper(void* __stream, int8_t* __restrict__ __module_data, uint64_t i, generic_val* __restrict__ args) noexcept __attribute__((nonnull (2,4)));
static void mul__6680_closure_337_0wrapper(void* __stream, int8_t* __restrict__ __module_data, uint64_t i, generic_val* __restrict__ args) noexcept __attribute__((nonnull (2,4)));
static void mul__6670_closure_338_0wrapper(void* __stream, int8_t* __restrict__ __module_data, uint64_t i, generic_val* __restrict__ args) noexcept __attribute__((nonnull (2,4)));
static void reorder__5520_closure_339_0wrapper(void* __stream, int8_t* __restrict__ __module_data, uint64_t i, generic_val* __restrict__ args) noexcept __attribute__((nonnull (2,4)));
static void mul__6700_closure_340_0wrapper(void* __stream, int8_t* __restrict__ __module_data, uint64_t i, generic_val* __restrict__ args) noexcept __attribute__((nonnull (2,4)));
static void mul__6690_closure_341_0wrapper(void* __stream, int8_t* __restrict__ __module_data, uint64_t i, generic_val* __restrict__ args) noexcept __attribute__((nonnull (2,4)));
static void reorder__5550_closure_342_0wrapper(void* __stream, int8_t* __restrict__ __module_data, uint64_t i, generic_val* __restrict__ args) noexcept __attribute__((nonnull (2,4)));
static void res5b_conv_2_cast_mul_add_cast_add_cast_reorder__6760_closure_343_0wrapper(void* __stream, int8_t* __restrict__ __module_data, uint64_t i, generic_val* __restrict__ args) noexcept __attribute__((nonnull (2,4)));
static void res5c_conv_0_cast_mul_add_relu_cast_reorder__6750_closure_344_0wrapper(void* __stream, int8_t* __restrict__ __module_data, uint64_t i, generic_val* __restrict__ args) noexcept __attribute__((nonnull (2,4)));
static void res5c_conv_0_cast_mul_add_relu_cast_reorder__6750_closure_345_0wrapper(void* __stream, int8_t* __restrict__ __module_data, uint64_t i, generic_val* __restrict__ args) noexcept __attribute__((nonnull (2,4)));
static void res5c_conv_1_cast_mul_add_relu_cast_reorder__6740_closure_346_0wrapper(void* __stream, int8_t* __restrict__ __module_data, uint64_t i, generic_val* __restrict__ args) noexcept __attribute__((nonnull (2,4)));
static void mul__6720_closure_347_0wrapper(void* __stream, int8_t* __restrict__ __module_data, uint64_t i, generic_val* __restrict__ args) noexcept __attribute__((nonnull (2,4)));
static void mul__6710_closure_348_0wrapper(void* __stream, int8_t* __restrict__ __module_data, uint64_t i, generic_val* __restrict__ args) noexcept __attribute__((nonnull (2,4)));
static void reorder__5580_closure_349_0wrapper(void* __stream, int8_t* __restrict__ __module_data, uint64_t i, generic_val* __restrict__ args) noexcept __attribute__((nonnull (2,4)));
static void res5c_conv_2_cast_mul_add_cast_add_cast_cast__6730_closure_350_0wrapper(void* __stream, int8_t* __restrict__ __module_data, uint64_t i, generic_val* __restrict__ args) noexcept __attribute__((nonnull (2,4)));
static void reorder__1050_closure_351_0wrapper(void* __stream, int8_t* __restrict__ __module_data, uint64_t i, generic_val* __restrict__ args) noexcept __attribute__((nonnull (2,4)));
static bool reorder__481(float* __restrict__ __outs_0, float* __restrict__ __ins_0) noexcept __attribute__((nonnull (1,2)));
static bool reorder__488(float* __restrict__ __outs_0, float* __restrict__ __ins_0) noexcept __attribute__((nonnull (1,2)));
static bool reorder__504(float* __restrict__ __outs_0, float* __restrict__ __ins_0) noexcept __attribute__((nonnull (1,2)));
static bool reorder__513(float* __restrict__ __outs_0, float* __restrict__ __ins_0) noexcept __attribute__((nonnull (1,2)));
static bool reorder__520(float* __restrict__ __outs_0, float* __restrict__ __ins_0) noexcept __attribute__((nonnull (1,2)));
static bool reorder__529(float* __restrict__ __outs_0, float* __restrict__ __ins_0) noexcept __attribute__((nonnull (1,2)));
static bool reorder__420(float* __restrict__ __outs_0, float* __restrict__ __ins_0) noexcept __attribute__((nonnull (1,2)));
static bool reorder__424(float* __restrict__ __outs_0, float* __restrict__ __ins_0) noexcept __attribute__((nonnull (1,2)));
static bool reorder__427(float* __restrict__ __outs_0, float* __restrict__ __ins_0) noexcept __attribute__((nonnull (1,2)));
static bool reorder__431(float* __restrict__ __outs_0, float* __restrict__ __ins_0) noexcept __attribute__((nonnull (1,2)));
static bool reorder__434(float* __restrict__ __outs_0, float* __restrict__ __ins_0) noexcept __attribute__((nonnull (1,2)));
static bool reorder__437(float* __restrict__ __outs_0, float* __restrict__ __ins_0) noexcept __attribute__((nonnull (1,2)));
static bool reorder__440(float* __restrict__ __outs_0, float* __restrict__ __ins_0) noexcept __attribute__((nonnull (1,2)));
static bool reorder__443(float* __restrict__ __outs_0, float* __restrict__ __ins_0) noexcept __attribute__((nonnull (1,2)));
static bool reorder__446(float* __restrict__ __outs_0, float* __restrict__ __ins_0) noexcept __attribute__((nonnull (1,2)));
static bool reorder__449(float* __restrict__ __outs_0, float* __restrict__ __ins_0) noexcept __attribute__((nonnull (1,2)));
static bool reorder__452(float* __restrict__ __outs_0, float* __restrict__ __ins_0) noexcept __attribute__((nonnull (1,2)));
static bool reorder__455(float* __restrict__ __outs_0, float* __restrict__ __ins_0) noexcept __attribute__((nonnull (1,2)));
static bool reorder__458(float* __restrict__ __outs_0, float* __restrict__ __ins_0) noexcept __attribute__((nonnull (1,2)));
static bool reorder__462(float* __restrict__ __outs_0, float* __restrict__ __ins_0) noexcept __attribute__((nonnull (1,2)));
static bool reorder__466(float* __restrict__ __outs_0, float* __restrict__ __ins_0) noexcept __attribute__((nonnull (1,2)));
static bool reorder__469(float* __restrict__ __outs_0, float* __restrict__ __ins_0) noexcept __attribute__((nonnull (1,2)));
static bool reorder__472(float* __restrict__ __outs_0, float* __restrict__ __ins_0) noexcept __attribute__((nonnull (1,2)));
static bool reorder__475(float* __restrict__ __outs_0, float* __restrict__ __ins_0) noexcept __attribute__((nonnull (1,2)));
static bool reorder__478(float* __restrict__ __outs_0, float* __restrict__ __ins_0) noexcept __attribute__((nonnull (1,2)));
static bool reorder__485(float* __restrict__ __outs_0, float* __restrict__ __ins_0) noexcept __attribute__((nonnull (1,2)));
static bool reorder__491(float* __restrict__ __outs_0, float* __restrict__ __ins_0) noexcept __attribute__((nonnull (1,2)));
static bool reorder__494(float* __restrict__ __outs_0, float* __restrict__ __ins_0) noexcept __attribute__((nonnull (1,2)));
static bool reorder__498(float* __restrict__ __outs_0, float* __restrict__ __ins_0) noexcept __attribute__((nonnull (1,2)));
static bool reorder__501(float* __restrict__ __outs_0, float* __restrict__ __ins_0) noexcept __attribute__((nonnull (1,2)));
static bool reorder__507(float* __restrict__ __outs_0, float* __restrict__ __ins_0) noexcept __attribute__((nonnull (1,2)));
static bool reorder__510(float* __restrict__ __outs_0, float* __restrict__ __ins_0) noexcept __attribute__((nonnull (1,2)));
static bool reorder__517(float* __restrict__ __outs_0, float* __restrict__ __ins_0) noexcept __attribute__((nonnull (1,2)));
static bool reorder__523(float* __restrict__ __outs_0, float* __restrict__ __ins_0) noexcept __attribute__((nonnull (1,2)));
static bool reorder__526(float* __restrict__ __outs_0, float* __restrict__ __ins_0) noexcept __attribute__((nonnull (1,2)));
static bool reorder__532(float* __restrict__ __outs_0, float* __restrict__ __ins_0) noexcept __attribute__((nonnull (1,2)));
static bool reorder__535(float* __restrict__ __outs_0, float* __restrict__ __ins_0) noexcept __attribute__((nonnull (1,2)));
static bool reorder__538(float* __restrict__ __outs_0, float* __restrict__ __ins_0) noexcept __attribute__((nonnull (1,2)));
static bool reorder__541(float* __restrict__ __outs_0, float* __restrict__ __ins_0) noexcept __attribute__((nonnull (1,2)));
static bool reorder__544(float* __restrict__ __outs_0, float* __restrict__ __ins_0) noexcept __attribute__((nonnull (1,2)));
static bool reorder__547(float* __restrict__ __outs_0, float* __restrict__ __ins_0) noexcept __attribute__((nonnull (1,2)));
static bool reorder__550(float* __restrict__ __outs_0, float* __restrict__ __ins_0) noexcept __attribute__((nonnull (1,2)));
static bool reorder__553(float* __restrict__ __outs_0, float* __restrict__ __ins_0) noexcept __attribute__((nonnull (1,2)));
static bool reorder__556(float* __restrict__ __outs_0, float* __restrict__ __ins_0) noexcept __attribute__((nonnull (1,2)));
static bool reorder__559(float* __restrict__ __outs_0, float* __restrict__ __ins_0) noexcept __attribute__((nonnull (1,2)));
static bool reorder__425(float* __restrict__ __outs_0, float* __restrict__ __ins_0) noexcept __attribute__((nonnull (1,2)));
static bool reorder__432(float* __restrict__ __outs_0, float* __restrict__ __ins_0) noexcept __attribute__((nonnull (1,2)));
static bool reorder__438(float* __restrict__ __outs_0, float* __restrict__ __ins_0) noexcept __attribute__((nonnull (1,2)));
static bool reorder__441(float* __restrict__ __outs_0, float* __restrict__ __ins_0) noexcept __attribute__((nonnull (1,2)));
static bool reorder__450(float* __restrict__ __outs_0, float* __restrict__ __ins_0) noexcept __attribute__((nonnull (1,2)));
static bool reorder__453(float* __restrict__ __outs_0, float* __restrict__ __ins_0) noexcept __attribute__((nonnull (1,2)));
static bool reorder__459(float* __restrict__ __outs_0, float* __restrict__ __ins_0) noexcept __attribute__((nonnull (1,2)));
static bool reorder__467(float* __restrict__ __outs_0, float* __restrict__ __ins_0) noexcept __attribute__((nonnull (1,2)));
static bool reorder__473(float* __restrict__ __outs_0, float* __restrict__ __ins_0) noexcept __attribute__((nonnull (1,2)));
static bool reorder__476(float* __restrict__ __outs_0, float* __restrict__ __ins_0) noexcept __attribute__((nonnull (1,2)));
static bool reorder__421(float* __restrict__ __outs_0, float* __restrict__ __ins_0) noexcept __attribute__((nonnull (1,2)));
static bool reorder__428(float* __restrict__ __outs_0, float* __restrict__ __ins_0) noexcept __attribute__((nonnull (1,2)));
static bool reorder__435(float* __restrict__ __outs_0, float* __restrict__ __ins_0) noexcept __attribute__((nonnull (1,2)));
static bool reorder__444(float* __restrict__ __outs_0, float* __restrict__ __ins_0) noexcept __attribute__((nonnull (1,2)));
static bool reorder__486(float* __restrict__ __outs_0, float* __restrict__ __ins_0) noexcept __attribute__((nonnull (1,2)));
static bool reorder__492(float* __restrict__ __outs_0, float* __restrict__ __ins_0) noexcept __attribute__((nonnull (1,2)));
static bool reorder__495(float* __restrict__ __outs_0, float* __restrict__ __ins_0) noexcept __attribute__((nonnull (1,2)));
static bool reorder__499(float* __restrict__ __outs_0, float* __restrict__ __ins_0) noexcept __attribute__((nonnull (1,2)));
static bool reorder__502(float* __restrict__ __outs_0, float* __restrict__ __ins_0) noexcept __attribute__((nonnull (1,2)));
static bool reorder__508(float* __restrict__ __outs_0, float* __restrict__ __ins_0) noexcept __attribute__((nonnull (1,2)));
static bool reorder__511(float* __restrict__ __outs_0, float* __restrict__ __ins_0) noexcept __attribute__((nonnull (1,2)));
static bool reorder__518(float* __restrict__ __outs_0, float* __restrict__ __ins_0) noexcept __attribute__((nonnull (1,2)));
static bool reorder__524(float* __restrict__ __outs_0, float* __restrict__ __ins_0) noexcept __attribute__((nonnull (1,2)));
static bool reorder__527(float* __restrict__ __outs_0, float* __restrict__ __ins_0) noexcept __attribute__((nonnull (1,2)));
static bool reorder__447(float* __restrict__ __outs_0, float* __restrict__ __ins_0) noexcept __attribute__((nonnull (1,2)));
static bool reorder__456(float* __restrict__ __outs_0, float* __restrict__ __ins_0) noexcept __attribute__((nonnull (1,2)));
static bool reorder__463(float* __restrict__ __outs_0, float* __restrict__ __ins_0) noexcept __attribute__((nonnull (1,2)));
static bool reorder__470(float* __restrict__ __outs_0, float* __restrict__ __ins_0) noexcept __attribute__((nonnull (1,2)));
static bool reorder__479(float* __restrict__ __outs_0, float* __restrict__ __ins_0) noexcept __attribute__((nonnull (1,2)));
static bool reorder__536(float* __restrict__ __outs_0, float* __restrict__ __ins_0) noexcept __attribute__((nonnull (1,2)));
static bool reorder__539(float* __restrict__ __outs_0, float* __restrict__ __ins_0) noexcept __attribute__((nonnull (1,2)));
static bool reorder__545(float* __restrict__ __outs_0, float* __restrict__ __ins_0) noexcept __attribute__((nonnull (1,2)));
static bool reorder__548(float* __restrict__ __outs_0, float* __restrict__ __ins_0) noexcept __attribute__((nonnull (1,2)));
static bool reorder__554(float* __restrict__ __outs_0, float* __restrict__ __ins_0) noexcept __attribute__((nonnull (1,2)));
static bool reorder__557(float* __restrict__ __outs_0, float* __restrict__ __ins_0) noexcept __attribute__((nonnull (1,2)));
static bool reorder__482(float* __restrict__ __outs_0, float* __restrict__ __ins_0) noexcept __attribute__((nonnull (1,2)));
static bool reorder__489(float* __restrict__ __outs_0, float* __restrict__ __ins_0) noexcept __attribute__((nonnull (1,2)));
static bool reorder__505(float* __restrict__ __outs_0, float* __restrict__ __ins_0) noexcept __attribute__((nonnull (1,2)));
static bool reorder__514(float* __restrict__ __outs_0, float* __restrict__ __ins_0) noexcept __attribute__((nonnull (1,2)));
static bool reorder__521(float* __restrict__ __outs_0, float* __restrict__ __ins_0) noexcept __attribute__((nonnull (1,2)));
static bool reorder__530(float* __restrict__ __outs_0, float* __restrict__ __ins_0) noexcept __attribute__((nonnull (1,2)));
static bool reorder__533(float* __restrict__ __outs_0, float* __restrict__ __ins_0) noexcept __attribute__((nonnull (1,2)));
static bool reorder__542(float* __restrict__ __outs_0, float* __restrict__ __ins_0) noexcept __attribute__((nonnull (1,2)));
static bool reorder__551(float* __restrict__ __outs_0, float* __restrict__ __ins_0) noexcept __attribute__((nonnull (1,2)));
static bool reorder__560(float* __restrict__ __outs_0, float* __restrict__ __ins_0) noexcept __attribute__((nonnull (1,2)));
static bool mul__111(float* __restrict__ __outs_0, float* __restrict__ __ins_0, float* __restrict__ __ins_1) noexcept __attribute__((nonnull (1,2,3)));
static bool cast__112(int8_t* __restrict__ __outs_0, float* __restrict__ __ins_0) noexcept __attribute__((nonnull (1,2)));
static bool mul__108(float* __restrict__ __outs_0, float* __restrict__ __ins_0, float* __restrict__ __ins_1) noexcept __attribute__((nonnull (1,2,3)));
static bool cast__109(int8_t* __restrict__ __outs_0, float* __restrict__ __ins_0) noexcept __attribute__((nonnull (1,2)));
static bool mul__117(float* __restrict__ __outs_0, float* __restrict__ __ins_0, float* __restrict__ __ins_1) noexcept __attribute__((nonnull (1,2,3)));
static bool cast__118(int8_t* __restrict__ __outs_0, float* __restrict__ __ins_0) noexcept __attribute__((nonnull (1,2)));
static bool mul__126(float* __restrict__ __outs_0, float* __restrict__ __ins_0, float* __restrict__ __ins_1) noexcept __attribute__((nonnull (1,2,3)));
static bool cast__127(int8_t* __restrict__ __outs_0, float* __restrict__ __ins_0) noexcept __attribute__((nonnull (1,2)));
static bool mul__135(float* __restrict__ __outs_0, float* __restrict__ __ins_0, float* __restrict__ __ins_1) noexcept __attribute__((nonnull (1,2,3)));
static bool cast__136(int8_t* __restrict__ __outs_0, float* __restrict__ __ins_0) noexcept __attribute__((nonnull (1,2)));
static bool mul__120(float* __restrict__ __outs_0, float* __restrict__ __ins_0, float* __restrict__ __ins_1) noexcept __attribute__((nonnull (1,2,3)));
static bool cast__121(int8_t* __restrict__ __outs_0, float* __restrict__ __ins_0) noexcept __attribute__((nonnull (1,2)));
static bool mul__129(float* __restrict__ __outs_0, float* __restrict__ __ins_0, float* __restrict__ __ins_1) noexcept __attribute__((nonnull (1,2,3)));
static bool cast__130(int8_t* __restrict__ __outs_0, float* __restrict__ __ins_0) noexcept __attribute__((nonnull (1,2)));
static bool mul__141(float* __restrict__ __outs_0, float* __restrict__ __ins_0, float* __restrict__ __ins_1) noexcept __attribute__((nonnull (1,2,3)));
static bool cast__142(int8_t* __restrict__ __outs_0, float* __restrict__ __ins_0) noexcept __attribute__((nonnull (1,2)));
static bool mul__114(float* __restrict__ __outs_0, float* __restrict__ __ins_0, float* __restrict__ __ins_1) noexcept __attribute__((nonnull (1,2,3)));
static bool cast__115(int8_t* __restrict__ __outs_0, float* __restrict__ __ins_0) noexcept __attribute__((nonnull (1,2)));
static bool mul__123(float* __restrict__ __outs_0, float* __restrict__ __ins_0, float* __restrict__ __ins_1) noexcept __attribute__((nonnull (1,2,3)));
static bool cast__124(int8_t* __restrict__ __outs_0, float* __restrict__ __ins_0) noexcept __attribute__((nonnull (1,2)));
static bool mul__132(float* __restrict__ __outs_0, float* __restrict__ __ins_0, float* __restrict__ __ins_1) noexcept __attribute__((nonnull (1,2,3)));
static bool cast__133(int8_t* __restrict__ __outs_0, float* __restrict__ __ins_0) noexcept __attribute__((nonnull (1,2)));
static bool mul__147(float* __restrict__ __outs_0, float* __restrict__ __ins_0, float* __restrict__ __ins_1) noexcept __attribute__((nonnull (1,2,3)));
static bool cast__148(int8_t* __restrict__ __outs_0, float* __restrict__ __ins_0) noexcept __attribute__((nonnull (1,2)));
static bool mul__156(float* __restrict__ __outs_0, float* __restrict__ __ins_0, float* __restrict__ __ins_1) noexcept __attribute__((nonnull (1,2,3)));
static bool cast__157(int8_t* __restrict__ __outs_0, float* __restrict__ __ins_0) noexcept __attribute__((nonnull (1,2)));
static bool mul__165(float* __restrict__ __outs_0, float* __restrict__ __ins_0, float* __restrict__ __ins_1) noexcept __attribute__((nonnull (1,2,3)));
static bool cast__166(int8_t* __restrict__ __outs_0, float* __restrict__ __ins_0) noexcept __attribute__((nonnull (1,2)));
static bool mul__174(float* __restrict__ __outs_0, float* __restrict__ __ins_0, float* __restrict__ __ins_1) noexcept __attribute__((nonnull (1,2,3)));
static bool cast__175(int8_t* __restrict__ __outs_0, float* __restrict__ __ins_0) noexcept __attribute__((nonnull (1,2)));
static bool mul__150(float* __restrict__ __outs_0, float* __restrict__ __ins_0, float* __restrict__ __ins_1) noexcept __attribute__((nonnull (1,2,3)));
static bool cast__151(int8_t* __restrict__ __outs_0, float* __restrict__ __ins_0) noexcept __attribute__((nonnull (1,2)));
static bool mul__159(float* __restrict__ __outs_0, float* __restrict__ __ins_0, float* __restrict__ __ins_1) noexcept __attribute__((nonnull (1,2,3)));
static bool cast__160(int8_t* __restrict__ __outs_0, float* __restrict__ __ins_0) noexcept __attribute__((nonnull (1,2)));
static bool mul__168(float* __restrict__ __outs_0, float* __restrict__ __ins_0, float* __restrict__ __ins_1) noexcept __attribute__((nonnull (1,2,3)));
static bool cast__169(int8_t* __restrict__ __outs_0, float* __restrict__ __ins_0) noexcept __attribute__((nonnull (1,2)));
static bool mul__138(float* __restrict__ __outs_0, float* __restrict__ __ins_0, float* __restrict__ __ins_1) noexcept __attribute__((nonnull (1,2,3)));
static bool cast__139(int8_t* __restrict__ __outs_0, float* __restrict__ __ins_0) noexcept __attribute__((nonnull (1,2)));
static bool mul__180(float* __restrict__ __outs_0, float* __restrict__ __ins_0, float* __restrict__ __ins_1) noexcept __attribute__((nonnull (1,2,3)));
static bool cast__181(int8_t* __restrict__ __outs_0, float* __restrict__ __ins_0) noexcept __attribute__((nonnull (1,2)));
static bool mul__144(float* __restrict__ __outs_0, float* __restrict__ __ins_0, float* __restrict__ __ins_1) noexcept __attribute__((nonnull (1,2,3)));
static bool cast__145(int8_t* __restrict__ __outs_0, float* __restrict__ __ins_0) noexcept __attribute__((nonnull (1,2)));
static bool mul__153(float* __restrict__ __outs_0, float* __restrict__ __ins_0, float* __restrict__ __ins_1) noexcept __attribute__((nonnull (1,2,3)));
static bool cast__154(int8_t* __restrict__ __outs_0, float* __restrict__ __ins_0) noexcept __attribute__((nonnull (1,2)));
static bool mul__162(float* __restrict__ __outs_0, float* __restrict__ __ins_0, float* __restrict__ __ins_1) noexcept __attribute__((nonnull (1,2,3)));
static bool cast__163(int8_t* __restrict__ __outs_0, float* __restrict__ __ins_0) noexcept __attribute__((nonnull (1,2)));
static bool mul__171(float* __restrict__ __outs_0, float* __restrict__ __ins_0, float* __restrict__ __ins_1) noexcept __attribute__((nonnull (1,2,3)));
static bool cast__172(int8_t* __restrict__ __outs_0, float* __restrict__ __ins_0) noexcept __attribute__((nonnull (1,2)));
static bool mul__186(float* __restrict__ __outs_0, float* __restrict__ __ins_0, float* __restrict__ __ins_1) noexcept __attribute__((nonnull (1,2,3)));
static bool cast__187(int8_t* __restrict__ __outs_0, float* __restrict__ __ins_0) noexcept __attribute__((nonnull (1,2)));
static bool mul__195(float* __restrict__ __outs_0, float* __restrict__ __ins_0, float* __restrict__ __ins_1) noexcept __attribute__((nonnull (1,2,3)));
static bool cast__196(int8_t* __restrict__ __outs_0, float* __restrict__ __ins_0) noexcept __attribute__((nonnull (1,2)));
static bool mul__204(float* __restrict__ __outs_0, float* __restrict__ __ins_0, float* __restrict__ __ins_1) noexcept __attribute__((nonnull (1,2,3)));
static bool cast__205(int8_t* __restrict__ __outs_0, float* __restrict__ __ins_0) noexcept __attribute__((nonnull (1,2)));
static bool mul__213(float* __restrict__ __outs_0, float* __restrict__ __ins_0, float* __restrict__ __ins_1) noexcept __attribute__((nonnull (1,2,3)));
static bool cast__214(int8_t* __restrict__ __outs_0, float* __restrict__ __ins_0) noexcept __attribute__((nonnull (1,2)));
static bool mul__222(float* __restrict__ __outs_0, float* __restrict__ __ins_0, float* __restrict__ __ins_1) noexcept __attribute__((nonnull (1,2,3)));
static bool cast__223(int8_t* __restrict__ __outs_0, float* __restrict__ __ins_0) noexcept __attribute__((nonnull (1,2)));
static bool mul__231(float* __restrict__ __outs_0, float* __restrict__ __ins_0, float* __restrict__ __ins_1) noexcept __attribute__((nonnull (1,2,3)));
static bool cast__232(int8_t* __restrict__ __outs_0, float* __restrict__ __ins_0) noexcept __attribute__((nonnull (1,2)));
static bool mul__189(float* __restrict__ __outs_0, float* __restrict__ __ins_0, float* __restrict__ __ins_1) noexcept __attribute__((nonnull (1,2,3)));
static bool cast__190(int8_t* __restrict__ __outs_0, float* __restrict__ __ins_0) noexcept __attribute__((nonnull (1,2)));
static bool mul__198(float* __restrict__ __outs_0, float* __restrict__ __ins_0, float* __restrict__ __ins_1) noexcept __attribute__((nonnull (1,2,3)));
static bool cast__199(int8_t* __restrict__ __outs_0, float* __restrict__ __ins_0) noexcept __attribute__((nonnull (1,2)));
static bool mul__207(float* __restrict__ __outs_0, float* __restrict__ __ins_0, float* __restrict__ __ins_1) noexcept __attribute__((nonnull (1,2,3)));
static bool cast__208(int8_t* __restrict__ __outs_0, float* __restrict__ __ins_0) noexcept __attribute__((nonnull (1,2)));
static bool mul__216(float* __restrict__ __outs_0, float* __restrict__ __ins_0, float* __restrict__ __ins_1) noexcept __attribute__((nonnull (1,2,3)));
static bool cast__217(int8_t* __restrict__ __outs_0, float* __restrict__ __ins_0) noexcept __attribute__((nonnull (1,2)));
static bool mul__225(float* __restrict__ __outs_0, float* __restrict__ __ins_0, float* __restrict__ __ins_1) noexcept __attribute__((nonnull (1,2,3)));
static bool cast__226(int8_t* __restrict__ __outs_0, float* __restrict__ __ins_0) noexcept __attribute__((nonnull (1,2)));
static bool mul__177(float* __restrict__ __outs_0, float* __restrict__ __ins_0, float* __restrict__ __ins_1) noexcept __attribute__((nonnull (1,2,3)));
static bool cast__178(int8_t* __restrict__ __outs_0, float* __restrict__ __ins_0) noexcept __attribute__((nonnull (1,2)));
static bool mul__237(float* __restrict__ __outs_0, float* __restrict__ __ins_0, float* __restrict__ __ins_1) noexcept __attribute__((nonnull (1,2,3)));
static bool cast__238(int8_t* __restrict__ __outs_0, float* __restrict__ __ins_0) noexcept __attribute__((nonnull (1,2)));
static bool mul__183(float* __restrict__ __outs_0, float* __restrict__ __ins_0, float* __restrict__ __ins_1) noexcept __attribute__((nonnull (1,2,3)));
static bool cast__184(int8_t* __restrict__ __outs_0, float* __restrict__ __ins_0) noexcept __attribute__((nonnull (1,2)));
static bool mul__192(float* __restrict__ __outs_0, float* __restrict__ __ins_0, float* __restrict__ __ins_1) noexcept __attribute__((nonnull (1,2,3)));
static bool cast__193(int8_t* __restrict__ __outs_0, float* __restrict__ __ins_0) noexcept __attribute__((nonnull (1,2)));
static bool mul__201(float* __restrict__ __outs_0, float* __restrict__ __ins_0, float* __restrict__ __ins_1) noexcept __attribute__((nonnull (1,2,3)));
static bool cast__202(int8_t* __restrict__ __outs_0, float* __restrict__ __ins_0) noexcept __attribute__((nonnull (1,2)));
static bool mul__210(float* __restrict__ __outs_0, float* __restrict__ __ins_0, float* __restrict__ __ins_1) noexcept __attribute__((nonnull (1,2,3)));
static bool cast__211(int8_t* __restrict__ __outs_0, float* __restrict__ __ins_0) noexcept __attribute__((nonnull (1,2)));
static bool mul__219(float* __restrict__ __outs_0, float* __restrict__ __ins_0, float* __restrict__ __ins_1) noexcept __attribute__((nonnull (1,2,3)));
static bool cast__220(int8_t* __restrict__ __outs_0, float* __restrict__ __ins_0) noexcept __attribute__((nonnull (1,2)));
static bool mul__228(float* __restrict__ __outs_0, float* __restrict__ __ins_0, float* __restrict__ __ins_1) noexcept __attribute__((nonnull (1,2,3)));
static bool cast__229(int8_t* __restrict__ __outs_0, float* __restrict__ __ins_0) noexcept __attribute__((nonnull (1,2)));
static bool reorder__525(int8_t* __restrict__ __outs_0, int8_t* __restrict__ __ins_0) noexcept __attribute__((nonnull (1,2)));
static bool mul__652(float* __restrict__ __outs_0, float* __restrict__ __ins_0, float* __restrict__ __ins_1) noexcept __attribute__((nonnull (1,2,3)));
static bool mul__651(float* __restrict__ __outs_0, float* __restrict__ __ins_0, float* __restrict__ __ins_1) noexcept __attribute__((nonnull (1,2,3)));
static bool mul__646(float* __restrict__ __outs_0, float* __restrict__ __ins_0, float* __restrict__ __ins_1) noexcept __attribute__((nonnull (1,2,3)));
static bool mul__645(float* __restrict__ __outs_0, float* __restrict__ __ins_0, float* __restrict__ __ins_1) noexcept __attribute__((nonnull (1,2,3)));
static bool mul__640(float* __restrict__ __outs_0, float* __restrict__ __ins_0, float* __restrict__ __ins_1) noexcept __attribute__((nonnull (1,2,3)));
static bool mul__639(float* __restrict__ __outs_0, float* __restrict__ __ins_0, float* __restrict__ __ins_1) noexcept __attribute__((nonnull (1,2,3)));
static bool mul__634(float* __restrict__ __outs_0, float* __restrict__ __ins_0, float* __restrict__ __ins_1) noexcept __attribute__((nonnull (1,2,3)));
static bool mul__633(float* __restrict__ __outs_0, float* __restrict__ __ins_0, float* __restrict__ __ins_1) noexcept __attribute__((nonnull (1,2,3)));
static bool mul__628(float* __restrict__ __outs_0, float* __restrict__ __ins_0, float* __restrict__ __ins_1) noexcept __attribute__((nonnull (1,2,3)));
static bool mul__627(float* __restrict__ __outs_0, float* __restrict__ __ins_0, float* __restrict__ __ins_1) noexcept __attribute__((nonnull (1,2,3)));
static bool mul__622(float* __restrict__ __outs_0, float* __restrict__ __ins_0, float* __restrict__ __ins_1) noexcept __attribute__((nonnull (1,2,3)));
static bool mul__621(float* __restrict__ __outs_0, float* __restrict__ __ins_0, float* __restrict__ __ins_1) noexcept __attribute__((nonnull (1,2,3)));
static bool mul__616(float* __restrict__ __outs_0, float* __restrict__ __ins_0, float* __restrict__ __ins_1) noexcept __attribute__((nonnull (1,2,3)));
static bool mul__615(float* __restrict__ __outs_0, float* __restrict__ __ins_0, float* __restrict__ __ins_1) noexcept __attribute__((nonnull (1,2,3)));
static bool mul__614(float* __restrict__ __outs_0, float* __restrict__ __ins_0, float* __restrict__ __ins_1) noexcept __attribute__((nonnull (1,2,3)));
static bool mul__613(float* __restrict__ __outs_0, float* __restrict__ __ins_0, float* __restrict__ __ins_1) noexcept __attribute__((nonnull (1,2,3)));
static bool mul__608(float* __restrict__ __outs_0, float* __restrict__ __ins_0, float* __restrict__ __ins_1) noexcept __attribute__((nonnull (1,2,3)));
static bool mul__607(float* __restrict__ __outs_0, float* __restrict__ __ins_0, float* __restrict__ __ins_1) noexcept __attribute__((nonnull (1,2,3)));
static bool mul__602(float* __restrict__ __outs_0, float* __restrict__ __ins_0, float* __restrict__ __ins_1) noexcept __attribute__((nonnull (1,2,3)));
static bool mul__601(float* __restrict__ __outs_0, float* __restrict__ __ins_0, float* __restrict__ __ins_1) noexcept __attribute__((nonnull (1,2,3)));
static bool mul__596(float* __restrict__ __outs_0, float* __restrict__ __ins_0, float* __restrict__ __ins_1) noexcept __attribute__((nonnull (1,2,3)));
static bool mul__595(float* __restrict__ __outs_0, float* __restrict__ __ins_0, float* __restrict__ __ins_1) noexcept __attribute__((nonnull (1,2,3)));
static bool mul__590(float* __restrict__ __outs_0, float* __restrict__ __ins_0, float* __restrict__ __ins_1) noexcept __attribute__((nonnull (1,2,3)));
static bool mul__589(float* __restrict__ __outs_0, float* __restrict__ __ins_0, float* __restrict__ __ins_1) noexcept __attribute__((nonnull (1,2,3)));
static bool mul__650(float* __restrict__ __outs_0, float* __restrict__ __ins_0, float* __restrict__ __ins_1) noexcept __attribute__((nonnull (1,2,3)));
static bool mul__649(float* __restrict__ __outs_0, float* __restrict__ __ins_0, float* __restrict__ __ins_1) noexcept __attribute__((nonnull (1,2,3)));
static bool mul__648(float* __restrict__ __outs_0, float* __restrict__ __ins_0, float* __restrict__ __ins_1) noexcept __attribute__((nonnull (1,2,3)));
static bool mul__647(float* __restrict__ __outs_0, float* __restrict__ __ins_0, float* __restrict__ __ins_1) noexcept __attribute__((nonnull (1,2,3)));
static bool mul__644(float* __restrict__ __outs_0, float* __restrict__ __ins_0, float* __restrict__ __ins_1) noexcept __attribute__((nonnull (1,2,3)));
static bool mul__643(float* __restrict__ __outs_0, float* __restrict__ __ins_0, float* __restrict__ __ins_1) noexcept __attribute__((nonnull (1,2,3)));
static bool mul__642(float* __restrict__ __outs_0, float* __restrict__ __ins_0, float* __restrict__ __ins_1) noexcept __attribute__((nonnull (1,2,3)));
static bool mul__641(float* __restrict__ __outs_0, float* __restrict__ __ins_0, float* __restrict__ __ins_1) noexcept __attribute__((nonnull (1,2,3)));
static bool mul__638(float* __restrict__ __outs_0, float* __restrict__ __ins_0, float* __restrict__ __ins_1) noexcept __attribute__((nonnull (1,2,3)));
static bool mul__637(float* __restrict__ __outs_0, float* __restrict__ __ins_0, float* __restrict__ __ins_1) noexcept __attribute__((nonnull (1,2,3)));
static bool mul__636(float* __restrict__ __outs_0, float* __restrict__ __ins_0, float* __restrict__ __ins_1) noexcept __attribute__((nonnull (1,2,3)));
static bool mul__635(float* __restrict__ __outs_0, float* __restrict__ __ins_0, float* __restrict__ __ins_1) noexcept __attribute__((nonnull (1,2,3)));
static bool mul__632(float* __restrict__ __outs_0, float* __restrict__ __ins_0, float* __restrict__ __ins_1) noexcept __attribute__((nonnull (1,2,3)));
static bool mul__631(float* __restrict__ __outs_0, float* __restrict__ __ins_0, float* __restrict__ __ins_1) noexcept __attribute__((nonnull (1,2,3)));
static bool mul__630(float* __restrict__ __outs_0, float* __restrict__ __ins_0, float* __restrict__ __ins_1) noexcept __attribute__((nonnull (1,2,3)));
static bool mul__629(float* __restrict__ __outs_0, float* __restrict__ __ins_0, float* __restrict__ __ins_1) noexcept __attribute__((nonnull (1,2,3)));
static bool mul__626(float* __restrict__ __outs_0, float* __restrict__ __ins_0, float* __restrict__ __ins_1) noexcept __attribute__((nonnull (1,2,3)));
static bool mul__625(float* __restrict__ __outs_0, float* __restrict__ __ins_0, float* __restrict__ __ins_1) noexcept __attribute__((nonnull (1,2,3)));
static bool mul__624(float* __restrict__ __outs_0, float* __restrict__ __ins_0, float* __restrict__ __ins_1) noexcept __attribute__((nonnull (1,2,3)));
static bool mul__623(float* __restrict__ __outs_0, float* __restrict__ __ins_0, float* __restrict__ __ins_1) noexcept __attribute__((nonnull (1,2,3)));
static bool mul__620(float* __restrict__ __outs_0, float* __restrict__ __ins_0, float* __restrict__ __ins_1) noexcept __attribute__((nonnull (1,2,3)));
static bool mul__619(float* __restrict__ __outs_0, float* __restrict__ __ins_0, float* __restrict__ __ins_1) noexcept __attribute__((nonnull (1,2,3)));
static bool mul__618(float* __restrict__ __outs_0, float* __restrict__ __ins_0, float* __restrict__ __ins_1) noexcept __attribute__((nonnull (1,2,3)));
static bool mul__617(float* __restrict__ __outs_0, float* __restrict__ __ins_0, float* __restrict__ __ins_1) noexcept __attribute__((nonnull (1,2,3)));
static bool mul__588(float* __restrict__ __outs_0, float* __restrict__ __ins_0, float* __restrict__ __ins_1) noexcept __attribute__((nonnull (1,2,3)));
static bool mul__587(float* __restrict__ __outs_0, float* __restrict__ __ins_0, float* __restrict__ __ins_1) noexcept __attribute__((nonnull (1,2,3)));
static bool mul__582(float* __restrict__ __outs_0, float* __restrict__ __ins_0, float* __restrict__ __ins_1) noexcept __attribute__((nonnull (1,2,3)));
static bool mul__581(float* __restrict__ __outs_0, float* __restrict__ __ins_0, float* __restrict__ __ins_1) noexcept __attribute__((nonnull (1,2,3)));
static bool mul__576(float* __restrict__ __outs_0, float* __restrict__ __ins_0, float* __restrict__ __ins_1) noexcept __attribute__((nonnull (1,2,3)));
static bool mul__575(float* __restrict__ __outs_0, float* __restrict__ __ins_0, float* __restrict__ __ins_1) noexcept __attribute__((nonnull (1,2,3)));
static bool mul__570(float* __restrict__ __outs_0, float* __restrict__ __ins_0, float* __restrict__ __ins_1) noexcept __attribute__((nonnull (1,2,3)));
static bool mul__569(float* __restrict__ __outs_0, float* __restrict__ __ins_0, float* __restrict__ __ins_1) noexcept __attribute__((nonnull (1,2,3)));
static bool reorder__522(int8_t* __restrict__ __outs_0, int8_t* __restrict__ __ins_0) noexcept __attribute__((nonnull (1,2)));
static bool reorder__515(int8_t* __restrict__ __outs_0, int8_t* __restrict__ __ins_0) noexcept __attribute__((nonnull (1,2)));
static bool reorder__506(int8_t* __restrict__ __outs_0, int8_t* __restrict__ __ins_0) noexcept __attribute__((nonnull (1,2)));
static bool reorder__497(int8_t* __restrict__ __outs_0, int8_t* __restrict__ __ins_0) noexcept __attribute__((nonnull (1,2)));
static bool reorder__490(int8_t* __restrict__ __outs_0, int8_t* __restrict__ __ins_0) noexcept __attribute__((nonnull (1,2)));
static bool reorder__528(int8_t* __restrict__ __outs_0, int8_t* __restrict__ __ins_0) noexcept __attribute__((nonnull (1,2)));
static bool reorder__519(int8_t* __restrict__ __outs_0, int8_t* __restrict__ __ins_0) noexcept __attribute__((nonnull (1,2)));
static bool reorder__512(int8_t* __restrict__ __outs_0, int8_t* __restrict__ __ins_0) noexcept __attribute__((nonnull (1,2)));
static bool reorder__503(int8_t* __restrict__ __outs_0, int8_t* __restrict__ __ins_0) noexcept __attribute__((nonnull (1,2)));
static bool reorder__496(int8_t* __restrict__ __outs_0, int8_t* __restrict__ __ins_0) noexcept __attribute__((nonnull (1,2)));
static bool reorder__487(int8_t* __restrict__ __outs_0, int8_t* __restrict__ __ins_0) noexcept __attribute__((nonnull (1,2)));
static bool reorder__422(int8_t* __restrict__ __outs_0, int8_t* __restrict__ __ins_0) noexcept __attribute__((nonnull (1,2)));
static bool mul__612(float* __restrict__ __outs_0, float* __restrict__ __ins_0, float* __restrict__ __ins_1) noexcept __attribute__((nonnull (1,2,3)));
static bool mul__611(float* __restrict__ __outs_0, float* __restrict__ __ins_0, float* __restrict__ __ins_1) noexcept __attribute__((nonnull (1,2,3)));
static bool mul__610(float* __restrict__ __outs_0, float* __restrict__ __ins_0, float* __restrict__ __ins_1) noexcept __attribute__((nonnull (1,2,3)));
static bool mul__609(float* __restrict__ __outs_0, float* __restrict__ __ins_0, float* __restrict__ __ins_1) noexcept __attribute__((nonnull (1,2,3)));
static bool mul__606(float* __restrict__ __outs_0, float* __restrict__ __ins_0, float* __restrict__ __ins_1) noexcept __attribute__((nonnull (1,2,3)));
static bool mul__605(float* __restrict__ __outs_0, float* __restrict__ __ins_0, float* __restrict__ __ins_1) noexcept __attribute__((nonnull (1,2,3)));
static bool mul__604(float* __restrict__ __outs_0, float* __restrict__ __ins_0, float* __restrict__ __ins_1) noexcept __attribute__((nonnull (1,2,3)));
static bool mul__603(float* __restrict__ __outs_0, float* __restrict__ __ins_0, float* __restrict__ __ins_1) noexcept __attribute__((nonnull (1,2,3)));
static bool mul__600(float* __restrict__ __outs_0, float* __restrict__ __ins_0, float* __restrict__ __ins_1) noexcept __attribute__((nonnull (1,2,3)));
static bool mul__599(float* __restrict__ __outs_0, float* __restrict__ __ins_0, float* __restrict__ __ins_1) noexcept __attribute__((nonnull (1,2,3)));
static bool mul__598(float* __restrict__ __outs_0, float* __restrict__ __ins_0, float* __restrict__ __ins_1) noexcept __attribute__((nonnull (1,2,3)));
static bool mul__597(float* __restrict__ __outs_0, float* __restrict__ __ins_0, float* __restrict__ __ins_1) noexcept __attribute__((nonnull (1,2,3)));
static bool mul__594(float* __restrict__ __outs_0, float* __restrict__ __ins_0, float* __restrict__ __ins_1) noexcept __attribute__((nonnull (1,2,3)));
static bool mul__593(float* __restrict__ __outs_0, float* __restrict__ __ins_0, float* __restrict__ __ins_1) noexcept __attribute__((nonnull (1,2,3)));
static bool mul__592(float* __restrict__ __outs_0, float* __restrict__ __ins_0, float* __restrict__ __ins_1) noexcept __attribute__((nonnull (1,2,3)));
static bool mul__591(float* __restrict__ __outs_0, float* __restrict__ __ins_0, float* __restrict__ __ins_1) noexcept __attribute__((nonnull (1,2,3)));
static bool mul__586(float* __restrict__ __outs_0, float* __restrict__ __ins_0, float* __restrict__ __ins_1) noexcept __attribute__((nonnull (1,2,3)));
static bool mul__585(float* __restrict__ __outs_0, float* __restrict__ __ins_0, float* __restrict__ __ins_1) noexcept __attribute__((nonnull (1,2,3)));
static bool mul__584(float* __restrict__ __outs_0, float* __restrict__ __ins_0, float* __restrict__ __ins_1) noexcept __attribute__((nonnull (1,2,3)));
static bool mul__583(float* __restrict__ __outs_0, float* __restrict__ __ins_0, float* __restrict__ __ins_1) noexcept __attribute__((nonnull (1,2,3)));
static bool mul__580(float* __restrict__ __outs_0, float* __restrict__ __ins_0, float* __restrict__ __ins_1) noexcept __attribute__((nonnull (1,2,3)));
static bool mul__579(float* __restrict__ __outs_0, float* __restrict__ __ins_0, float* __restrict__ __ins_1) noexcept __attribute__((nonnull (1,2,3)));
static bool mul__578(float* __restrict__ __outs_0, float* __restrict__ __ins_0, float* __restrict__ __ins_1) noexcept __attribute__((nonnull (1,2,3)));
static bool mul__577(float* __restrict__ __outs_0, float* __restrict__ __ins_0, float* __restrict__ __ins_1) noexcept __attribute__((nonnull (1,2,3)));
static bool mul__574(float* __restrict__ __outs_0, float* __restrict__ __ins_0, float* __restrict__ __ins_1) noexcept __attribute__((nonnull (1,2,3)));
static bool mul__573(float* __restrict__ __outs_0, float* __restrict__ __ins_0, float* __restrict__ __ins_1) noexcept __attribute__((nonnull (1,2,3)));
static bool mul__572(float* __restrict__ __outs_0, float* __restrict__ __ins_0, float* __restrict__ __ins_1) noexcept __attribute__((nonnull (1,2,3)));
static bool mul__571(float* __restrict__ __outs_0, float* __restrict__ __ins_0, float* __restrict__ __ins_1) noexcept __attribute__((nonnull (1,2,3)));
static bool reorder__516(int8_t* __restrict__ __outs_0, int8_t* __restrict__ __ins_0) noexcept __attribute__((nonnull (1,2)));
static bool reorder__509(int8_t* __restrict__ __outs_0, int8_t* __restrict__ __ins_0) noexcept __attribute__((nonnull (1,2)));
static bool reorder__500(int8_t* __restrict__ __outs_0, int8_t* __restrict__ __ins_0) noexcept __attribute__((nonnull (1,2)));
static bool reorder__493(int8_t* __restrict__ __outs_0, int8_t* __restrict__ __ins_0) noexcept __attribute__((nonnull (1,2)));
static bool reorder__484(int8_t* __restrict__ __outs_0, int8_t* __restrict__ __ins_0) noexcept __attribute__((nonnull (1,2)));
static bool reorder__480(int8_t* __restrict__ __outs_0, int8_t* __restrict__ __ins_0) noexcept __attribute__((nonnull (1,2)));
static bool reorder__474(int8_t* __restrict__ __outs_0, int8_t* __restrict__ __ins_0) noexcept __attribute__((nonnull (1,2)));
static bool reorder__465(int8_t* __restrict__ __outs_0, int8_t* __restrict__ __ins_0) noexcept __attribute__((nonnull (1,2)));
static bool reorder__460(int8_t* __restrict__ __outs_0, int8_t* __restrict__ __ins_0) noexcept __attribute__((nonnull (1,2)));
static bool reorder__451(int8_t* __restrict__ __outs_0, int8_t* __restrict__ __ins_0) noexcept __attribute__((nonnull (1,2)));
static bool reorder__483(int8_t* __restrict__ __outs_0, int8_t* __restrict__ __ins_0) noexcept __attribute__((nonnull (1,2)));
static bool reorder__445(int8_t* __restrict__ __outs_0, int8_t* __restrict__ __ins_0) noexcept __attribute__((nonnull (1,2)));
static bool reorder__471(int8_t* __restrict__ __outs_0, int8_t* __restrict__ __ins_0) noexcept __attribute__((nonnull (1,2)));
static bool reorder__464(int8_t* __restrict__ __outs_0, int8_t* __restrict__ __ins_0) noexcept __attribute__((nonnull (1,2)));
static bool reorder__457(int8_t* __restrict__ __outs_0, int8_t* __restrict__ __ins_0) noexcept __attribute__((nonnull (1,2)));
static bool reorder__477(int8_t* __restrict__ __outs_0, int8_t* __restrict__ __ins_0) noexcept __attribute__((nonnull (1,2)));
static bool reorder__468(int8_t* __restrict__ __outs_0, int8_t* __restrict__ __ins_0) noexcept __attribute__((nonnull (1,2)));
static bool reorder__461(int8_t* __restrict__ __outs_0, int8_t* __restrict__ __ins_0) noexcept __attribute__((nonnull (1,2)));
static bool reorder__454(int8_t* __restrict__ __outs_0, int8_t* __restrict__ __ins_0) noexcept __attribute__((nonnull (1,2)));
static bool reorder__439(int8_t* __restrict__ __outs_0, int8_t* __restrict__ __ins_0) noexcept __attribute__((nonnull (1,2)));
static bool reorder__430(int8_t* __restrict__ __outs_0, int8_t* __restrict__ __ins_0) noexcept __attribute__((nonnull (1,2)));
static bool reorder__423(int8_t* __restrict__ __outs_0, int8_t* __restrict__ __ins_0) noexcept __attribute__((nonnull (1,2)));
static bool reorder__448(int8_t* __restrict__ __outs_0, int8_t* __restrict__ __ins_0) noexcept __attribute__((nonnull (1,2)));
static bool reorder__436(int8_t* __restrict__ __outs_0, int8_t* __restrict__ __ins_0) noexcept __attribute__((nonnull (1,2)));
static bool reorder__429(int8_t* __restrict__ __outs_0, int8_t* __restrict__ __ins_0) noexcept __attribute__((nonnull (1,2)));
static bool reorder__442(int8_t* __restrict__ __outs_0, int8_t* __restrict__ __ins_0) noexcept __attribute__((nonnull (1,2)));
static bool reorder__433(int8_t* __restrict__ __outs_0, int8_t* __restrict__ __ins_0) noexcept __attribute__((nonnull (1,2)));
static bool reorder__426(int8_t* __restrict__ __outs_0, int8_t* __restrict__ __ins_0) noexcept __attribute__((nonnull (1,2)));
static bool reorder__419(int8_t* __restrict__ __outs_0, int8_t* __restrict__ __ins_0) noexcept __attribute__((nonnull (1,2)));
static bool mul__656(float* __restrict__ __outs_0, float* __restrict__ __ins_0, float* __restrict__ __ins_1) noexcept __attribute__((nonnull (1,2,3)));
static bool mul__655(float* __restrict__ __outs_0, float* __restrict__ __ins_0, float* __restrict__ __ins_1) noexcept __attribute__((nonnull (1,2,3)));
static bool reorder__534(int8_t* __restrict__ __outs_0, int8_t* __restrict__ __ins_0) noexcept __attribute__((nonnull (1,2)));
static bool mul__243(float* __restrict__ __outs_0, float* __restrict__ __ins_0, float* __restrict__ __ins_1) noexcept __attribute__((nonnull (1,2,3)));
static bool cast__244(int8_t* __restrict__ __outs_0, float* __restrict__ __ins_0) noexcept __attribute__((nonnull (1,2)));
static bool mul__252(float* __restrict__ __outs_0, float* __restrict__ __ins_0, float* __restrict__ __ins_1) noexcept __attribute__((nonnull (1,2,3)));
static bool cast__253(int8_t* __restrict__ __outs_0, float* __restrict__ __ins_0) noexcept __attribute__((nonnull (1,2)));
static bool mul__261(float* __restrict__ __outs_0, float* __restrict__ __ins_0, float* __restrict__ __ins_1) noexcept __attribute__((nonnull (1,2,3)));
static bool cast__262(int8_t* __restrict__ __outs_0, float* __restrict__ __ins_0) noexcept __attribute__((nonnull (1,2)));
static bool mul__246(float* __restrict__ __outs_0, float* __restrict__ __ins_0, float* __restrict__ __ins_1) noexcept __attribute__((nonnull (1,2,3)));
static bool cast__247(int8_t* __restrict__ __outs_0, float* __restrict__ __ins_0) noexcept __attribute__((nonnull (1,2)));
static bool mul__255(float* __restrict__ __outs_0, float* __restrict__ __ins_0, float* __restrict__ __ins_1) noexcept __attribute__((nonnull (1,2,3)));
static bool cast__256(int8_t* __restrict__ __outs_0, float* __restrict__ __ins_0) noexcept __attribute__((nonnull (1,2)));
static bool mul__234(float* __restrict__ __outs_0, float* __restrict__ __ins_0, float* __restrict__ __ins_1) noexcept __attribute__((nonnull (1,2,3)));
static bool cast__235(int8_t* __restrict__ __outs_0, float* __restrict__ __ins_0) noexcept __attribute__((nonnull (1,2)));
static bool reorder__531(int8_t* __restrict__ __outs_0, int8_t* __restrict__ __ins_0) noexcept __attribute__((nonnull (1,2)));
static bool mul__654(float* __restrict__ __outs_0, float* __restrict__ __ins_0, float* __restrict__ __ins_1) noexcept __attribute__((nonnull (1,2,3)));
static bool mul__653(float* __restrict__ __outs_0, float* __restrict__ __ins_0, float* __restrict__ __ins_1) noexcept __attribute__((nonnull (1,2,3)));
static bool mul__240(float* __restrict__ __outs_0, float* __restrict__ __ins_0, float* __restrict__ __ins_1) noexcept __attribute__((nonnull (1,2,3)));
static bool cast__241(int8_t* __restrict__ __outs_0, float* __restrict__ __ins_0) noexcept __attribute__((nonnull (1,2)));
static bool reorder__537(int8_t* __restrict__ __outs_0, int8_t* __restrict__ __ins_0) noexcept __attribute__((nonnull (1,2)));
static bool mul__658(float* __restrict__ __outs_0, float* __restrict__ __ins_0, float* __restrict__ __ins_1) noexcept __attribute__((nonnull (1,2,3)));
static bool mul__657(float* __restrict__ __outs_0, float* __restrict__ __ins_0, float* __restrict__ __ins_1) noexcept __attribute__((nonnull (1,2,3)));
static bool mul__660(float* __restrict__ __outs_0, float* __restrict__ __ins_0, float* __restrict__ __ins_1) noexcept __attribute__((nonnull (1,2,3)));
static bool mul__659(float* __restrict__ __outs_0, float* __restrict__ __ins_0, float* __restrict__ __ins_1) noexcept __attribute__((nonnull (1,2,3)));
static bool reorder__540(int8_t* __restrict__ __outs_0, int8_t* __restrict__ __ins_0) noexcept __attribute__((nonnull (1,2)));
static bool mul__662(float* __restrict__ __outs_0, float* __restrict__ __ins_0, float* __restrict__ __ins_1) noexcept __attribute__((nonnull (1,2,3)));
static bool mul__661(float* __restrict__ __outs_0, float* __restrict__ __ins_0, float* __restrict__ __ins_1) noexcept __attribute__((nonnull (1,2,3)));
static bool reorder__543(int8_t* __restrict__ __outs_0, int8_t* __restrict__ __ins_0) noexcept __attribute__((nonnull (1,2)));
static bool mul__249(float* __restrict__ __outs_0, float* __restrict__ __ins_0, float* __restrict__ __ins_1) noexcept __attribute__((nonnull (1,2,3)));
static bool cast__250(int8_t* __restrict__ __outs_0, float* __restrict__ __ins_0) noexcept __attribute__((nonnull (1,2)));
static bool mul__258(float* __restrict__ __outs_0, float* __restrict__ __ins_0, float* __restrict__ __ins_1) noexcept __attribute__((nonnull (1,2,3)));
static bool cast__259(int8_t* __restrict__ __outs_0, float* __restrict__ __ins_0) noexcept __attribute__((nonnull (1,2)));
static bool mul__664(float* __restrict__ __outs_0, float* __restrict__ __ins_0, float* __restrict__ __ins_1) noexcept __attribute__((nonnull (1,2,3)));
static bool mul__663(float* __restrict__ __outs_0, float* __restrict__ __ins_0, float* __restrict__ __ins_1) noexcept __attribute__((nonnull (1,2,3)));
static bool reorder__546(int8_t* __restrict__ __outs_0, int8_t* __restrict__ __ins_0) noexcept __attribute__((nonnull (1,2)));
static bool mul__666(float* __restrict__ __outs_0, float* __restrict__ __ins_0, float* __restrict__ __ins_1) noexcept __attribute__((nonnull (1,2,3)));
static bool mul__665(float* __restrict__ __outs_0, float* __restrict__ __ins_0, float* __restrict__ __ins_1) noexcept __attribute__((nonnull (1,2,3)));
static bool reorder__549(int8_t* __restrict__ __outs_0, int8_t* __restrict__ __ins_0) noexcept __attribute__((nonnull (1,2)));
static bool mul__668(float* __restrict__ __outs_0, float* __restrict__ __ins_0, float* __restrict__ __ins_1) noexcept __attribute__((nonnull (1,2,3)));
static bool mul__667(float* __restrict__ __outs_0, float* __restrict__ __ins_0, float* __restrict__ __ins_1) noexcept __attribute__((nonnull (1,2,3)));
static bool reorder__552(int8_t* __restrict__ __outs_0, int8_t* __restrict__ __ins_0) noexcept __attribute__((nonnull (1,2)));
static bool mul__670(float* __restrict__ __outs_0, float* __restrict__ __ins_0, float* __restrict__ __ins_1) noexcept __attribute__((nonnull (1,2,3)));
static bool mul__669(float* __restrict__ __outs_0, float* __restrict__ __ins_0, float* __restrict__ __ins_1) noexcept __attribute__((nonnull (1,2,3)));
static bool reorder__555(int8_t* __restrict__ __outs_0, int8_t* __restrict__ __ins_0) noexcept __attribute__((nonnull (1,2)));
static bool mul__672(float* __restrict__ __outs_0, float* __restrict__ __ins_0, float* __restrict__ __ins_1) noexcept __attribute__((nonnull (1,2,3)));
static bool mul__671(float* __restrict__ __outs_0, float* __restrict__ __ins_0, float* __restrict__ __ins_1) noexcept __attribute__((nonnull (1,2,3)));
static bool reorder__558(int8_t* __restrict__ __outs_0, int8_t* __restrict__ __ins_0) noexcept __attribute__((nonnull (1,2)));
static void reorder__4810_closure_0(uint64_t fused_0_fuseiter_3791___fuseiter_3792_978, float* __restrict__ __ins_0, float* __restrict__ __outs_0) noexcept __attribute__((nonnull (2,3)));
static void reorder__4880_closure_1(uint64_t fused_0fused_0fused_0_fuseiter_3796___fuseiter_3797_979___fuseiter_3798_980___fuseiter_3799_981, float* __restrict__ __ins_0, float* __restrict__ __outs_0) noexcept __attribute__((nonnull (2,3)));
static void reorder__5040_closure_2(uint64_t fused_0fused_0fused_0_fuseiter_3801___fuseiter_3802_982___fuseiter_3803_983___fuseiter_3804_984, float* __restrict__ __ins_0, float* __restrict__ __outs_0) noexcept __attribute__((nonnull (2,3)));
static void reorder__5130_closure_3(uint64_t fused_0fused_0fused_0_fuseiter_3806___fuseiter_3807_985___fuseiter_3808_986___fuseiter_3809_987, float* __restrict__ __ins_0, float* __restrict__ __outs_0) noexcept __attribute__((nonnull (2,3)));
static void reorder__5200_closure_4(uint64_t fused_0fused_0fused_0_fuseiter_3811___fuseiter_3812_988___fuseiter_3813_989___fuseiter_3814_990, float* __restrict__ __ins_0, float* __restrict__ __outs_0) noexcept __attribute__((nonnull (2,3)));
static void reorder__5290_closure_5(uint64_t fused_0fused_0fused_0_fuseiter_3816___fuseiter_3817_991___fuseiter_3818_992___fuseiter_3819_993, float* __restrict__ __ins_0, float* __restrict__ __outs_0) noexcept __attribute__((nonnull (2,3)));
static void reorder__4200_closure_6(uint64_t fused_0fused_0fused_0_fuseiter_3821___fuseiter_3822_994___fuseiter_3823_995___fuseiter_3824_996, float* __restrict__ __ins_0, float* __restrict__ __outs_0) noexcept __attribute__((nonnull (2,3)));
static void reorder__4240_closure_7(uint64_t fused_0fused_0fused_0_fuseiter_3826___fuseiter_3827_997___fuseiter_3828_998___fuseiter_3829_999, float* __restrict__ __ins_0, float* __restrict__ __outs_0) noexcept __attribute__((nonnull (2,3)));
static void reorder__4270_closure_8(uint64_t fused_0fused_0fused_0_fuseiter_3831___fuseiter_3832_1000___fuseiter_3833_1001___fuseiter_3834_1002, float* __restrict__ __ins_0, float* __restrict__ __outs_0) noexcept __attribute__((nonnull (2,3)));
static void reorder__4310_closure_9(uint64_t fused_0fused_0fused_0_fuseiter_3836___fuseiter_3837_1003___fuseiter_3838_1004___fuseiter_3839_1005, float* __restrict__ __ins_0, float* __restrict__ __outs_0) noexcept __attribute__((nonnull (2,3)));
static void reorder__4340_closure_10(uint64_t fused_0fused_0fused_0_fuseiter_3841___fuseiter_3842_1006___fuseiter_3843_1007___fuseiter_3844_1008, float* __restrict__ __ins_0, float* __restrict__ __outs_0) noexcept __attribute__((nonnull (2,3)));
static void reorder__4370_closure_11(uint64_t fused_0fused_0fused_0_fuseiter_3846___fuseiter_3847_1009___fuseiter_3848_1010___fuseiter_3849_1011, float* __restrict__ __ins_0, float* __restrict__ __outs_0) noexcept __attribute__((nonnull (2,3)));
static void reorder__4400_closure_12(uint64_t fused_0fused_0fused_0_fuseiter_3851___fuseiter_3852_1012___fuseiter_3853_1013___fuseiter_3854_1014, float* __restrict__ __ins_0, float* __restrict__ __outs_0) noexcept __attribute__((nonnull (2,3)));
static void reorder__4430_closure_13(uint64_t fused_0fused_0fused_0_fuseiter_3856___fuseiter_3857_1015___fuseiter_3858_1016___fuseiter_3859_1017, float* __restrict__ __ins_0, float* __restrict__ __outs_0) noexcept __attribute__((nonnull (2,3)));
static void reorder__4460_closure_14(uint64_t fused_0fused_0fused_0_fuseiter_3861___fuseiter_3862_1018___fuseiter_3863_1019___fuseiter_3864_1020, float* __restrict__ __ins_0, float* __restrict__ __outs_0) noexcept __attribute__((nonnull (2,3)));
static void reorder__4490_closure_15(uint64_t fused_0fused_0fused_0_fuseiter_3866___fuseiter_3867_1021___fuseiter_3868_1022___fuseiter_3869_1023, float* __restrict__ __ins_0, float* __restrict__ __outs_0) noexcept __attribute__((nonnull (2,3)));
static void reorder__4520_closure_16(uint64_t fused_0fused_0fused_0_fuseiter_3871___fuseiter_3872_1024___fuseiter_3873_1025___fuseiter_3874_1026, float* __restrict__ __ins_0, float* __restrict__ __outs_0) noexcept __attribute__((nonnull (2,3)));
static void reorder__4550_closure_17(uint64_t fused_0fused_0fused_0_fuseiter_3876___fuseiter_3877_1027___fuseiter_3878_1028___fuseiter_3879_1029, float* __restrict__ __ins_0, float* __restrict__ __outs_0) noexcept __attribute__((nonnull (2,3)));
static void reorder__4580_closure_18(uint64_t fused_0fused_0fused_0_fuseiter_3881___fuseiter_3882_1030___fuseiter_3883_1031___fuseiter_3884_1032, float* __restrict__ __ins_0, float* __restrict__ __outs_0) noexcept __attribute__((nonnull (2,3)));
static void reorder__4620_closure_19(uint64_t fused_0fused_0fused_0_fuseiter_3886___fuseiter_3887_1033___fuseiter_3888_1034___fuseiter_3889_1035, float* __restrict__ __ins_0, float* __restrict__ __outs_0) noexcept __attribute__((nonnull (2,3)));
static void reorder__4660_closure_20(uint64_t fused_0fused_0fused_0_fuseiter_3891___fuseiter_3892_1036___fuseiter_3893_1037___fuseiter_3894_1038, float* __restrict__ __ins_0, float* __restrict__ __outs_0) noexcept __attribute__((nonnull (2,3)));
static void reorder__4690_closure_21(uint64_t fused_0fused_0fused_0_fuseiter_3896___fuseiter_3897_1039___fuseiter_3898_1040___fuseiter_3899_1041, float* __restrict__ __ins_0, float* __restrict__ __outs_0) noexcept __attribute__((nonnull (2,3)));
static void reorder__4720_closure_22(uint64_t fused_0fused_0fused_0_fuseiter_3901___fuseiter_3902_1042___fuseiter_3903_1043___fuseiter_3904_1044, float* __restrict__ __ins_0, float* __restrict__ __outs_0) noexcept __attribute__((nonnull (2,3)));
static void reorder__4750_closure_23(uint64_t fused_0fused_0fused_0_fuseiter_3906___fuseiter_3907_1045___fuseiter_3908_1046___fuseiter_3909_1047, float* __restrict__ __ins_0, float* __restrict__ __outs_0) noexcept __attribute__((nonnull (2,3)));
static void reorder__4780_closure_24(uint64_t fused_0fused_0fused_0_fuseiter_3911___fuseiter_3912_1048___fuseiter_3913_1049___fuseiter_3914_1050, float* __restrict__ __ins_0, float* __restrict__ __outs_0) noexcept __attribute__((nonnull (2,3)));
static void reorder__4850_closure_25(uint64_t fused_0fused_0fused_0_fuseiter_3916___fuseiter_3917_1051___fuseiter_3918_1052___fuseiter_3919_1053, float* __restrict__ __ins_0, float* __restrict__ __outs_0) noexcept __attribute__((nonnull (2,3)));
static void reorder__4910_closure_26(uint64_t fused_0fused_0fused_0_fuseiter_3921___fuseiter_3922_1054___fuseiter_3923_1055___fuseiter_3924_1056, float* __restrict__ __ins_0, float* __restrict__ __outs_0) noexcept __attribute__((nonnull (2,3)));
static void reorder__4940_closure_27(uint64_t fused_0fused_0fused_0_fuseiter_3926___fuseiter_3927_1057___fuseiter_3928_1058___fuseiter_3929_1059, float* __restrict__ __ins_0, float* __restrict__ __outs_0) noexcept __attribute__((nonnull (2,3)));
static void reorder__4980_closure_28(uint64_t fused_0fused_0fused_0_fuseiter_3931___fuseiter_3932_1060___fuseiter_3933_1061___fuseiter_3934_1062, float* __restrict__ __ins_0, float* __restrict__ __outs_0) noexcept __attribute__((nonnull (2,3)));
static void reorder__5010_closure_29(uint64_t fused_0fused_0fused_0_fuseiter_3936___fuseiter_3937_1063___fuseiter_3938_1064___fuseiter_3939_1065, float* __restrict__ __ins_0, float* __restrict__ __outs_0) noexcept __attribute__((nonnull (2,3)));
static void reorder__5070_closure_30(uint64_t fused_0fused_0fused_0_fuseiter_3941___fuseiter_3942_1066___fuseiter_3943_1067___fuseiter_3944_1068, float* __restrict__ __ins_0, float* __restrict__ __outs_0) noexcept __attribute__((nonnull (2,3)));
static void reorder__5100_closure_31(uint64_t fused_0fused_0fused_0_fuseiter_3946___fuseiter_3947_1069___fuseiter_3948_1070___fuseiter_3949_1071, float* __restrict__ __ins_0, float* __restrict__ __outs_0) noexcept __attribute__((nonnull (2,3)));
static void reorder__5170_closure_32(uint64_t fused_0fused_0fused_0_fuseiter_3951___fuseiter_3952_1072___fuseiter_3953_1073___fuseiter_3954_1074, float* __restrict__ __ins_0, float* __restrict__ __outs_0) noexcept __attribute__((nonnull (2,3)));
static void reorder__5230_closure_33(uint64_t fused_0fused_0fused_0_fuseiter_3956___fuseiter_3957_1075___fuseiter_3958_1076___fuseiter_3959_1077, float* __restrict__ __ins_0, float* __restrict__ __outs_0) noexcept __attribute__((nonnull (2,3)));
static void reorder__5260_closure_34(uint64_t fused_0fused_0fused_0_fuseiter_3961___fuseiter_3962_1078___fuseiter_3963_1079___fuseiter_3964_1080, float* __restrict__ __ins_0, float* __restrict__ __outs_0) noexcept __attribute__((nonnull (2,3)));
static void reorder__5320_closure_35(uint64_t fused_0_fuseiter_3966___fuseiter_3967_1081, float* __restrict__ __ins_0, float* __restrict__ __outs_0) noexcept __attribute__((nonnull (2,3)));
static void reorder__5350_closure_36(uint64_t fused_0fused_0fused_0_fuseiter_3971___fuseiter_3972_1082___fuseiter_3973_1083___fuseiter_3974_1084, float* __restrict__ __ins_0, float* __restrict__ __outs_0) noexcept __attribute__((nonnull (2,3)));
static void reorder__5380_closure_37(uint64_t fused_0fused_0fused_0_fuseiter_3976___fuseiter_3977_1085___fuseiter_3978_1086___fuseiter_3979_1087, float* __restrict__ __ins_0, float* __restrict__ __outs_0) noexcept __attribute__((nonnull (2,3)));
static void reorder__5410_closure_38(uint64_t fused_0_fuseiter_3981___fuseiter_3982_1088, float* __restrict__ __ins_0, float* __restrict__ __outs_0) noexcept __attribute__((nonnull (2,3)));
static void reorder__5440_closure_39(uint64_t fused_0fused_0fused_0_fuseiter_3986___fuseiter_3987_1089___fuseiter_3988_1090___fuseiter_3989_1091, float* __restrict__ __ins_0, float* __restrict__ __outs_0) noexcept __attribute__((nonnull (2,3)));
static void reorder__5470_closure_40(uint64_t fused_0_fuseiter_3991___fuseiter_3992_1092, float* __restrict__ __ins_0, float* __restrict__ __outs_0) noexcept __attribute__((nonnull (2,3)));
static void reorder__5500_closure_41(uint64_t fused_0_fuseiter_3996___fuseiter_3997_1093, float* __restrict__ __ins_0, float* __restrict__ __outs_0) noexcept __attribute__((nonnull (2,3)));
static void reorder__5530_closure_42(uint64_t fused_0fused_0fused_0_fuseiter_4001___fuseiter_4002_1094___fuseiter_4003_1095___fuseiter_4004_1096, float* __restrict__ __ins_0, float* __restrict__ __outs_0) noexcept __attribute__((nonnull (2,3)));
static void reorder__5560_closure_43(uint64_t fused_0fused_0fused_0_fuseiter_4006___fuseiter_4007_1097___fuseiter_4008_1098___fuseiter_4009_1099, float* __restrict__ __ins_0, float* __restrict__ __outs_0) noexcept __attribute__((nonnull (2,3)));
static void reorder__5590_closure_44(uint64_t fused_0fused_0fused_0_fuseiter_4011___fuseiter_4012_1100___fuseiter_4013_1101___fuseiter_4014_1102, float* __restrict__ __ins_0, float* __restrict__ __outs_0) noexcept __attribute__((nonnull (2,3)));
static void reorder__4250_closure_45(uint64_t fused_0fused_0fused_0_fuseiter_4016___fuseiter_4017_1103___fuseiter_4018_1104___fuseiter_4019_1105, float* __restrict__ __ins_0, float* __restrict__ __outs_0) noexcept __attribute__((nonnull (2,3)));
static void reorder__4320_closure_46(uint64_t fused_0fused_0fused_0_fuseiter_4021___fuseiter_4022_1106___fuseiter_4023_1107___fuseiter_4024_1108, float* __restrict__ __ins_0, float* __restrict__ __outs_0) noexcept __attribute__((nonnull (2,3)));
static void reorder__4380_closure_47(uint64_t fused_0fused_0fused_0_fuseiter_4026___fuseiter_4027_1109___fuseiter_4028_1110___fuseiter_4029_1111, float* __restrict__ __ins_0, float* __restrict__ __outs_0) noexcept __attribute__((nonnull (2,3)));
static void reorder__4410_closure_48(uint64_t fused_0fused_0fused_0_fuseiter_4031___fuseiter_4032_1112___fuseiter_4033_1113___fuseiter_4034_1114, float* __restrict__ __ins_0, float* __restrict__ __outs_0) noexcept __attribute__((nonnull (2,3)));
static void reorder__4500_closure_49(uint64_t fused_0fused_0fused_0_fuseiter_4036___fuseiter_4037_1115___fuseiter_4038_1116___fuseiter_4039_1117, float* __restrict__ __ins_0, float* __restrict__ __outs_0) noexcept __attribute__((nonnull (2,3)));
static void reorder__4530_closure_50(uint64_t fused_0fused_0fused_0_fuseiter_4041___fuseiter_4042_1118___fuseiter_4043_1119___fuseiter_4044_1120, float* __restrict__ __ins_0, float* __restrict__ __outs_0) noexcept __attribute__((nonnull (2,3)));
static void reorder__4590_closure_51(uint64_t fused_0fused_0fused_0_fuseiter_4046___fuseiter_4047_1121___fuseiter_4048_1122___fuseiter_4049_1123, float* __restrict__ __ins_0, float* __restrict__ __outs_0) noexcept __attribute__((nonnull (2,3)));
static void reorder__4670_closure_52(uint64_t fused_0fused_0fused_0_fuseiter_4051___fuseiter_4052_1124___fuseiter_4053_1125___fuseiter_4054_1126, float* __restrict__ __ins_0, float* __restrict__ __outs_0) noexcept __attribute__((nonnull (2,3)));
static void reorder__4730_closure_53(uint64_t fused_0fused_0fused_0_fuseiter_4056___fuseiter_4057_1127___fuseiter_4058_1128___fuseiter_4059_1129, float* __restrict__ __ins_0, float* __restrict__ __outs_0) noexcept __attribute__((nonnull (2,3)));
static void reorder__4760_closure_54(uint64_t fused_0fused_0fused_0_fuseiter_4061___fuseiter_4062_1130___fuseiter_4063_1131___fuseiter_4064_1132, float* __restrict__ __ins_0, float* __restrict__ __outs_0) noexcept __attribute__((nonnull (2,3)));
static void reorder__4210_closure_55(uint64_t fused_0fused_0fused_0_fuseiter_4066___fuseiter_4067_1133___fuseiter_4068_1134___fuseiter_4069_1135, float* __restrict__ __ins_0, float* __restrict__ __outs_0) noexcept __attribute__((nonnull (2,3)));
static void reorder__4280_closure_56(uint64_t fused_0fused_0fused_0_fuseiter_4071___fuseiter_4072_1136___fuseiter_4073_1137___fuseiter_4074_1138, float* __restrict__ __ins_0, float* __restrict__ __outs_0) noexcept __attribute__((nonnull (2,3)));
static void reorder__4350_closure_57(uint64_t fused_0fused_0fused_0_fuseiter_4076___fuseiter_4077_1139___fuseiter_4078_1140___fuseiter_4079_1141, float* __restrict__ __ins_0, float* __restrict__ __outs_0) noexcept __attribute__((nonnull (2,3)));
static void reorder__4440_closure_58(uint64_t fused_0fused_0fused_0_fuseiter_4081___fuseiter_4082_1142___fuseiter_4083_1143___fuseiter_4084_1144, float* __restrict__ __ins_0, float* __restrict__ __outs_0) noexcept __attribute__((nonnull (2,3)));
static void reorder__4860_closure_59(uint64_t fused_0fused_0fused_0_fuseiter_4086___fuseiter_4087_1145___fuseiter_4088_1146___fuseiter_4089_1147, float* __restrict__ __ins_0, float* __restrict__ __outs_0) noexcept __attribute__((nonnull (2,3)));
static void reorder__4920_closure_60(uint64_t fused_0fused_0fused_0_fuseiter_4091___fuseiter_4092_1148___fuseiter_4093_1149___fuseiter_4094_1150, float* __restrict__ __ins_0, float* __restrict__ __outs_0) noexcept __attribute__((nonnull (2,3)));
static void reorder__4950_closure_61(uint64_t fused_0fused_0fused_0_fuseiter_4096___fuseiter_4097_1151___fuseiter_4098_1152___fuseiter_4099_1153, float* __restrict__ __ins_0, float* __restrict__ __outs_0) noexcept __attribute__((nonnull (2,3)));
static void reorder__4990_closure_62(uint64_t fused_0fused_0fused_0_fuseiter_4101___fuseiter_4102_1154___fuseiter_4103_1155___fuseiter_4104_1156, float* __restrict__ __ins_0, float* __restrict__ __outs_0) noexcept __attribute__((nonnull (2,3)));
static void reorder__5020_closure_63(uint64_t fused_0fused_0fused_0_fuseiter_4106___fuseiter_4107_1157___fuseiter_4108_1158___fuseiter_4109_1159, float* __restrict__ __ins_0, float* __restrict__ __outs_0) noexcept __attribute__((nonnull (2,3)));
static void reorder__5080_closure_64(uint64_t fused_0fused_0fused_0_fuseiter_4111___fuseiter_4112_1160___fuseiter_4113_1161___fuseiter_4114_1162, float* __restrict__ __ins_0, float* __restrict__ __outs_0) noexcept __attribute__((nonnull (2,3)));
static void reorder__5110_closure_65(uint64_t fused_0fused_0fused_0_fuseiter_4116___fuseiter_4117_1163___fuseiter_4118_1164___fuseiter_4119_1165, float* __restrict__ __ins_0, float* __restrict__ __outs_0) noexcept __attribute__((nonnull (2,3)));
static void reorder__5180_closure_66(uint64_t fused_0fused_0fused_0_fuseiter_4121___fuseiter_4122_1166___fuseiter_4123_1167___fuseiter_4124_1168, float* __restrict__ __ins_0, float* __restrict__ __outs_0) noexcept __attribute__((nonnull (2,3)));
static void reorder__5240_closure_67(uint64_t fused_0fused_0fused_0_fuseiter_4126___fuseiter_4127_1169___fuseiter_4128_1170___fuseiter_4129_1171, float* __restrict__ __ins_0, float* __restrict__ __outs_0) noexcept __attribute__((nonnull (2,3)));
static void reorder__5270_closure_68(uint64_t fused_0fused_0fused_0_fuseiter_4131___fuseiter_4132_1172___fuseiter_4133_1173___fuseiter_4134_1174, float* __restrict__ __ins_0, float* __restrict__ __outs_0) noexcept __attribute__((nonnull (2,3)));
static void reorder__4470_closure_69(uint64_t fused_0fused_0fused_0_fuseiter_4136___fuseiter_4137_1175___fuseiter_4138_1176___fuseiter_4139_1177, float* __restrict__ __ins_0, float* __restrict__ __outs_0) noexcept __attribute__((nonnull (2,3)));
static void reorder__4560_closure_70(uint64_t fused_0fused_0fused_0_fuseiter_4141___fuseiter_4142_1178___fuseiter_4143_1179___fuseiter_4144_1180, float* __restrict__ __ins_0, float* __restrict__ __outs_0) noexcept __attribute__((nonnull (2,3)));
static void reorder__4630_closure_71(uint64_t fused_0fused_0fused_0_fuseiter_4146___fuseiter_4147_1181___fuseiter_4148_1182___fuseiter_4149_1183, float* __restrict__ __ins_0, float* __restrict__ __outs_0) noexcept __attribute__((nonnull (2,3)));
static void reorder__4700_closure_72(uint64_t fused_0fused_0fused_0_fuseiter_4151___fuseiter_4152_1184___fuseiter_4153_1185___fuseiter_4154_1186, float* __restrict__ __ins_0, float* __restrict__ __outs_0) noexcept __attribute__((nonnull (2,3)));
static void reorder__4790_closure_73(uint64_t fused_0fused_0fused_0_fuseiter_4156___fuseiter_4157_1187___fuseiter_4158_1188___fuseiter_4159_1189, float* __restrict__ __ins_0, float* __restrict__ __outs_0) noexcept __attribute__((nonnull (2,3)));
static void reorder__5360_closure_74(uint64_t fused_0fused_0fused_0_fuseiter_4161___fuseiter_4162_1190___fuseiter_4163_1191___fuseiter_4164_1192, float* __restrict__ __ins_0, float* __restrict__ __outs_0) noexcept __attribute__((nonnull (2,3)));
static void reorder__5390_closure_75(uint64_t fused_0fused_0fused_0_fuseiter_4166___fuseiter_4167_1193___fuseiter_4168_1194___fuseiter_4169_1195, float* __restrict__ __ins_0, float* __restrict__ __outs_0) noexcept __attribute__((nonnull (2,3)));
static void reorder__5450_closure_76(uint64_t fused_0fused_0fused_0_fuseiter_4171___fuseiter_4172_1196___fuseiter_4173_1197___fuseiter_4174_1198, float* __restrict__ __ins_0, float* __restrict__ __outs_0) noexcept __attribute__((nonnull (2,3)));
static void reorder__5480_closure_77(uint64_t fused_0_fuseiter_4176___fuseiter_4177_1199, float* __restrict__ __ins_0, float* __restrict__ __outs_0) noexcept __attribute__((nonnull (2,3)));
static void reorder__5540_closure_78(uint64_t fused_0fused_0fused_0_fuseiter_4181___fuseiter_4182_1200___fuseiter_4183_1201___fuseiter_4184_1202, float* __restrict__ __ins_0, float* __restrict__ __outs_0) noexcept __attribute__((nonnull (2,3)));
static void reorder__5570_closure_79(uint64_t fused_0fused_0fused_0_fuseiter_4186___fuseiter_4187_1203___fuseiter_4188_1204___fuseiter_4189_1205, float* __restrict__ __ins_0, float* __restrict__ __outs_0) noexcept __attribute__((nonnull (2,3)));
static void reorder__4820_closure_80(uint64_t fused_0_fuseiter_4191___fuseiter_4192_1206, float* __restrict__ __ins_0, float* __restrict__ __outs_0) noexcept __attribute__((nonnull (2,3)));
static void reorder__4890_closure_81(uint64_t fused_0fused_0fused_0_fuseiter_4196___fuseiter_4197_1207___fuseiter_4198_1208___fuseiter_4199_1209, float* __restrict__ __ins_0, float* __restrict__ __outs_0) noexcept __attribute__((nonnull (2,3)));
static void reorder__5050_closure_82(uint64_t fused_0fused_0fused_0_fuseiter_4201___fuseiter_4202_1210___fuseiter_4203_1211___fuseiter_4204_1212, float* __restrict__ __ins_0, float* __restrict__ __outs_0) noexcept __attribute__((nonnull (2,3)));
static void reorder__5140_closure_83(uint64_t fused_0fused_0fused_0_fuseiter_4206___fuseiter_4207_1213___fuseiter_4208_1214___fuseiter_4209_1215, float* __restrict__ __ins_0, float* __restrict__ __outs_0) noexcept __attribute__((nonnull (2,3)));
static void reorder__5210_closure_84(uint64_t fused_0fused_0fused_0_fuseiter_4211___fuseiter_4212_1216___fuseiter_4213_1217___fuseiter_4214_1218, float* __restrict__ __ins_0, float* __restrict__ __outs_0) noexcept __attribute__((nonnull (2,3)));
static void reorder__5300_closure_85(uint64_t fused_0fused_0fused_0_fuseiter_4216___fuseiter_4217_1219___fuseiter_4218_1220___fuseiter_4219_1221, float* __restrict__ __ins_0, float* __restrict__ __outs_0) noexcept __attribute__((nonnull (2,3)));
static void reorder__5330_closure_86(uint64_t fused_0_fuseiter_4221___fuseiter_4222_1222, float* __restrict__ __ins_0, float* __restrict__ __outs_0) noexcept __attribute__((nonnull (2,3)));
static void reorder__5420_closure_87(uint64_t fused_0_fuseiter_4226___fuseiter_4227_1223, float* __restrict__ __ins_0, float* __restrict__ __outs_0) noexcept __attribute__((nonnull (2,3)));
static void reorder__5510_closure_88(uint64_t fused_0_fuseiter_4231___fuseiter_4232_1224, float* __restrict__ __ins_0, float* __restrict__ __outs_0) noexcept __attribute__((nonnull (2,3)));
static void reorder__5600_closure_89(uint64_t fused_0fused_0fused_0_fuseiter_4236___fuseiter_4237_1225___fuseiter_4238_1226___fuseiter_4239_1227, float* __restrict__ __ins_0, float* __restrict__ __outs_0) noexcept __attribute__((nonnull (2,3)));
static void mul__1110_closure_90(uint64_t fused_0fused_0__itr_0____itr_1_1228____itr_2_1229, float* __restrict__ __ins_0, float* __restrict__ __ins_1, float* __restrict__ __outs_0) noexcept __attribute__((nonnull (2,3,4)));
static void cast__1120_closure_91(uint64_t fused_0fused_0__itr_0____itr_1_1230____itr_2_1231, float* __restrict__ __ins_0, int8_t* __restrict__ __outs_0) noexcept __attribute__((nonnull (2,3)));
static void mul__1080_closure_92(uint64_t fused_0fused_0__itr_0____itr_1_1232____itr_2_1233, float* __restrict__ __ins_0, float* __restrict__ __ins_1, float* __restrict__ __outs_0) noexcept __attribute__((nonnull (2,3,4)));
static void cast__1090_closure_93(uint64_t fused_0fused_0__itr_0____itr_1_1234____itr_2_1235, float* __restrict__ __ins_0, int8_t* __restrict__ __outs_0) noexcept __attribute__((nonnull (2,3)));
static void mul__1170_closure_94(uint64_t fused_0fused_0__itr_0____itr_1_1236____itr_2_1237, float* __restrict__ __ins_0, float* __restrict__ __ins_1, float* __restrict__ __outs_0) noexcept __attribute__((nonnull (2,3,4)));
static void cast__1180_closure_95(uint64_t fused_0fused_0__itr_0____itr_1_1238____itr_2_1239, float* __restrict__ __ins_0, int8_t* __restrict__ __outs_0) noexcept __attribute__((nonnull (2,3)));
static void mul__1260_closure_96(uint64_t fused_0fused_0__itr_0____itr_1_1240____itr_2_1241, float* __restrict__ __ins_0, float* __restrict__ __ins_1, float* __restrict__ __outs_0) noexcept __attribute__((nonnull (2,3,4)));
static void cast__1270_closure_97(uint64_t fused_0fused_0__itr_0____itr_1_1242____itr_2_1243, float* __restrict__ __ins_0, int8_t* __restrict__ __outs_0) noexcept __attribute__((nonnull (2,3)));
static void mul__1350_closure_98(uint64_t fused_0fused_0__itr_0____itr_1_1244____itr_2_1245, float* __restrict__ __ins_0, float* __restrict__ __ins_1, float* __restrict__ __outs_0) noexcept __attribute__((nonnull (2,3,4)));
static void cast__1360_closure_99(uint64_t fused_0fused_0__itr_0____itr_1_1246____itr_2_1247, float* __restrict__ __ins_0, int8_t* __restrict__ __outs_0) noexcept __attribute__((nonnull (2,3)));
static void mul__1200_closure_100(uint64_t fused_0fused_0__itr_0____itr_1_1248____itr_2_1249, float* __restrict__ __ins_0, float* __restrict__ __ins_1, float* __restrict__ __outs_0) noexcept __attribute__((nonnull (2,3,4)));
static void cast__1210_closure_101(uint64_t fused_0fused_0__itr_0____itr_1_1250____itr_2_1251, float* __restrict__ __ins_0, int8_t* __restrict__ __outs_0) noexcept __attribute__((nonnull (2,3)));
static void mul__1290_closure_102(uint64_t fused_0fused_0__itr_0____itr_1_1252____itr_2_1253, float* __restrict__ __ins_0, float* __restrict__ __ins_1, float* __restrict__ __outs_0) noexcept __attribute__((nonnull (2,3,4)));
static void cast__1300_closure_103(uint64_t fused_0fused_0__itr_0____itr_1_1254____itr_2_1255, float* __restrict__ __ins_0, int8_t* __restrict__ __outs_0) noexcept __attribute__((nonnull (2,3)));
static void mul__1410_closure_104(uint64_t fused_0fused_0__itr_0____itr_1_1256____itr_2_1257, float* __restrict__ __ins_0, float* __restrict__ __ins_1, float* __restrict__ __outs_0) noexcept __attribute__((nonnull (2,3,4)));
static void cast__1420_closure_105(uint64_t fused_0fused_0__itr_0____itr_1_1258____itr_2_1259, float* __restrict__ __ins_0, int8_t* __restrict__ __outs_0) noexcept __attribute__((nonnull (2,3)));
static void mul__1140_closure_106(uint64_t fused_0fused_0__itr_0____itr_1_1260____itr_2_1261, float* __restrict__ __ins_0, float* __restrict__ __ins_1, float* __restrict__ __outs_0) noexcept __attribute__((nonnull (2,3,4)));
static void cast__1150_closure_107(uint64_t fused_0fused_0__itr_0____itr_1_1262____itr_2_1263, float* __restrict__ __ins_0, int8_t* __restrict__ __outs_0) noexcept __attribute__((nonnull (2,3)));
static void mul__1230_closure_108(uint64_t fused_0fused_0__itr_0____itr_1_1264____itr_2_1265, float* __restrict__ __ins_0, float* __restrict__ __ins_1, float* __restrict__ __outs_0) noexcept __attribute__((nonnull (2,3,4)));
static void cast__1240_closure_109(uint64_t fused_0fused_0__itr_0____itr_1_1266____itr_2_1267, float* __restrict__ __ins_0, int8_t* __restrict__ __outs_0) noexcept __attribute__((nonnull (2,3)));
static void mul__1320_closure_110(uint64_t fused_0fused_0__itr_0____itr_1_1268____itr_2_1269, float* __restrict__ __ins_0, float* __restrict__ __ins_1, float* __restrict__ __outs_0) noexcept __attribute__((nonnull (2,3,4)));
static void cast__1330_closure_111(uint64_t fused_0fused_0__itr_0____itr_1_1270____itr_2_1271, float* __restrict__ __ins_0, int8_t* __restrict__ __outs_0) noexcept __attribute__((nonnull (2,3)));
static void mul__1470_closure_112(uint64_t fused_0fused_0__itr_0____itr_1_1272____itr_2_1273, float* __restrict__ __ins_0, float* __restrict__ __ins_1, float* __restrict__ __outs_0) noexcept __attribute__((nonnull (2,3,4)));
static void cast__1480_closure_113(uint64_t fused_0fused_0__itr_0____itr_1_1274____itr_2_1275, float* __restrict__ __ins_0, int8_t* __restrict__ __outs_0) noexcept __attribute__((nonnull (2,3)));
static void mul__1560_closure_114(uint64_t fused_0fused_0__itr_0____itr_1_1276____itr_2_1277, float* __restrict__ __ins_0, float* __restrict__ __ins_1, float* __restrict__ __outs_0) noexcept __attribute__((nonnull (2,3,4)));
static void cast__1570_closure_115(uint64_t fused_0fused_0__itr_0____itr_1_1278____itr_2_1279, float* __restrict__ __ins_0, int8_t* __restrict__ __outs_0) noexcept __attribute__((nonnull (2,3)));
static void mul__1650_closure_116(uint64_t fused_0fused_0__itr_0____itr_1_1280____itr_2_1281, float* __restrict__ __ins_0, float* __restrict__ __ins_1, float* __restrict__ __outs_0) noexcept __attribute__((nonnull (2,3,4)));
static void cast__1660_closure_117(uint64_t fused_0fused_0__itr_0____itr_1_1282____itr_2_1283, float* __restrict__ __ins_0, int8_t* __restrict__ __outs_0) noexcept __attribute__((nonnull (2,3)));
static void mul__1740_closure_118(uint64_t fused_0fused_0__itr_0____itr_1_1284____itr_2_1285, float* __restrict__ __ins_0, float* __restrict__ __ins_1, float* __restrict__ __outs_0) noexcept __attribute__((nonnull (2,3,4)));
static void cast__1750_closure_119(uint64_t fused_0fused_0__itr_0____itr_1_1286____itr_2_1287, float* __restrict__ __ins_0, int8_t* __restrict__ __outs_0) noexcept __attribute__((nonnull (2,3)));
static void mul__1500_closure_120(uint64_t fused_0fused_0__itr_0____itr_1_1288____itr_2_1289, float* __restrict__ __ins_0, float* __restrict__ __ins_1, float* __restrict__ __outs_0) noexcept __attribute__((nonnull (2,3,4)));
static void cast__1510_closure_121(uint64_t fused_0fused_0__itr_0____itr_1_1290____itr_2_1291, float* __restrict__ __ins_0, int8_t* __restrict__ __outs_0) noexcept __attribute__((nonnull (2,3)));
static void mul__1590_closure_122(uint64_t fused_0fused_0__itr_0____itr_1_1292____itr_2_1293, float* __restrict__ __ins_0, float* __restrict__ __ins_1, float* __restrict__ __outs_0) noexcept __attribute__((nonnull (2,3,4)));
static void cast__1600_closure_123(uint64_t fused_0fused_0__itr_0____itr_1_1294____itr_2_1295, float* __restrict__ __ins_0, int8_t* __restrict__ __outs_0) noexcept __attribute__((nonnull (2,3)));
static void mul__1680_closure_124(uint64_t fused_0fused_0__itr_0____itr_1_1296____itr_2_1297, float* __restrict__ __ins_0, float* __restrict__ __ins_1, float* __restrict__ __outs_0) noexcept __attribute__((nonnull (2,3,4)));
static void cast__1690_closure_125(uint64_t fused_0fused_0__itr_0____itr_1_1298____itr_2_1299, float* __restrict__ __ins_0, int8_t* __restrict__ __outs_0) noexcept __attribute__((nonnull (2,3)));
static void mul__1380_closure_126(uint64_t fused_0fused_0__itr_0____itr_1_1300____itr_2_1301, float* __restrict__ __ins_0, float* __restrict__ __ins_1, float* __restrict__ __outs_0) noexcept __attribute__((nonnull (2,3,4)));
static void cast__1390_closure_127(uint64_t fused_0fused_0__itr_0____itr_1_1302____itr_2_1303, float* __restrict__ __ins_0, int8_t* __restrict__ __outs_0) noexcept __attribute__((nonnull (2,3)));
static void mul__1800_closure_128(uint64_t fused_0fused_0__itr_0____itr_1_1304____itr_2_1305, float* __restrict__ __ins_0, float* __restrict__ __ins_1, float* __restrict__ __outs_0) noexcept __attribute__((nonnull (2,3,4)));
static void cast__1810_closure_129(uint64_t fused_0fused_0__itr_0____itr_1_1306____itr_2_1307, float* __restrict__ __ins_0, int8_t* __restrict__ __outs_0) noexcept __attribute__((nonnull (2,3)));
static void mul__1440_closure_130(uint64_t fused_0fused_0__itr_0____itr_1_1308____itr_2_1309, float* __restrict__ __ins_0, float* __restrict__ __ins_1, float* __restrict__ __outs_0) noexcept __attribute__((nonnull (2,3,4)));
static void cast__1450_closure_131(uint64_t fused_0fused_0__itr_0____itr_1_1310____itr_2_1311, float* __restrict__ __ins_0, int8_t* __restrict__ __outs_0) noexcept __attribute__((nonnull (2,3)));
static void mul__1530_closure_132(uint64_t fused_0fused_0__itr_0____itr_1_1312____itr_2_1313, float* __restrict__ __ins_0, float* __restrict__ __ins_1, float* __restrict__ __outs_0) noexcept __attribute__((nonnull (2,3,4)));
static void cast__1540_closure_133(uint64_t fused_0fused_0__itr_0____itr_1_1314____itr_2_1315, float* __restrict__ __ins_0, int8_t* __restrict__ __outs_0) noexcept __attribute__((nonnull (2,3)));
static void mul__1620_closure_134(uint64_t fused_0fused_0__itr_0____itr_1_1316____itr_2_1317, float* __restrict__ __ins_0, float* __restrict__ __ins_1, float* __restrict__ __outs_0) noexcept __attribute__((nonnull (2,3,4)));
static void cast__1630_closure_135(uint64_t fused_0fused_0__itr_0____itr_1_1318____itr_2_1319, float* __restrict__ __ins_0, int8_t* __restrict__ __outs_0) noexcept __attribute__((nonnull (2,3)));
static void mul__1710_closure_136(uint64_t fused_0fused_0__itr_0____itr_1_1320____itr_2_1321, float* __restrict__ __ins_0, float* __restrict__ __ins_1, float* __restrict__ __outs_0) noexcept __attribute__((nonnull (2,3,4)));
static void cast__1720_closure_137(uint64_t fused_0fused_0__itr_0____itr_1_1322____itr_2_1323, float* __restrict__ __ins_0, int8_t* __restrict__ __outs_0) noexcept __attribute__((nonnull (2,3)));
static void mul__1860_closure_138(uint64_t fused_0fused_0__itr_0____itr_1_1324____itr_2_1325, float* __restrict__ __ins_0, float* __restrict__ __ins_1, float* __restrict__ __outs_0) noexcept __attribute__((nonnull (2,3,4)));
static void cast__1870_closure_139(uint64_t fused_0fused_0__itr_0____itr_1_1326____itr_2_1327, float* __restrict__ __ins_0, int8_t* __restrict__ __outs_0) noexcept __attribute__((nonnull (2,3)));
static void mul__1950_closure_140(uint64_t fused_0fused_0__itr_0____itr_1_1328____itr_2_1329, float* __restrict__ __ins_0, float* __restrict__ __ins_1, float* __restrict__ __outs_0) noexcept __attribute__((nonnull (2,3,4)));
static void cast__1960_closure_141(uint64_t fused_0fused_0__itr_0____itr_1_1330____itr_2_1331, float* __restrict__ __ins_0, int8_t* __restrict__ __outs_0) noexcept __attribute__((nonnull (2,3)));
static void mul__2040_closure_142(uint64_t fused_0fused_0__itr_0____itr_1_1332____itr_2_1333, float* __restrict__ __ins_0, float* __restrict__ __ins_1, float* __restrict__ __outs_0) noexcept __attribute__((nonnull (2,3,4)));
static void cast__2050_closure_143(uint64_t fused_0fused_0__itr_0____itr_1_1334____itr_2_1335, float* __restrict__ __ins_0, int8_t* __restrict__ __outs_0) noexcept __attribute__((nonnull (2,3)));
static void mul__2130_closure_144(uint64_t fused_0fused_0__itr_0____itr_1_1336____itr_2_1337, float* __restrict__ __ins_0, float* __restrict__ __ins_1, float* __restrict__ __outs_0) noexcept __attribute__((nonnull (2,3,4)));
static void cast__2140_closure_145(uint64_t fused_0fused_0__itr_0____itr_1_1338____itr_2_1339, float* __restrict__ __ins_0, int8_t* __restrict__ __outs_0) noexcept __attribute__((nonnull (2,3)));
static void mul__2220_closure_146(uint64_t fused_0fused_0__itr_0____itr_1_1340____itr_2_1341, float* __restrict__ __ins_0, float* __restrict__ __ins_1, float* __restrict__ __outs_0) noexcept __attribute__((nonnull (2,3,4)));
static void cast__2230_closure_147(uint64_t fused_0fused_0__itr_0____itr_1_1342____itr_2_1343, float* __restrict__ __ins_0, int8_t* __restrict__ __outs_0) noexcept __attribute__((nonnull (2,3)));
static void mul__2310_closure_148(uint64_t fused_0fused_0__itr_0____itr_1_1344____itr_2_1345, float* __restrict__ __ins_0, float* __restrict__ __ins_1, float* __restrict__ __outs_0) noexcept __attribute__((nonnull (2,3,4)));
static void cast__2320_closure_149(uint64_t fused_0fused_0__itr_0____itr_1_1346____itr_2_1347, float* __restrict__ __ins_0, int8_t* __restrict__ __outs_0) noexcept __attribute__((nonnull (2,3)));
static void mul__1890_closure_150(uint64_t fused_0fused_0__itr_0____itr_1_1348____itr_2_1349, float* __restrict__ __ins_0, float* __restrict__ __ins_1, float* __restrict__ __outs_0) noexcept __attribute__((nonnull (2,3,4)));
static void cast__1900_closure_151(uint64_t fused_0fused_0__itr_0____itr_1_1350____itr_2_1351, float* __restrict__ __ins_0, int8_t* __restrict__ __outs_0) noexcept __attribute__((nonnull (2,3)));
static void mul__1980_closure_152(uint64_t fused_0fused_0__itr_0____itr_1_1352____itr_2_1353, float* __restrict__ __ins_0, float* __restrict__ __ins_1, float* __restrict__ __outs_0) noexcept __attribute__((nonnull (2,3,4)));
static void cast__1990_closure_153(uint64_t fused_0fused_0__itr_0____itr_1_1354____itr_2_1355, float* __restrict__ __ins_0, int8_t* __restrict__ __outs_0) noexcept __attribute__((nonnull (2,3)));
static void mul__2070_closure_154(uint64_t fused_0fused_0__itr_0____itr_1_1356____itr_2_1357, float* __restrict__ __ins_0, float* __restrict__ __ins_1, float* __restrict__ __outs_0) noexcept __attribute__((nonnull (2,3,4)));
static void cast__2080_closure_155(uint64_t fused_0fused_0__itr_0____itr_1_1358____itr_2_1359, float* __restrict__ __ins_0, int8_t* __restrict__ __outs_0) noexcept __attribute__((nonnull (2,3)));
static void mul__2160_closure_156(uint64_t fused_0fused_0__itr_0____itr_1_1360____itr_2_1361, float* __restrict__ __ins_0, float* __restrict__ __ins_1, float* __restrict__ __outs_0) noexcept __attribute__((nonnull (2,3,4)));
static void cast__2170_closure_157(uint64_t fused_0fused_0__itr_0____itr_1_1362____itr_2_1363, float* __restrict__ __ins_0, int8_t* __restrict__ __outs_0) noexcept __attribute__((nonnull (2,3)));
static void mul__2250_closure_158(uint64_t fused_0fused_0__itr_0____itr_1_1364____itr_2_1365, float* __restrict__ __ins_0, float* __restrict__ __ins_1, float* __restrict__ __outs_0) noexcept __attribute__((nonnull (2,3,4)));
static void cast__2260_closure_159(uint64_t fused_0fused_0__itr_0____itr_1_1366____itr_2_1367, float* __restrict__ __ins_0, int8_t* __restrict__ __outs_0) noexcept __attribute__((nonnull (2,3)));
static void mul__1770_closure_160(uint64_t fused_0fused_0__itr_0____itr_1_1368____itr_2_1369, float* __restrict__ __ins_0, float* __restrict__ __ins_1, float* __restrict__ __outs_0) noexcept __attribute__((nonnull (2,3,4)));
static void cast__1780_closure_161(uint64_t fused_0fused_0__itr_0____itr_1_1370____itr_2_1371, float* __restrict__ __ins_0, int8_t* __restrict__ __outs_0) noexcept __attribute__((nonnull (2,3)));
static void mul__2370_closure_162(uint64_t fused_0fused_0__itr_0____itr_1_1372____itr_2_1373, float* __restrict__ __ins_0, float* __restrict__ __ins_1, float* __restrict__ __outs_0) noexcept __attribute__((nonnull (2,3,4)));
static void cast__2380_closure_163(uint64_t fused_0fused_0__itr_0____itr_1_1374____itr_2_1375, float* __restrict__ __ins_0, int8_t* __restrict__ __outs_0) noexcept __attribute__((nonnull (2,3)));
static void mul__1830_closure_164(uint64_t fused_0fused_0__itr_0____itr_1_1376____itr_2_1377, float* __restrict__ __ins_0, float* __restrict__ __ins_1, float* __restrict__ __outs_0) noexcept __attribute__((nonnull (2,3,4)));
static void cast__1840_closure_165(uint64_t fused_0fused_0__itr_0____itr_1_1378____itr_2_1379, float* __restrict__ __ins_0, int8_t* __restrict__ __outs_0) noexcept __attribute__((nonnull (2,3)));
static void mul__1920_closure_166(uint64_t fused_0fused_0__itr_0____itr_1_1380____itr_2_1381, float* __restrict__ __ins_0, float* __restrict__ __ins_1, float* __restrict__ __outs_0) noexcept __attribute__((nonnull (2,3,4)));
static void cast__1930_closure_167(uint64_t fused_0fused_0__itr_0____itr_1_1382____itr_2_1383, float* __restrict__ __ins_0, int8_t* __restrict__ __outs_0) noexcept __attribute__((nonnull (2,3)));
static void mul__2010_closure_168(uint64_t fused_0fused_0__itr_0____itr_1_1384____itr_2_1385, float* __restrict__ __ins_0, float* __restrict__ __ins_1, float* __restrict__ __outs_0) noexcept __attribute__((nonnull (2,3,4)));
static void cast__2020_closure_169(uint64_t fused_0fused_0__itr_0____itr_1_1386____itr_2_1387, float* __restrict__ __ins_0, int8_t* __restrict__ __outs_0) noexcept __attribute__((nonnull (2,3)));
static void mul__2100_closure_170(uint64_t fused_0fused_0__itr_0____itr_1_1388____itr_2_1389, float* __restrict__ __ins_0, float* __restrict__ __ins_1, float* __restrict__ __outs_0) noexcept __attribute__((nonnull (2,3,4)));
static void cast__2110_closure_171(uint64_t fused_0fused_0__itr_0____itr_1_1390____itr_2_1391, float* __restrict__ __ins_0, int8_t* __restrict__ __outs_0) noexcept __attribute__((nonnull (2,3)));
static void mul__2190_closure_172(uint64_t fused_0fused_0__itr_0____itr_1_1392____itr_2_1393, float* __restrict__ __ins_0, float* __restrict__ __ins_1, float* __restrict__ __outs_0) noexcept __attribute__((nonnull (2,3,4)));
static void cast__2200_closure_173(uint64_t fused_0fused_0__itr_0____itr_1_1394____itr_2_1395, float* __restrict__ __ins_0, int8_t* __restrict__ __outs_0) noexcept __attribute__((nonnull (2,3)));
static void mul__2280_closure_174(uint64_t fused_0fused_0__itr_0____itr_1_1396____itr_2_1397, float* __restrict__ __ins_0, float* __restrict__ __ins_1, float* __restrict__ __outs_0) noexcept __attribute__((nonnull (2,3,4)));
static void cast__2290_closure_175(uint64_t fused_0fused_0__itr_0____itr_1_1398____itr_2_1399, float* __restrict__ __ins_0, int8_t* __restrict__ __outs_0) noexcept __attribute__((nonnull (2,3)));
static void reorder__5250_closure_176(uint64_t fused_0fused_0_fuseiter_4671___fuseiter_4672_1400___fuseiter_4673_1401, int8_t* __restrict__ __ins_0, int8_t* __restrict__ __outs_0) noexcept __attribute__((nonnull (2,3)));
static void mul__6520_closure_177(uint64_t fused_0fused_0fused_0__itr_0____itr_1_1402____itr_2_1403____itr_3_1404, float* __restrict__ __ins_0, float* __restrict__ __ins_1, float* __restrict__ __outs_0) noexcept __attribute__((nonnull (2,3,4)));
static void mul__6510_closure_178(uint64_t fused_0fused_0fused_0__itr_0____itr_1_1405____itr_2_1406____itr_3_1407, float* __restrict__ __ins_0, float* __restrict__ __ins_1, float* __restrict__ __outs_0) noexcept __attribute__((nonnull (2,3,4)));
static void mul__6460_closure_179(uint64_t fused_0fused_0fused_0__itr_0____itr_1_1408____itr_2_1409____itr_3_1410, float* __restrict__ __ins_0, float* __restrict__ __ins_1, float* __restrict__ __outs_0) noexcept __attribute__((nonnull (2,3,4)));
static void mul__6450_closure_180(uint64_t fused_0fused_0fused_0__itr_0____itr_1_1411____itr_2_1412____itr_3_1413, float* __restrict__ __ins_0, float* __restrict__ __ins_1, float* __restrict__ __outs_0) noexcept __attribute__((nonnull (2,3,4)));
static void mul__6400_closure_181(uint64_t fused_0fused_0fused_0__itr_0____itr_1_1414____itr_2_1415____itr_3_1416, float* __restrict__ __ins_0, float* __restrict__ __ins_1, float* __restrict__ __outs_0) noexcept __attribute__((nonnull (2,3,4)));
static void mul__6390_closure_182(uint64_t fused_0fused_0fused_0__itr_0____itr_1_1417____itr_2_1418____itr_3_1419, float* __restrict__ __ins_0, float* __restrict__ __ins_1, float* __restrict__ __outs_0) noexcept __attribute__((nonnull (2,3,4)));
static void mul__6340_closure_183(uint64_t fused_0fused_0fused_0__itr_0____itr_1_1420____itr_2_1421____itr_3_1422, float* __restrict__ __ins_0, float* __restrict__ __ins_1, float* __restrict__ __outs_0) noexcept __attribute__((nonnull (2,3,4)));
static void mul__6330_closure_184(uint64_t fused_0fused_0fused_0__itr_0____itr_1_1423____itr_2_1424____itr_3_1425, float* __restrict__ __ins_0, float* __restrict__ __ins_1, float* __restrict__ __outs_0) noexcept __attribute__((nonnull (2,3,4)));
static void mul__6220_closure_185(uint64_t fused_0fused_0fused_0__itr_0____itr_1_1432____itr_2_1433____itr_3_1434, float* __restrict__ __ins_0, float* __restrict__ __ins_1, float* __restrict__ __outs_0) noexcept __attribute__((nonnull (2,3,4)));
static void mul__6210_closure_186(uint64_t fused_0fused_0fused_0__itr_0____itr_1_1435____itr_2_1436____itr_3_1437, float* __restrict__ __ins_0, float* __restrict__ __ins_1, float* __restrict__ __outs_0) noexcept __attribute__((nonnull (2,3,4)));
static void mul__6160_closure_187(uint64_t fused_0fused_0fused_0__itr_0____itr_1_1438____itr_2_1439____itr_3_1440, float* __restrict__ __ins_0, float* __restrict__ __ins_1, float* __restrict__ __outs_0) noexcept __attribute__((nonnull (2,3,4)));
static void mul__6150_closure_188(uint64_t fused_0fused_0fused_0__itr_0____itr_1_1441____itr_2_1442____itr_3_1443, float* __restrict__ __ins_0, float* __restrict__ __ins_1, float* __restrict__ __outs_0) noexcept __attribute__((nonnull (2,3,4)));
static void mul__6140_closure_189(uint64_t fused_0fused_0fused_0__itr_0____itr_1_1444____itr_2_1445____itr_3_1446, float* __restrict__ __ins_0, float* __restrict__ __ins_1, float* __restrict__ __outs_0) noexcept __attribute__((nonnull (2,3,4)));
static void mul__6130_closure_190(uint64_t fused_0fused_0fused_0__itr_0____itr_1_1447____itr_2_1448____itr_3_1449, float* __restrict__ __ins_0, float* __restrict__ __ins_1, float* __restrict__ __outs_0) noexcept __attribute__((nonnull (2,3,4)));
static void mul__6080_closure_191(uint64_t fused_0fused_0fused_0__itr_0____itr_1_1450____itr_2_1451____itr_3_1452, float* __restrict__ __ins_0, float* __restrict__ __ins_1, float* __restrict__ __outs_0) noexcept __attribute__((nonnull (2,3,4)));
static void mul__6070_closure_192(uint64_t fused_0fused_0fused_0__itr_0____itr_1_1453____itr_2_1454____itr_3_1455, float* __restrict__ __ins_0, float* __restrict__ __ins_1, float* __restrict__ __outs_0) noexcept __attribute__((nonnull (2,3,4)));
static void mul__6020_closure_193(uint64_t fused_0fused_0fused_0__itr_0____itr_1_1456____itr_2_1457____itr_3_1458, float* __restrict__ __ins_0, float* __restrict__ __ins_1, float* __restrict__ __outs_0) noexcept __attribute__((nonnull (2,3,4)));
static void mul__6010_closure_194(uint64_t fused_0fused_0fused_0__itr_0____itr_1_1459____itr_2_1460____itr_3_1461, float* __restrict__ __ins_0, float* __restrict__ __ins_1, float* __restrict__ __outs_0) noexcept __attribute__((nonnull (2,3,4)));
static void mul__5960_closure_195(uint64_t fused_0fused_0fused_0__itr_0____itr_1_1462____itr_2_1463____itr_3_1464, float* __restrict__ __ins_0, float* __restrict__ __ins_1, float* __restrict__ __outs_0) noexcept __attribute__((nonnull (2,3,4)));
static void mul__5950_closure_196(uint64_t fused_0fused_0fused_0__itr_0____itr_1_1465____itr_2_1466____itr_3_1467, float* __restrict__ __ins_0, float* __restrict__ __ins_1, float* __restrict__ __outs_0) noexcept __attribute__((nonnull (2,3,4)));
static void mul__5900_closure_197(uint64_t fused_0fused_0fused_0__itr_0____itr_1_1468____itr_2_1469____itr_3_1470, float* __restrict__ __ins_0, float* __restrict__ __ins_1, float* __restrict__ __outs_0) noexcept __attribute__((nonnull (2,3,4)));
static void mul__5890_closure_198(uint64_t fused_0fused_0fused_0__itr_0____itr_1_1471____itr_2_1472____itr_3_1473, float* __restrict__ __ins_0, float* __restrict__ __ins_1, float* __restrict__ __outs_0) noexcept __attribute__((nonnull (2,3,4)));
static void mul__6500_closure_199(uint64_t fused_0fused_0fused_0__itr_0____itr_1_1474____itr_2_1475____itr_3_1476, float* __restrict__ __ins_0, float* __restrict__ __ins_1, float* __restrict__ __outs_0) noexcept __attribute__((nonnull (2,3,4)));
static void mul__6490_closure_200(uint64_t fused_0fused_0fused_0__itr_0____itr_1_1477____itr_2_1478____itr_3_1479, float* __restrict__ __ins_0, float* __restrict__ __ins_1, float* __restrict__ __outs_0) noexcept __attribute__((nonnull (2,3,4)));
static void mul__6480_closure_201(uint64_t fused_0fused_0fused_0__itr_0____itr_1_1480____itr_2_1481____itr_3_1482, float* __restrict__ __ins_0, float* __restrict__ __ins_1, float* __restrict__ __outs_0) noexcept __attribute__((nonnull (2,3,4)));
static void mul__6470_closure_202(uint64_t fused_0fused_0fused_0__itr_0____itr_1_1483____itr_2_1484____itr_3_1485, float* __restrict__ __ins_0, float* __restrict__ __ins_1, float* __restrict__ __outs_0) noexcept __attribute__((nonnull (2,3,4)));
static void mul__6440_closure_203(uint64_t fused_0fused_0fused_0__itr_0____itr_1_1486____itr_2_1487____itr_3_1488, float* __restrict__ __ins_0, float* __restrict__ __ins_1, float* __restrict__ __outs_0) noexcept __attribute__((nonnull (2,3,4)));
static void mul__6430_closure_204(uint64_t fused_0fused_0fused_0__itr_0____itr_1_1489____itr_2_1490____itr_3_1491, float* __restrict__ __ins_0, float* __restrict__ __ins_1, float* __restrict__ __outs_0) noexcept __attribute__((nonnull (2,3,4)));
static void mul__6380_closure_205(uint64_t fused_0fused_0fused_0__itr_0____itr_1_1498____itr_2_1499____itr_3_1500, float* __restrict__ __ins_0, float* __restrict__ __ins_1, float* __restrict__ __outs_0) noexcept __attribute__((nonnull (2,3,4)));
static void mul__6370_closure_206(uint64_t fused_0fused_0fused_0__itr_0____itr_1_1501____itr_2_1502____itr_3_1503, float* __restrict__ __ins_0, float* __restrict__ __ins_1, float* __restrict__ __outs_0) noexcept __attribute__((nonnull (2,3,4)));
static void mul__6360_closure_207(uint64_t fused_0fused_0fused_0__itr_0____itr_1_1504____itr_2_1505____itr_3_1506, float* __restrict__ __ins_0, float* __restrict__ __ins_1, float* __restrict__ __outs_0) noexcept __attribute__((nonnull (2,3,4)));
static void mul__6350_closure_208(uint64_t fused_0fused_0fused_0__itr_0____itr_1_1507____itr_2_1508____itr_3_1509, float* __restrict__ __ins_0, float* __restrict__ __ins_1, float* __restrict__ __outs_0) noexcept __attribute__((nonnull (2,3,4)));
static void mul__6320_closure_209(uint64_t fused_0fused_0fused_0__itr_0____itr_1_1510____itr_2_1511____itr_3_1512, float* __restrict__ __ins_0, float* __restrict__ __ins_1, float* __restrict__ __outs_0) noexcept __attribute__((nonnull (2,3,4)));
static void mul__6310_closure_210(uint64_t fused_0fused_0fused_0__itr_0____itr_1_1513____itr_2_1514____itr_3_1515, float* __restrict__ __ins_0, float* __restrict__ __ins_1, float* __restrict__ __outs_0) noexcept __attribute__((nonnull (2,3,4)));
static void mul__6300_closure_211(uint64_t fused_0fused_0fused_0__itr_0____itr_1_1516____itr_2_1517____itr_3_1518, float* __restrict__ __ins_0, float* __restrict__ __ins_1, float* __restrict__ __outs_0) noexcept __attribute__((nonnull (2,3,4)));
static void mul__6290_closure_212(uint64_t fused_0fused_0fused_0__itr_0____itr_1_1519____itr_2_1520____itr_3_1521, float* __restrict__ __ins_0, float* __restrict__ __ins_1, float* __restrict__ __outs_0) noexcept __attribute__((nonnull (2,3,4)));
static void mul__6260_closure_213(uint64_t fused_0fused_0fused_0__itr_0____itr_1_1522____itr_2_1523____itr_3_1524, float* __restrict__ __ins_0, float* __restrict__ __ins_1, float* __restrict__ __outs_0) noexcept __attribute__((nonnull (2,3,4)));
static void mul__6250_closure_214(uint64_t fused_0fused_0fused_0__itr_0____itr_1_1525____itr_2_1526____itr_3_1527, float* __restrict__ __ins_0, float* __restrict__ __ins_1, float* __restrict__ __outs_0) noexcept __attribute__((nonnull (2,3,4)));
static void mul__6240_closure_215(uint64_t fused_0fused_0fused_0__itr_0____itr_1_1528____itr_2_1529____itr_3_1530, float* __restrict__ __ins_0, float* __restrict__ __ins_1, float* __restrict__ __outs_0) noexcept __attribute__((nonnull (2,3,4)));
static void mul__6230_closure_216(uint64_t fused_0fused_0fused_0__itr_0____itr_1_1531____itr_2_1532____itr_3_1533, float* __restrict__ __ins_0, float* __restrict__ __ins_1, float* __restrict__ __outs_0) noexcept __attribute__((nonnull (2,3,4)));
static void mul__6200_closure_217(uint64_t fused_0fused_0fused_0__itr_0____itr_1_1534____itr_2_1535____itr_3_1536, float* __restrict__ __ins_0, float* __restrict__ __ins_1, float* __restrict__ __outs_0) noexcept __attribute__((nonnull (2,3,4)));
static void mul__6190_closure_218(uint64_t fused_0fused_0fused_0__itr_0____itr_1_1537____itr_2_1538____itr_3_1539, float* __restrict__ __ins_0, float* __restrict__ __ins_1, float* __restrict__ __outs_0) noexcept __attribute__((nonnull (2,3,4)));
static void mul__5880_closure_219(uint64_t fused_0fused_0fused_0__itr_0____itr_1_1546____itr_2_1547____itr_3_1548, float* __restrict__ __ins_0, float* __restrict__ __ins_1, float* __restrict__ __outs_0) noexcept __attribute__((nonnull (2,3,4)));
static void mul__5870_closure_220(uint64_t fused_0fused_0fused_0__itr_0____itr_1_1549____itr_2_1550____itr_3_1551, float* __restrict__ __ins_0, float* __restrict__ __ins_1, float* __restrict__ __outs_0) noexcept __attribute__((nonnull (2,3,4)));
static void mul__5820_closure_221(uint64_t fused_0fused_0fused_0__itr_0____itr_1_1552____itr_2_1553____itr_3_1554, float* __restrict__ __ins_0, float* __restrict__ __ins_1, float* __restrict__ __outs_0) noexcept __attribute__((nonnull (2,3,4)));
static void mul__5810_closure_222(uint64_t fused_0fused_0fused_0__itr_0____itr_1_1555____itr_2_1556____itr_3_1557, float* __restrict__ __ins_0, float* __restrict__ __ins_1, float* __restrict__ __outs_0) noexcept __attribute__((nonnull (2,3,4)));
static void mul__5760_closure_223(uint64_t fused_0fused_0fused_0__itr_0____itr_1_1558____itr_2_1559____itr_3_1560, float* __restrict__ __ins_0, float* __restrict__ __ins_1, float* __restrict__ __outs_0) noexcept __attribute__((nonnull (2,3,4)));
static void mul__5750_closure_224(uint64_t fused_0fused_0fused_0__itr_0____itr_1_1561____itr_2_1562____itr_3_1563, float* __restrict__ __ins_0, float* __restrict__ __ins_1, float* __restrict__ __outs_0) noexcept __attribute__((nonnull (2,3,4)));
static void mul__5700_closure_225(uint64_t fused_0fused_0fused_0__itr_0____itr_1_1564____itr_2_1565____itr_3_1566, float* __restrict__ __ins_0, float* __restrict__ __ins_1, float* __restrict__ __outs_0) noexcept __attribute__((nonnull (2,3,4)));
static void mul__5690_closure_226(uint64_t fused_0fused_0fused_0__itr_0____itr_1_1567____itr_2_1568____itr_3_1569, float* __restrict__ __ins_0, float* __restrict__ __ins_1, float* __restrict__ __outs_0) noexcept __attribute__((nonnull (2,3,4)));
static void reorder__5220_closure_227(uint64_t fused_0fused_0fused_0fused_0_fuseiter_5014___fuseiter_5015_1570___fuseiter_5016_1571___fuseiter_5017_1572___fuseiter_5018_1573, int8_t* __restrict__ __ins_0, int8_t* __restrict__ __outs_0) noexcept __attribute__((nonnull (2,3)));
static void reorder__5150_closure_228(uint64_t fused_0fused_0fused_0fused_0_fuseiter_5021___fuseiter_5022_1574___fuseiter_5023_1575___fuseiter_5024_1576___fuseiter_5025_1577, int8_t* __restrict__ __ins_0, int8_t* __restrict__ __outs_0) noexcept __attribute__((nonnull (2,3)));
static void reorder__5060_closure_229(uint64_t fused_0_fuseiter_5028___fuseiter_5029_1578, int8_t* __restrict__ __ins_0, int8_t* __restrict__ __outs_0) noexcept __attribute__((nonnull (2,3)));
static void reorder__4970_closure_230(uint64_t fused_0_fuseiter_5035___fuseiter_5036_1579, int8_t* __restrict__ __ins_0, int8_t* __restrict__ __outs_0) noexcept __attribute__((nonnull (2,3)));
static void reorder__4900_closure_231(uint64_t fused_0fused_0fused_0fused_0_fuseiter_5042___fuseiter_5043_1580___fuseiter_5044_1581___fuseiter_5045_1582___fuseiter_5046_1583, int8_t* __restrict__ __ins_0, int8_t* __restrict__ __outs_0) noexcept __attribute__((nonnull (2,3)));
static void reorder__5280_closure_232(uint64_t fused_0fused_0fused_0fused_0_fuseiter_5049___fuseiter_5050_1584___fuseiter_5051_1585___fuseiter_5052_1586___fuseiter_5053_1587, int8_t* __restrict__ __ins_0, int8_t* __restrict__ __outs_0) noexcept __attribute__((nonnull (2,3)));
static void reorder__5190_closure_233(uint64_t fused_0fused_0fused_0fused_0_fuseiter_5056___fuseiter_5057_1588___fuseiter_5058_1589___fuseiter_5059_1590___fuseiter_5060_1591, int8_t* __restrict__ __ins_0, int8_t* __restrict__ __outs_0) noexcept __attribute__((nonnull (2,3)));
static void reorder__5120_closure_234(uint64_t fused_0fused_0fused_0fused_0_fuseiter_5063___fuseiter_5064_1592___fuseiter_5065_1593___fuseiter_5066_1594___fuseiter_5067_1595, int8_t* __restrict__ __ins_0, int8_t* __restrict__ __outs_0) noexcept __attribute__((nonnull (2,3)));
static void reorder__5030_closure_235(uint64_t fused_0fused_0fused_0fused_0_fuseiter_5070___fuseiter_5071_1596___fuseiter_5072_1597___fuseiter_5073_1598___fuseiter_5074_1599, int8_t* __restrict__ __ins_0, int8_t* __restrict__ __outs_0) noexcept __attribute__((nonnull (2,3)));
static void reorder__4960_closure_236(uint64_t fused_0fused_0fused_0fused_0_fuseiter_5077___fuseiter_5078_1600___fuseiter_5079_1601___fuseiter_5080_1602___fuseiter_5081_1603, int8_t* __restrict__ __ins_0, int8_t* __restrict__ __outs_0) noexcept __attribute__((nonnull (2,3)));
static void reorder__4870_closure_237(uint64_t fused_0fused_0fused_0fused_0_fuseiter_5084___fuseiter_5085_1604___fuseiter_5086_1605___fuseiter_5087_1606___fuseiter_5088_1607, int8_t* __restrict__ __ins_0, int8_t* __restrict__ __outs_0) noexcept __attribute__((nonnull (2,3)));
static void reorder__4220_closure_238(uint64_t fused_0fused_0fused_0fused_0fused_0_fuseiter_5091___fuseiter_5092_1608___fuseiter_5093_1609___fuseiter_5094_1610___fuseiter_5095_1611___fuseiter_5096_1612, int8_t* __restrict__ __ins_0, int8_t* __restrict__ __outs_0) noexcept __attribute__((nonnull (2,3)));
static void mul__6120_closure_239(uint64_t fused_0fused_0fused_0__itr_0____itr_1_1613____itr_2_1614____itr_3_1615, float* __restrict__ __ins_0, float* __restrict__ __ins_1, float* __restrict__ __outs_0) noexcept __attribute__((nonnull (2,3,4)));
static void mul__6110_closure_240(uint64_t fused_0fused_0fused_0__itr_0____itr_1_1616____itr_2_1617____itr_3_1618, float* __restrict__ __ins_0, float* __restrict__ __ins_1, float* __restrict__ __outs_0) noexcept __attribute__((nonnull (2,3,4)));
static void mul__6100_closure_241(uint64_t fused_0fused_0fused_0__itr_0____itr_1_1619____itr_2_1620____itr_3_1621, float* __restrict__ __ins_0, float* __restrict__ __ins_1, float* __restrict__ __outs_0) noexcept __attribute__((nonnull (2,3,4)));
static void mul__6090_closure_242(uint64_t fused_0fused_0fused_0__itr_0____itr_1_1622____itr_2_1623____itr_3_1624, float* __restrict__ __ins_0, float* __restrict__ __ins_1, float* __restrict__ __outs_0) noexcept __attribute__((nonnull (2,3,4)));
static void mul__6060_closure_243(uint64_t fused_0fused_0fused_0__itr_0____itr_1_1625____itr_2_1626____itr_3_1627, float* __restrict__ __ins_0, float* __restrict__ __ins_1, float* __restrict__ __outs_0) noexcept __attribute__((nonnull (2,3,4)));
static void mul__6050_closure_244(uint64_t fused_0fused_0fused_0__itr_0____itr_1_1628____itr_2_1629____itr_3_1630, float* __restrict__ __ins_0, float* __restrict__ __ins_1, float* __restrict__ __outs_0) noexcept __attribute__((nonnull (2,3,4)));
static void mul__5980_closure_245(uint64_t fused_0fused_0fused_0__itr_0____itr_1_1643____itr_2_1644____itr_3_1645, float* __restrict__ __ins_0, float* __restrict__ __ins_1, float* __restrict__ __outs_0) noexcept __attribute__((nonnull (2,3,4)));
static void mul__5970_closure_246(uint64_t fused_0fused_0fused_0__itr_0____itr_1_1646____itr_2_1647____itr_3_1648, float* __restrict__ __ins_0, float* __restrict__ __ins_1, float* __restrict__ __outs_0) noexcept __attribute__((nonnull (2,3,4)));
static void mul__5940_closure_247(uint64_t fused_0fused_0fused_0__itr_0____itr_1_1649____itr_2_1650____itr_3_1651, float* __restrict__ __ins_0, float* __restrict__ __ins_1, float* __restrict__ __outs_0) noexcept __attribute__((nonnull (2,3,4)));
static void mul__5930_closure_248(uint64_t fused_0fused_0fused_0__itr_0____itr_1_1652____itr_2_1653____itr_3_1654, float* __restrict__ __ins_0, float* __restrict__ __ins_1, float* __restrict__ __outs_0) noexcept __attribute__((nonnull (2,3,4)));
static void mul__5920_closure_249(uint64_t fused_0fused_0fused_0__itr_0____itr_1_1655____itr_2_1656____itr_3_1657, float* __restrict__ __ins_0, float* __restrict__ __ins_1, float* __restrict__ __outs_0) noexcept __attribute__((nonnull (2,3,4)));
static void mul__5910_closure_250(uint64_t fused_0fused_0fused_0__itr_0____itr_1_1658____itr_2_1659____itr_3_1660, float* __restrict__ __ins_0, float* __restrict__ __ins_1, float* __restrict__ __outs_0) noexcept __attribute__((nonnull (2,3,4)));
static void mul__5860_closure_251(uint64_t fused_0fused_0fused_0__itr_0____itr_1_1661____itr_2_1662____itr_3_1663, float* __restrict__ __ins_0, float* __restrict__ __ins_1, float* __restrict__ __outs_0) noexcept __attribute__((nonnull (2,3,4)));
static void mul__5850_closure_252(uint64_t fused_0fused_0fused_0__itr_0____itr_1_1664____itr_2_1665____itr_3_1666, float* __restrict__ __ins_0, float* __restrict__ __ins_1, float* __restrict__ __outs_0) noexcept __attribute__((nonnull (2,3,4)));
static void mul__5840_closure_253(uint64_t fused_0fused_0fused_0__itr_0____itr_1_1667____itr_2_1668____itr_3_1669, float* __restrict__ __ins_0, float* __restrict__ __ins_1, float* __restrict__ __outs_0) noexcept __attribute__((nonnull (2,3,4)));
static void mul__5830_closure_254(uint64_t fused_0fused_0fused_0__itr_0____itr_1_1670____itr_2_1671____itr_3_1672, float* __restrict__ __ins_0, float* __restrict__ __ins_1, float* __restrict__ __outs_0) noexcept __attribute__((nonnull (2,3,4)));
static void mul__5800_closure_255(uint64_t fused_0fused_0fused_0__itr_0____itr_1_1673____itr_2_1674____itr_3_1675, float* __restrict__ __ins_0, float* __restrict__ __ins_1, float* __restrict__ __outs_0) noexcept __attribute__((nonnull (2,3,4)));
static void mul__5790_closure_256(uint64_t fused_0fused_0fused_0__itr_0____itr_1_1676____itr_2_1677____itr_3_1678, float* __restrict__ __ins_0, float* __restrict__ __ins_1, float* __restrict__ __outs_0) noexcept __attribute__((nonnull (2,3,4)));
static void mul__5740_closure_257(uint64_t fused_0fused_0fused_0__itr_0____itr_1_1685____itr_2_1686____itr_3_1687, float* __restrict__ __ins_0, float* __restrict__ __ins_1, float* __restrict__ __outs_0) noexcept __attribute__((nonnull (2,3,4)));
static void mul__5730_closure_258(uint64_t fused_0fused_0fused_0__itr_0____itr_1_1688____itr_2_1689____itr_3_1690, float* __restrict__ __ins_0, float* __restrict__ __ins_1, float* __restrict__ __outs_0) noexcept __attribute__((nonnull (2,3,4)));
static void reorder__5160_closure_259(uint64_t fused_0fused_0_fuseiter_5266___fuseiter_5267_1697___fuseiter_5268_1698, int8_t* __restrict__ __ins_0, int8_t* __restrict__ __outs_0) noexcept __attribute__((nonnull (2,3)));
static void reorder__5090_closure_260(uint64_t fused_0fused_0_fuseiter_5273___fuseiter_5274_1699___fuseiter_5275_1700, int8_t* __restrict__ __ins_0, int8_t* __restrict__ __outs_0) noexcept __attribute__((nonnull (2,3)));
static void reorder__5000_closure_261(uint64_t fused_0fused_0_fuseiter_5280___fuseiter_5281_1701___fuseiter_5282_1702, int8_t* __restrict__ __ins_0, int8_t* __restrict__ __outs_0) noexcept __attribute__((nonnull (2,3)));
static void reorder__4930_closure_262(uint64_t fused_0fused_0_fuseiter_5287___fuseiter_5288_1703___fuseiter_5289_1704, int8_t* __restrict__ __ins_0, int8_t* __restrict__ __outs_0) noexcept __attribute__((nonnull (2,3)));
static void reorder__4840_closure_263(uint64_t fused_0fused_0fused_0fused_0_fuseiter_5294___fuseiter_5295_1705___fuseiter_5296_1706___fuseiter_5297_1707___fuseiter_5298_1708, int8_t* __restrict__ __ins_0, int8_t* __restrict__ __outs_0) noexcept __attribute__((nonnull (2,3)));
static void reorder__4800_closure_264(uint64_t _fuseiter_5301, int8_t* __restrict__ __ins_0, int8_t* __restrict__ __outs_0) noexcept __attribute__((nonnull (2,3)));
static void reorder__4740_closure_265(uint64_t fused_0fused_0fused_0_fuseiter_5308___fuseiter_5309_1709___fuseiter_5310_1710___fuseiter_5311_1711, int8_t* __restrict__ __ins_0, int8_t* __restrict__ __outs_0) noexcept __attribute__((nonnull (2,3)));
static void reorder__4650_closure_266(uint64_t fused_0fused_0_fuseiter_5315___fuseiter_5316_1712___fuseiter_5317_1713, int8_t* __restrict__ __ins_0, int8_t* __restrict__ __outs_0) noexcept __attribute__((nonnull (2,3)));
static void reorder__4600_closure_267(uint64_t fused_0fused_0fused_0fused_0_fuseiter_5322___fuseiter_5323_1714___fuseiter_5324_1715___fuseiter_5325_1716___fuseiter_5326_1717, int8_t* __restrict__ __ins_0, int8_t* __restrict__ __outs_0) noexcept __attribute__((nonnull (2,3)));
static void reorder__4510_closure_268(uint64_t fused_0fused_0fused_0_fuseiter_5329___fuseiter_5330_1718___fuseiter_5331_1719___fuseiter_5332_1720, int8_t* __restrict__ __ins_0, int8_t* __restrict__ __outs_0) noexcept __attribute__((nonnull (2,3)));
static void reorder__4830_closure_269(uint64_t fused_0fused_0fused_0fused_0_fuseiter_5336___fuseiter_5337_1721___fuseiter_5338_1722___fuseiter_5339_1723___fuseiter_5340_1724, int8_t* __restrict__ __ins_0, int8_t* __restrict__ __outs_0) noexcept __attribute__((nonnull (2,3)));
static void reorder__4450_closure_270(uint64_t fused_0_fuseiter_5343___fuseiter_5344_1725, int8_t* __restrict__ __ins_0, int8_t* __restrict__ __outs_0) noexcept __attribute__((nonnull (2,3)));
static void reorder__4710_closure_271(uint64_t fused_0fused_0fused_0fused_0_fuseiter_5350___fuseiter_5351_1726___fuseiter_5352_1727___fuseiter_5353_1728___fuseiter_5354_1729, int8_t* __restrict__ __ins_0, int8_t* __restrict__ __outs_0) noexcept __attribute__((nonnull (2,3)));
static void reorder__4640_closure_272(uint64_t fused_0fused_0fused_0fused_0_fuseiter_5357___fuseiter_5358_1730___fuseiter_5359_1731___fuseiter_5360_1732___fuseiter_5361_1733, int8_t* __restrict__ __ins_0, int8_t* __restrict__ __outs_0) noexcept __attribute__((nonnull (2,3)));
static void reorder__4570_closure_273(uint64_t fused_0fused_0fused_0fused_0_fuseiter_5364___fuseiter_5365_1734___fuseiter_5366_1735___fuseiter_5367_1736___fuseiter_5368_1737, int8_t* __restrict__ __ins_0, int8_t* __restrict__ __outs_0) noexcept __attribute__((nonnull (2,3)));
static void reorder__4770_closure_274(uint64_t fused_0fused_0fused_0fused_0_fuseiter_5371___fuseiter_5372_1738___fuseiter_5373_1739___fuseiter_5374_1740___fuseiter_5375_1741, int8_t* __restrict__ __ins_0, int8_t* __restrict__ __outs_0) noexcept __attribute__((nonnull (2,3)));
static void reorder__4680_closure_275(uint64_t fused_0fused_0fused_0fused_0_fuseiter_5378___fuseiter_5379_1742___fuseiter_5380_1743___fuseiter_5381_1744___fuseiter_5382_1745, int8_t* __restrict__ __ins_0, int8_t* __restrict__ __outs_0) noexcept __attribute__((nonnull (2,3)));
static void reorder__4610_closure_276(uint64_t fused_0fused_0fused_0fused_0_fuseiter_5385___fuseiter_5386_1746___fuseiter_5387_1747___fuseiter_5388_1748___fuseiter_5389_1749, int8_t* __restrict__ __ins_0, int8_t* __restrict__ __outs_0) noexcept __attribute__((nonnull (2,3)));
static void reorder__4540_closure_277(uint64_t fused_0_fuseiter_5392___fuseiter_5393_1750, int8_t* __restrict__ __ins_0, int8_t* __restrict__ __outs_0) noexcept __attribute__((nonnull (2,3)));
static void reorder__4390_closure_278(uint64_t fused_0fused_0fused_0fused_0_fuseiter_5399___fuseiter_5400_1751___fuseiter_5401_1752___fuseiter_5402_1753___fuseiter_5403_1754, int8_t* __restrict__ __ins_0, int8_t* __restrict__ __outs_0) noexcept __attribute__((nonnull (2,3)));
static void reorder__4300_closure_279(uint64_t fused_0fused_0fused_0_fuseiter_5406___fuseiter_5407_1755___fuseiter_5408_1756___fuseiter_5409_1757, int8_t* __restrict__ __ins_0, int8_t* __restrict__ __outs_0) noexcept __attribute__((nonnull (2,3)));
static void reorder__4230_closure_280(uint64_t fused_0fused_0fused_0fused_0_fuseiter_5413___fuseiter_5414_1758___fuseiter_5415_1759___fuseiter_5416_1760___fuseiter_5417_1761, int8_t* __restrict__ __ins_0, int8_t* __restrict__ __outs_0) noexcept __attribute__((nonnull (2,3)));
static void reorder__4480_closure_281(uint64_t fused_0fused_0fused_0fused_0_fuseiter_5420___fuseiter_5421_1762___fuseiter_5422_1763___fuseiter_5423_1764___fuseiter_5424_1765, int8_t* __restrict__ __ins_0, int8_t* __restrict__ __outs_0) noexcept __attribute__((nonnull (2,3)));
static void reorder__4360_closure_282(uint64_t fused_0fused_0fused_0fused_0_fuseiter_5427___fuseiter_5428_1766___fuseiter_5429_1767___fuseiter_5430_1768___fuseiter_5431_1769, int8_t* __restrict__ __ins_0, int8_t* __restrict__ __outs_0) noexcept __attribute__((nonnull (2,3)));
static void reorder__4290_closure_283(uint64_t fused_0fused_0fused_0fused_0_fuseiter_5434___fuseiter_5435_1770___fuseiter_5436_1771___fuseiter_5437_1772___fuseiter_5438_1773, int8_t* __restrict__ __ins_0, int8_t* __restrict__ __outs_0) noexcept __attribute__((nonnull (2,3)));
static void reorder__4420_closure_284(uint64_t fused_0fused_0fused_0fused_0_fuseiter_5441___fuseiter_5442_1774___fuseiter_5443_1775___fuseiter_5444_1776___fuseiter_5445_1777, int8_t* __restrict__ __ins_0, int8_t* __restrict__ __outs_0) noexcept __attribute__((nonnull (2,3)));
static void reorder__4330_closure_285(uint64_t fused_0fused_0fused_0fused_0_fuseiter_5448___fuseiter_5449_1778___fuseiter_5450_1779___fuseiter_5451_1780___fuseiter_5452_1781, int8_t* __restrict__ __ins_0, int8_t* __restrict__ __outs_0) noexcept __attribute__((nonnull (2,3)));
static void reorder__4260_closure_286(uint64_t fused_0fused_0fused_0fused_0_fuseiter_5455___fuseiter_5456_1782___fuseiter_5457_1783___fuseiter_5458_1784___fuseiter_5459_1785, int8_t* __restrict__ __ins_0, int8_t* __restrict__ __outs_0) noexcept __attribute__((nonnull (2,3)));
static void reorder__4190_closure_287(uint64_t fused_0fused_0fused_0fused_0_fuseiter_5462___fuseiter_5463_1786___fuseiter_5464_1787___fuseiter_5465_1788___fuseiter_5466_1789, int8_t* __restrict__ __ins_0, int8_t* __restrict__ __outs_0) noexcept __attribute__((nonnull (2,3)));
static void mul__6560_closure_288(uint64_t fused_0fused_0fused_0__itr_0____itr_1_1790____itr_2_1791____itr_3_1792, float* __restrict__ __ins_0, float* __restrict__ __ins_1, float* __restrict__ __outs_0) noexcept __attribute__((nonnull (2,3,4)));
static void mul__6550_closure_289(uint64_t fused_0fused_0fused_0__itr_0____itr_1_1793____itr_2_1794____itr_3_1795, float* __restrict__ __ins_0, float* __restrict__ __ins_1, float* __restrict__ __outs_0) noexcept __attribute__((nonnull (2,3,4)));
static void reorder__5340_closure_290(uint64_t fused_0_fuseiter_5481___fuseiter_5482_1796, int8_t* __restrict__ __ins_0, int8_t* __restrict__ __outs_0) noexcept __attribute__((nonnull (2,3)));
static void mul__2430_closure_291(uint64_t fused_0fused_0__itr_0____itr_1_1797____itr_2_1798, float* __restrict__ __ins_0, float* __restrict__ __ins_1, float* __restrict__ __outs_0) noexcept __attribute__((nonnull (2,3,4)));
static void cast__2440_closure_292(uint64_t fused_0fused_0__itr_0____itr_1_1799____itr_2_1800, float* __restrict__ __ins_0, int8_t* __restrict__ __outs_0) noexcept __attribute__((nonnull (2,3)));
static void mul__2520_closure_293(uint64_t fused_0fused_0__itr_0____itr_1_1801____itr_2_1802, float* __restrict__ __ins_0, float* __restrict__ __ins_1, float* __restrict__ __outs_0) noexcept __attribute__((nonnull (2,3,4)));
static void cast__2530_closure_294(uint64_t fused_0fused_0__itr_0____itr_1_1803____itr_2_1804, float* __restrict__ __ins_0, int8_t* __restrict__ __outs_0) noexcept __attribute__((nonnull (2,3)));
static void mul__2610_closure_295(uint64_t fused_0fused_0__itr_0____itr_1_1805____itr_2_1806, float* __restrict__ __ins_0, float* __restrict__ __ins_1, float* __restrict__ __outs_0) noexcept __attribute__((nonnull (2,3,4)));
static void cast__2620_closure_296(uint64_t fused_0fused_0__itr_0____itr_1_1807____itr_2_1808, float* __restrict__ __ins_0, int8_t* __restrict__ __outs_0) noexcept __attribute__((nonnull (2,3)));
static void mul__2460_closure_297(uint64_t fused_0fused_0__itr_0____itr_1_1809____itr_2_1810, float* __restrict__ __ins_0, float* __restrict__ __ins_1, float* __restrict__ __outs_0) noexcept __attribute__((nonnull (2,3,4)));
static void cast__2470_closure_298(uint64_t fused_0fused_0__itr_0____itr_1_1811____itr_2_1812, float* __restrict__ __ins_0, int8_t* __restrict__ __outs_0) noexcept __attribute__((nonnull (2,3)));
static void mul__2550_closure_299(uint64_t fused_0fused_0__itr_0____itr_1_1813____itr_2_1814, float* __restrict__ __ins_0, float* __restrict__ __ins_1, float* __restrict__ __outs_0) noexcept __attribute__((nonnull (2,3,4)));
static void cast__2560_closure_300(uint64_t fused_0fused_0__itr_0____itr_1_1815____itr_2_1816, float* __restrict__ __ins_0, int8_t* __restrict__ __outs_0) noexcept __attribute__((nonnull (2,3)));
static void mul__2340_closure_301(uint64_t fused_0fused_0__itr_0____itr_1_1817____itr_2_1818, float* __restrict__ __ins_0, float* __restrict__ __ins_1, float* __restrict__ __outs_0) noexcept __attribute__((nonnull (2,3,4)));
static void cast__2350_closure_302(uint64_t fused_0fused_0__itr_0____itr_1_1819____itr_2_1820, float* __restrict__ __ins_0, int8_t* __restrict__ __outs_0) noexcept __attribute__((nonnull (2,3)));
static void batchwise_4_fused_res2a_conv_b_cast_mul_add_cast_reorder_res2a_conv_0_cast_mul_add_relu_cast_res2a_conv_1_cast_mul_add_relu_cast_res2a_conv_2_cast_mul_add_cast_add_cast_reorder_res2b_conv_0_cast_mul_add_relu_cast_res2b_conv_1_cast_mul_add_relu_cast_reorder_res2b_conv_2_cast_mul_add_cast_add_cast_reorder_res2c_conv_0_cast_mul_add_relu_cast_reorder_res2c_conv_1_cast_mul_add_relu_cast_reorder_res2c_conv_2_cast_mul_add_cast_add_cast_reorder_res3a_conv_b_cast_mul_add_cast_res3a_conv_0_cast_mul_add_relu_cast_reorder_res3a_conv_1_cast_mul_add_relu_cast_res3a_conv_2_cast_mul_add_cast_add_cast_reorder_res3b_conv_0_cast_mul_add_relu_cast_reorder_res3b_conv_1_cast_mul_add_relu_cast_reorder_res3b_conv_2_cast_mul_add_cast_add_cast_reorder_res3c_conv_0_cast_mul_add_relu_cast_reorder_res3c_conv_1_cast_mul_add_relu_cast_reorder_res3c_conv_2_cast_mul_add_cast_add_cast_reorder_res3d_conv_0_cast_mul_add_relu_cast_reorder_res3d_conv_1_cast_mul_add_relu_cast_res3d_conv_2_cast_mul_add_cast_add_cast_res4a_conv_b_cast_mul_add_cast_reorder_res4a_conv_0_cast_mul_add_relu_cast_res4a_conv_1_cast_mul_add_relu_cast_reorder_res4a_conv_2_cast_mul_add_cast_add_cast_reorder_res4b_conv_0_cast_mul_add_relu_cast_reorder_res4b_conv_1_cast_mul_add_relu_cast_reorder_res4b_conv_2_cast_mul_add_cast_add_cast_reorder_res4c_conv_0_cast_mul_add_relu_cast_reorder_res4c_conv_1_cast_mul_add_relu_cast_reorder_res4c_conv_2_cast_mul_add_cast_add_cast_res4d_conv_0_cast_mul_add_relu_cast_res4d_conv_1_cast_mul_add_relu_cast_reorder_res4d_conv_2_cast_mul_add_cast_add_cast_res4e_conv_0_cast_mul_add_relu_cast_reorder_res4e_conv_1_cast_mul_add_relu_cast_reorder_res4e_conv_2_cast_mul_add_cast_add_cast_reorder_reorder_res4f_conv_0_cast_mul_add_relu_cast_reorder_res4f_conv_1_cast_mul_add_relu_cast_reorder_res4f_conv_2_cast_mul_add_cast_add_cast__6830_closure_303(uint64_t __batchwise_iter_0, int8_t* __restrict__ __ins_0, int8_t* __restrict__ __ins_4, float* __restrict__ __ins_5, float* __restrict__ __ins_6, int8_t* __restrict__ __ins_7, float* __restrict__ __ins_8, float* __restrict__ __ins_9, int8_t* __restrict__ __ins_1, float* __restrict__ __ins_2, float* __restrict__ __ins_3, int8_t* __restrict__ __ins_10, float* __restrict__ __ins_11, float* __restrict__ __ins_12, int8_t* __restrict__ __ins_13, float* __restrict__ __ins_14, float* __restrict__ __ins_15, int8_t* __restrict__ __ins_16, float* __restrict__ __ins_17, float* __restrict__ __ins_18, int8_t* __restrict__ __ins_19, float* __restrict__ __ins_20, float* __restrict__ __ins_21, int8_t* __restrict__ __ins_22, float* __restrict__ __ins_23, float* __restrict__ __ins_24, int8_t* __restrict__ __ins_25, float* __restrict__ __ins_26, float* __restrict__ __ins_27, int8_t* __restrict__ __ins_28, float* __restrict__ __ins_29, float* __restrict__ __ins_30, int8_t* __restrict__ __ins_34, float* __restrict__ __ins_35, float* __restrict__ __ins_36, int8_t* __restrict__ __ins_37, float* __restrict__ __ins_38, float* __restrict__ __ins_39, int8_t* __restrict__ __ins_31, float* __restrict__ __ins_32, float* __restrict__ __ins_33, int8_t* __restrict__ __ins_40, float* __restrict__ __ins_41, float* __restrict__ __ins_42, int8_t* __restrict__ __ins_43, float* __restrict__ __ins_44, float* __restrict__ __ins_45, int8_t* __restrict__ __ins_46, float* __restrict__ __ins_47, float* __restrict__ __ins_48, int8_t* __restrict__ __ins_49, float* __restrict__ __ins_50, float* __restrict__ __ins_51, int8_t* __restrict__ __ins_52, float* __restrict__ __ins_53, float* __restrict__ __ins_54, int8_t* __restrict__ __ins_55, float* __restrict__ __ins_56, float* __restrict__ __ins_57, int8_t* __restrict__ __ins_58, float* __restrict__ __ins_59, float* __restrict__ __ins_60, int8_t* __restrict__ __ins_61, float* __restrict__ __ins_62, float* __restrict__ __ins_63, int8_t* __restrict__ __ins_64, float* __restrict__ __ins_65, float* __restrict__ __ins_66, int8_t* __restrict__ __ins_67, float* __restrict__ __ins_68, float* __restrict__ __ins_69, int8_t* __restrict__ __ins_73, float* __restrict__ __ins_74, float* __restrict__ __ins_75, int8_t* __restrict__ __ins_76, float* __restrict__ __ins_77, float* __restrict__ __ins_78, int8_t* __restrict__ __ins_70, float* __restrict__ __ins_71, float* __restrict__ __ins_72, int8_t* __restrict__ __ins_79, float* __restrict__ __ins_80, float* __restrict__ __ins_81, int8_t* __restrict__ __ins_82, float* __restrict__ __ins_83, float* __restrict__ __ins_84, int8_t* __restrict__ __ins_85, float* __restrict__ __ins_86, float* __restrict__ __ins_87, int8_t* __restrict__ __ins_88, float* __restrict__ __ins_89, float* __restrict__ __ins_90, int8_t* __restrict__ __ins_91, float* __restrict__ __ins_92, float* __restrict__ __ins_93, int8_t* __restrict__ __ins_94, float* __restrict__ __ins_95, float* __restrict__ __ins_96, int8_t* __restrict__ __ins_97, float* __restrict__ __ins_98, float* __restrict__ __ins_99, int8_t* __restrict__ __ins_100, float* __restrict__ __ins_101, float* __restrict__ __ins_102, int8_t* __restrict__ __ins_103, float* __restrict__ __ins_104, float* __restrict__ __ins_105, int8_t* __restrict__ __ins_106, float* __restrict__ __ins_107, float* __restrict__ __ins_108, int8_t* __restrict__ __ins_109, float* __restrict__ __ins_110, float* __restrict__ __ins_111, int8_t* __restrict__ __ins_112, float* __restrict__ __ins_113, float* __restrict__ __ins_114, int8_t* __restrict__ __ins_115, float* __restrict__ __ins_116, float* __restrict__ __ins_117, int8_t* __restrict__ __ins_118, float* __restrict__ __ins_119, float* __restrict__ __ins_120, int8_t* __restrict__ __ins_121, float* __restrict__ __ins_122, float* __restrict__ __ins_123, uint8_t* __restrict__ __outs_0, int8_t* __restrict__ __ins_124, float* __restrict__ __ins_125, float* __restrict__ __ins_126) noexcept __attribute__((nonnull (2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25,26,27,28,29,30,31,32,33,34,35,36,37,38,39,40,41,42,43,44,45,46,47,48,49,50,51,52,53,54,55,56,57,58,59,60,61,62,63,64,65,66,67,68,69,70,71,72,73,74,75,76,77,78,79,80,81,82,83,84,85,86,87,88,89,90,91,92,93,94,95,96,97,98,99,100,101,102,103,104,105,106,107,108,109,110,111,112,113,114,115,116,117,118,119,120,121,122,123,124,125,126,127,128,129)));
static void reorder__5310_closure_304(uint64_t _fuseiter_7023, int8_t* __restrict__ __ins_0, int8_t* __restrict__ __outs_0) noexcept __attribute__((nonnull (2,3)));
static void mul__6540_closure_305(uint64_t fused_0fused_0fused_0__itr_0____itr_1_1881____itr_2_1882____itr_3_1883, float* __restrict__ __ins_0, float* __restrict__ __ins_1, float* __restrict__ __outs_0) noexcept __attribute__((nonnull (2,3,4)));
static void mul__6530_closure_306(uint64_t fused_0fused_0fused_0__itr_0____itr_1_1884____itr_2_1885____itr_3_1886, float* __restrict__ __ins_0, float* __restrict__ __ins_1, float* __restrict__ __outs_0) noexcept __attribute__((nonnull (2,3,4)));
static void mul__2400_closure_307(uint64_t fused_0fused_0__itr_0____itr_1_1887____itr_2_1888, float* __restrict__ __ins_0, float* __restrict__ __ins_1, float* __restrict__ __outs_0) noexcept __attribute__((nonnull (2,3,4)));
static void cast__2410_closure_308(uint64_t fused_0fused_0__itr_0____itr_1_1889____itr_2_1890, float* __restrict__ __ins_0, int8_t* __restrict__ __outs_0) noexcept __attribute__((nonnull (2,3)));
static void reorder__5370_closure_309(uint64_t fused_0fused_0_fuseiter_7052___fuseiter_7053_1891___fuseiter_7054_1892, int8_t* __restrict__ __ins_0, int8_t* __restrict__ __outs_0) noexcept __attribute__((nonnull (2,3)));
static void res5a_conv_0_cast_mul_add_relu_cast_reorder__6810_closure_310(uint64_t fused_0n__k_1893, int8_t* __restrict__ __outs_0) noexcept __attribute__((nonnull (2)));
static void res5a_conv_0_cast_mul_add_relu_cast_reorder__6810_closure_311(uint64_t fused_0n__k_1894, uint8_t* __restrict__ __ins_0, int8_t* __restrict__ __ins_1, float* __restrict__ __ins_2, float* __restrict__ __ins_3, int8_t* __restrict__ __outs_0) noexcept __attribute__((nonnull (2,3,4,5,6)));
static void mul__6580_closure_312(uint64_t fused_0fused_0fused_0__itr_0____itr_1_1895____itr_2_1896____itr_3_1897, float* __restrict__ __ins_0, float* __restrict__ __ins_1, float* __restrict__ __outs_0) noexcept __attribute__((nonnull (2,3,4)));
static void mul__6570_closure_313(uint64_t fused_0fused_0fused_0__itr_0____itr_1_1898____itr_2_1899____itr_3_1900, float* __restrict__ __ins_0, float* __restrict__ __ins_1, float* __restrict__ __outs_0) noexcept __attribute__((nonnull (2,3,4)));
static void res5a_conv_1_cast_mul_add_relu_cast_reorder__6800_closure_314(uint64_t fused_0k_o__n_1901, int32_t* __restrict__ conv_os_acc_size, int32_t* __restrict__ conv_os_blk_size, int8_t* __restrict__ __ins_0, int8_t* __restrict__ __ins_1, float* __restrict__ __ins_2, float* __restrict__ __ins_3, int8_t* __restrict__ __outs_0) noexcept __attribute__((nonnull (2,3,4,5,6,7,8)));
static void res5a_conv_b_cast_mul_add_cast_reorder__6820_closure_315(uint64_t fused_0n__c_o_1902, uint8_t* __restrict__ __ins_0, uint8_t* __restrict__ input_tmp) noexcept __attribute__((nonnull (2,3)));
static void res5a_conv_b_cast_mul_add_cast_reorder__6820_closure_316(uint64_t fused_0k__n_1903, uint8_t* __restrict__ input_tmp, int8_t* __restrict__ __ins_1, float* __restrict__ __ins_2, float* __restrict__ __ins_3, int8_t* __restrict__ __outs_0) noexcept __attribute__((nonnull (2,3,4,5,6)));
static void mul__6600_closure_317(uint64_t fused_0fused_0fused_0__itr_0____itr_1_1904____itr_2_1905____itr_3_1906, float* __restrict__ __ins_0, float* __restrict__ __ins_1, float* __restrict__ __outs_0) noexcept __attribute__((nonnull (2,3,4)));
static void mul__6590_closure_318(uint64_t fused_0fused_0fused_0__itr_0____itr_1_1907____itr_2_1908____itr_3_1909, float* __restrict__ __ins_0, float* __restrict__ __ins_1, float* __restrict__ __outs_0) noexcept __attribute__((nonnull (2,3,4)));
static void reorder__5400_closure_319(uint64_t _fuseiter_7182, int8_t* __restrict__ __ins_0, int8_t* __restrict__ __outs_0) noexcept __attribute__((nonnull (2,3)));
static void mul__6620_closure_320(uint64_t fused_0fused_0fused_0__itr_0____itr_1_1910____itr_2_1911____itr_3_1912, float* __restrict__ __ins_0, float* __restrict__ __ins_1, float* __restrict__ __outs_0) noexcept __attribute__((nonnull (2,3,4)));
static void mul__6610_closure_321(uint64_t fused_0fused_0fused_0__itr_0____itr_1_1913____itr_2_1914____itr_3_1915, float* __restrict__ __ins_0, float* __restrict__ __ins_1, float* __restrict__ __outs_0) noexcept __attribute__((nonnull (2,3,4)));
static void reorder__5430_closure_322(uint64_t fused_0_fuseiter_7201___fuseiter_7202_1916, int8_t* __restrict__ __ins_0, int8_t* __restrict__ __outs_0) noexcept __attribute__((nonnull (2,3)));
static void mul__2490_closure_323(uint64_t fused_0fused_0__itr_0____itr_1_1917____itr_2_1918, float* __restrict__ __ins_0, float* __restrict__ __ins_1, float* __restrict__ __outs_0) noexcept __attribute__((nonnull (2,3,4)));
static void cast__2500_closure_324(uint64_t fused_0fused_0__itr_0____itr_1_1919____itr_2_1920, float* __restrict__ __ins_0, int8_t* __restrict__ __outs_0) noexcept __attribute__((nonnull (2,3)));
static void mul__2580_closure_325(uint64_t fused_0fused_0__itr_0____itr_1_1921____itr_2_1922, float* __restrict__ __ins_0, float* __restrict__ __ins_1, float* __restrict__ __outs_0) noexcept __attribute__((nonnull (2,3,4)));
static void cast__2590_closure_326(uint64_t fused_0fused_0__itr_0____itr_1_1923____itr_2_1924, float* __restrict__ __ins_0, int8_t* __restrict__ __outs_0) noexcept __attribute__((nonnull (2,3)));
static void mul__6640_closure_327(uint64_t fused_0fused_0fused_0__itr_0____itr_1_1925____itr_2_1926____itr_3_1927, float* __restrict__ __ins_0, float* __restrict__ __ins_1, float* __restrict__ __outs_0) noexcept __attribute__((nonnull (2,3,4)));
static void mul__6630_closure_328(uint64_t fused_0fused_0fused_0__itr_0____itr_1_1928____itr_2_1929____itr_3_1930, float* __restrict__ __ins_0, float* __restrict__ __ins_1, float* __restrict__ __outs_0) noexcept __attribute__((nonnull (2,3,4)));
static void reorder__5460_closure_329(uint64_t _fuseiter_7240, int8_t* __restrict__ __ins_0, int8_t* __restrict__ __outs_0) noexcept __attribute__((nonnull (2,3)));
static void res5a_conv_2_cast_mul_add_cast_add_cast__6790_closure_330(uint64_t fused_0k__n_1931, int8_t* __restrict__ __ins_0, int8_t* __restrict__ __ins_1, float* __restrict__ __ins_2, float* __restrict__ __ins_3, int8_t* __restrict__ __ins_4, uint8_t* __restrict__ __outs_0) noexcept __attribute__((nonnull (2,3,4,5,6,7)));
static void res5b_conv_0_cast_mul_add_relu_cast_reorder__6780_closure_331(uint64_t fused_0n__k_1932, int8_t* __restrict__ __outs_0) noexcept __attribute__((nonnull (2)));
static void res5b_conv_0_cast_mul_add_relu_cast_reorder__6780_closure_332(uint64_t fused_0n__k_1933, uint8_t* __restrict__ __ins_0, int8_t* __restrict__ __ins_1, float* __restrict__ __ins_2, float* __restrict__ __ins_3, int8_t* __restrict__ __outs_0) noexcept __attribute__((nonnull (2,3,4,5,6)));
static void res5b_conv_1_cast_mul_add_relu_cast_reorder__6770_closure_333(uint64_t fused_0k_o__n_1934, int32_t* __restrict__ conv_os_acc_size, int32_t* __restrict__ conv_os_blk_size, int8_t* __restrict__ __ins_0, int8_t* __restrict__ __ins_1, float* __restrict__ __ins_2, float* __restrict__ __ins_3, int8_t* __restrict__ __outs_0) noexcept __attribute__((nonnull (2,3,4,5,6,7,8)));
static void mul__6660_closure_334(uint64_t fused_0fused_0fused_0__itr_0____itr_1_1935____itr_2_1936____itr_3_1937, float* __restrict__ __ins_0, float* __restrict__ __ins_1, float* __restrict__ __outs_0) noexcept __attribute__((nonnull (2,3,4)));
static void mul__6650_closure_335(uint64_t fused_0fused_0fused_0__itr_0____itr_1_1938____itr_2_1939____itr_3_1940, float* __restrict__ __ins_0, float* __restrict__ __ins_1, float* __restrict__ __outs_0) noexcept __attribute__((nonnull (2,3,4)));
static void reorder__5490_closure_336(uint64_t _fuseiter_7365, int8_t* __restrict__ __ins_0, int8_t* __restrict__ __outs_0) noexcept __attribute__((nonnull (2,3)));
static void mul__6680_closure_337(uint64_t fused_0fused_0fused_0__itr_0____itr_1_1941____itr_2_1942____itr_3_1943, float* __restrict__ __ins_0, float* __restrict__ __ins_1, float* __restrict__ __outs_0) noexcept __attribute__((nonnull (2,3,4)));
static void mul__6670_closure_338(uint64_t fused_0fused_0fused_0__itr_0____itr_1_1944____itr_2_1945____itr_3_1946, float* __restrict__ __ins_0, float* __restrict__ __ins_1, float* __restrict__ __outs_0) noexcept __attribute__((nonnull (2,3,4)));
static void reorder__5520_closure_339(uint64_t fused_0_fuseiter_7384___fuseiter_7385_1947, int8_t* __restrict__ __ins_0, int8_t* __restrict__ __outs_0) noexcept __attribute__((nonnull (2,3)));
static void mul__6700_closure_340(uint64_t fused_0fused_0fused_0__itr_0____itr_1_1948____itr_2_1949____itr_3_1950, float* __restrict__ __ins_0, float* __restrict__ __ins_1, float* __restrict__ __outs_0) noexcept __attribute__((nonnull (2,3,4)));
static void mul__6690_closure_341(uint64_t fused_0fused_0fused_0__itr_0____itr_1_1951____itr_2_1952____itr_3_1953, float* __restrict__ __ins_0, float* __restrict__ __ins_1, float* __restrict__ __outs_0) noexcept __attribute__((nonnull (2,3,4)));
static void reorder__5550_closure_342(uint64_t fused_0fused_0_fuseiter_7403___fuseiter_7404_1954___fuseiter_7405_1955, int8_t* __restrict__ __ins_0, int8_t* __restrict__ __outs_0) noexcept __attribute__((nonnull (2,3)));
static void res5b_conv_2_cast_mul_add_cast_add_cast_reorder__6760_closure_343(uint64_t fused_0n__k_1956, int8_t* __restrict__ __ins_0, int8_t* __restrict__ __ins_1, float* __restrict__ __ins_2, float* __restrict__ __ins_3, uint8_t* __restrict__ __ins_4, uint8_t* __restrict__ __outs_0) noexcept __attribute__((nonnull (2,3,4,5,6,7)));
static void res5c_conv_0_cast_mul_add_relu_cast_reorder__6750_closure_344(uint64_t fused_0n__k_1957, int8_t* __restrict__ __outs_0) noexcept __attribute__((nonnull (2)));
static void res5c_conv_0_cast_mul_add_relu_cast_reorder__6750_closure_345(uint64_t fused_0n__k_1958, uint8_t* __restrict__ __ins_0, int8_t* __restrict__ __ins_1, float* __restrict__ __ins_2, float* __restrict__ __ins_3, int8_t* __restrict__ __outs_0) noexcept __attribute__((nonnull (2,3,4,5,6)));
static void res5c_conv_1_cast_mul_add_relu_cast_reorder__6740_closure_346(uint64_t fused_0k_o__n_1959, int32_t* __restrict__ conv_os_acc_size, int32_t* __restrict__ conv_os_blk_size, int8_t* __restrict__ __ins_0, int8_t* __restrict__ __ins_1, float* __restrict__ __ins_2, float* __restrict__ __ins_3, int8_t* __restrict__ __outs_0) noexcept __attribute__((nonnull (2,3,4,5,6,7,8)));
static void mul__6720_closure_347(uint64_t fused_0fused_0fused_0__itr_0____itr_1_1960____itr_2_1961____itr_3_1962, float* __restrict__ __ins_0, float* __restrict__ __ins_1, float* __restrict__ __outs_0) noexcept __attribute__((nonnull (2,3,4)));
static void mul__6710_closure_348(uint64_t fused_0fused_0fused_0__itr_0____itr_1_1963____itr_2_1964____itr_3_1965, float* __restrict__ __ins_0, float* __restrict__ __ins_1, float* __restrict__ __outs_0) noexcept __attribute__((nonnull (2,3,4)));
static void reorder__5580_closure_349(uint64_t fused_0fused_0fused_0fused_0_fuseiter_7533___fuseiter_7534_1966___fuseiter_7535_1967___fuseiter_7536_1968___fuseiter_7537_1969, int8_t* __restrict__ __ins_0, int8_t* __restrict__ __outs_0) noexcept __attribute__((nonnull (2,3)));
static void res5c_conv_2_cast_mul_add_cast_add_cast_cast__6730_closure_350(uint64_t fused_0k__n_1970, int8_t* __restrict__ __ins_0, int8_t* __restrict__ __ins_1, float* __restrict__ __ins_2, float* __restrict__ __ins_3, uint8_t* __restrict__ __ins_4, int8_t* __restrict__ __outs_0) noexcept __attribute__((nonnull (2,3,4,5,6,7)));
static void reorder__1050_closure_351(uint64_t fused_0n__h_1971, int8_t* __restrict__ __ins_0, int8_t* __restrict__ __outs_0) noexcept __attribute__((nonnull (2,3)));


extern "C" void rn50_backbone_bs4(int8_t* __restrict__ backbone_output, int8_t* __restrict__ backbone_input, float* __restrict__ res2a_weight_b, float* __restrict__ res2a_bias_b, float* __restrict__ res2a_weight_0, float* __restrict__ res2a_bias_0, float* __restrict__ res2a_weight_1, float* __restrict__ res2a_bias_1, float* __restrict__ res2a_weight_2, float* __restrict__ res2a_bias_2, float* __restrict__ res2b_weight_0, float* __restrict__ res2b_bias_0, float* __restrict__ res2b_weight_1, float* __restrict__ res2b_bias_1, float* __restrict__ res2b_weight_2, float* __restrict__ res2b_bias_2, float* __restrict__ res2c_weight_0, float* __restrict__ res2c_bias_0, float* __restrict__ res2c_weight_1, float* __restrict__ res2c_bias_1, float* __restrict__ res2c_weight_2, float* __restrict__ res2c_bias_2, float* __restrict__ res3a_weight_b, float* __restrict__ res3a_bias_b, float* __restrict__ res3a_weight_0, float* __restrict__ res3a_bias_0, float* __restrict__ res3a_weight_1, float* __restrict__ res3a_bias_1, float* __restrict__ res3a_weight_2, float* __restrict__ res3a_bias_2, float* __restrict__ res3b_weight_0, float* __restrict__ res3b_bias_0, float* __restrict__ res3b_weight_1, float* __restrict__ res3b_bias_1, float* __restrict__ res3b_weight_2, float* __restrict__ res3b_bias_2, float* __restrict__ res3c_weight_0, float* __restrict__ res3c_bias_0, float* __restrict__ res3c_weight_1, float* __restrict__ res3c_bias_1, float* __restrict__ res3c_weight_2, float* __restrict__ res3c_bias_2, float* __restrict__ res3d_weight_0, float* __restrict__ res3d_bias_0, float* __restrict__ res3d_weight_1, float* __restrict__ res3d_bias_1, float* __restrict__ res3d_weight_2, float* __restrict__ res3d_bias_2, float* __restrict__ res4a_weight_b, float* __restrict__ res4a_bias_b, float* __restrict__ res4a_weight_0, float* __restrict__ res4a_bias_0, float* __restrict__ res4a_weight_1, float* __restrict__ res4a_bias_1, float* __restrict__ res4a_weight_2, float* __restrict__ res4a_bias_2, float* __restrict__ res4b_weight_0, float* __restrict__ res4b_bias_0, float* __restrict__ res4b_weight_1, float* __restrict__ res4b_bias_1, float* __restrict__ res4b_weight_2, float* __restrict__ res4b_bias_2, float* __restrict__ res4c_weight_0, float* __restrict__ res4c_bias_0, float* __restrict__ res4c_weight_1, float* __restrict__ res4c_bias_1, float* __restrict__ res4c_weight_2, float* __restrict__ res4c_bias_2, float* __restrict__ res4d_weight_0, float* __restrict__ res4d_bias_0, float* __restrict__ res4d_weight_1, float* __restrict__ res4d_bias_1, float* __restrict__ res4d_weight_2, float* __restrict__ res4d_bias_2, float* __restrict__ res4e_weight_0, float* __restrict__ res4e_bias_0, float* __restrict__ res4e_weight_1, float* __restrict__ res4e_bias_1, float* __restrict__ res4e_weight_2, float* __restrict__ res4e_bias_2, float* __restrict__ res4f_weight_0, float* __restrict__ res4f_bias_0, float* __restrict__ res4f_weight_1, float* __restrict__ res4f_bias_1, float* __restrict__ res4f_weight_2, float* __restrict__ res4f_bias_2, float* __restrict__ res5a_weight_b, float* __restrict__ res5a_bias_b, float* __restrict__ res5a_weight_0, float* __restrict__ res5a_bias_0, float* __restrict__ res5a_weight_1, float* __restrict__ res5a_bias_1, float* __restrict__ res5a_weight_2, float* __restrict__ res5a_bias_2, float* __restrict__ res5b_weight_0, float* __restrict__ res5b_bias_0, float* __restrict__ res5b_weight_1, float* __restrict__ res5b_bias_1, float* __restrict__ res5b_weight_2, float* __restrict__ res5b_bias_2, float* __restrict__ res5c_weight_0, float* __restrict__ res5c_bias_0, float* __restrict__ res5c_weight_1, float* __restrict__ res5c_bias_1, float* __restrict__ res5c_weight_2, float* __restrict__ res5c_bias_2) noexcept{
  bool& is_init = *(bool*)(__module_data + 0);
  int8_t* folded_const_281 = (int8_t*)&__uninitialized_data[8608768UL];
  float* folded_const_212 = (float*)&__uninitialized_data[699392UL];
  float* folded_const_211 = (float*)&__uninitialized_data[698368UL];
  int8_t* folded_const_224 = (int8_t*)&__uninitialized_data[3584000UL];
  float* folded_const_252 = (float*)&__uninitialized_data[3599104UL];
  float* folded_const_251 = (float*)&__uninitialized_data[3598848UL];
  int8_t* folded_const_274 = (int8_t*)&__uninitialized_data[8457216UL];
  float* folded_const_250 = (float*)&__uninitialized_data[3598592UL];
  float* folded_const_249 = (float*)&__uninitialized_data[3598336UL];
  int8_t* folded_const_280 = (int8_t*)&__uninitialized_data[8592384UL];
  float* folded_const_210 = (float*)&__uninitialized_data[697344UL];
  float* folded_const_209 = (float*)&__uninitialized_data[696320UL];
  int8_t* folded_const_277 = (int8_t*)&__uninitialized_data[8543232UL];
  float* folded_const_248 = (float*)&__uninitialized_data[3598080UL];
  float* folded_const_247 = (float*)&__uninitialized_data[3597824UL];
  int8_t* folded_const_273 = (int8_t*)&__uninitialized_data[8420352UL];
  float* folded_const_246 = (float*)&__uninitialized_data[3597568UL];
  float* folded_const_245 = (float*)&__uninitialized_data[3597312UL];
  int8_t* folded_const_279 = (int8_t*)&__uninitialized_data[8576000UL];
  float* folded_const_208 = (float*)&__uninitialized_data[695296UL];
  float* folded_const_207 = (float*)&__uninitialized_data[694272UL];
  int8_t* folded_const_276 = (int8_t*)&__uninitialized_data[8526848UL];
  float* folded_const_244 = (float*)&__uninitialized_data[3597056UL];
  float* folded_const_243 = (float*)&__uninitialized_data[3596800UL];
  int8_t* folded_const_272 = (int8_t*)&__uninitialized_data[8383488UL];
  float* folded_const_242 = (float*)&__uninitialized_data[3596544UL];
  float* folded_const_241 = (float*)&__uninitialized_data[3596288UL];
  int8_t* folded_const_278 = (int8_t*)&__uninitialized_data[8559616UL];
  float* folded_const_206 = (float*)&__uninitialized_data[693248UL];
  float* folded_const_205 = (float*)&__uninitialized_data[692224UL];
  int8_t* folded_const_264 = (int8_t*)&__uninitialized_data[7793664UL];
  float* folded_const_180 = (float*)&__uninitialized_data[665600UL];
  float* folded_const_179 = (float*)&__uninitialized_data[663552UL];
  int8_t* folded_const_275 = (int8_t*)&__uninitialized_data[8494080UL];
  float* folded_const_240 = (float*)&__uninitialized_data[3595776UL];
  float* folded_const_239 = (float*)&__uninitialized_data[3595264UL];
  int8_t* folded_const_262 = (int8_t*)&__uninitialized_data[7515136UL];
  float* folded_const_238 = (float*)&__uninitialized_data[3594752UL];
  float* folded_const_237 = (float*)&__uninitialized_data[3594240UL];
  int8_t* folded_const_271 = (int8_t*)&__uninitialized_data[8317952UL];
  float* folded_const_178 = (float*)&__uninitialized_data[661504UL];
  float* folded_const_177 = (float*)&__uninitialized_data[659456UL];
  int8_t* folded_const_267 = (int8_t*)&__uninitialized_data[8055808UL];
  float* folded_const_236 = (float*)&__uninitialized_data[3593728UL];
  float* folded_const_235 = (float*)&__uninitialized_data[3593216UL];
  int8_t* folded_const_261 = (int8_t*)&__uninitialized_data[7367680UL];
  float* folded_const_234 = (float*)&__uninitialized_data[3592704UL];
  float* folded_const_233 = (float*)&__uninitialized_data[3592192UL];
  int8_t* folded_const_270 = (int8_t*)&__uninitialized_data[8252416UL];
  float* folded_const_176 = (float*)&__uninitialized_data[657408UL];
  float* folded_const_175 = (float*)&__uninitialized_data[655360UL];
  int8_t* folded_const_266 = (int8_t*)&__uninitialized_data[7990272UL];
  float* folded_const_232 = (float*)&__uninitialized_data[3591680UL];
  float* folded_const_231 = (float*)&__uninitialized_data[3591168UL];
  int8_t* folded_const_260 = (int8_t*)&__uninitialized_data[7220224UL];
  float* folded_const_230 = (float*)&__uninitialized_data[3590656UL];
  float* folded_const_229 = (float*)&__uninitialized_data[3590144UL];
  int8_t* folded_const_269 = (int8_t*)&__uninitialized_data[8186880UL];
  float* folded_const_174 = (float*)&__uninitialized_data[653312UL];
  float* folded_const_173 = (float*)&__uninitialized_data[651264UL];
  int8_t* folded_const_265 = (int8_t*)&__uninitialized_data[7924736UL];
  float* folded_const_228 = (float*)&__uninitialized_data[3589632UL];
  float* folded_const_227 = (float*)&__uninitialized_data[3589120UL];
  int8_t* folded_const_259 = (int8_t*)&__uninitialized_data[7072768UL];
  float* folded_const_226 = (float*)&__uninitialized_data[3588608UL];
  float* folded_const_225 = (float*)&__uninitialized_data[3588096UL];
  int8_t* folded_const_268 = (int8_t*)&__uninitialized_data[8121344UL];
  float* folded_const_172 = (float*)&__uninitialized_data[649216UL];
  float* folded_const_171 = (float*)&__uninitialized_data[647168UL];
  int8_t* folded_const_258 = (int8_t*)&__uninitialized_data[6548480UL];
  float* folded_const_170 = (float*)&__uninitialized_data[643072UL];
  float* folded_const_169 = (float*)&__uninitialized_data[638976UL];
  int8_t* folded_const_263 = (int8_t*)&__uninitialized_data[7662592UL];
  float* folded_const_204 = (float*)&__uninitialized_data[691200UL];
  float* folded_const_203 = (float*)&__uninitialized_data[690176UL];
  int8_t* folded_const_257 = (int8_t*)&__uninitialized_data[5958656UL];
  float* folded_const_202 = (float*)&__uninitialized_data[689152UL];
  float* folded_const_201 = (float*)&__uninitialized_data[688128UL];
  int8_t* folded_const_223 = (int8_t*)&__uninitialized_data[3321856UL];
  float* folded_const_168 = (float*)&__uninitialized_data[634880UL];
  float* folded_const_167 = (float*)&__uninitialized_data[630784UL];
  int8_t* folded_const_217 = (int8_t*)&__uninitialized_data[1748992UL];
  float* folded_const_200 = (float*)&__uninitialized_data[687104UL];
  float* folded_const_199 = (float*)&__uninitialized_data[686080UL];
  int8_t* folded_const_256 = (int8_t*)&__uninitialized_data[5368832UL];
  float* folded_const_198 = (float*)&__uninitialized_data[685056UL];
  float* folded_const_197 = (float*)&__uninitialized_data[684032UL];
  int8_t* folded_const_222 = (int8_t*)&__uninitialized_data[3059712UL];
  float* folded_const_166 = (float*)&__uninitialized_data[626688UL];
  float* folded_const_165 = (float*)&__uninitialized_data[622592UL];
  int8_t* folded_const_216 = (int8_t*)&__uninitialized_data[1486848UL];
  float* folded_const_196 = (float*)&__uninitialized_data[683008UL];
  float* folded_const_195 = (float*)&__uninitialized_data[681984UL];
  int8_t* folded_const_255 = (int8_t*)&__uninitialized_data[4779008UL];
  float* folded_const_194 = (float*)&__uninitialized_data[680960UL];
  float* folded_const_193 = (float*)&__uninitialized_data[679936UL];
  int8_t* folded_const_221 = (int8_t*)&__uninitialized_data[2797568UL];
  float* folded_const_164 = (float*)&__uninitialized_data[618496UL];
  float* folded_const_163 = (float*)&__uninitialized_data[614400UL];
  int8_t* folded_const_215 = (int8_t*)&__uninitialized_data[1224704UL];
  float* folded_const_192 = (float*)&__uninitialized_data[678912UL];
  float* folded_const_191 = (float*)&__uninitialized_data[677888UL];
  int8_t* folded_const_254 = (int8_t*)&__uninitialized_data[4189184UL];
  float* folded_const_190 = (float*)&__uninitialized_data[676864UL];
  float* folded_const_189 = (float*)&__uninitialized_data[675840UL];
  int8_t* folded_const_220 = (int8_t*)&__uninitialized_data[2535424UL];
  float* folded_const_162 = (float*)&__uninitialized_data[610304UL];
  float* folded_const_161 = (float*)&__uninitialized_data[606208UL];
  int8_t* folded_const_214 = (int8_t*)&__uninitialized_data[962560UL];
  float* folded_const_188 = (float*)&__uninitialized_data[674816UL];
  float* folded_const_187 = (float*)&__uninitialized_data[673792UL];
  int8_t* folded_const_253 = (int8_t*)&__uninitialized_data[3599360UL];
  float* folded_const_186 = (float*)&__uninitialized_data[672768UL];
  float* folded_const_185 = (float*)&__uninitialized_data[671744UL];
  int8_t* folded_const_219 = (int8_t*)&__uninitialized_data[2273280UL];
  float* folded_const_160 = (float*)&__uninitialized_data[602112UL];
  float* folded_const_159 = (float*)&__uninitialized_data[598016UL];
  int8_t* folded_const_213 = (int8_t*)&__uninitialized_data[700416UL];
  float* folded_const_184 = (float*)&__uninitialized_data[670720UL];
  float* folded_const_183 = (float*)&__uninitialized_data[669696UL];
  int8_t* folded_const_156 = (int8_t*)&__uninitialized_data[0UL];
  float* folded_const_182 = (float*)&__uninitialized_data[668672UL];
  float* folded_const_181 = (float*)&__uninitialized_data[667648UL];
  int8_t* folded_const_218 = (int8_t*)&__uninitialized_data[2011136UL];
  float* folded_const_158 = (float*)&__uninitialized_data[593920UL];
  float* folded_const_157 = (float*)&__uninitialized_data[589824UL];
  int8_t* folded_const_284 = (int8_t*)&__uninitialized_data[8629248UL];
  float* folded_const_283 = (float*)&__uninitialized_data[8627200UL];
  float* folded_const_282 = (float*)&__uninitialized_data[8625152UL];
  int8_t* folded_const_288 = (int8_t*)&__uninitialized_data[11267072UL];
  float* folded_const_290 = (float*)&__uninitialized_data[13628416UL];
  float* folded_const_289 = (float*)&__uninitialized_data[13626368UL];
  int8_t* folded_const_285 = (int8_t*)&__uninitialized_data[9153536UL];
  float* folded_const_287 = (float*)&__uninitialized_data[11258880UL];
  float* folded_const_286 = (float*)&__uninitialized_data[11250688UL];
  int8_t* folded_const_293 = (int8_t*)&__uninitialized_data[13646848UL];
  float* folded_const_292 = (float*)&__uninitialized_data[13638656UL];
  float* folded_const_291 = (float*)&__uninitialized_data[13630464UL];
  int8_t* folded_const_296 = (int8_t*)&__uninitialized_data[14699520UL];
  float* folded_const_295 = (float*)&__uninitialized_data[14697472UL];
  float* folded_const_294 = (float*)&__uninitialized_data[14695424UL];
  int8_t* folded_const_299 = (int8_t*)&__uninitialized_data[15752192UL];
  float* folded_const_298 = (float*)&__uninitialized_data[15750144UL];
  float* folded_const_297 = (float*)&__uninitialized_data[15748096UL];
  int8_t* folded_const_302 = (int8_t*)&__uninitialized_data[18127872UL];
  float* folded_const_301 = (float*)&__uninitialized_data[18119680UL];
  float* folded_const_300 = (float*)&__uninitialized_data[18111488UL];
  int8_t* folded_const_305 = (int8_t*)&__uninitialized_data[19180544UL];
  float* folded_const_304 = (float*)&__uninitialized_data[19178496UL];
  float* folded_const_303 = (float*)&__uninitialized_data[19176448UL];
  int8_t* folded_const_308 = (int8_t*)&__uninitialized_data[20233216UL];
  float* folded_const_307 = (float*)&__uninitialized_data[20231168UL];
  float* folded_const_306 = (float*)&__uninitialized_data[20229120UL];
  int8_t* folded_const_311 = (int8_t*)&__uninitialized_data[22608896UL];
  float* folded_const_310 = (float*)&__uninitialized_data[22600704UL];
  float* folded_const_309 = (float*)&__uninitialized_data[22592512UL];
  int8_t* __rescheduled_0 = (int8_t*)sc_aligned_malloc(__stream, 1427456UL);
  if (!is_init) {
    __init_const_globals(backbone_output, backbone_input, res2a_weight_b, res2a_bias_b, res2a_weight_0, res2a_bias_0, res2a_weight_1, res2a_bias_1, res2a_weight_2, res2a_bias_2, res2b_weight_0, res2b_bias_0, res2b_weight_1, res2b_bias_1, res2b_weight_2, res2b_bias_2, res2c_weight_0, res2c_bias_0, res2c_weight_1, res2c_bias_1, res2c_weight_2, res2c_bias_2, res3a_weight_b, res3a_bias_b, res3a_weight_0, res3a_bias_0, res3a_weight_1, res3a_bias_1, res3a_weight_2, res3a_bias_2, res3b_weight_0, res3b_bias_0, res3b_weight_1, res3b_bias_1, res3b_weight_2, res3b_bias_2, res3c_weight_0, res3c_bias_0, res3c_weight_1, res3c_bias_1, res3c_weight_2, res3c_bias_2, res3d_weight_0, res3d_bias_0, res3d_weight_1, res3d_bias_1, res3d_weight_2, res3d_bias_2, res4a_weight_b, res4a_bias_b, res4a_weight_0, res4a_bias_0, res4a_weight_1, res4a_bias_1, res4a_weight_2, res4a_bias_2, res4b_weight_0, res4b_bias_0, res4b_weight_1, res4b_bias_1, res4b_weight_2, res4b_bias_2, res4c_weight_0, res4c_bias_0, res4c_weight_1, res4c_bias_1, res4c_weight_2, res4c_bias_2, res4d_weight_0, res4d_bias_0, res4d_weight_1, res4d_bias_1, res4d_weight_2, res4d_bias_2, res4e_weight_0, res4e_bias_0, res4e_weight_1, res4e_bias_1, res4e_weight_2, res4e_bias_2, res4f_weight_0, res4f_bias_0, res4f_weight_1, res4f_bias_1, res4f_weight_2, res4f_bias_2, res5a_weight_b, res5a_bias_b, res5a_weight_0, res5a_bias_0, res5a_weight_1, res5a_bias_1, res5a_weight_2, res5a_bias_2, res5b_weight_0, res5b_bias_0, res5b_weight_1, res5b_bias_1, res5b_weight_2, res5b_bias_2, res5c_weight_0, res5c_bias_0, res5c_weight_1, res5c_bias_1, res5c_weight_2, res5c_bias_2);
  }
  // [u8 [4, 4, 14, 14, 256] @ ABCD256b]
  uint8_t* buffer_578 = (uint8_t*)&__rescheduled_0[0UL];
  batchwise_4_fused_res2a_conv_b_cast_mul_add_cast_reorder_res2a_conv_0_cast_mul_add_relu_cast_res2a_conv_1_cast_mul_add_relu_cast_res2a_conv_2_cast_mul_add_cast_add_cast_reorder_res2b_conv_0_cast_mul_add_relu_cast_res2b_conv_1_cast_mul_add_relu_cast_reorder_res2b_conv_2_cast_mul_add_cast_add_cast_reorder_res2c_conv_0_cast_mul_add_relu_cast_reorder_res2c_conv_1_cast_mul_add_relu_cast_reorder_res2c_conv_2_cast_mul_add_cast_add_cast_reorder_res3a_conv_b_cast_mul_add_cast_res3a_conv_0_cast_mul_add_relu_cast_reorder_res3a_conv_1_cast_mul_add_relu_cast_res3a_conv_2_cast_mul_add_cast_add_cast_reorder_res3b_conv_0_cast_mul_add_relu_cast_reorder_res3b_conv_1_cast_mul_add_relu_cast_reorder_res3b_conv_2_cast_mul_add_cast_add_cast_reorder_res3c_conv_0_cast_mul_add_relu_cast_reorder_res3c_conv_1_cast_mul_add_relu_cast_reorder_res3c_conv_2_cast_mul_add_cast_add_cast_reorder_res3d_conv_0_cast_mul_add_relu_cast_reorder_res3d_conv_1_cast_mul_add_relu_cast_res3d_conv_2_cast_mul_add_cast_add_cast_res4a_conv_b_cast_mul_add_cast_reorder_res4a_conv_0_cast_mul_add_relu_cast_res4a_conv_1_cast_mul_add_relu_cast_reorder_res4a_conv_2_cast_mul_add_cast_add_cast_reorder_res4b_conv_0_cast_mul_add_relu_cast_reorder_res4b_conv_1_cast_mul_add_relu_cast_reorder_res4b_conv_2_cast_mul_add_cast_add_cast_reorder_res4c_conv_0_cast_mul_add_relu_cast_reorder_res4c_conv_1_cast_mul_add_relu_cast_reorder_res4c_conv_2_cast_mul_add_cast_add_cast_res4d_conv_0_cast_mul_add_relu_cast_res4d_conv_1_cast_mul_add_relu_cast_reorder_res4d_conv_2_cast_mul_add_cast_add_cast_res4e_conv_0_cast_mul_add_relu_cast_reorder_res4e_conv_1_cast_mul_add_relu_cast_reorder_res4e_conv_2_cast_mul_add_cast_add_cast_reorder_reorder_res4f_conv_0_cast_mul_add_relu_cast_reorder_res4f_conv_1_cast_mul_add_relu_cast_reorder_res4f_conv_2_cast_mul_add_cast_add_cast__683(buffer_578, &backbone_input[0UL], folded_const_281, folded_const_212, folded_const_211, folded_const_224, folded_const_252, folded_const_251, folded_const_274, folded_const_250, folded_const_249, folded_const_280, folded_const_210, folded_const_209, folded_const_277, folded_const_248, folded_const_247, folded_const_273, folded_const_246, folded_const_245, folded_const_279, folded_const_208, folded_const_207, folded_const_276, folded_const_244, folded_const_243, folded_const_272, folded_const_242, folded_const_241, folded_const_278, folded_const_206, folded_const_205, folded_const_264, folded_const_180, folded_const_179, folded_const_275, folded_const_240, folded_const_239, folded_const_262, folded_const_238, folded_const_237, folded_const_271, folded_const_178, folded_const_177, folded_const_267, folded_const_236, folded_const_235, folded_const_261, folded_const_234, folded_const_233, folded_const_270, folded_const_176, folded_const_175, folded_const_266, folded_const_232, folded_const_231, folded_const_260, folded_const_230, folded_const_229, folded_const_269, folded_const_174, folded_const_173, folded_const_265, folded_const_228, folded_const_227, folded_const_259, folded_const_226, folded_const_225, folded_const_268, folded_const_172, folded_const_171, folded_const_258, folded_const_170, folded_const_169, folded_const_263, folded_const_204, folded_const_203, folded_const_257, folded_const_202, folded_const_201, folded_const_223, folded_const_168, folded_const_167, folded_const_217, folded_const_200, folded_const_199, folded_const_256, folded_const_198, folded_const_197, folded_const_222, folded_const_166, folded_const_165, folded_const_216, folded_const_196, folded_const_195, folded_const_255, folded_const_194, folded_const_193, folded_const_221, folded_const_164, folded_const_163, folded_const_215, folded_const_192, folded_const_191, folded_const_254, folded_const_190, folded_const_189, folded_const_220, folded_const_162, folded_const_161, folded_const_214, folded_const_188, folded_const_187, folded_const_253, folded_const_186, folded_const_185, folded_const_219, folded_const_160, folded_const_159, folded_const_213, folded_const_184, folded_const_183, folded_const_156, folded_const_182, folded_const_181, folded_const_218, folded_const_158, folded_const_157);
  // [s8 [4, 2, 16, 16, 256] @ ABCD256b]
  int8_t* buffer_585 = (int8_t*)&__rescheduled_0[802816UL];
  res5a_conv_0_cast_mul_add_relu_cast_reorder__681(buffer_585, buffer_578, folded_const_284, folded_const_283, folded_const_282);
  // [s8 [4, 2, 7, 7, 256] @ ABCD256b]
  int8_t* buffer_588 = (int8_t*)&__rescheduled_0[1327104UL];
  res5a_conv_1_cast_mul_add_relu_cast_reorder__680(buffer_588, buffer_585, folded_const_288, folded_const_290, folded_const_289);
  // [s8 [4, 32, 7, 7, 64] @ ABCD64b]
  int8_t* buffer_589 = (int8_t*)&__rescheduled_0[802816UL];
  res5a_conv_b_cast_mul_add_cast_reorder__682(buffer_589, buffer_578, folded_const_285, folded_const_287, folded_const_286);
  // [u8 [4, 32, 7, 7, 64] @ ABCD64b]
  uint8_t* buffer_603 = (uint8_t*)&__rescheduled_0[0UL];
  res5a_conv_2_cast_mul_add_cast_add_cast__679(buffer_603, buffer_588, folded_const_293, folded_const_292, folded_const_291, buffer_589);
  // [s8 [4, 1, 9, 9, 512] @ ABCD512b]
  int8_t* buffer_604 = (int8_t*)&__rescheduled_0[401408UL];
  res5b_conv_0_cast_mul_add_relu_cast_reorder__678(buffer_604, buffer_603, folded_const_296, folded_const_295, folded_const_294);
  // [s8 [4, 2, 7, 7, 256] @ ABCD256b]
  int8_t* buffer_605 = (int8_t*)&__rescheduled_0[567296UL];
  res5b_conv_1_cast_mul_add_relu_cast_reorder__677(buffer_605, buffer_604, folded_const_299, folded_const_298, folded_const_297);
  // [u8 [4, 4, 7, 7, 512] @ ABCD512b]
  uint8_t* buffer_615 = (uint8_t*)&__rescheduled_0[667648UL];
  res5b_conv_2_cast_mul_add_cast_add_cast_reorder__676(buffer_615, buffer_605, folded_const_302, folded_const_301, folded_const_300, buffer_603);
  // [s8 [4, 2, 9, 9, 256] @ ABCD256b]
  int8_t* buffer_616 = (int8_t*)&__rescheduled_0[1069056UL];
  res5c_conv_0_cast_mul_add_relu_cast_reorder__675(buffer_616, buffer_615, folded_const_305, folded_const_304, folded_const_303);
  // [s8 [4, 1, 7, 7, 512] @ ABCD512b]
  int8_t* buffer_617 = (int8_t*)&__rescheduled_0[1234944UL];
  res5c_conv_1_cast_mul_add_relu_cast_reorder__674(buffer_617, buffer_616, folded_const_308, folded_const_307, folded_const_306);
  // [s8 [4, 4, 7, 7, 512] @ ABCD512b]
  int8_t* buffer_621 = (int8_t*)&__rescheduled_0[0UL];
  res5c_conv_2_cast_mul_add_cast_add_cast_cast__673(buffer_621, buffer_617, folded_const_311, folded_const_310, folded_const_309, buffer_615);
  reorder__105(backbone_output, buffer_621);
  sc_aligned_free(__stream, __rescheduled_0);
}

static bool reorder__481(float* __restrict__ __outs_0, float* __restrict__ __ins_0) noexcept{
  generic_val __tempargs0[2UL];
  __tempargs0[0UL] = __ins_0;
  __tempargs0[1UL] = __outs_0;
  sc_parallel_call_cpu_with_env((void*)&reorder__4810_closure_0_0wrapper, __stream, __module_data, 0UL, 32UL, 1UL, __tempargs0);
  return true;
}

static bool reorder__488(float* __restrict__ __outs_0, float* __restrict__ __ins_0) noexcept{
  generic_val __tempargs1[2UL];
  __tempargs1[0UL] = __ins_0;
  __tempargs1[1UL] = __outs_0;
  sc_parallel_call_cpu_with_env((void*)&reorder__4880_closure_1_0wrapper, __stream, __module_data, 0UL, 8UL, 1UL, __tempargs1);
  return true;
}

static bool reorder__504(float* __restrict__ __outs_0, float* __restrict__ __ins_0) noexcept{
  generic_val __tempargs2[2UL];
  __tempargs2[0UL] = __ins_0;
  __tempargs2[1UL] = __outs_0;
  sc_parallel_call_cpu_with_env((void*)&reorder__5040_closure_2_0wrapper, __stream, __module_data, 0UL, 8UL, 1UL, __tempargs2);
  return true;
}

static bool reorder__513(float* __restrict__ __outs_0, float* __restrict__ __ins_0) noexcept{
  generic_val __tempargs3[2UL];
  __tempargs3[0UL] = __ins_0;
  __tempargs3[1UL] = __outs_0;
  sc_parallel_call_cpu_with_env((void*)&reorder__5130_closure_3_0wrapper, __stream, __module_data, 0UL, 8UL, 1UL, __tempargs3);
  return true;
}

static bool reorder__520(float* __restrict__ __outs_0, float* __restrict__ __ins_0) noexcept{
  generic_val __tempargs4[2UL];
  __tempargs4[0UL] = __ins_0;
  __tempargs4[1UL] = __outs_0;
  sc_parallel_call_cpu_with_env((void*)&reorder__5200_closure_4_0wrapper, __stream, __module_data, 0UL, 8UL, 1UL, __tempargs4);
  return true;
}

static bool reorder__529(float* __restrict__ __outs_0, float* __restrict__ __ins_0) noexcept{
  generic_val __tempargs5[2UL];
  __tempargs5[0UL] = __ins_0;
  __tempargs5[1UL] = __outs_0;
  sc_parallel_call_cpu_with_env((void*)&reorder__5290_closure_5_0wrapper, __stream, __module_data, 0UL, 4UL, 1UL, __tempargs5);
  return true;
}

static bool reorder__420(float* __restrict__ __outs_0, float* __restrict__ __ins_0) noexcept{
  generic_val __tempargs6[2UL];
  __tempargs6[0UL] = __ins_0;
  __tempargs6[1UL] = __outs_0;
  sc_parallel_call_cpu_with_env((void*)&reorder__4200_closure_6_0wrapper, __stream, __module_data, 0UL, 16UL, 1UL, __tempargs6);
  return true;
}

static bool reorder__424(float* __restrict__ __outs_0, float* __restrict__ __ins_0) noexcept{
  generic_val __tempargs7[2UL];
  __tempargs7[0UL] = __ins_0;
  __tempargs7[1UL] = __outs_0;
  sc_parallel_call_cpu_with_env((void*)&reorder__4240_closure_7_0wrapper, __stream, __module_data, 0UL, 2UL, 1UL, __tempargs7);
  return true;
}

static bool reorder__427(float* __restrict__ __outs_0, float* __restrict__ __ins_0) noexcept{
  generic_val __tempargs8[2UL];
  __tempargs8[0UL] = __ins_0;
  __tempargs8[1UL] = __outs_0;
  sc_parallel_call_cpu_with_env((void*)&reorder__4270_closure_8_0wrapper, __stream, __module_data, 0UL, 4UL, 1UL, __tempargs8);
  return true;
}

static bool reorder__431(float* __restrict__ __outs_0, float* __restrict__ __ins_0) noexcept{
  generic_val __tempargs9[2UL];
  __tempargs9[0UL] = __ins_0;
  __tempargs9[1UL] = __outs_0;
  sc_parallel_call_cpu_with_env((void*)&reorder__4310_closure_9_0wrapper, __stream, __module_data, 0UL, 4UL, 1UL, __tempargs9);
  return true;
}

static bool reorder__434(float* __restrict__ __outs_0, float* __restrict__ __ins_0) noexcept{
  generic_val __tempargs10[2UL];
  __tempargs10[0UL] = __ins_0;
  __tempargs10[1UL] = __outs_0;
  sc_parallel_call_cpu_with_env((void*)&reorder__4340_closure_10_0wrapper, __stream, __module_data, 0UL, 8UL, 1UL, __tempargs10);
  return true;
}

static bool reorder__437(float* __restrict__ __outs_0, float* __restrict__ __ins_0) noexcept{
  generic_val __tempargs11[2UL];
  __tempargs11[0UL] = __ins_0;
  __tempargs11[1UL] = __outs_0;
  sc_parallel_call_cpu_with_env((void*)&reorder__4370_closure_11_0wrapper, __stream, __module_data, 0UL, 2UL, 1UL, __tempargs11);
  return true;
}

static bool reorder__440(float* __restrict__ __outs_0, float* __restrict__ __ins_0) noexcept{
  generic_val __tempargs12[2UL];
  __tempargs12[0UL] = __ins_0;
  __tempargs12[1UL] = __outs_0;
  sc_parallel_call_cpu_with_env((void*)&reorder__4400_closure_12_0wrapper, __stream, __module_data, 0UL, 2UL, 1UL, __tempargs12);
  return true;
}

static bool reorder__443(float* __restrict__ __outs_0, float* __restrict__ __ins_0) noexcept{
  generic_val __tempargs13[2UL];
  __tempargs13[0UL] = __ins_0;
  __tempargs13[1UL] = __outs_0;
  sc_parallel_call_cpu_with_env((void*)&reorder__4430_closure_13_0wrapper, __stream, __module_data, 0UL, 4UL, 1UL, __tempargs13);
  return true;
}

static bool reorder__446(float* __restrict__ __outs_0, float* __restrict__ __ins_0) noexcept{
  generic_val __tempargs14[2UL];
  __tempargs14[0UL] = __ins_0;
  __tempargs14[1UL] = __outs_0;
  sc_parallel_call_cpu_with_env((void*)&reorder__4460_closure_14_0wrapper, __stream, __module_data, 0UL, 16UL, 1UL, __tempargs14);
  return true;
}

static bool reorder__449(float* __restrict__ __outs_0, float* __restrict__ __ins_0) noexcept{
  generic_val __tempargs15[2UL];
  __tempargs15[0UL] = __ins_0;
  __tempargs15[1UL] = __outs_0;
  sc_parallel_call_cpu_with_env((void*)&reorder__4490_closure_15_0wrapper, __stream, __module_data, 0UL, 2UL, 1UL, __tempargs15);
  return true;
}

static bool reorder__452(float* __restrict__ __outs_0, float* __restrict__ __ins_0) noexcept{
  generic_val __tempargs16[2UL];
  __tempargs16[0UL] = __ins_0;
  __tempargs16[1UL] = __outs_0;
  sc_parallel_call_cpu_with_env((void*)&reorder__4520_closure_16_0wrapper, __stream, __module_data, 0UL, 4UL, 1UL, __tempargs16);
  return true;
}

static bool reorder__455(float* __restrict__ __outs_0, float* __restrict__ __ins_0) noexcept{
  generic_val __tempargs17[2UL];
  __tempargs17[0UL] = __ins_0;
  __tempargs17[1UL] = __outs_0;
  sc_parallel_call_cpu_with_env((void*)&reorder__4550_closure_17_0wrapper, __stream, __module_data, 0UL, 16UL, 1UL, __tempargs17);
  return true;
}

static bool reorder__458(float* __restrict__ __outs_0, float* __restrict__ __ins_0) noexcept{
  generic_val __tempargs18[2UL];
  __tempargs18[0UL] = __ins_0;
  __tempargs18[1UL] = __outs_0;
  sc_parallel_call_cpu_with_env((void*)&reorder__4580_closure_18_0wrapper, __stream, __module_data, 0UL, 4UL, 1UL, __tempargs18);
  return true;
}

static bool reorder__462(float* __restrict__ __outs_0, float* __restrict__ __ins_0) noexcept{
  generic_val __tempargs19[2UL];
  __tempargs19[0UL] = __ins_0;
  __tempargs19[1UL] = __outs_0;
  sc_parallel_call_cpu_with_env((void*)&reorder__4620_closure_19_0wrapper, __stream, __module_data, 0UL, 4UL, 1UL, __tempargs19);
  return true;
}

static bool reorder__466(float* __restrict__ __outs_0, float* __restrict__ __ins_0) noexcept{
  generic_val __tempargs20[2UL];
  __tempargs20[0UL] = __ins_0;
  __tempargs20[1UL] = __outs_0;
  sc_parallel_call_cpu_with_env((void*)&reorder__4660_closure_20_0wrapper, __stream, __module_data, 0UL, 4UL, 1UL, __tempargs20);
  return true;
}

static bool reorder__469(float* __restrict__ __outs_0, float* __restrict__ __ins_0) noexcept{
  generic_val __tempargs21[2UL];
  __tempargs21[0UL] = __ins_0;
  __tempargs21[1UL] = __outs_0;
  sc_parallel_call_cpu_with_env((void*)&reorder__4690_closure_21_0wrapper, __stream, __module_data, 0UL, 8UL, 1UL, __tempargs21);
  return true;
}

static bool reorder__472(float* __restrict__ __outs_0, float* __restrict__ __ins_0) noexcept{
  generic_val __tempargs22[2UL];
  __tempargs22[0UL] = __ins_0;
  __tempargs22[1UL] = __outs_0;
  sc_parallel_call_cpu_with_env((void*)&reorder__4720_closure_22_0wrapper, __stream, __module_data, 0UL, 2UL, 1UL, __tempargs22);
  return true;
}

static bool reorder__475(float* __restrict__ __outs_0, float* __restrict__ __ins_0) noexcept{
  generic_val __tempargs23[2UL];
  __tempargs23[0UL] = __ins_0;
  __tempargs23[1UL] = __outs_0;
  sc_parallel_call_cpu_with_env((void*)&reorder__4750_closure_23_0wrapper, __stream, __module_data, 0UL, 4UL, 1UL, __tempargs23);
  return true;
}

static bool reorder__478(float* __restrict__ __outs_0, float* __restrict__ __ins_0) noexcept{
  generic_val __tempargs24[2UL];
  __tempargs24[0UL] = __ins_0;
  __tempargs24[1UL] = __outs_0;
  sc_parallel_call_cpu_with_env((void*)&reorder__4780_closure_24_0wrapper, __stream, __module_data, 0UL, 4UL, 1UL, __tempargs24);
  return true;
}

static bool reorder__485(float* __restrict__ __outs_0, float* __restrict__ __ins_0) noexcept{
  generic_val __tempargs25[2UL];
  __tempargs25[0UL] = __ins_0;
  __tempargs25[1UL] = __outs_0;
  sc_parallel_call_cpu_with_env((void*)&reorder__4850_closure_25_0wrapper, __stream, __module_data, 0UL, 2UL, 1UL, __tempargs25);
  return true;
}

static bool reorder__491(float* __restrict__ __outs_0, float* __restrict__ __ins_0) noexcept{
  generic_val __tempargs26[2UL];
  __tempargs26[0UL] = __ins_0;
  __tempargs26[1UL] = __outs_0;
  sc_parallel_call_cpu_with_env((void*)&reorder__4910_closure_26_0wrapper, __stream, __module_data, 0UL, 4UL, 1UL, __tempargs26);
  return true;
}

static bool reorder__494(float* __restrict__ __outs_0, float* __restrict__ __ins_0) noexcept{
  generic_val __tempargs27[2UL];
  __tempargs27[0UL] = __ins_0;
  __tempargs27[1UL] = __outs_0;
  sc_parallel_call_cpu_with_env((void*)&reorder__4940_closure_27_0wrapper, __stream, __module_data, 0UL, 16UL, 1UL, __tempargs27);
  return true;
}

static bool reorder__498(float* __restrict__ __outs_0, float* __restrict__ __ins_0) noexcept{
  generic_val __tempargs28[2UL];
  __tempargs28[0UL] = __ins_0;
  __tempargs28[1UL] = __outs_0;
  sc_parallel_call_cpu_with_env((void*)&reorder__4980_closure_28_0wrapper, __stream, __module_data, 0UL, 8UL, 1UL, __tempargs28);
  return true;
}

static bool reorder__501(float* __restrict__ __outs_0, float* __restrict__ __ins_0) noexcept{
  generic_val __tempargs29[2UL];
  __tempargs29[0UL] = __ins_0;
  __tempargs29[1UL] = __outs_0;
  sc_parallel_call_cpu_with_env((void*)&reorder__5010_closure_29_0wrapper, __stream, __module_data, 0UL, 8UL, 1UL, __tempargs29);
  return true;
}

static bool reorder__507(float* __restrict__ __outs_0, float* __restrict__ __ins_0) noexcept{
  generic_val __tempargs30[2UL];
  __tempargs30[0UL] = __ins_0;
  __tempargs30[1UL] = __outs_0;
  sc_parallel_call_cpu_with_env((void*)&reorder__5070_closure_30_0wrapper, __stream, __module_data, 0UL, 4UL, 1UL, __tempargs30);
  return true;
}

static bool reorder__510(float* __restrict__ __outs_0, float* __restrict__ __ins_0) noexcept{
  generic_val __tempargs31[2UL];
  __tempargs31[0UL] = __ins_0;
  __tempargs31[1UL] = __outs_0;
  sc_parallel_call_cpu_with_env((void*)&reorder__5100_closure_31_0wrapper, __stream, __module_data, 0UL, 4UL, 1UL, __tempargs31);
  return true;
}

static bool reorder__517(float* __restrict__ __outs_0, float* __restrict__ __ins_0) noexcept{
  generic_val __tempargs32[2UL];
  __tempargs32[0UL] = __ins_0;
  __tempargs32[1UL] = __outs_0;
  sc_parallel_call_cpu_with_env((void*)&reorder__5170_closure_32_0wrapper, __stream, __module_data, 0UL, 8UL, 1UL, __tempargs32);
  return true;
}

static bool reorder__523(float* __restrict__ __outs_0, float* __restrict__ __ins_0) noexcept{
  generic_val __tempargs33[2UL];
  __tempargs33[0UL] = __ins_0;
  __tempargs33[1UL] = __outs_0;
  sc_parallel_call_cpu_with_env((void*)&reorder__5230_closure_33_0wrapper, __stream, __module_data, 0UL, 8UL, 1UL, __tempargs33);
  return true;
}

static bool reorder__526(float* __restrict__ __outs_0, float* __restrict__ __ins_0) noexcept{
  generic_val __tempargs34[2UL];
  __tempargs34[0UL] = __ins_0;
  __tempargs34[1UL] = __outs_0;
  sc_parallel_call_cpu_with_env((void*)&reorder__5260_closure_34_0wrapper, __stream, __module_data, 0UL, 4UL, 1UL, __tempargs34);
  return true;
}

static bool reorder__532(float* __restrict__ __outs_0, float* __restrict__ __ins_0) noexcept{
  generic_val __tempargs35[2UL];
  __tempargs35[0UL] = __ins_0;
  __tempargs35[1UL] = __outs_0;
  sc_parallel_call_cpu_with_env((void*)&reorder__5320_closure_35_0wrapper, __stream, __module_data, 0UL, 128UL, 1UL, __tempargs35);
  return true;
}

static bool reorder__535(float* __restrict__ __outs_0, float* __restrict__ __ins_0) noexcept{
  generic_val __tempargs36[2UL];
  __tempargs36[0UL] = __ins_0;
  __tempargs36[1UL] = __outs_0;
  sc_parallel_call_cpu_with_env((void*)&reorder__5350_closure_36_0wrapper, __stream, __module_data, 0UL, 8UL, 1UL, __tempargs36);
  return true;
}

static bool reorder__538(float* __restrict__ __outs_0, float* __restrict__ __ins_0) noexcept{
  generic_val __tempargs37[2UL];
  __tempargs37[0UL] = __ins_0;
  __tempargs37[1UL] = __outs_0;
  sc_parallel_call_cpu_with_env((void*)&reorder__5380_closure_37_0wrapper, __stream, __module_data, 0UL, 4UL, 1UL, __tempargs37);
  return true;
}

static bool reorder__541(float* __restrict__ __outs_0, float* __restrict__ __ins_0) noexcept{
  generic_val __tempargs38[2UL];
  __tempargs38[0UL] = __ins_0;
  __tempargs38[1UL] = __outs_0;
  sc_parallel_call_cpu_with_env((void*)&reorder__5410_closure_38_0wrapper, __stream, __module_data, 0UL, 32UL, 1UL, __tempargs38);
  return true;
}

static bool reorder__544(float* __restrict__ __outs_0, float* __restrict__ __ins_0) noexcept{
  generic_val __tempargs39[2UL];
  __tempargs39[0UL] = __ins_0;
  __tempargs39[1UL] = __outs_0;
  sc_parallel_call_cpu_with_env((void*)&reorder__5440_closure_39_0wrapper, __stream, __module_data, 0UL, 4UL, 1UL, __tempargs39);
  return true;
}

static bool reorder__547(float* __restrict__ __outs_0, float* __restrict__ __ins_0) noexcept{
  generic_val __tempargs40[2UL];
  __tempargs40[0UL] = __ins_0;
  __tempargs40[1UL] = __outs_0;
  sc_parallel_call_cpu_with_env((void*)&reorder__5470_closure_40_0wrapper, __stream, __module_data, 0UL, 32UL, 1UL, __tempargs40);
  return true;
}

static bool reorder__550(float* __restrict__ __outs_0, float* __restrict__ __ins_0) noexcept{
  generic_val __tempargs41[2UL];
  __tempargs41[0UL] = __ins_0;
  __tempargs41[1UL] = __outs_0;
  sc_parallel_call_cpu_with_env((void*)&reorder__5500_closure_41_0wrapper, __stream, __module_data, 0UL, 32UL, 1UL, __tempargs41);
  return true;
}

static bool reorder__553(float* __restrict__ __outs_0, float* __restrict__ __ins_0) noexcept{
  generic_val __tempargs42[2UL];
  __tempargs42[0UL] = __ins_0;
  __tempargs42[1UL] = __outs_0;
  sc_parallel_call_cpu_with_env((void*)&reorder__5530_closure_42_0wrapper, __stream, __module_data, 0UL, 16UL, 1UL, __tempargs42);
  return true;
}

static bool reorder__556(float* __restrict__ __outs_0, float* __restrict__ __ins_0) noexcept{
  generic_val __tempargs43[2UL];
  __tempargs43[0UL] = __ins_0;
  __tempargs43[1UL] = __outs_0;
  sc_parallel_call_cpu_with_env((void*)&reorder__5560_closure_43_0wrapper, __stream, __module_data, 0UL, 4UL, 1UL, __tempargs43);
  return true;
}

static bool reorder__559(float* __restrict__ __outs_0, float* __restrict__ __ins_0) noexcept{
  generic_val __tempargs44[2UL];
  __tempargs44[0UL] = __ins_0;
  __tempargs44[1UL] = __outs_0;
  sc_parallel_call_cpu_with_env((void*)&reorder__5590_closure_44_0wrapper, __stream, __module_data, 0UL, 4UL, 1UL, __tempargs44);
  return true;
}

static bool reorder__425(float* __restrict__ __outs_0, float* __restrict__ __ins_0) noexcept{
  generic_val __tempargs45[2UL];
  __tempargs45[0UL] = __ins_0;
  __tempargs45[1UL] = __outs_0;
  sc_parallel_call_cpu_with_env((void*)&reorder__4250_closure_45_0wrapper, __stream, __module_data, 0UL, 2UL, 1UL, __tempargs45);
  return true;
}

static bool reorder__432(float* __restrict__ __outs_0, float* __restrict__ __ins_0) noexcept{
  generic_val __tempargs46[2UL];
  __tempargs46[0UL] = __ins_0;
  __tempargs46[1UL] = __outs_0;
  sc_parallel_call_cpu_with_env((void*)&reorder__4320_closure_46_0wrapper, __stream, __module_data, 0UL, 4UL, 1UL, __tempargs46);
  return true;
}

static bool reorder__438(float* __restrict__ __outs_0, float* __restrict__ __ins_0) noexcept{
  generic_val __tempargs47[2UL];
  __tempargs47[0UL] = __ins_0;
  __tempargs47[1UL] = __outs_0;
  sc_parallel_call_cpu_with_env((void*)&reorder__4380_closure_47_0wrapper, __stream, __module_data, 0UL, 2UL, 1UL, __tempargs47);
  return true;
}

static bool reorder__441(float* __restrict__ __outs_0, float* __restrict__ __ins_0) noexcept{
  generic_val __tempargs48[2UL];
  __tempargs48[0UL] = __ins_0;
  __tempargs48[1UL] = __outs_0;
  sc_parallel_call_cpu_with_env((void*)&reorder__4410_closure_48_0wrapper, __stream, __module_data, 0UL, 2UL, 1UL, __tempargs48);
  return true;
}

static bool reorder__450(float* __restrict__ __outs_0, float* __restrict__ __ins_0) noexcept{
  generic_val __tempargs49[2UL];
  __tempargs49[0UL] = __ins_0;
  __tempargs49[1UL] = __outs_0;
  sc_parallel_call_cpu_with_env((void*)&reorder__4500_closure_49_0wrapper, __stream, __module_data, 0UL, 2UL, 1UL, __tempargs49);
  return true;
}

static bool reorder__453(float* __restrict__ __outs_0, float* __restrict__ __ins_0) noexcept{
  generic_val __tempargs50[2UL];
  __tempargs50[0UL] = __ins_0;
  __tempargs50[1UL] = __outs_0;
  sc_parallel_call_cpu_with_env((void*)&reorder__4530_closure_50_0wrapper, __stream, __module_data, 0UL, 4UL, 1UL, __tempargs50);
  return true;
}

static bool reorder__459(float* __restrict__ __outs_0, float* __restrict__ __ins_0) noexcept{
  generic_val __tempargs51[2UL];
  __tempargs51[0UL] = __ins_0;
  __tempargs51[1UL] = __outs_0;
  sc_parallel_call_cpu_with_env((void*)&reorder__4590_closure_51_0wrapper, __stream, __module_data, 0UL, 4UL, 1UL, __tempargs51);
  return true;
}

static bool reorder__467(float* __restrict__ __outs_0, float* __restrict__ __ins_0) noexcept{
  generic_val __tempargs52[2UL];
  __tempargs52[0UL] = __ins_0;
  __tempargs52[1UL] = __outs_0;
  sc_parallel_call_cpu_with_env((void*)&reorder__4670_closure_52_0wrapper, __stream, __module_data, 0UL, 4UL, 1UL, __tempargs52);
  return true;
}

static bool reorder__473(float* __restrict__ __outs_0, float* __restrict__ __ins_0) noexcept{
  generic_val __tempargs53[2UL];
  __tempargs53[0UL] = __ins_0;
  __tempargs53[1UL] = __outs_0;
  sc_parallel_call_cpu_with_env((void*)&reorder__4730_closure_53_0wrapper, __stream, __module_data, 0UL, 2UL, 1UL, __tempargs53);
  return true;
}

static bool reorder__476(float* __restrict__ __outs_0, float* __restrict__ __ins_0) noexcept{
  generic_val __tempargs54[2UL];
  __tempargs54[0UL] = __ins_0;
  __tempargs54[1UL] = __outs_0;
  sc_parallel_call_cpu_with_env((void*)&reorder__4760_closure_54_0wrapper, __stream, __module_data, 0UL, 4UL, 1UL, __tempargs54);
  return true;
}

static bool reorder__421(float* __restrict__ __outs_0, float* __restrict__ __ins_0) noexcept{
  generic_val __tempargs55[2UL];
  __tempargs55[0UL] = __ins_0;
  __tempargs55[1UL] = __outs_0;
  sc_parallel_call_cpu_with_env((void*)&reorder__4210_closure_55_0wrapper, __stream, __module_data, 0UL, 16UL, 1UL, __tempargs55);
  return true;
}

static bool reorder__428(float* __restrict__ __outs_0, float* __restrict__ __ins_0) noexcept{
  generic_val __tempargs56[2UL];
  __tempargs56[0UL] = __ins_0;
  __tempargs56[1UL] = __outs_0;
  sc_parallel_call_cpu_with_env((void*)&reorder__4280_closure_56_0wrapper, __stream, __module_data, 0UL, 4UL, 1UL, __tempargs56);
  return true;
}

static bool reorder__435(float* __restrict__ __outs_0, float* __restrict__ __ins_0) noexcept{
  generic_val __tempargs57[2UL];
  __tempargs57[0UL] = __ins_0;
  __tempargs57[1UL] = __outs_0;
  sc_parallel_call_cpu_with_env((void*)&reorder__4350_closure_57_0wrapper, __stream, __module_data, 0UL, 8UL, 1UL, __tempargs57);
  return true;
}

static bool reorder__444(float* __restrict__ __outs_0, float* __restrict__ __ins_0) noexcept{
  generic_val __tempargs58[2UL];
  __tempargs58[0UL] = __ins_0;
  __tempargs58[1UL] = __outs_0;
  sc_parallel_call_cpu_with_env((void*)&reorder__4440_closure_58_0wrapper, __stream, __module_data, 0UL, 4UL, 1UL, __tempargs58);
  return true;
}

static bool reorder__486(float* __restrict__ __outs_0, float* __restrict__ __ins_0) noexcept{
  generic_val __tempargs59[2UL];
  __tempargs59[0UL] = __ins_0;
  __tempargs59[1UL] = __outs_0;
  sc_parallel_call_cpu_with_env((void*)&reorder__4860_closure_59_0wrapper, __stream, __module_data, 0UL, 2UL, 1UL, __tempargs59);
  return true;
}

static bool reorder__492(float* __restrict__ __outs_0, float* __restrict__ __ins_0) noexcept{
  generic_val __tempargs60[2UL];
  __tempargs60[0UL] = __ins_0;
  __tempargs60[1UL] = __outs_0;
  sc_parallel_call_cpu_with_env((void*)&reorder__4920_closure_60_0wrapper, __stream, __module_data, 0UL, 4UL, 1UL, __tempargs60);
  return true;
}

static bool reorder__495(float* __restrict__ __outs_0, float* __restrict__ __ins_0) noexcept{
  generic_val __tempargs61[2UL];
  __tempargs61[0UL] = __ins_0;
  __tempargs61[1UL] = __outs_0;
  sc_parallel_call_cpu_with_env((void*)&reorder__4950_closure_61_0wrapper, __stream, __module_data, 0UL, 16UL, 1UL, __tempargs61);
  return true;
}

static bool reorder__499(float* __restrict__ __outs_0, float* __restrict__ __ins_0) noexcept{
  generic_val __tempargs62[2UL];
  __tempargs62[0UL] = __ins_0;
  __tempargs62[1UL] = __outs_0;
  sc_parallel_call_cpu_with_env((void*)&reorder__4990_closure_62_0wrapper, __stream, __module_data, 0UL, 8UL, 1UL, __tempargs62);
  return true;
}

static bool reorder__502(float* __restrict__ __outs_0, float* __restrict__ __ins_0) noexcept{
  generic_val __tempargs63[2UL];
  __tempargs63[0UL] = __ins_0;
  __tempargs63[1UL] = __outs_0;
  sc_parallel_call_cpu_with_env((void*)&reorder__5020_closure_63_0wrapper, __stream, __module_data, 0UL, 8UL, 1UL, __tempargs63);
  return true;
}

static bool reorder__508(float* __restrict__ __outs_0, float* __restrict__ __ins_0) noexcept{
  generic_val __tempargs64[2UL];
  __tempargs64[0UL] = __ins_0;
  __tempargs64[1UL] = __outs_0;
  sc_parallel_call_cpu_with_env((void*)&reorder__5080_closure_64_0wrapper, __stream, __module_data, 0UL, 4UL, 1UL, __tempargs64);
  return true;
}

static bool reorder__511(float* __restrict__ __outs_0, float* __restrict__ __ins_0) noexcept{
  generic_val __tempargs65[2UL];
  __tempargs65[0UL] = __ins_0;
  __tempargs65[1UL] = __outs_0;
  sc_parallel_call_cpu_with_env((void*)&reorder__5110_closure_65_0wrapper, __stream, __module_data, 0UL, 4UL, 1UL, __tempargs65);
  return true;
}

static bool reorder__518(float* __restrict__ __outs_0, float* __restrict__ __ins_0) noexcept{
  generic_val __tempargs66[2UL];
  __tempargs66[0UL] = __ins_0;
  __tempargs66[1UL] = __outs_0;
  sc_parallel_call_cpu_with_env((void*)&reorder__5180_closure_66_0wrapper, __stream, __module_data, 0UL, 8UL, 1UL, __tempargs66);
  return true;
}

static bool reorder__524(float* __restrict__ __outs_0, float* __restrict__ __ins_0) noexcept{
  generic_val __tempargs67[2UL];
  __tempargs67[0UL] = __ins_0;
  __tempargs67[1UL] = __outs_0;
  sc_parallel_call_cpu_with_env((void*)&reorder__5240_closure_67_0wrapper, __stream, __module_data, 0UL, 8UL, 1UL, __tempargs67);
  return true;
}

static bool reorder__527(float* __restrict__ __outs_0, float* __restrict__ __ins_0) noexcept{
  generic_val __tempargs68[2UL];
  __tempargs68[0UL] = __ins_0;
  __tempargs68[1UL] = __outs_0;
  sc_parallel_call_cpu_with_env((void*)&reorder__5270_closure_68_0wrapper, __stream, __module_data, 0UL, 4UL, 1UL, __tempargs68);
  return true;
}

static bool reorder__447(float* __restrict__ __outs_0, float* __restrict__ __ins_0) noexcept{
  generic_val __tempargs69[2UL];
  __tempargs69[0UL] = __ins_0;
  __tempargs69[1UL] = __outs_0;
  sc_parallel_call_cpu_with_env((void*)&reorder__4470_closure_69_0wrapper, __stream, __module_data, 0UL, 16UL, 1UL, __tempargs69);
  return true;
}

static bool reorder__456(float* __restrict__ __outs_0, float* __restrict__ __ins_0) noexcept{
  generic_val __tempargs70[2UL];
  __tempargs70[0UL] = __ins_0;
  __tempargs70[1UL] = __outs_0;
  sc_parallel_call_cpu_with_env((void*)&reorder__4560_closure_70_0wrapper, __stream, __module_data, 0UL, 16UL, 1UL, __tempargs70);
  return true;
}

static bool reorder__463(float* __restrict__ __outs_0, float* __restrict__ __ins_0) noexcept{
  generic_val __tempargs71[2UL];
  __tempargs71[0UL] = __ins_0;
  __tempargs71[1UL] = __outs_0;
  sc_parallel_call_cpu_with_env((void*)&reorder__4630_closure_71_0wrapper, __stream, __module_data, 0UL, 4UL, 1UL, __tempargs71);
  return true;
}

static bool reorder__470(float* __restrict__ __outs_0, float* __restrict__ __ins_0) noexcept{
  generic_val __tempargs72[2UL];
  __tempargs72[0UL] = __ins_0;
  __tempargs72[1UL] = __outs_0;
  sc_parallel_call_cpu_with_env((void*)&reorder__4700_closure_72_0wrapper, __stream, __module_data, 0UL, 8UL, 1UL, __tempargs72);
  return true;
}

static bool reorder__479(float* __restrict__ __outs_0, float* __restrict__ __ins_0) noexcept{
  generic_val __tempargs73[2UL];
  __tempargs73[0UL] = __ins_0;
  __tempargs73[1UL] = __outs_0;
  sc_parallel_call_cpu_with_env((void*)&reorder__4790_closure_73_0wrapper, __stream, __module_data, 0UL, 4UL, 1UL, __tempargs73);
  return true;
}

static bool reorder__536(float* __restrict__ __outs_0, float* __restrict__ __ins_0) noexcept{
  generic_val __tempargs74[2UL];
  __tempargs74[0UL] = __ins_0;
  __tempargs74[1UL] = __outs_0;
  sc_parallel_call_cpu_with_env((void*)&reorder__5360_closure_74_0wrapper, __stream, __module_data, 0UL, 8UL, 1UL, __tempargs74);
  return true;
}

static bool reorder__539(float* __restrict__ __outs_0, float* __restrict__ __ins_0) noexcept{
  generic_val __tempargs75[2UL];
  __tempargs75[0UL] = __ins_0;
  __tempargs75[1UL] = __outs_0;
  sc_parallel_call_cpu_with_env((void*)&reorder__5390_closure_75_0wrapper, __stream, __module_data, 0UL, 4UL, 1UL, __tempargs75);
  return true;
}

static bool reorder__545(float* __restrict__ __outs_0, float* __restrict__ __ins_0) noexcept{
  generic_val __tempargs76[2UL];
  __tempargs76[0UL] = __ins_0;
  __tempargs76[1UL] = __outs_0;
  sc_parallel_call_cpu_with_env((void*)&reorder__5450_closure_76_0wrapper, __stream, __module_data, 0UL, 4UL, 1UL, __tempargs76);
  return true;
}

static bool reorder__548(float* __restrict__ __outs_0, float* __restrict__ __ins_0) noexcept{
  generic_val __tempargs77[2UL];
  __tempargs77[0UL] = __ins_0;
  __tempargs77[1UL] = __outs_0;
  sc_parallel_call_cpu_with_env((void*)&reorder__5480_closure_77_0wrapper, __stream, __module_data, 0UL, 32UL, 1UL, __tempargs77);
  return true;
}

static bool reorder__554(float* __restrict__ __outs_0, float* __restrict__ __ins_0) noexcept{
  generic_val __tempargs78[2UL];
  __tempargs78[0UL] = __ins_0;
  __tempargs78[1UL] = __outs_0;
  sc_parallel_call_cpu_with_env((void*)&reorder__5540_closure_78_0wrapper, __stream, __module_data, 0UL, 16UL, 1UL, __tempargs78);
  return true;
}

static bool reorder__557(float* __restrict__ __outs_0, float* __restrict__ __ins_0) noexcept{
  generic_val __tempargs79[2UL];
  __tempargs79[0UL] = __ins_0;
  __tempargs79[1UL] = __outs_0;
  sc_parallel_call_cpu_with_env((void*)&reorder__5570_closure_79_0wrapper, __stream, __module_data, 0UL, 4UL, 1UL, __tempargs79);
  return true;
}

static bool reorder__482(float* __restrict__ __outs_0, float* __restrict__ __ins_0) noexcept{
  generic_val __tempargs80[2UL];
  __tempargs80[0UL] = __ins_0;
  __tempargs80[1UL] = __outs_0;
  sc_parallel_call_cpu_with_env((void*)&reorder__4820_closure_80_0wrapper, __stream, __module_data, 0UL, 32UL, 1UL, __tempargs80);
  return true;
}

static bool reorder__489(float* __restrict__ __outs_0, float* __restrict__ __ins_0) noexcept{
  generic_val __tempargs81[2UL];
  __tempargs81[0UL] = __ins_0;
  __tempargs81[1UL] = __outs_0;
  sc_parallel_call_cpu_with_env((void*)&reorder__4890_closure_81_0wrapper, __stream, __module_data, 0UL, 8UL, 1UL, __tempargs81);
  return true;
}

static bool reorder__505(float* __restrict__ __outs_0, float* __restrict__ __ins_0) noexcept{
  generic_val __tempargs82[2UL];
  __tempargs82[0UL] = __ins_0;
  __tempargs82[1UL] = __outs_0;
  sc_parallel_call_cpu_with_env((void*)&reorder__5050_closure_82_0wrapper, __stream, __module_data, 0UL, 8UL, 1UL, __tempargs82);
  return true;
}

static bool reorder__514(float* __restrict__ __outs_0, float* __restrict__ __ins_0) noexcept{
  generic_val __tempargs83[2UL];
  __tempargs83[0UL] = __ins_0;
  __tempargs83[1UL] = __outs_0;
  sc_parallel_call_cpu_with_env((void*)&reorder__5140_closure_83_0wrapper, __stream, __module_data, 0UL, 8UL, 1UL, __tempargs83);
  return true;
}

static bool reorder__521(float* __restrict__ __outs_0, float* __restrict__ __ins_0) noexcept{
  generic_val __tempargs84[2UL];
  __tempargs84[0UL] = __ins_0;
  __tempargs84[1UL] = __outs_0;
  sc_parallel_call_cpu_with_env((void*)&reorder__5210_closure_84_0wrapper, __stream, __module_data, 0UL, 8UL, 1UL, __tempargs84);
  return true;
}

static bool reorder__530(float* __restrict__ __outs_0, float* __restrict__ __ins_0) noexcept{
  generic_val __tempargs85[2UL];
  __tempargs85[0UL] = __ins_0;
  __tempargs85[1UL] = __outs_0;
  sc_parallel_call_cpu_with_env((void*)&reorder__5300_closure_85_0wrapper, __stream, __module_data, 0UL, 4UL, 1UL, __tempargs85);
  return true;
}

static bool reorder__533(float* __restrict__ __outs_0, float* __restrict__ __ins_0) noexcept{
  generic_val __tempargs86[2UL];
  __tempargs86[0UL] = __ins_0;
  __tempargs86[1UL] = __outs_0;
  sc_parallel_call_cpu_with_env((void*)&reorder__5330_closure_86_0wrapper, __stream, __module_data, 0UL, 128UL, 1UL, __tempargs86);
  return true;
}

static bool reorder__542(float* __restrict__ __outs_0, float* __restrict__ __ins_0) noexcept{
  generic_val __tempargs87[2UL];
  __tempargs87[0UL] = __ins_0;
  __tempargs87[1UL] = __outs_0;
  sc_parallel_call_cpu_with_env((void*)&reorder__5420_closure_87_0wrapper, __stream, __module_data, 0UL, 32UL, 1UL, __tempargs87);
  return true;
}

static bool reorder__551(float* __restrict__ __outs_0, float* __restrict__ __ins_0) noexcept{
  generic_val __tempargs88[2UL];
  __tempargs88[0UL] = __ins_0;
  __tempargs88[1UL] = __outs_0;
  sc_parallel_call_cpu_with_env((void*)&reorder__5510_closure_88_0wrapper, __stream, __module_data, 0UL, 32UL, 1UL, __tempargs88);
  return true;
}

static bool reorder__560(float* __restrict__ __outs_0, float* __restrict__ __ins_0) noexcept{
  generic_val __tempargs89[2UL];
  __tempargs89[0UL] = __ins_0;
  __tempargs89[1UL] = __outs_0;
  sc_parallel_call_cpu_with_env((void*)&reorder__5600_closure_89_0wrapper, __stream, __module_data, 0UL, 4UL, 1UL, __tempargs89);
  return true;
}

static bool mul__111(float* __restrict__ __outs_0, float* __restrict__ __ins_0, float* __restrict__ __ins_1) noexcept{
  generic_val __tempargs90[3UL];
  __tempargs90[0UL] = __ins_0;
  __tempargs90[1UL] = __ins_1;
  __tempargs90[2UL] = __outs_0;
  sc_parallel_call_cpu_with_env((void*)&mul__1110_closure_90_0wrapper, __stream, __module_data, 0UL, 4096UL, 1UL, __tempargs90);
  return true;
}

static bool cast__112(int8_t* __restrict__ __outs_0, float* __restrict__ __ins_0) noexcept{
  generic_val __tempargs91[2UL];
  __tempargs91[0UL] = __ins_0;
  __tempargs91[1UL] = __outs_0;
  sc_parallel_call_cpu_with_env((void*)&cast__1120_closure_91_0wrapper, __stream, __module_data, 0UL, 4096UL, 1UL, __tempargs91);
  return true;
}

static bool mul__108(float* __restrict__ __outs_0, float* __restrict__ __ins_0, float* __restrict__ __ins_1) noexcept{
  generic_val __tempargs92[3UL];
  __tempargs92[0UL] = __ins_0;
  __tempargs92[1UL] = __ins_1;
  __tempargs92[2UL] = __outs_0;
  sc_parallel_call_cpu_with_env((void*)&mul__1080_closure_92_0wrapper, __stream, __module_data, 0UL, 16384UL, 1UL, __tempargs92);
  return true;
}

static bool cast__109(int8_t* __restrict__ __outs_0, float* __restrict__ __ins_0) noexcept{
  generic_val __tempargs93[2UL];
  __tempargs93[0UL] = __ins_0;
  __tempargs93[1UL] = __outs_0;
  sc_parallel_call_cpu_with_env((void*)&cast__1090_closure_93_0wrapper, __stream, __module_data, 0UL, 16384UL, 1UL, __tempargs93);
  return true;
}

static bool mul__117(float* __restrict__ __outs_0, float* __restrict__ __ins_0, float* __restrict__ __ins_1) noexcept{
  generic_val __tempargs94[3UL];
  __tempargs94[0UL] = __ins_0;
  __tempargs94[1UL] = __ins_1;
  __tempargs94[2UL] = __outs_0;
  sc_parallel_call_cpu_with_env((void*)&mul__1170_closure_94_0wrapper, __stream, __module_data, 0UL, 16384UL, 1UL, __tempargs94);
  return true;
}

static bool cast__118(int8_t* __restrict__ __outs_0, float* __restrict__ __ins_0) noexcept{
  generic_val __tempargs95[2UL];
  __tempargs95[0UL] = __ins_0;
  __tempargs95[1UL] = __outs_0;
  sc_parallel_call_cpu_with_env((void*)&cast__1180_closure_95_0wrapper, __stream, __module_data, 0UL, 16384UL, 1UL, __tempargs95);
  return true;
}

static bool mul__126(float* __restrict__ __outs_0, float* __restrict__ __ins_0, float* __restrict__ __ins_1) noexcept{
  generic_val __tempargs96[3UL];
  __tempargs96[0UL] = __ins_0;
  __tempargs96[1UL] = __ins_1;
  __tempargs96[2UL] = __outs_0;
  sc_parallel_call_cpu_with_env((void*)&mul__1260_closure_96_0wrapper, __stream, __module_data, 0UL, 16384UL, 1UL, __tempargs96);
  return true;
}

static bool cast__127(int8_t* __restrict__ __outs_0, float* __restrict__ __ins_0) noexcept{
  generic_val __tempargs97[2UL];
  __tempargs97[0UL] = __ins_0;
  __tempargs97[1UL] = __outs_0;
  sc_parallel_call_cpu_with_env((void*)&cast__1270_closure_97_0wrapper, __stream, __module_data, 0UL, 16384UL, 1UL, __tempargs97);
  return true;
}

static bool mul__135(float* __restrict__ __outs_0, float* __restrict__ __ins_0, float* __restrict__ __ins_1) noexcept{
  generic_val __tempargs98[3UL];
  __tempargs98[0UL] = __ins_0;
  __tempargs98[1UL] = __ins_1;
  __tempargs98[2UL] = __outs_0;
  sc_parallel_call_cpu_with_env((void*)&mul__1350_closure_98_0wrapper, __stream, __module_data, 0UL, 16384UL, 1UL, __tempargs98);
  return true;
}

static bool cast__136(int8_t* __restrict__ __outs_0, float* __restrict__ __ins_0) noexcept{
  generic_val __tempargs99[2UL];
  __tempargs99[0UL] = __ins_0;
  __tempargs99[1UL] = __outs_0;
  sc_parallel_call_cpu_with_env((void*)&cast__1360_closure_99_0wrapper, __stream, __module_data, 0UL, 16384UL, 1UL, __tempargs99);
  return true;
}

static bool mul__120(float* __restrict__ __outs_0, float* __restrict__ __ins_0, float* __restrict__ __ins_1) noexcept{
  generic_val __tempargs100[3UL];
  __tempargs100[0UL] = __ins_0;
  __tempargs100[1UL] = __ins_1;
  __tempargs100[2UL] = __outs_0;
  sc_parallel_call_cpu_with_env((void*)&mul__1200_closure_100_0wrapper, __stream, __module_data, 0UL, 16384UL, 1UL, __tempargs100);
  return true;
}

static bool cast__121(int8_t* __restrict__ __outs_0, float* __restrict__ __ins_0) noexcept{
  generic_val __tempargs101[2UL];
  __tempargs101[0UL] = __ins_0;
  __tempargs101[1UL] = __outs_0;
  sc_parallel_call_cpu_with_env((void*)&cast__1210_closure_101_0wrapper, __stream, __module_data, 0UL, 16384UL, 1UL, __tempargs101);
  return true;
}

static bool mul__129(float* __restrict__ __outs_0, float* __restrict__ __ins_0, float* __restrict__ __ins_1) noexcept{
  generic_val __tempargs102[3UL];
  __tempargs102[0UL] = __ins_0;
  __tempargs102[1UL] = __ins_1;
  __tempargs102[2UL] = __outs_0;
  sc_parallel_call_cpu_with_env((void*)&mul__1290_closure_102_0wrapper, __stream, __module_data, 0UL, 16384UL, 1UL, __tempargs102);
  return true;
}

static bool cast__130(int8_t* __restrict__ __outs_0, float* __restrict__ __ins_0) noexcept{
  generic_val __tempargs103[2UL];
  __tempargs103[0UL] = __ins_0;
  __tempargs103[1UL] = __outs_0;
  sc_parallel_call_cpu_with_env((void*)&cast__1300_closure_103_0wrapper, __stream, __module_data, 0UL, 16384UL, 1UL, __tempargs103);
  return true;
}

static bool mul__141(float* __restrict__ __outs_0, float* __restrict__ __ins_0, float* __restrict__ __ins_1) noexcept{
  generic_val __tempargs104[3UL];
  __tempargs104[0UL] = __ins_0;
  __tempargs104[1UL] = __ins_1;
  __tempargs104[2UL] = __outs_0;
  sc_parallel_call_cpu_with_env((void*)&mul__1410_closure_104_0wrapper, __stream, __module_data, 0UL, 32768UL, 1UL, __tempargs104);
  return true;
}

static bool cast__142(int8_t* __restrict__ __outs_0, float* __restrict__ __ins_0) noexcept{
  generic_val __tempargs105[2UL];
  __tempargs105[0UL] = __ins_0;
  __tempargs105[1UL] = __outs_0;
  sc_parallel_call_cpu_with_env((void*)&cast__1420_closure_105_0wrapper, __stream, __module_data, 0UL, 32768UL, 1UL, __tempargs105);
  return true;
}

static bool mul__114(float* __restrict__ __outs_0, float* __restrict__ __ins_0, float* __restrict__ __ins_1) noexcept{
  generic_val __tempargs106[3UL];
  __tempargs106[0UL] = __ins_0;
  __tempargs106[1UL] = __ins_1;
  __tempargs106[2UL] = __outs_0;
  sc_parallel_call_cpu_with_env((void*)&mul__1140_closure_106_0wrapper, __stream, __module_data, 0UL, 12288UL, 1UL, __tempargs106);
  return true;
}

static bool cast__115(int8_t* __restrict__ __outs_0, float* __restrict__ __ins_0) noexcept{
  generic_val __tempargs107[2UL];
  __tempargs107[0UL] = __ins_0;
  __tempargs107[1UL] = __outs_0;
  sc_parallel_call_cpu_with_env((void*)&cast__1150_closure_107_0wrapper, __stream, __module_data, 0UL, 12288UL, 1UL, __tempargs107);
  return true;
}

static bool mul__123(float* __restrict__ __outs_0, float* __restrict__ __ins_0, float* __restrict__ __ins_1) noexcept{
  generic_val __tempargs108[3UL];
  __tempargs108[0UL] = __ins_0;
  __tempargs108[1UL] = __ins_1;
  __tempargs108[2UL] = __outs_0;
  sc_parallel_call_cpu_with_env((void*)&mul__1230_closure_108_0wrapper, __stream, __module_data, 0UL, 12288UL, 1UL, __tempargs108);
  return true;
}

static bool cast__124(int8_t* __restrict__ __outs_0, float* __restrict__ __ins_0) noexcept{
  generic_val __tempargs109[2UL];
  __tempargs109[0UL] = __ins_0;
  __tempargs109[1UL] = __outs_0;
  sc_parallel_call_cpu_with_env((void*)&cast__1240_closure_109_0wrapper, __stream, __module_data, 0UL, 12288UL, 1UL, __tempargs109);
  return true;
}

static bool mul__132(float* __restrict__ __outs_0, float* __restrict__ __ins_0, float* __restrict__ __ins_1) noexcept{
  generic_val __tempargs110[3UL];
  __tempargs110[0UL] = __ins_0;
  __tempargs110[1UL] = __ins_1;
  __tempargs110[2UL] = __outs_0;
  sc_parallel_call_cpu_with_env((void*)&mul__1320_closure_110_0wrapper, __stream, __module_data, 0UL, 12288UL, 1UL, __tempargs110);
  return true;
}

static bool cast__133(int8_t* __restrict__ __outs_0, float* __restrict__ __ins_0) noexcept{
  generic_val __tempargs111[2UL];
  __tempargs111[0UL] = __ins_0;
  __tempargs111[1UL] = __outs_0;
  sc_parallel_call_cpu_with_env((void*)&cast__1330_closure_111_0wrapper, __stream, __module_data, 0UL, 12288UL, 1UL, __tempargs111);
  return true;
}

static bool mul__147(float* __restrict__ __outs_0, float* __restrict__ __ins_0, float* __restrict__ __ins_1) noexcept{
  generic_val __tempargs112[3UL];
  __tempargs112[0UL] = __ins_0;
  __tempargs112[1UL] = __ins_1;
  __tempargs112[2UL] = __outs_0;
  sc_parallel_call_cpu_with_env((void*)&mul__1470_closure_112_0wrapper, __stream, __module_data, 0UL, 65536UL, 1UL, __tempargs112);
  return true;
}

static bool cast__148(int8_t* __restrict__ __outs_0, float* __restrict__ __ins_0) noexcept{
  generic_val __tempargs113[2UL];
  __tempargs113[0UL] = __ins_0;
  __tempargs113[1UL] = __outs_0;
  sc_parallel_call_cpu_with_env((void*)&cast__1480_closure_113_0wrapper, __stream, __module_data, 0UL, 65536UL, 1UL, __tempargs113);
  return true;
}

static bool mul__156(float* __restrict__ __outs_0, float* __restrict__ __ins_0, float* __restrict__ __ins_1) noexcept{
  generic_val __tempargs114[3UL];
  __tempargs114[0UL] = __ins_0;
  __tempargs114[1UL] = __ins_1;
  __tempargs114[2UL] = __outs_0;
  sc_parallel_call_cpu_with_env((void*)&mul__1560_closure_114_0wrapper, __stream, __module_data, 0UL, 65536UL, 1UL, __tempargs114);
  return true;
}

static bool cast__157(int8_t* __restrict__ __outs_0, float* __restrict__ __ins_0) noexcept{
  generic_val __tempargs115[2UL];
  __tempargs115[0UL] = __ins_0;
  __tempargs115[1UL] = __outs_0;
  sc_parallel_call_cpu_with_env((void*)&cast__1570_closure_115_0wrapper, __stream, __module_data, 0UL, 65536UL, 1UL, __tempargs115);
  return true;
}

static bool mul__165(float* __restrict__ __outs_0, float* __restrict__ __ins_0, float* __restrict__ __ins_1) noexcept{
  generic_val __tempargs116[3UL];
  __tempargs116[0UL] = __ins_0;
  __tempargs116[1UL] = __ins_1;
  __tempargs116[2UL] = __outs_0;
  sc_parallel_call_cpu_with_env((void*)&mul__1650_closure_116_0wrapper, __stream, __module_data, 0UL, 65536UL, 1UL, __tempargs116);
  return true;
}

static bool cast__166(int8_t* __restrict__ __outs_0, float* __restrict__ __ins_0) noexcept{
  generic_val __tempargs117[2UL];
  __tempargs117[0UL] = __ins_0;
  __tempargs117[1UL] = __outs_0;
  sc_parallel_call_cpu_with_env((void*)&cast__1660_closure_117_0wrapper, __stream, __module_data, 0UL, 65536UL, 1UL, __tempargs117);
  return true;
}

static bool mul__174(float* __restrict__ __outs_0, float* __restrict__ __ins_0, float* __restrict__ __ins_1) noexcept{
  generic_val __tempargs118[3UL];
  __tempargs118[0UL] = __ins_0;
  __tempargs118[1UL] = __ins_1;
  __tempargs118[2UL] = __outs_0;
  sc_parallel_call_cpu_with_env((void*)&mul__1740_closure_118_0wrapper, __stream, __module_data, 0UL, 65536UL, 1UL, __tempargs118);
  return true;
}

static bool cast__175(int8_t* __restrict__ __outs_0, float* __restrict__ __ins_0) noexcept{
  generic_val __tempargs119[2UL];
  __tempargs119[0UL] = __ins_0;
  __tempargs119[1UL] = __outs_0;
  sc_parallel_call_cpu_with_env((void*)&cast__1750_closure_119_0wrapper, __stream, __module_data, 0UL, 65536UL, 1UL, __tempargs119);
  return true;
}

static bool mul__150(float* __restrict__ __outs_0, float* __restrict__ __ins_0, float* __restrict__ __ins_1) noexcept{
  generic_val __tempargs120[3UL];
  __tempargs120[0UL] = __ins_0;
  __tempargs120[1UL] = __ins_1;
  __tempargs120[2UL] = __outs_0;
  sc_parallel_call_cpu_with_env((void*)&mul__1500_closure_120_0wrapper, __stream, __module_data, 0UL, 65536UL, 1UL, __tempargs120);
  return true;
}

static bool cast__151(int8_t* __restrict__ __outs_0, float* __restrict__ __ins_0) noexcept{
  generic_val __tempargs121[2UL];
  __tempargs121[0UL] = __ins_0;
  __tempargs121[1UL] = __outs_0;
  sc_parallel_call_cpu_with_env((void*)&cast__1510_closure_121_0wrapper, __stream, __module_data, 0UL, 65536UL, 1UL, __tempargs121);
  return true;
}

static bool mul__159(float* __restrict__ __outs_0, float* __restrict__ __ins_0, float* __restrict__ __ins_1) noexcept{
  generic_val __tempargs122[3UL];
  __tempargs122[0UL] = __ins_0;
  __tempargs122[1UL] = __ins_1;
  __tempargs122[2UL] = __outs_0;
  sc_parallel_call_cpu_with_env((void*)&mul__1590_closure_122_0wrapper, __stream, __module_data, 0UL, 65536UL, 1UL, __tempargs122);
  return true;
}

static bool cast__160(int8_t* __restrict__ __outs_0, float* __restrict__ __ins_0) noexcept{
  generic_val __tempargs123[2UL];
  __tempargs123[0UL] = __ins_0;
  __tempargs123[1UL] = __outs_0;
  sc_parallel_call_cpu_with_env((void*)&cast__1600_closure_123_0wrapper, __stream, __module_data, 0UL, 65536UL, 1UL, __tempargs123);
  return true;
}

static bool mul__168(float* __restrict__ __outs_0, float* __restrict__ __ins_0, float* __restrict__ __ins_1) noexcept{
  generic_val __tempargs124[3UL];
  __tempargs124[0UL] = __ins_0;
  __tempargs124[1UL] = __ins_1;
  __tempargs124[2UL] = __outs_0;
  sc_parallel_call_cpu_with_env((void*)&mul__1680_closure_124_0wrapper, __stream, __module_data, 0UL, 65536UL, 1UL, __tempargs124);
  return true;
}

static bool cast__169(int8_t* __restrict__ __outs_0, float* __restrict__ __ins_0) noexcept{
  generic_val __tempargs125[2UL];
  __tempargs125[0UL] = __ins_0;
  __tempargs125[1UL] = __outs_0;
  sc_parallel_call_cpu_with_env((void*)&cast__1690_closure_125_0wrapper, __stream, __module_data, 0UL, 65536UL, 1UL, __tempargs125);
  return true;
}

static bool mul__138(float* __restrict__ __outs_0, float* __restrict__ __ins_0, float* __restrict__ __ins_1) noexcept{
  generic_val __tempargs126[3UL];
  __tempargs126[0UL] = __ins_0;
  __tempargs126[1UL] = __ins_1;
  __tempargs126[2UL] = __outs_0;
  sc_parallel_call_cpu_with_env((void*)&mul__1380_closure_126_0wrapper, __stream, __module_data, 0UL, 131072UL, 1UL, __tempargs126);
  return true;
}

static bool cast__139(int8_t* __restrict__ __outs_0, float* __restrict__ __ins_0) noexcept{
  generic_val __tempargs127[2UL];
  __tempargs127[0UL] = __ins_0;
  __tempargs127[1UL] = __outs_0;
  sc_parallel_call_cpu_with_env((void*)&cast__1390_closure_127_0wrapper, __stream, __module_data, 0UL, 131072UL, 1UL, __tempargs127);
  return true;
}

static bool mul__180(float* __restrict__ __outs_0, float* __restrict__ __ins_0, float* __restrict__ __ins_1) noexcept{
  generic_val __tempargs128[3UL];
  __tempargs128[0UL] = __ins_0;
  __tempargs128[1UL] = __ins_1;
  __tempargs128[2UL] = __outs_0;
  sc_parallel_call_cpu_with_env((void*)&mul__1800_closure_128_0wrapper, __stream, __module_data, 0UL, 131072UL, 1UL, __tempargs128);
  return true;
}

static bool cast__181(int8_t* __restrict__ __outs_0, float* __restrict__ __ins_0) noexcept{
  generic_val __tempargs129[2UL];
  __tempargs129[0UL] = __ins_0;
  __tempargs129[1UL] = __outs_0;
  sc_parallel_call_cpu_with_env((void*)&cast__1810_closure_129_0wrapper, __stream, __module_data, 0UL, 131072UL, 1UL, __tempargs129);
  return true;
}

static bool mul__144(float* __restrict__ __outs_0, float* __restrict__ __ins_0, float* __restrict__ __ins_1) noexcept{
  generic_val __tempargs130[3UL];
  __tempargs130[0UL] = __ins_0;
  __tempargs130[1UL] = __ins_1;
  __tempargs130[2UL] = __outs_0;
  sc_parallel_call_cpu_with_env((void*)&mul__1440_closure_130_0wrapper, __stream, __module_data, 0UL, 49152UL, 1UL, __tempargs130);
  return true;
}

static bool cast__145(int8_t* __restrict__ __outs_0, float* __restrict__ __ins_0) noexcept{
  generic_val __tempargs131[2UL];
  __tempargs131[0UL] = __ins_0;
  __tempargs131[1UL] = __outs_0;
  sc_parallel_call_cpu_with_env((void*)&cast__1450_closure_131_0wrapper, __stream, __module_data, 0UL, 49152UL, 1UL, __tempargs131);
  return true;
}

static bool mul__153(float* __restrict__ __outs_0, float* __restrict__ __ins_0, float* __restrict__ __ins_1) noexcept{
  generic_val __tempargs132[3UL];
  __tempargs132[0UL] = __ins_0;
  __tempargs132[1UL] = __ins_1;
  __tempargs132[2UL] = __outs_0;
  sc_parallel_call_cpu_with_env((void*)&mul__1530_closure_132_0wrapper, __stream, __module_data, 0UL, 49152UL, 1UL, __tempargs132);
  return true;
}

static bool cast__154(int8_t* __restrict__ __outs_0, float* __restrict__ __ins_0) noexcept{
  generic_val __tempargs133[2UL];
  __tempargs133[0UL] = __ins_0;
  __tempargs133[1UL] = __outs_0;
  sc_parallel_call_cpu_with_env((void*)&cast__1540_closure_133_0wrapper, __stream, __module_data, 0UL, 49152UL, 1UL, __tempargs133);
  return true;
}

static bool mul__162(float* __restrict__ __outs_0, float* __restrict__ __ins_0, float* __restrict__ __ins_1) noexcept{
  generic_val __tempargs134[3UL];
  __tempargs134[0UL] = __ins_0;
  __tempargs134[1UL] = __ins_1;
  __tempargs134[2UL] = __outs_0;
  sc_parallel_call_cpu_with_env((void*)&mul__1620_closure_134_0wrapper, __stream, __module_data, 0UL, 49152UL, 1UL, __tempargs134);
  return true;
}

static bool cast__163(int8_t* __restrict__ __outs_0, float* __restrict__ __ins_0) noexcept{
  generic_val __tempargs135[2UL];
  __tempargs135[0UL] = __ins_0;
  __tempargs135[1UL] = __outs_0;
  sc_parallel_call_cpu_with_env((void*)&cast__1630_closure_135_0wrapper, __stream, __module_data, 0UL, 49152UL, 1UL, __tempargs135);
  return true;
}

static bool mul__171(float* __restrict__ __outs_0, float* __restrict__ __ins_0, float* __restrict__ __ins_1) noexcept{
  generic_val __tempargs136[3UL];
  __tempargs136[0UL] = __ins_0;
  __tempargs136[1UL] = __ins_1;
  __tempargs136[2UL] = __outs_0;
  sc_parallel_call_cpu_with_env((void*)&mul__1710_closure_136_0wrapper, __stream, __module_data, 0UL, 49152UL, 1UL, __tempargs136);
  return true;
}

static bool cast__172(int8_t* __restrict__ __outs_0, float* __restrict__ __ins_0) noexcept{
  generic_val __tempargs137[2UL];
  __tempargs137[0UL] = __ins_0;
  __tempargs137[1UL] = __outs_0;
  sc_parallel_call_cpu_with_env((void*)&cast__1720_closure_137_0wrapper, __stream, __module_data, 0UL, 49152UL, 1UL, __tempargs137);
  return true;
}

static bool mul__186(float* __restrict__ __outs_0, float* __restrict__ __ins_0, float* __restrict__ __ins_1) noexcept{
  generic_val __tempargs138[3UL];
  __tempargs138[0UL] = __ins_0;
  __tempargs138[1UL] = __ins_1;
  __tempargs138[2UL] = __outs_0;
  sc_parallel_call_cpu_with_env((void*)&mul__1860_closure_138_0wrapper, __stream, __module_data, 0UL, 262144UL, 1UL, __tempargs138);
  return true;
}

static bool cast__187(int8_t* __restrict__ __outs_0, float* __restrict__ __ins_0) noexcept{
  generic_val __tempargs139[2UL];
  __tempargs139[0UL] = __ins_0;
  __tempargs139[1UL] = __outs_0;
  sc_parallel_call_cpu_with_env((void*)&cast__1870_closure_139_0wrapper, __stream, __module_data, 0UL, 262144UL, 1UL, __tempargs139);
  return true;
}

static bool mul__195(float* __restrict__ __outs_0, float* __restrict__ __ins_0, float* __restrict__ __ins_1) noexcept{
  generic_val __tempargs140[3UL];
  __tempargs140[0UL] = __ins_0;
  __tempargs140[1UL] = __ins_1;
  __tempargs140[2UL] = __outs_0;
  sc_parallel_call_cpu_with_env((void*)&mul__1950_closure_140_0wrapper, __stream, __module_data, 0UL, 262144UL, 1UL, __tempargs140);
  return true;
}

static bool cast__196(int8_t* __restrict__ __outs_0, float* __restrict__ __ins_0) noexcept{
  generic_val __tempargs141[2UL];
  __tempargs141[0UL] = __ins_0;
  __tempargs141[1UL] = __outs_0;
  sc_parallel_call_cpu_with_env((void*)&cast__1960_closure_141_0wrapper, __stream, __module_data, 0UL, 262144UL, 1UL, __tempargs141);
  return true;
}

static bool mul__204(float* __restrict__ __outs_0, float* __restrict__ __ins_0, float* __restrict__ __ins_1) noexcept{
  generic_val __tempargs142[3UL];
  __tempargs142[0UL] = __ins_0;
  __tempargs142[1UL] = __ins_1;
  __tempargs142[2UL] = __outs_0;
  sc_parallel_call_cpu_with_env((void*)&mul__2040_closure_142_0wrapper, __stream, __module_data, 0UL, 262144UL, 1UL, __tempargs142);
  return true;
}

static bool cast__205(int8_t* __restrict__ __outs_0, float* __restrict__ __ins_0) noexcept{
  generic_val __tempargs143[2UL];
  __tempargs143[0UL] = __ins_0;
  __tempargs143[1UL] = __outs_0;
  sc_parallel_call_cpu_with_env((void*)&cast__2050_closure_143_0wrapper, __stream, __module_data, 0UL, 262144UL, 1UL, __tempargs143);
  return true;
}

static bool mul__213(float* __restrict__ __outs_0, float* __restrict__ __ins_0, float* __restrict__ __ins_1) noexcept{
  generic_val __tempargs144[3UL];
  __tempargs144[0UL] = __ins_0;
  __tempargs144[1UL] = __ins_1;
  __tempargs144[2UL] = __outs_0;
  sc_parallel_call_cpu_with_env((void*)&mul__2130_closure_144_0wrapper, __stream, __module_data, 0UL, 262144UL, 1UL, __tempargs144);
  return true;
}

static bool cast__214(int8_t* __restrict__ __outs_0, float* __restrict__ __ins_0) noexcept{
  generic_val __tempargs145[2UL];
  __tempargs145[0UL] = __ins_0;
  __tempargs145[1UL] = __outs_0;
  sc_parallel_call_cpu_with_env((void*)&cast__2140_closure_145_0wrapper, __stream, __module_data, 0UL, 262144UL, 1UL, __tempargs145);
  return true;
}

static bool mul__222(float* __restrict__ __outs_0, float* __restrict__ __ins_0, float* __restrict__ __ins_1) noexcept{
  generic_val __tempargs146[3UL];
  __tempargs146[0UL] = __ins_0;
  __tempargs146[1UL] = __ins_1;
  __tempargs146[2UL] = __outs_0;
  sc_parallel_call_cpu_with_env((void*)&mul__2220_closure_146_0wrapper, __stream, __module_data, 0UL, 262144UL, 1UL, __tempargs146);
  return true;
}

static bool cast__223(int8_t* __restrict__ __outs_0, float* __restrict__ __ins_0) noexcept{
  generic_val __tempargs147[2UL];
  __tempargs147[0UL] = __ins_0;
  __tempargs147[1UL] = __outs_0;
  sc_parallel_call_cpu_with_env((void*)&cast__2230_closure_147_0wrapper, __stream, __module_data, 0UL, 262144UL, 1UL, __tempargs147);
  return true;
}

static bool mul__231(float* __restrict__ __outs_0, float* __restrict__ __ins_0, float* __restrict__ __ins_1) noexcept{
  generic_val __tempargs148[3UL];
  __tempargs148[0UL] = __ins_0;
  __tempargs148[1UL] = __ins_1;
  __tempargs148[2UL] = __outs_0;
  sc_parallel_call_cpu_with_env((void*)&mul__2310_closure_148_0wrapper, __stream, __module_data, 0UL, 262144UL, 1UL, __tempargs148);
  return true;
}

static bool cast__232(int8_t* __restrict__ __outs_0, float* __restrict__ __ins_0) noexcept{
  generic_val __tempargs149[2UL];
  __tempargs149[0UL] = __ins_0;
  __tempargs149[1UL] = __outs_0;
  sc_parallel_call_cpu_with_env((void*)&cast__2320_closure_149_0wrapper, __stream, __module_data, 0UL, 262144UL, 1UL, __tempargs149);
  return true;
}

static bool mul__189(float* __restrict__ __outs_0, float* __restrict__ __ins_0, float* __restrict__ __ins_1) noexcept{
  generic_val __tempargs150[3UL];
  __tempargs150[0UL] = __ins_0;
  __tempargs150[1UL] = __ins_1;
  __tempargs150[2UL] = __outs_0;
  sc_parallel_call_cpu_with_env((void*)&mul__1890_closure_150_0wrapper, __stream, __module_data, 0UL, 262144UL, 1UL, __tempargs150);
  return true;
}

static bool cast__190(int8_t* __restrict__ __outs_0, float* __restrict__ __ins_0) noexcept{
  generic_val __tempargs151[2UL];
  __tempargs151[0UL] = __ins_0;
  __tempargs151[1UL] = __outs_0;
  sc_parallel_call_cpu_with_env((void*)&cast__1900_closure_151_0wrapper, __stream, __module_data, 0UL, 262144UL, 1UL, __tempargs151);
  return true;
}

static bool mul__198(float* __restrict__ __outs_0, float* __restrict__ __ins_0, float* __restrict__ __ins_1) noexcept{
  generic_val __tempargs152[3UL];
  __tempargs152[0UL] = __ins_0;
  __tempargs152[1UL] = __ins_1;
  __tempargs152[2UL] = __outs_0;
  sc_parallel_call_cpu_with_env((void*)&mul__1980_closure_152_0wrapper, __stream, __module_data, 0UL, 262144UL, 1UL, __tempargs152);
  return true;
}

static bool cast__199(int8_t* __restrict__ __outs_0, float* __restrict__ __ins_0) noexcept{
  generic_val __tempargs153[2UL];
  __tempargs153[0UL] = __ins_0;
  __tempargs153[1UL] = __outs_0;
  sc_parallel_call_cpu_with_env((void*)&cast__1990_closure_153_0wrapper, __stream, __module_data, 0UL, 262144UL, 1UL, __tempargs153);
  return true;
}

static bool mul__207(float* __restrict__ __outs_0, float* __restrict__ __ins_0, float* __restrict__ __ins_1) noexcept{
  generic_val __tempargs154[3UL];
  __tempargs154[0UL] = __ins_0;
  __tempargs154[1UL] = __ins_1;
  __tempargs154[2UL] = __outs_0;
  sc_parallel_call_cpu_with_env((void*)&mul__2070_closure_154_0wrapper, __stream, __module_data, 0UL, 262144UL, 1UL, __tempargs154);
  return true;
}

static bool cast__208(int8_t* __restrict__ __outs_0, float* __restrict__ __ins_0) noexcept{
  generic_val __tempargs155[2UL];
  __tempargs155[0UL] = __ins_0;
  __tempargs155[1UL] = __outs_0;
  sc_parallel_call_cpu_with_env((void*)&cast__2080_closure_155_0wrapper, __stream, __module_data, 0UL, 262144UL, 1UL, __tempargs155);
  return true;
}

static bool mul__216(float* __restrict__ __outs_0, float* __restrict__ __ins_0, float* __restrict__ __ins_1) noexcept{
  generic_val __tempargs156[3UL];
  __tempargs156[0UL] = __ins_0;
  __tempargs156[1UL] = __ins_1;
  __tempargs156[2UL] = __outs_0;
  sc_parallel_call_cpu_with_env((void*)&mul__2160_closure_156_0wrapper, __stream, __module_data, 0UL, 262144UL, 1UL, __tempargs156);
  return true;
}

static bool cast__217(int8_t* __restrict__ __outs_0, float* __restrict__ __ins_0) noexcept{
  generic_val __tempargs157[2UL];
  __tempargs157[0UL] = __ins_0;
  __tempargs157[1UL] = __outs_0;
  sc_parallel_call_cpu_with_env((void*)&cast__2170_closure_157_0wrapper, __stream, __module_data, 0UL, 262144UL, 1UL, __tempargs157);
  return true;
}

static bool mul__225(float* __restrict__ __outs_0, float* __restrict__ __ins_0, float* __restrict__ __ins_1) noexcept{
  generic_val __tempargs158[3UL];
  __tempargs158[0UL] = __ins_0;
  __tempargs158[1UL] = __ins_1;
  __tempargs158[2UL] = __outs_0;
  sc_parallel_call_cpu_with_env((void*)&mul__2250_closure_158_0wrapper, __stream, __module_data, 0UL, 262144UL, 1UL, __tempargs158);
  return true;
}

static bool cast__226(int8_t* __restrict__ __outs_0, float* __restrict__ __ins_0) noexcept{
  generic_val __tempargs159[2UL];
  __tempargs159[0UL] = __ins_0;
  __tempargs159[1UL] = __outs_0;
  sc_parallel_call_cpu_with_env((void*)&cast__2260_closure_159_0wrapper, __stream, __module_data, 0UL, 262144UL, 1UL, __tempargs159);
  return true;
}

static bool mul__177(float* __restrict__ __outs_0, float* __restrict__ __ins_0, float* __restrict__ __ins_1) noexcept{
  generic_val __tempargs160[3UL];
  __tempargs160[0UL] = __ins_0;
  __tempargs160[1UL] = __ins_1;
  __tempargs160[2UL] = __outs_0;
  sc_parallel_call_cpu_with_env((void*)&mul__1770_closure_160_0wrapper, __stream, __module_data, 0UL, 524288UL, 1UL, __tempargs160);
  return true;
}

static bool cast__178(int8_t* __restrict__ __outs_0, float* __restrict__ __ins_0) noexcept{
  generic_val __tempargs161[2UL];
  __tempargs161[0UL] = __ins_0;
  __tempargs161[1UL] = __outs_0;
  sc_parallel_call_cpu_with_env((void*)&cast__1780_closure_161_0wrapper, __stream, __module_data, 0UL, 524288UL, 1UL, __tempargs161);
  return true;
}

static bool mul__237(float* __restrict__ __outs_0, float* __restrict__ __ins_0, float* __restrict__ __ins_1) noexcept{
  generic_val __tempargs162[3UL];
  __tempargs162[0UL] = __ins_0;
  __tempargs162[1UL] = __ins_1;
  __tempargs162[2UL] = __outs_0;
  sc_parallel_call_cpu_with_env((void*)&mul__2370_closure_162_0wrapper, __stream, __module_data, 0UL, 524288UL, 1UL, __tempargs162);
  return true;
}

static bool cast__238(int8_t* __restrict__ __outs_0, float* __restrict__ __ins_0) noexcept{
  generic_val __tempargs163[2UL];
  __tempargs163[0UL] = __ins_0;
  __tempargs163[1UL] = __outs_0;
  sc_parallel_call_cpu_with_env((void*)&cast__2380_closure_163_0wrapper, __stream, __module_data, 0UL, 524288UL, 1UL, __tempargs163);
  return true;
}

static bool mul__183(float* __restrict__ __outs_0, float* __restrict__ __ins_0, float* __restrict__ __ins_1) noexcept{
  generic_val __tempargs164[3UL];
  __tempargs164[0UL] = __ins_0;
  __tempargs164[1UL] = __ins_1;
  __tempargs164[2UL] = __outs_0;
  sc_parallel_call_cpu_with_env((void*)&mul__1830_closure_164_0wrapper, __stream, __module_data, 0UL, 196608UL, 1UL, __tempargs164);
  return true;
}

static bool cast__184(int8_t* __restrict__ __outs_0, float* __restrict__ __ins_0) noexcept{
  generic_val __tempargs165[2UL];
  __tempargs165[0UL] = __ins_0;
  __tempargs165[1UL] = __outs_0;
  sc_parallel_call_cpu_with_env((void*)&cast__1840_closure_165_0wrapper, __stream, __module_data, 0UL, 196608UL, 1UL, __tempargs165);
  return true;
}

static bool mul__192(float* __restrict__ __outs_0, float* __restrict__ __ins_0, float* __restrict__ __ins_1) noexcept{
  generic_val __tempargs166[3UL];
  __tempargs166[0UL] = __ins_0;
  __tempargs166[1UL] = __ins_1;
  __tempargs166[2UL] = __outs_0;
  sc_parallel_call_cpu_with_env((void*)&mul__1920_closure_166_0wrapper, __stream, __module_data, 0UL, 196608UL, 1UL, __tempargs166);
  return true;
}

static bool cast__193(int8_t* __restrict__ __outs_0, float* __restrict__ __ins_0) noexcept{
  generic_val __tempargs167[2UL];
  __tempargs167[0UL] = __ins_0;
  __tempargs167[1UL] = __outs_0;
  sc_parallel_call_cpu_with_env((void*)&cast__1930_closure_167_0wrapper, __stream, __module_data, 0UL, 196608UL, 1UL, __tempargs167);
  return true;
}

static bool mul__201(float* __restrict__ __outs_0, float* __restrict__ __ins_0, float* __restrict__ __ins_1) noexcept{
  generic_val __tempargs168[3UL];
  __tempargs168[0UL] = __ins_0;
  __tempargs168[1UL] = __ins_1;
  __tempargs168[2UL] = __outs_0;
  sc_parallel_call_cpu_with_env((void*)&mul__2010_closure_168_0wrapper, __stream, __module_data, 0UL, 196608UL, 1UL, __tempargs168);
  return true;
}

static bool cast__202(int8_t* __restrict__ __outs_0, float* __restrict__ __ins_0) noexcept{
  generic_val __tempargs169[2UL];
  __tempargs169[0UL] = __ins_0;
  __tempargs169[1UL] = __outs_0;
  sc_parallel_call_cpu_with_env((void*)&cast__2020_closure_169_0wrapper, __stream, __module_data, 0UL, 196608UL, 1UL, __tempargs169);
  return true;
}

static bool mul__210(float* __restrict__ __outs_0, float* __restrict__ __ins_0, float* __restrict__ __ins_1) noexcept{
  generic_val __tempargs170[3UL];
  __tempargs170[0UL] = __ins_0;
  __tempargs170[1UL] = __ins_1;
  __tempargs170[2UL] = __outs_0;
  sc_parallel_call_cpu_with_env((void*)&mul__2100_closure_170_0wrapper, __stream, __module_data, 0UL, 196608UL, 1UL, __tempargs170);
  return true;
}

static bool cast__211(int8_t* __restrict__ __outs_0, float* __restrict__ __ins_0) noexcept{
  generic_val __tempargs171[2UL];
  __tempargs171[0UL] = __ins_0;
  __tempargs171[1UL] = __outs_0;
  sc_parallel_call_cpu_with_env((void*)&cast__2110_closure_171_0wrapper, __stream, __module_data, 0UL, 196608UL, 1UL, __tempargs171);
  return true;
}

static bool mul__219(float* __restrict__ __outs_0, float* __restrict__ __ins_0, float* __restrict__ __ins_1) noexcept{
  generic_val __tempargs172[3UL];
  __tempargs172[0UL] = __ins_0;
  __tempargs172[1UL] = __ins_1;
  __tempargs172[2UL] = __outs_0;
  sc_parallel_call_cpu_with_env((void*)&mul__2190_closure_172_0wrapper, __stream, __module_data, 0UL, 196608UL, 1UL, __tempargs172);
  return true;
}

static bool cast__220(int8_t* __restrict__ __outs_0, float* __restrict__ __ins_0) noexcept{
  generic_val __tempargs173[2UL];
  __tempargs173[0UL] = __ins_0;
  __tempargs173[1UL] = __outs_0;
  sc_parallel_call_cpu_with_env((void*)&cast__2200_closure_173_0wrapper, __stream, __module_data, 0UL, 196608UL, 1UL, __tempargs173);
  return true;
}

static bool mul__228(float* __restrict__ __outs_0, float* __restrict__ __ins_0, float* __restrict__ __ins_1) noexcept{
  generic_val __tempargs174[3UL];
  __tempargs174[0UL] = __ins_0;
  __tempargs174[1UL] = __ins_1;
  __tempargs174[2UL] = __outs_0;
  sc_parallel_call_cpu_with_env((void*)&mul__2280_closure_174_0wrapper, __stream, __module_data, 0UL, 196608UL, 1UL, __tempargs174);
  return true;
}

static bool cast__229(int8_t* __restrict__ __outs_0, float* __restrict__ __ins_0) noexcept{
  generic_val __tempargs175[2UL];
  __tempargs175[0UL] = __ins_0;
  __tempargs175[1UL] = __outs_0;
  sc_parallel_call_cpu_with_env((void*)&cast__2290_closure_175_0wrapper, __stream, __module_data, 0UL, 196608UL, 1UL, __tempargs175);
  return true;
}

static bool reorder__525(int8_t* __restrict__ __outs_0, int8_t* __restrict__ __ins_0) noexcept{
  generic_val __tempargs176[2UL];
  __tempargs176[0UL] = __ins_0;
  __tempargs176[1UL] = __outs_0;
  sc_parallel_call_cpu_with_env((void*)&reorder__5250_closure_176_0wrapper, __stream, __module_data, 0UL, 48UL, 1UL, __tempargs176);
  return true;
}

static bool mul__652(float* __restrict__ __outs_0, float* __restrict__ __ins_0, float* __restrict__ __ins_1) noexcept{
  generic_val __tempargs177[3UL];
  __tempargs177[0UL] = __ins_0;
  __tempargs177[1UL] = __ins_1;
  __tempargs177[2UL] = __outs_0;
  sc_parallel_call_cpu_with_env((void*)&mul__6520_closure_177_0wrapper, __stream, __module_data, 0UL, 4UL, 1UL, __tempargs177);
  return true;
}

static bool mul__651(float* __restrict__ __outs_0, float* __restrict__ __ins_0, float* __restrict__ __ins_1) noexcept{
  generic_val __tempargs178[3UL];
  __tempargs178[0UL] = __ins_0;
  __tempargs178[1UL] = __ins_1;
  __tempargs178[2UL] = __outs_0;
  sc_parallel_call_cpu_with_env((void*)&mul__6510_closure_178_0wrapper, __stream, __module_data, 0UL, 4UL, 1UL, __tempargs178);
  return true;
}

static bool mul__646(float* __restrict__ __outs_0, float* __restrict__ __ins_0, float* __restrict__ __ins_1) noexcept{
  generic_val __tempargs179[3UL];
  __tempargs179[0UL] = __ins_0;
  __tempargs179[1UL] = __ins_1;
  __tempargs179[2UL] = __outs_0;
  sc_parallel_call_cpu_with_env((void*)&mul__6460_closure_179_0wrapper, __stream, __module_data, 0UL, 8UL, 1UL, __tempargs179);
  return true;
}

static bool mul__645(float* __restrict__ __outs_0, float* __restrict__ __ins_0, float* __restrict__ __ins_1) noexcept{
  generic_val __tempargs180[3UL];
  __tempargs180[0UL] = __ins_0;
  __tempargs180[1UL] = __ins_1;
  __tempargs180[2UL] = __outs_0;
  sc_parallel_call_cpu_with_env((void*)&mul__6450_closure_180_0wrapper, __stream, __module_data, 0UL, 8UL, 1UL, __tempargs180);
  return true;
}

static bool mul__640(float* __restrict__ __outs_0, float* __restrict__ __ins_0, float* __restrict__ __ins_1) noexcept{
  generic_val __tempargs181[3UL];
  __tempargs181[0UL] = __ins_0;
  __tempargs181[1UL] = __ins_1;
  __tempargs181[2UL] = __outs_0;
  sc_parallel_call_cpu_with_env((void*)&mul__6400_closure_181_0wrapper, __stream, __module_data, 0UL, 8UL, 1UL, __tempargs181);
  return true;
}

static bool mul__639(float* __restrict__ __outs_0, float* __restrict__ __ins_0, float* __restrict__ __ins_1) noexcept{
  generic_val __tempargs182[3UL];
  __tempargs182[0UL] = __ins_0;
  __tempargs182[1UL] = __ins_1;
  __tempargs182[2UL] = __outs_0;
  sc_parallel_call_cpu_with_env((void*)&mul__6390_closure_182_0wrapper, __stream, __module_data, 0UL, 8UL, 1UL, __tempargs182);
  return true;
}

static bool mul__634(float* __restrict__ __outs_0, float* __restrict__ __ins_0, float* __restrict__ __ins_1) noexcept{
  generic_val __tempargs183[3UL];
  __tempargs183[0UL] = __ins_0;
  __tempargs183[1UL] = __ins_1;
  __tempargs183[2UL] = __outs_0;
  sc_parallel_call_cpu_with_env((void*)&mul__6340_closure_183_0wrapper, __stream, __module_data, 0UL, 8UL, 1UL, __tempargs183);
  return true;
}

static bool mul__633(float* __restrict__ __outs_0, float* __restrict__ __ins_0, float* __restrict__ __ins_1) noexcept{
  generic_val __tempargs184[3UL];
  __tempargs184[0UL] = __ins_0;
  __tempargs184[1UL] = __ins_1;
  __tempargs184[2UL] = __outs_0;
  sc_parallel_call_cpu_with_env((void*)&mul__6330_closure_184_0wrapper, __stream, __module_data, 0UL, 8UL, 1UL, __tempargs184);
  return true;
}

static bool mul__628(float* __restrict__ __outs_0, float* __restrict__ __ins_0, float* __restrict__ __ins_1) noexcept{
  for (uint64_t _fuseiter_4730 = 0UL; _fuseiter_4730 < 1024UL; _fuseiter_4730 += 16UL) {
    vec_f32x16 __cached_0;
    __cached_0 = vec_f32x16::load(&__ins_0[_fuseiter_4730]);
    float __cached_1;
    __cached_1 = __ins_1[0];
    vec_f32x16 __cached_2;
    __cached_2 = (__cached_0 * vec_f32x16(__cached_1));
    vec_f32x16::store(__cached_2, &__outs_0[_fuseiter_4730]);
  }
  return true;
}

static bool mul__627(float* __restrict__ __outs_0, float* __restrict__ __ins_0, float* __restrict__ __ins_1) noexcept{
  for (uint64_t _fuseiter_4736 = 0UL; _fuseiter_4736 < 1024UL; _fuseiter_4736 += 16UL) {
    vec_f32x16 __cached_0;
    __cached_0 = vec_f32x16::load(&__ins_0[_fuseiter_4736]);
    float __cached_1;
    __cached_1 = __ins_1[0];
    vec_f32x16 __cached_2;
    __cached_2 = (__cached_0 * vec_f32x16(__cached_1));
    vec_f32x16::store(__cached_2, &__outs_0[_fuseiter_4736]);
  }
  return true;
}

static bool mul__622(float* __restrict__ __outs_0, float* __restrict__ __ins_0, float* __restrict__ __ins_1) noexcept{
  generic_val __tempargs185[3UL];
  __tempargs185[0UL] = __ins_0;
  __tempargs185[1UL] = __ins_1;
  __tempargs185[2UL] = __outs_0;
  sc_parallel_call_cpu_with_env((void*)&mul__6220_closure_185_0wrapper, __stream, __module_data, 0UL, 8UL, 1UL, __tempargs185);
  return true;
}

static bool mul__621(float* __restrict__ __outs_0, float* __restrict__ __ins_0, float* __restrict__ __ins_1) noexcept{
  generic_val __tempargs186[3UL];
  __tempargs186[0UL] = __ins_0;
  __tempargs186[1UL] = __ins_1;
  __tempargs186[2UL] = __outs_0;
  sc_parallel_call_cpu_with_env((void*)&mul__6210_closure_186_0wrapper, __stream, __module_data, 0UL, 8UL, 1UL, __tempargs186);
  return true;
}

static bool mul__616(float* __restrict__ __outs_0, float* __restrict__ __ins_0, float* __restrict__ __ins_1) noexcept{
  generic_val __tempargs187[3UL];
  __tempargs187[0UL] = __ins_0;
  __tempargs187[1UL] = __ins_1;
  __tempargs187[2UL] = __outs_0;
  sc_parallel_call_cpu_with_env((void*)&mul__6160_closure_187_0wrapper, __stream, __module_data, 0UL, 32UL, 1UL, __tempargs187);
  return true;
}

static bool mul__615(float* __restrict__ __outs_0, float* __restrict__ __ins_0, float* __restrict__ __ins_1) noexcept{
  generic_val __tempargs188[3UL];
  __tempargs188[0UL] = __ins_0;
  __tempargs188[1UL] = __ins_1;
  __tempargs188[2UL] = __outs_0;
  sc_parallel_call_cpu_with_env((void*)&mul__6150_closure_188_0wrapper, __stream, __module_data, 0UL, 32UL, 1UL, __tempargs188);
  return true;
}

static bool mul__614(float* __restrict__ __outs_0, float* __restrict__ __ins_0, float* __restrict__ __ins_1) noexcept{
  generic_val __tempargs189[3UL];
  __tempargs189[0UL] = __ins_0;
  __tempargs189[1UL] = __ins_1;
  __tempargs189[2UL] = __outs_0;
  sc_parallel_call_cpu_with_env((void*)&mul__6140_closure_189_0wrapper, __stream, __module_data, 0UL, 4UL, 1UL, __tempargs189);
  return true;
}

static bool mul__613(float* __restrict__ __outs_0, float* __restrict__ __ins_0, float* __restrict__ __ins_1) noexcept{
  generic_val __tempargs190[3UL];
  __tempargs190[0UL] = __ins_0;
  __tempargs190[1UL] = __ins_1;
  __tempargs190[2UL] = __outs_0;
  sc_parallel_call_cpu_with_env((void*)&mul__6130_closure_190_0wrapper, __stream, __module_data, 0UL, 4UL, 1UL, __tempargs190);
  return true;
}

static bool mul__608(float* __restrict__ __outs_0, float* __restrict__ __ins_0, float* __restrict__ __ins_1) noexcept{
  generic_val __tempargs191[3UL];
  __tempargs191[0UL] = __ins_0;
  __tempargs191[1UL] = __ins_1;
  __tempargs191[2UL] = __outs_0;
  sc_parallel_call_cpu_with_env((void*)&mul__6080_closure_191_0wrapper, __stream, __module_data, 0UL, 8UL, 1UL, __tempargs191);
  return true;
}

static bool mul__607(float* __restrict__ __outs_0, float* __restrict__ __ins_0, float* __restrict__ __ins_1) noexcept{
  generic_val __tempargs192[3UL];
  __tempargs192[0UL] = __ins_0;
  __tempargs192[1UL] = __ins_1;
  __tempargs192[2UL] = __outs_0;
  sc_parallel_call_cpu_with_env((void*)&mul__6070_closure_192_0wrapper, __stream, __module_data, 0UL, 8UL, 1UL, __tempargs192);
  return true;
}

static bool mul__602(float* __restrict__ __outs_0, float* __restrict__ __ins_0, float* __restrict__ __ins_1) noexcept{
  generic_val __tempargs193[3UL];
  __tempargs193[0UL] = __ins_0;
  __tempargs193[1UL] = __ins_1;
  __tempargs193[2UL] = __outs_0;
  sc_parallel_call_cpu_with_env((void*)&mul__6020_closure_193_0wrapper, __stream, __module_data, 0UL, 4UL, 1UL, __tempargs193);
  return true;
}

static bool mul__601(float* __restrict__ __outs_0, float* __restrict__ __ins_0, float* __restrict__ __ins_1) noexcept{
  generic_val __tempargs194[3UL];
  __tempargs194[0UL] = __ins_0;
  __tempargs194[1UL] = __ins_1;
  __tempargs194[2UL] = __outs_0;
  sc_parallel_call_cpu_with_env((void*)&mul__6010_closure_194_0wrapper, __stream, __module_data, 0UL, 4UL, 1UL, __tempargs194);
  return true;
}

static bool mul__596(float* __restrict__ __outs_0, float* __restrict__ __ins_0, float* __restrict__ __ins_1) noexcept{
  generic_val __tempargs195[3UL];
  __tempargs195[0UL] = __ins_0;
  __tempargs195[1UL] = __ins_1;
  __tempargs195[2UL] = __outs_0;
  sc_parallel_call_cpu_with_env((void*)&mul__5960_closure_195_0wrapper, __stream, __module_data, 0UL, 16UL, 1UL, __tempargs195);
  return true;
}

static bool mul__595(float* __restrict__ __outs_0, float* __restrict__ __ins_0, float* __restrict__ __ins_1) noexcept{
  generic_val __tempargs196[3UL];
  __tempargs196[0UL] = __ins_0;
  __tempargs196[1UL] = __ins_1;
  __tempargs196[2UL] = __outs_0;
  sc_parallel_call_cpu_with_env((void*)&mul__5950_closure_196_0wrapper, __stream, __module_data, 0UL, 16UL, 1UL, __tempargs196);
  return true;
}

static bool mul__590(float* __restrict__ __outs_0, float* __restrict__ __ins_0, float* __restrict__ __ins_1) noexcept{
  generic_val __tempargs197[3UL];
  __tempargs197[0UL] = __ins_0;
  __tempargs197[1UL] = __ins_1;
  __tempargs197[2UL] = __outs_0;
  sc_parallel_call_cpu_with_env((void*)&mul__5900_closure_197_0wrapper, __stream, __module_data, 0UL, 16UL, 1UL, __tempargs197);
  return true;
}

static bool mul__589(float* __restrict__ __outs_0, float* __restrict__ __ins_0, float* __restrict__ __ins_1) noexcept{
  generic_val __tempargs198[3UL];
  __tempargs198[0UL] = __ins_0;
  __tempargs198[1UL] = __ins_1;
  __tempargs198[2UL] = __outs_0;
  sc_parallel_call_cpu_with_env((void*)&mul__5890_closure_198_0wrapper, __stream, __module_data, 0UL, 16UL, 1UL, __tempargs198);
  return true;
}

static bool mul__650(float* __restrict__ __outs_0, float* __restrict__ __ins_0, float* __restrict__ __ins_1) noexcept{
  generic_val __tempargs199[3UL];
  __tempargs199[0UL] = __ins_0;
  __tempargs199[1UL] = __ins_1;
  __tempargs199[2UL] = __outs_0;
  sc_parallel_call_cpu_with_env((void*)&mul__6500_closure_199_0wrapper, __stream, __module_data, 0UL, 4UL, 1UL, __tempargs199);
  return true;
}

static bool mul__649(float* __restrict__ __outs_0, float* __restrict__ __ins_0, float* __restrict__ __ins_1) noexcept{
  generic_val __tempargs200[3UL];
  __tempargs200[0UL] = __ins_0;
  __tempargs200[1UL] = __ins_1;
  __tempargs200[2UL] = __outs_0;
  sc_parallel_call_cpu_with_env((void*)&mul__6490_closure_200_0wrapper, __stream, __module_data, 0UL, 4UL, 1UL, __tempargs200);
  return true;
}

static bool mul__648(float* __restrict__ __outs_0, float* __restrict__ __ins_0, float* __restrict__ __ins_1) noexcept{
  generic_val __tempargs201[3UL];
  __tempargs201[0UL] = __ins_0;
  __tempargs201[1UL] = __ins_1;
  __tempargs201[2UL] = __outs_0;
  sc_parallel_call_cpu_with_env((void*)&mul__6480_closure_201_0wrapper, __stream, __module_data, 0UL, 8UL, 1UL, __tempargs201);
  return true;
}

static bool mul__647(float* __restrict__ __outs_0, float* __restrict__ __ins_0, float* __restrict__ __ins_1) noexcept{
  generic_val __tempargs202[3UL];
  __tempargs202[0UL] = __ins_0;
  __tempargs202[1UL] = __ins_1;
  __tempargs202[2UL] = __outs_0;
  sc_parallel_call_cpu_with_env((void*)&mul__6470_closure_202_0wrapper, __stream, __module_data, 0UL, 8UL, 1UL, __tempargs202);
  return true;
}

static bool mul__644(float* __restrict__ __outs_0, float* __restrict__ __ins_0, float* __restrict__ __ins_1) noexcept{
  generic_val __tempargs203[3UL];
  __tempargs203[0UL] = __ins_0;
  __tempargs203[1UL] = __ins_1;
  __tempargs203[2UL] = __outs_0;
  sc_parallel_call_cpu_with_env((void*)&mul__6440_closure_203_0wrapper, __stream, __module_data, 0UL, 8UL, 1UL, __tempargs203);
  return true;
}

static bool mul__643(float* __restrict__ __outs_0, float* __restrict__ __ins_0, float* __restrict__ __ins_1) noexcept{
  generic_val __tempargs204[3UL];
  __tempargs204[0UL] = __ins_0;
  __tempargs204[1UL] = __ins_1;
  __tempargs204[2UL] = __outs_0;
  sc_parallel_call_cpu_with_env((void*)&mul__6430_closure_204_0wrapper, __stream, __module_data, 0UL, 8UL, 1UL, __tempargs204);
  return true;
}

static bool mul__642(float* __restrict__ __outs_0, float* __restrict__ __ins_0, float* __restrict__ __ins_1) noexcept{
  for (uint64_t _fuseiter_4862 = 0UL; _fuseiter_4862 < 256UL; _fuseiter_4862 += 16UL) {
    vec_f32x16 __cached_0;
    __cached_0 = vec_f32x16::load(&__ins_0[_fuseiter_4862]);
    float __cached_1;
    __cached_1 = __ins_1[0];
    vec_f32x16 __cached_2;
    __cached_2 = (__cached_0 * vec_f32x16(__cached_1));
    vec_f32x16::store(__cached_2, &__outs_0[_fuseiter_4862]);
  }
  return true;
}

static bool mul__641(float* __restrict__ __outs_0, float* __restrict__ __ins_0, float* __restrict__ __ins_1) noexcept{
  for (uint64_t _fuseiter_4868 = 0UL; _fuseiter_4868 < 256UL; _fuseiter_4868 += 16UL) {
    vec_f32x16 __cached_0;
    __cached_0 = vec_f32x16::load(&__ins_0[_fuseiter_4868]);
    float __cached_1;
    __cached_1 = __ins_1[0];
    vec_f32x16 __cached_2;
    __cached_2 = (__cached_0 * vec_f32x16(__cached_1));
    vec_f32x16::store(__cached_2, &__outs_0[_fuseiter_4868]);
  }
  return true;
}

static bool mul__638(float* __restrict__ __outs_0, float* __restrict__ __ins_0, float* __restrict__ __ins_1) noexcept{
  generic_val __tempargs205[3UL];
  __tempargs205[0UL] = __ins_0;
  __tempargs205[1UL] = __ins_1;
  __tempargs205[2UL] = __outs_0;
  sc_parallel_call_cpu_with_env((void*)&mul__6380_closure_205_0wrapper, __stream, __module_data, 0UL, 4UL, 1UL, __tempargs205);
  return true;
}

static bool mul__637(float* __restrict__ __outs_0, float* __restrict__ __ins_0, float* __restrict__ __ins_1) noexcept{
  generic_val __tempargs206[3UL];
  __tempargs206[0UL] = __ins_0;
  __tempargs206[1UL] = __ins_1;
  __tempargs206[2UL] = __outs_0;
  sc_parallel_call_cpu_with_env((void*)&mul__6370_closure_206_0wrapper, __stream, __module_data, 0UL, 4UL, 1UL, __tempargs206);
  return true;
}

static bool mul__636(float* __restrict__ __outs_0, float* __restrict__ __ins_0, float* __restrict__ __ins_1) noexcept{
  generic_val __tempargs207[3UL];
  __tempargs207[0UL] = __ins_0;
  __tempargs207[1UL] = __ins_1;
  __tempargs207[2UL] = __outs_0;
  sc_parallel_call_cpu_with_env((void*)&mul__6360_closure_207_0wrapper, __stream, __module_data, 0UL, 4UL, 1UL, __tempargs207);
  return true;
}

static bool mul__635(float* __restrict__ __outs_0, float* __restrict__ __ins_0, float* __restrict__ __ins_1) noexcept{
  generic_val __tempargs208[3UL];
  __tempargs208[0UL] = __ins_0;
  __tempargs208[1UL] = __ins_1;
  __tempargs208[2UL] = __outs_0;
  sc_parallel_call_cpu_with_env((void*)&mul__6350_closure_208_0wrapper, __stream, __module_data, 0UL, 4UL, 1UL, __tempargs208);
  return true;
}

static bool mul__632(float* __restrict__ __outs_0, float* __restrict__ __ins_0, float* __restrict__ __ins_1) noexcept{
  generic_val __tempargs209[3UL];
  __tempargs209[0UL] = __ins_0;
  __tempargs209[1UL] = __ins_1;
  __tempargs209[2UL] = __outs_0;
  sc_parallel_call_cpu_with_env((void*)&mul__6320_closure_209_0wrapper, __stream, __module_data, 0UL, 8UL, 1UL, __tempargs209);
  return true;
}

static bool mul__631(float* __restrict__ __outs_0, float* __restrict__ __ins_0, float* __restrict__ __ins_1) noexcept{
  generic_val __tempargs210[3UL];
  __tempargs210[0UL] = __ins_0;
  __tempargs210[1UL] = __ins_1;
  __tempargs210[2UL] = __outs_0;
  sc_parallel_call_cpu_with_env((void*)&mul__6310_closure_210_0wrapper, __stream, __module_data, 0UL, 8UL, 1UL, __tempargs210);
  return true;
}

static bool mul__630(float* __restrict__ __outs_0, float* __restrict__ __ins_0, float* __restrict__ __ins_1) noexcept{
  generic_val __tempargs211[3UL];
  __tempargs211[0UL] = __ins_0;
  __tempargs211[1UL] = __ins_1;
  __tempargs211[2UL] = __outs_0;
  sc_parallel_call_cpu_with_env((void*)&mul__6300_closure_211_0wrapper, __stream, __module_data, 0UL, 8UL, 1UL, __tempargs211);
  return true;
}

static bool mul__629(float* __restrict__ __outs_0, float* __restrict__ __ins_0, float* __restrict__ __ins_1) noexcept{
  generic_val __tempargs212[3UL];
  __tempargs212[0UL] = __ins_0;
  __tempargs212[1UL] = __ins_1;
  __tempargs212[2UL] = __outs_0;
  sc_parallel_call_cpu_with_env((void*)&mul__6290_closure_212_0wrapper, __stream, __module_data, 0UL, 8UL, 1UL, __tempargs212);
  return true;
}

static bool mul__626(float* __restrict__ __outs_0, float* __restrict__ __ins_0, float* __restrict__ __ins_1) noexcept{
  generic_val __tempargs213[3UL];
  __tempargs213[0UL] = __ins_0;
  __tempargs213[1UL] = __ins_1;
  __tempargs213[2UL] = __outs_0;
  sc_parallel_call_cpu_with_env((void*)&mul__6260_closure_213_0wrapper, __stream, __module_data, 0UL, 16UL, 1UL, __tempargs213);
  return true;
}

static bool mul__625(float* __restrict__ __outs_0, float* __restrict__ __ins_0, float* __restrict__ __ins_1) noexcept{
  generic_val __tempargs214[3UL];
  __tempargs214[0UL] = __ins_0;
  __tempargs214[1UL] = __ins_1;
  __tempargs214[2UL] = __outs_0;
  sc_parallel_call_cpu_with_env((void*)&mul__6250_closure_214_0wrapper, __stream, __module_data, 0UL, 16UL, 1UL, __tempargs214);
  return true;
}

static bool mul__624(float* __restrict__ __outs_0, float* __restrict__ __ins_0, float* __restrict__ __ins_1) noexcept{
  generic_val __tempargs215[3UL];
  __tempargs215[0UL] = __ins_0;
  __tempargs215[1UL] = __ins_1;
  __tempargs215[2UL] = __outs_0;
  sc_parallel_call_cpu_with_env((void*)&mul__6240_closure_215_0wrapper, __stream, __module_data, 0UL, 4UL, 1UL, __tempargs215);
  return true;
}

static bool mul__623(float* __restrict__ __outs_0, float* __restrict__ __ins_0, float* __restrict__ __ins_1) noexcept{
  generic_val __tempargs216[3UL];
  __tempargs216[0UL] = __ins_0;
  __tempargs216[1UL] = __ins_1;
  __tempargs216[2UL] = __outs_0;
  sc_parallel_call_cpu_with_env((void*)&mul__6230_closure_216_0wrapper, __stream, __module_data, 0UL, 4UL, 1UL, __tempargs216);
  return true;
}

static bool mul__620(float* __restrict__ __outs_0, float* __restrict__ __ins_0, float* __restrict__ __ins_1) noexcept{
  generic_val __tempargs217[3UL];
  __tempargs217[0UL] = __ins_0;
  __tempargs217[1UL] = __ins_1;
  __tempargs217[2UL] = __outs_0;
  sc_parallel_call_cpu_with_env((void*)&mul__6200_closure_217_0wrapper, __stream, __module_data, 0UL, 2UL, 1UL, __tempargs217);
  return true;
}

static bool mul__619(float* __restrict__ __outs_0, float* __restrict__ __ins_0, float* __restrict__ __ins_1) noexcept{
  generic_val __tempargs218[3UL];
  __tempargs218[0UL] = __ins_0;
  __tempargs218[1UL] = __ins_1;
  __tempargs218[2UL] = __outs_0;
  sc_parallel_call_cpu_with_env((void*)&mul__6190_closure_218_0wrapper, __stream, __module_data, 0UL, 2UL, 1UL, __tempargs218);
  return true;
}

static bool mul__618(float* __restrict__ __outs_0, float* __restrict__ __ins_0, float* __restrict__ __ins_1) noexcept{
  for (uint64_t _fuseiter_4958 = 0UL; _fuseiter_4958 < 256UL; _fuseiter_4958 += 16UL) {
    vec_f32x16 __cached_0;
    __cached_0 = vec_f32x16::load(&__ins_0[_fuseiter_4958]);
    float __cached_1;
    __cached_1 = __ins_1[0];
    vec_f32x16 __cached_2;
    __cached_2 = (__cached_0 * vec_f32x16(__cached_1));
    vec_f32x16::store(__cached_2, &__outs_0[_fuseiter_4958]);
  }
  return true;
}

static bool mul__617(float* __restrict__ __outs_0, float* __restrict__ __ins_0, float* __restrict__ __ins_1) noexcept{
  for (uint64_t _fuseiter_4964 = 0UL; _fuseiter_4964 < 256UL; _fuseiter_4964 += 16UL) {
    vec_f32x16 __cached_0;
    __cached_0 = vec_f32x16::load(&__ins_0[_fuseiter_4964]);
    float __cached_1;
    __cached_1 = __ins_1[0];
    vec_f32x16 __cached_2;
    __cached_2 = (__cached_0 * vec_f32x16(__cached_1));
    vec_f32x16::store(__cached_2, &__outs_0[_fuseiter_4964]);
  }
  return true;
}

static bool mul__588(float* __restrict__ __outs_0, float* __restrict__ __ins_0, float* __restrict__ __ins_1) noexcept{
  generic_val __tempargs219[3UL];
  __tempargs219[0UL] = __ins_0;
  __tempargs219[1UL] = __ins_1;
  __tempargs219[2UL] = __outs_0;
  sc_parallel_call_cpu_with_env((void*)&mul__5880_closure_219_0wrapper, __stream, __module_data, 0UL, 4UL, 1UL, __tempargs219);
  return true;
}

static bool mul__587(float* __restrict__ __outs_0, float* __restrict__ __ins_0, float* __restrict__ __ins_1) noexcept{
  generic_val __tempargs220[3UL];
  __tempargs220[0UL] = __ins_0;
  __tempargs220[1UL] = __ins_1;
  __tempargs220[2UL] = __outs_0;
  sc_parallel_call_cpu_with_env((void*)&mul__5870_closure_220_0wrapper, __stream, __module_data, 0UL, 4UL, 1UL, __tempargs220);
  return true;
}

static bool mul__582(float* __restrict__ __outs_0, float* __restrict__ __ins_0, float* __restrict__ __ins_1) noexcept{
  generic_val __tempargs221[3UL];
  __tempargs221[0UL] = __ins_0;
  __tempargs221[1UL] = __ins_1;
  __tempargs221[2UL] = __outs_0;
  sc_parallel_call_cpu_with_env((void*)&mul__5820_closure_221_0wrapper, __stream, __module_data, 0UL, 8UL, 1UL, __tempargs221);
  return true;
}

static bool mul__581(float* __restrict__ __outs_0, float* __restrict__ __ins_0, float* __restrict__ __ins_1) noexcept{
  generic_val __tempargs222[3UL];
  __tempargs222[0UL] = __ins_0;
  __tempargs222[1UL] = __ins_1;
  __tempargs222[2UL] = __outs_0;
  sc_parallel_call_cpu_with_env((void*)&mul__5810_closure_222_0wrapper, __stream, __module_data, 0UL, 8UL, 1UL, __tempargs222);
  return true;
}

static bool mul__576(float* __restrict__ __outs_0, float* __restrict__ __ins_0, float* __restrict__ __ins_1) noexcept{
  generic_val __tempargs223[3UL];
  __tempargs223[0UL] = __ins_0;
  __tempargs223[1UL] = __ins_1;
  __tempargs223[2UL] = __outs_0;
  sc_parallel_call_cpu_with_env((void*)&mul__5760_closure_223_0wrapper, __stream, __module_data, 0UL, 4UL, 1UL, __tempargs223);
  return true;
}

static bool mul__575(float* __restrict__ __outs_0, float* __restrict__ __ins_0, float* __restrict__ __ins_1) noexcept{
  generic_val __tempargs224[3UL];
  __tempargs224[0UL] = __ins_0;
  __tempargs224[1UL] = __ins_1;
  __tempargs224[2UL] = __outs_0;
  sc_parallel_call_cpu_with_env((void*)&mul__5750_closure_224_0wrapper, __stream, __module_data, 0UL, 4UL, 1UL, __tempargs224);
  return true;
}

static bool mul__570(float* __restrict__ __outs_0, float* __restrict__ __ins_0, float* __restrict__ __ins_1) noexcept{
  generic_val __tempargs225[3UL];
  __tempargs225[0UL] = __ins_0;
  __tempargs225[1UL] = __ins_1;
  __tempargs225[2UL] = __outs_0;
  sc_parallel_call_cpu_with_env((void*)&mul__5700_closure_225_0wrapper, __stream, __module_data, 0UL, 16UL, 1UL, __tempargs225);
  return true;
}

static bool mul__569(float* __restrict__ __outs_0, float* __restrict__ __ins_0, float* __restrict__ __ins_1) noexcept{
  generic_val __tempargs226[3UL];
  __tempargs226[0UL] = __ins_0;
  __tempargs226[1UL] = __ins_1;
  __tempargs226[2UL] = __outs_0;
  sc_parallel_call_cpu_with_env((void*)&mul__5690_closure_226_0wrapper, __stream, __module_data, 0UL, 16UL, 1UL, __tempargs226);
  return true;
}

static bool reorder__522(int8_t* __restrict__ __outs_0, int8_t* __restrict__ __ins_0) noexcept{
  generic_val __tempargs227[2UL];
  __tempargs227[0UL] = __ins_0;
  __tempargs227[1UL] = __outs_0;
  sc_parallel_call_cpu_with_env((void*)&reorder__5220_closure_227_0wrapper, __stream, __module_data, 0UL, 2048UL, 1UL, __tempargs227);
  return true;
}

static bool reorder__515(int8_t* __restrict__ __outs_0, int8_t* __restrict__ __ins_0) noexcept{
  generic_val __tempargs228[2UL];
  __tempargs228[0UL] = __ins_0;
  __tempargs228[1UL] = __outs_0;
  sc_parallel_call_cpu_with_env((void*)&reorder__5150_closure_228_0wrapper, __stream, __module_data, 0UL, 256UL, 1UL, __tempargs228);
  return true;
}

static bool reorder__506(int8_t* __restrict__ __outs_0, int8_t* __restrict__ __ins_0) noexcept{
  generic_val __tempargs229[2UL];
  __tempargs229[0UL] = __ins_0;
  __tempargs229[1UL] = __outs_0;
  sc_parallel_call_cpu_with_env((void*)&reorder__5060_closure_229_0wrapper, __stream, __module_data, 0UL, 32UL, 1UL, __tempargs229);
  return true;
}

static bool reorder__497(int8_t* __restrict__ __outs_0, int8_t* __restrict__ __ins_0) noexcept{
  generic_val __tempargs230[2UL];
  __tempargs230[0UL] = __ins_0;
  __tempargs230[1UL] = __outs_0;
  sc_parallel_call_cpu_with_env((void*)&reorder__4970_closure_230_0wrapper, __stream, __module_data, 0UL, 64UL, 1UL, __tempargs230);
  return true;
}

static bool reorder__490(int8_t* __restrict__ __outs_0, int8_t* __restrict__ __ins_0) noexcept{
  generic_val __tempargs231[2UL];
  __tempargs231[0UL] = __ins_0;
  __tempargs231[1UL] = __outs_0;
  sc_parallel_call_cpu_with_env((void*)&reorder__4900_closure_231_0wrapper, __stream, __module_data, 0UL, 1024UL, 1UL, __tempargs231);
  return true;
}

static bool reorder__528(int8_t* __restrict__ __outs_0, int8_t* __restrict__ __ins_0) noexcept{
  generic_val __tempargs232[2UL];
  __tempargs232[0UL] = __ins_0;
  __tempargs232[1UL] = __outs_0;
  sc_parallel_call_cpu_with_env((void*)&reorder__5280_closure_232_0wrapper, __stream, __module_data, 0UL, 256UL, 1UL, __tempargs232);
  return true;
}

static bool reorder__519(int8_t* __restrict__ __outs_0, int8_t* __restrict__ __ins_0) noexcept{
  generic_val __tempargs233[2UL];
  __tempargs233[0UL] = __ins_0;
  __tempargs233[1UL] = __outs_0;
  sc_parallel_call_cpu_with_env((void*)&reorder__5190_closure_233_0wrapper, __stream, __module_data, 0UL, 512UL, 1UL, __tempargs233);
  return true;
}

static bool reorder__512(int8_t* __restrict__ __outs_0, int8_t* __restrict__ __ins_0) noexcept{
  generic_val __tempargs234[2UL];
  __tempargs234[0UL] = __ins_0;
  __tempargs234[1UL] = __outs_0;
  sc_parallel_call_cpu_with_env((void*)&reorder__5120_closure_234_0wrapper, __stream, __module_data, 0UL, 512UL, 1UL, __tempargs234);
  return true;
}

static bool reorder__503(int8_t* __restrict__ __outs_0, int8_t* __restrict__ __ins_0) noexcept{
  generic_val __tempargs235[2UL];
  __tempargs235[0UL] = __ins_0;
  __tempargs235[1UL] = __outs_0;
  sc_parallel_call_cpu_with_env((void*)&reorder__5030_closure_235_0wrapper, __stream, __module_data, 0UL, 512UL, 1UL, __tempargs235);
  return true;
}

static bool reorder__496(int8_t* __restrict__ __outs_0, int8_t* __restrict__ __ins_0) noexcept{
  generic_val __tempargs236[2UL];
  __tempargs236[0UL] = __ins_0;
  __tempargs236[1UL] = __outs_0;
  sc_parallel_call_cpu_with_env((void*)&reorder__4960_closure_236_0wrapper, __stream, __module_data, 0UL, 64UL, 1UL, __tempargs236);
  return true;
}

static bool reorder__487(int8_t* __restrict__ __outs_0, int8_t* __restrict__ __ins_0) noexcept{
  generic_val __tempargs237[2UL];
  __tempargs237[0UL] = __ins_0;
  __tempargs237[1UL] = __outs_0;
  sc_parallel_call_cpu_with_env((void*)&reorder__4870_closure_237_0wrapper, __stream, __module_data, 0UL, 512UL, 1UL, __tempargs237);
  return true;
}

static bool reorder__422(int8_t* __restrict__ __outs_0, int8_t* __restrict__ __ins_0) noexcept{
  generic_val __tempargs238[2UL];
  __tempargs238[0UL] = __ins_0;
  __tempargs238[1UL] = __outs_0;
  sc_parallel_call_cpu_with_env((void*)&reorder__4220_closure_238_0wrapper, __stream, __module_data, 0UL, 1024UL, 1UL, __tempargs238);
  return true;
}

static bool mul__612(float* __restrict__ __outs_0, float* __restrict__ __ins_0, float* __restrict__ __ins_1) noexcept{
  generic_val __tempargs239[3UL];
  __tempargs239[0UL] = __ins_0;
  __tempargs239[1UL] = __ins_1;
  __tempargs239[2UL] = __outs_0;
  sc_parallel_call_cpu_with_env((void*)&mul__6120_closure_239_0wrapper, __stream, __module_data, 0UL, 4UL, 1UL, __tempargs239);
  return true;
}

static bool mul__611(float* __restrict__ __outs_0, float* __restrict__ __ins_0, float* __restrict__ __ins_1) noexcept{
  generic_val __tempargs240[3UL];
  __tempargs240[0UL] = __ins_0;
  __tempargs240[1UL] = __ins_1;
  __tempargs240[2UL] = __outs_0;
  sc_parallel_call_cpu_with_env((void*)&mul__6110_closure_240_0wrapper, __stream, __module_data, 0UL, 4UL, 1UL, __tempargs240);
  return true;
}

static bool mul__610(float* __restrict__ __outs_0, float* __restrict__ __ins_0, float* __restrict__ __ins_1) noexcept{
  generic_val __tempargs241[3UL];
  __tempargs241[0UL] = __ins_0;
  __tempargs241[1UL] = __ins_1;
  __tempargs241[2UL] = __outs_0;
  sc_parallel_call_cpu_with_env((void*)&mul__6100_closure_241_0wrapper, __stream, __module_data, 0UL, 2UL, 1UL, __tempargs241);
  return true;
}

static bool mul__609(float* __restrict__ __outs_0, float* __restrict__ __ins_0, float* __restrict__ __ins_1) noexcept{
  generic_val __tempargs242[3UL];
  __tempargs242[0UL] = __ins_0;
  __tempargs242[1UL] = __ins_1;
  __tempargs242[2UL] = __outs_0;
  sc_parallel_call_cpu_with_env((void*)&mul__6090_closure_242_0wrapper, __stream, __module_data, 0UL, 2UL, 1UL, __tempargs242);
  return true;
}

static bool mul__606(float* __restrict__ __outs_0, float* __restrict__ __ins_0, float* __restrict__ __ins_1) noexcept{
  generic_val __tempargs243[3UL];
  __tempargs243[0UL] = __ins_0;
  __tempargs243[1UL] = __ins_1;
  __tempargs243[2UL] = __outs_0;
  sc_parallel_call_cpu_with_env((void*)&mul__6060_closure_243_0wrapper, __stream, __module_data, 0UL, 4UL, 1UL, __tempargs243);
  return true;
}

static bool mul__605(float* __restrict__ __outs_0, float* __restrict__ __ins_0, float* __restrict__ __ins_1) noexcept{
  generic_val __tempargs244[3UL];
  __tempargs244[0UL] = __ins_0;
  __tempargs244[1UL] = __ins_1;
  __tempargs244[2UL] = __outs_0;
  sc_parallel_call_cpu_with_env((void*)&mul__6050_closure_244_0wrapper, __stream, __module_data, 0UL, 4UL, 1UL, __tempargs244);
  return true;
}

static bool mul__604(float* __restrict__ __outs_0, float* __restrict__ __ins_0, float* __restrict__ __ins_1) noexcept{
  for (uint64_t _fuseiter_5138 = 0UL; _fuseiter_5138 < 128UL; _fuseiter_5138 += 16UL) {
    vec_f32x16 __cached_0;
    __cached_0 = vec_f32x16::load(&__ins_0[_fuseiter_5138]);
    float __cached_1;
    __cached_1 = __ins_1[0];
    vec_f32x16 __cached_2;
    __cached_2 = (__cached_0 * vec_f32x16(__cached_1));
    vec_f32x16::store(__cached_2, &__outs_0[_fuseiter_5138]);
  }
  return true;
}

static bool mul__603(float* __restrict__ __outs_0, float* __restrict__ __ins_0, float* __restrict__ __ins_1) noexcept{
  for (uint64_t _fuseiter_5144 = 0UL; _fuseiter_5144 < 128UL; _fuseiter_5144 += 16UL) {
    vec_f32x16 __cached_0;
    __cached_0 = vec_f32x16::load(&__ins_0[_fuseiter_5144]);
    float __cached_1;
    __cached_1 = __ins_1[0];
    vec_f32x16 __cached_2;
    __cached_2 = (__cached_0 * vec_f32x16(__cached_1));
    vec_f32x16::store(__cached_2, &__outs_0[_fuseiter_5144]);
  }
  return true;
}

static bool mul__600(float* __restrict__ __outs_0, float* __restrict__ __ins_0, float* __restrict__ __ins_1) noexcept{
  for (uint64_t _fuseiter_5150 = 0UL; _fuseiter_5150 < 128UL; _fuseiter_5150 += 16UL) {
    vec_f32x16 __cached_0;
    __cached_0 = vec_f32x16::load(&__ins_0[_fuseiter_5150]);
    float __cached_1;
    __cached_1 = __ins_1[0];
    vec_f32x16 __cached_2;
    __cached_2 = (__cached_0 * vec_f32x16(__cached_1));
    vec_f32x16::store(__cached_2, &__outs_0[_fuseiter_5150]);
  }
  return true;
}

static bool mul__599(float* __restrict__ __outs_0, float* __restrict__ __ins_0, float* __restrict__ __ins_1) noexcept{
  for (uint64_t _fuseiter_5156 = 0UL; _fuseiter_5156 < 128UL; _fuseiter_5156 += 16UL) {
    vec_f32x16 __cached_0;
    __cached_0 = vec_f32x16::load(&__ins_0[_fuseiter_5156]);
    float __cached_1;
    __cached_1 = __ins_1[0];
    vec_f32x16 __cached_2;
    __cached_2 = (__cached_0 * vec_f32x16(__cached_1));
    vec_f32x16::store(__cached_2, &__outs_0[_fuseiter_5156]);
  }
  return true;
}

static bool mul__598(float* __restrict__ __outs_0, float* __restrict__ __ins_0, float* __restrict__ __ins_1) noexcept{
  generic_val __tempargs245[3UL];
  __tempargs245[0UL] = __ins_0;
  __tempargs245[1UL] = __ins_1;
  __tempargs245[2UL] = __outs_0;
  sc_parallel_call_cpu_with_env((void*)&mul__5980_closure_245_0wrapper, __stream, __module_data, 0UL, 4UL, 1UL, __tempargs245);
  return true;
}

static bool mul__597(float* __restrict__ __outs_0, float* __restrict__ __ins_0, float* __restrict__ __ins_1) noexcept{
  generic_val __tempargs246[3UL];
  __tempargs246[0UL] = __ins_0;
  __tempargs246[1UL] = __ins_1;
  __tempargs246[2UL] = __outs_0;
  sc_parallel_call_cpu_with_env((void*)&mul__5970_closure_246_0wrapper, __stream, __module_data, 0UL, 4UL, 1UL, __tempargs246);
  return true;
}

static bool mul__594(float* __restrict__ __outs_0, float* __restrict__ __ins_0, float* __restrict__ __ins_1) noexcept{
  generic_val __tempargs247[3UL];
  __tempargs247[0UL] = __ins_0;
  __tempargs247[1UL] = __ins_1;
  __tempargs247[2UL] = __outs_0;
  sc_parallel_call_cpu_with_env((void*)&mul__5940_closure_247_0wrapper, __stream, __module_data, 0UL, 4UL, 1UL, __tempargs247);
  return true;
}

static bool mul__593(float* __restrict__ __outs_0, float* __restrict__ __ins_0, float* __restrict__ __ins_1) noexcept{
  generic_val __tempargs248[3UL];
  __tempargs248[0UL] = __ins_0;
  __tempargs248[1UL] = __ins_1;
  __tempargs248[2UL] = __outs_0;
  sc_parallel_call_cpu_with_env((void*)&mul__5930_closure_248_0wrapper, __stream, __module_data, 0UL, 4UL, 1UL, __tempargs248);
  return true;
}

static bool mul__592(float* __restrict__ __outs_0, float* __restrict__ __ins_0, float* __restrict__ __ins_1) noexcept{
  generic_val __tempargs249[3UL];
  __tempargs249[0UL] = __ins_0;
  __tempargs249[1UL] = __ins_1;
  __tempargs249[2UL] = __outs_0;
  sc_parallel_call_cpu_with_env((void*)&mul__5920_closure_249_0wrapper, __stream, __module_data, 0UL, 2UL, 1UL, __tempargs249);
  return true;
}

static bool mul__591(float* __restrict__ __outs_0, float* __restrict__ __ins_0, float* __restrict__ __ins_1) noexcept{
  generic_val __tempargs250[3UL];
  __tempargs250[0UL] = __ins_0;
  __tempargs250[1UL] = __ins_1;
  __tempargs250[2UL] = __outs_0;
  sc_parallel_call_cpu_with_env((void*)&mul__5910_closure_250_0wrapper, __stream, __module_data, 0UL, 2UL, 1UL, __tempargs250);
  return true;
}

static bool mul__586(float* __restrict__ __outs_0, float* __restrict__ __ins_0, float* __restrict__ __ins_1) noexcept{
  generic_val __tempargs251[3UL];
  __tempargs251[0UL] = __ins_0;
  __tempargs251[1UL] = __ins_1;
  __tempargs251[2UL] = __outs_0;
  sc_parallel_call_cpu_with_env((void*)&mul__5860_closure_251_0wrapper, __stream, __module_data, 0UL, 2UL, 1UL, __tempargs251);
  return true;
}

static bool mul__585(float* __restrict__ __outs_0, float* __restrict__ __ins_0, float* __restrict__ __ins_1) noexcept{
  generic_val __tempargs252[3UL];
  __tempargs252[0UL] = __ins_0;
  __tempargs252[1UL] = __ins_1;
  __tempargs252[2UL] = __outs_0;
  sc_parallel_call_cpu_with_env((void*)&mul__5850_closure_252_0wrapper, __stream, __module_data, 0UL, 2UL, 1UL, __tempargs252);
  return true;
}

static bool mul__584(float* __restrict__ __outs_0, float* __restrict__ __ins_0, float* __restrict__ __ins_1) noexcept{
  generic_val __tempargs253[3UL];
  __tempargs253[0UL] = __ins_0;
  __tempargs253[1UL] = __ins_1;
  __tempargs253[2UL] = __outs_0;
  sc_parallel_call_cpu_with_env((void*)&mul__5840_closure_253_0wrapper, __stream, __module_data, 0UL, 2UL, 1UL, __tempargs253);
  return true;
}

static bool mul__583(float* __restrict__ __outs_0, float* __restrict__ __ins_0, float* __restrict__ __ins_1) noexcept{
  generic_val __tempargs254[3UL];
  __tempargs254[0UL] = __ins_0;
  __tempargs254[1UL] = __ins_1;
  __tempargs254[2UL] = __outs_0;
  sc_parallel_call_cpu_with_env((void*)&mul__5830_closure_254_0wrapper, __stream, __module_data, 0UL, 2UL, 1UL, __tempargs254);
  return true;
}

static bool mul__580(float* __restrict__ __outs_0, float* __restrict__ __ins_0, float* __restrict__ __ins_1) noexcept{
  generic_val __tempargs255[3UL];
  __tempargs255[0UL] = __ins_0;
  __tempargs255[1UL] = __ins_1;
  __tempargs255[2UL] = __outs_0;
  sc_parallel_call_cpu_with_env((void*)&mul__5800_closure_255_0wrapper, __stream, __module_data, 0UL, 4UL, 1UL, __tempargs255);
  return true;
}

static bool mul__579(float* __restrict__ __outs_0, float* __restrict__ __ins_0, float* __restrict__ __ins_1) noexcept{
  generic_val __tempargs256[3UL];
  __tempargs256[0UL] = __ins_0;
  __tempargs256[1UL] = __ins_1;
  __tempargs256[2UL] = __outs_0;
  sc_parallel_call_cpu_with_env((void*)&mul__5790_closure_256_0wrapper, __stream, __module_data, 0UL, 4UL, 1UL, __tempargs256);
  return true;
}

static bool mul__578(float* __restrict__ __outs_0, float* __restrict__ __ins_0, float* __restrict__ __ins_1) noexcept{
  for (uint64_t _fuseiter_5234 = 0UL; _fuseiter_5234 < 64UL; _fuseiter_5234 += 16UL) {
    vec_f32x16 __cached_0;
    __cached_0 = vec_f32x16::load(&__ins_0[_fuseiter_5234]);
    float __cached_1;
    __cached_1 = __ins_1[0];
    vec_f32x16 __cached_2;
    __cached_2 = (__cached_0 * vec_f32x16(__cached_1));
    vec_f32x16::store(__cached_2, &__outs_0[_fuseiter_5234]);
  }
  return true;
}

static bool mul__577(float* __restrict__ __outs_0, float* __restrict__ __ins_0, float* __restrict__ __ins_1) noexcept{
  for (uint64_t _fuseiter_5240 = 0UL; _fuseiter_5240 < 64UL; _fuseiter_5240 += 16UL) {
    vec_f32x16 __cached_0;
    __cached_0 = vec_f32x16::load(&__ins_0[_fuseiter_5240]);
    float __cached_1;
    __cached_1 = __ins_1[0];
    vec_f32x16 __cached_2;
    __cached_2 = (__cached_0 * vec_f32x16(__cached_1));
    vec_f32x16::store(__cached_2, &__outs_0[_fuseiter_5240]);
  }
  return true;
}

static bool mul__574(float* __restrict__ __outs_0, float* __restrict__ __ins_0, float* __restrict__ __ins_1) noexcept{
  generic_val __tempargs257[3UL];
  __tempargs257[0UL] = __ins_0;
  __tempargs257[1UL] = __ins_1;
  __tempargs257[2UL] = __outs_0;
  sc_parallel_call_cpu_with_env((void*)&mul__5740_closure_257_0wrapper, __stream, __module_data, 0UL, 2UL, 1UL, __tempargs257);
  return true;
}

static bool mul__573(float* __restrict__ __outs_0, float* __restrict__ __ins_0, float* __restrict__ __ins_1) noexcept{
  generic_val __tempargs258[3UL];
  __tempargs258[0UL] = __ins_0;
  __tempargs258[1UL] = __ins_1;
  __tempargs258[2UL] = __outs_0;
  sc_parallel_call_cpu_with_env((void*)&mul__5730_closure_258_0wrapper, __stream, __module_data, 0UL, 2UL, 1UL, __tempargs258);
  return true;
}

static bool mul__572(float* __restrict__ __outs_0, float* __restrict__ __ins_0, float* __restrict__ __ins_1) noexcept{
  for (uint64_t _fuseiter_5258 = 0UL; _fuseiter_5258 < 64UL; _fuseiter_5258 += 16UL) {
    vec_f32x16 __cached_0;
    __cached_0 = vec_f32x16::load(&__ins_0[_fuseiter_5258]);
    float __cached_1;
    __cached_1 = __ins_1[0];
    vec_f32x16 __cached_2;
    __cached_2 = (__cached_0 * vec_f32x16(__cached_1));
    vec_f32x16::store(__cached_2, &__outs_0[_fuseiter_5258]);
  }
  return true;
}

static bool mul__571(float* __restrict__ __outs_0, float* __restrict__ __ins_0, float* __restrict__ __ins_1) noexcept{
  for (uint64_t _fuseiter_5264 = 0UL; _fuseiter_5264 < 64UL; _fuseiter_5264 += 16UL) {
    vec_f32x16 __cached_0;
    __cached_0 = vec_f32x16::load(&__ins_0[_fuseiter_5264]);
    float __cached_1;
    __cached_1 = __ins_1[0];
    vec_f32x16 __cached_2;
    __cached_2 = (__cached_0 * vec_f32x16(__cached_1));
    vec_f32x16::store(__cached_2, &__outs_0[_fuseiter_5264]);
  }
  return true;
}

static bool reorder__516(int8_t* __restrict__ __outs_0, int8_t* __restrict__ __ins_0) noexcept{
  generic_val __tempargs259[2UL];
  __tempargs259[0UL] = __ins_0;
  __tempargs259[1UL] = __outs_0;
  sc_parallel_call_cpu_with_env((void*)&reorder__5160_closure_259_0wrapper, __stream, __module_data, 0UL, 48UL, 1UL, __tempargs259);
  return true;
}

static bool reorder__509(int8_t* __restrict__ __outs_0, int8_t* __restrict__ __ins_0) noexcept{
  generic_val __tempargs260[2UL];
  __tempargs260[0UL] = __ins_0;
  __tempargs260[1UL] = __outs_0;
  sc_parallel_call_cpu_with_env((void*)&reorder__5090_closure_260_0wrapper, __stream, __module_data, 0UL, 48UL, 1UL, __tempargs260);
  return true;
}

static bool reorder__500(int8_t* __restrict__ __outs_0, int8_t* __restrict__ __ins_0) noexcept{
  generic_val __tempargs261[2UL];
  __tempargs261[0UL] = __ins_0;
  __tempargs261[1UL] = __outs_0;
  sc_parallel_call_cpu_with_env((void*)&reorder__5000_closure_261_0wrapper, __stream, __module_data, 0UL, 48UL, 1UL, __tempargs261);
  return true;
}

static bool reorder__493(int8_t* __restrict__ __outs_0, int8_t* __restrict__ __ins_0) noexcept{
  generic_val __tempargs262[2UL];
  __tempargs262[0UL] = __ins_0;
  __tempargs262[1UL] = __outs_0;
  sc_parallel_call_cpu_with_env((void*)&reorder__4930_closure_262_0wrapper, __stream, __module_data, 0UL, 48UL, 1UL, __tempargs262);
  return true;
}

static bool reorder__484(int8_t* __restrict__ __outs_0, int8_t* __restrict__ __ins_0) noexcept{
  generic_val __tempargs263[2UL];
  __tempargs263[0UL] = __ins_0;
  __tempargs263[1UL] = __outs_0;
  sc_parallel_call_cpu_with_env((void*)&reorder__4840_closure_263_0wrapper, __stream, __module_data, 0UL, 1152UL, 1UL, __tempargs263);
  return true;
}

static bool reorder__480(int8_t* __restrict__ __outs_0, int8_t* __restrict__ __ins_0) noexcept{
  generic_val __tempargs264[2UL];
  __tempargs264[0UL] = __ins_0;
  __tempargs264[1UL] = __outs_0;
  sc_parallel_call_cpu_with_env((void*)&reorder__4800_closure_264_0wrapper, __stream, __module_data, 0UL, 32UL, 1UL, __tempargs264);
  return true;
}

static bool reorder__474(int8_t* __restrict__ __outs_0, int8_t* __restrict__ __ins_0) noexcept{
  generic_val __tempargs265[2UL];
  __tempargs265[0UL] = __ins_0;
  __tempargs265[1UL] = __outs_0;
  sc_parallel_call_cpu_with_env((void*)&reorder__4740_closure_265_0wrapper, __stream, __module_data, 0UL, 36UL, 1UL, __tempargs265);
  return true;
}

static bool reorder__465(int8_t* __restrict__ __outs_0, int8_t* __restrict__ __ins_0) noexcept{
  generic_val __tempargs266[2UL];
  __tempargs266[0UL] = __ins_0;
  __tempargs266[1UL] = __outs_0;
  sc_parallel_call_cpu_with_env((void*)&reorder__4650_closure_266_0wrapper, __stream, __module_data, 0UL, 24UL, 1UL, __tempargs266);
  return true;
}

static bool reorder__460(int8_t* __restrict__ __outs_0, int8_t* __restrict__ __ins_0) noexcept{
  generic_val __tempargs267[2UL];
  __tempargs267[0UL] = __ins_0;
  __tempargs267[1UL] = __outs_0;
  sc_parallel_call_cpu_with_env((void*)&reorder__4600_closure_267_0wrapper, __stream, __module_data, 0UL, 288UL, 1UL, __tempargs267);
  return true;
}

static bool reorder__451(int8_t* __restrict__ __outs_0, int8_t* __restrict__ __ins_0) noexcept{
  generic_val __tempargs268[2UL];
  __tempargs268[0UL] = __ins_0;
  __tempargs268[1UL] = __outs_0;
  sc_parallel_call_cpu_with_env((void*)&reorder__4510_closure_268_0wrapper, __stream, __module_data, 0UL, 36UL, 1UL, __tempargs268);
  return true;
}

static bool reorder__483(int8_t* __restrict__ __outs_0, int8_t* __restrict__ __ins_0) noexcept{
  generic_val __tempargs269[2UL];
  __tempargs269[0UL] = __ins_0;
  __tempargs269[1UL] = __outs_0;
  sc_parallel_call_cpu_with_env((void*)&reorder__4830_closure_269_0wrapper, __stream, __module_data, 0UL, 128UL, 1UL, __tempargs269);
  return true;
}

static bool reorder__445(int8_t* __restrict__ __outs_0, int8_t* __restrict__ __ins_0) noexcept{
  generic_val __tempargs270[2UL];
  __tempargs270[0UL] = __ins_0;
  __tempargs270[1UL] = __outs_0;
  sc_parallel_call_cpu_with_env((void*)&reorder__4450_closure_270_0wrapper, __stream, __module_data, 0UL, 32UL, 1UL, __tempargs270);
  return true;
}

static bool reorder__471(int8_t* __restrict__ __outs_0, int8_t* __restrict__ __ins_0) noexcept{
  generic_val __tempargs271[2UL];
  __tempargs271[0UL] = __ins_0;
  __tempargs271[1UL] = __outs_0;
  sc_parallel_call_cpu_with_env((void*)&reorder__4710_closure_271_0wrapper, __stream, __module_data, 0UL, 256UL, 1UL, __tempargs271);
  return true;
}

static bool reorder__464(int8_t* __restrict__ __outs_0, int8_t* __restrict__ __ins_0) noexcept{
  generic_val __tempargs272[2UL];
  __tempargs272[0UL] = __ins_0;
  __tempargs272[1UL] = __outs_0;
  sc_parallel_call_cpu_with_env((void*)&reorder__4640_closure_272_0wrapper, __stream, __module_data, 0UL, 128UL, 1UL, __tempargs272);
  return true;
}

static bool reorder__457(int8_t* __restrict__ __outs_0, int8_t* __restrict__ __ins_0) noexcept{
  generic_val __tempargs273[2UL];
  __tempargs273[0UL] = __ins_0;
  __tempargs273[1UL] = __outs_0;
  sc_parallel_call_cpu_with_env((void*)&reorder__4570_closure_273_0wrapper, __stream, __module_data, 0UL, 512UL, 1UL, __tempargs273);
  return true;
}

static bool reorder__477(int8_t* __restrict__ __outs_0, int8_t* __restrict__ __ins_0) noexcept{
  generic_val __tempargs274[2UL];
  __tempargs274[0UL] = __ins_0;
  __tempargs274[1UL] = __outs_0;
  sc_parallel_call_cpu_with_env((void*)&reorder__4770_closure_274_0wrapper, __stream, __module_data, 0UL, 128UL, 1UL, __tempargs274);
  return true;
}

static bool reorder__468(int8_t* __restrict__ __outs_0, int8_t* __restrict__ __ins_0) noexcept{
  generic_val __tempargs275[2UL];
  __tempargs275[0UL] = __ins_0;
  __tempargs275[1UL] = __outs_0;
  sc_parallel_call_cpu_with_env((void*)&reorder__4680_closure_275_0wrapper, __stream, __module_data, 0UL, 256UL, 1UL, __tempargs275);
  return true;
}

static bool reorder__461(int8_t* __restrict__ __outs_0, int8_t* __restrict__ __ins_0) noexcept{
  generic_val __tempargs276[2UL];
  __tempargs276[0UL] = __ins_0;
  __tempargs276[1UL] = __outs_0;
  sc_parallel_call_cpu_with_env((void*)&reorder__4610_closure_276_0wrapper, __stream, __module_data, 0UL, 128UL, 1UL, __tempargs276);
  return true;
}

static bool reorder__454(int8_t* __restrict__ __outs_0, int8_t* __restrict__ __ins_0) noexcept{
  generic_val __tempargs277[2UL];
  __tempargs277[0UL] = __ins_0;
  __tempargs277[1UL] = __outs_0;
  sc_parallel_call_cpu_with_env((void*)&reorder__4540_closure_277_0wrapper, __stream, __module_data, 0UL, 64UL, 1UL, __tempargs277);
  return true;
}

static bool reorder__439(int8_t* __restrict__ __outs_0, int8_t* __restrict__ __ins_0) noexcept{
  generic_val __tempargs278[2UL];
  __tempargs278[0UL] = __ins_0;
  __tempargs278[1UL] = __outs_0;
  sc_parallel_call_cpu_with_env((void*)&reorder__4390_closure_278_0wrapper, __stream, __module_data, 0UL, 288UL, 1UL, __tempargs278);
  return true;
}

static bool reorder__430(int8_t* __restrict__ __outs_0, int8_t* __restrict__ __ins_0) noexcept{
  generic_val __tempargs279[2UL];
  __tempargs279[0UL] = __ins_0;
  __tempargs279[1UL] = __outs_0;
  sc_parallel_call_cpu_with_env((void*)&reorder__4300_closure_279_0wrapper, __stream, __module_data, 0UL, 36UL, 1UL, __tempargs279);
  return true;
}

static bool reorder__423(int8_t* __restrict__ __outs_0, int8_t* __restrict__ __ins_0) noexcept{
  generic_val __tempargs280[2UL];
  __tempargs280[0UL] = __ins_0;
  __tempargs280[1UL] = __outs_0;
  sc_parallel_call_cpu_with_env((void*)&reorder__4230_closure_280_0wrapper, __stream, __module_data, 0UL, 288UL, 1UL, __tempargs280);
  return true;
}

static bool reorder__448(int8_t* __restrict__ __outs_0, int8_t* __restrict__ __ins_0) noexcept{
  generic_val __tempargs281[2UL];
  __tempargs281[0UL] = __ins_0;
  __tempargs281[1UL] = __outs_0;
  sc_parallel_call_cpu_with_env((void*)&reorder__4480_closure_281_0wrapper, __stream, __module_data, 0UL, 128UL, 1UL, __tempargs281);
  return true;
}

static bool reorder__436(int8_t* __restrict__ __outs_0, int8_t* __restrict__ __ins_0) noexcept{
  generic_val __tempargs282[2UL];
  __tempargs282[0UL] = __ins_0;
  __tempargs282[1UL] = __outs_0;
  sc_parallel_call_cpu_with_env((void*)&reorder__4360_closure_282_0wrapper, __stream, __module_data, 0UL, 128UL, 1UL, __tempargs282);
  return true;
}

static bool reorder__429(int8_t* __restrict__ __outs_0, int8_t* __restrict__ __ins_0) noexcept{
  generic_val __tempargs283[2UL];
  __tempargs283[0UL] = __ins_0;
  __tempargs283[1UL] = __outs_0;
  sc_parallel_call_cpu_with_env((void*)&reorder__4290_closure_283_0wrapper, __stream, __module_data, 0UL, 64UL, 1UL, __tempargs283);
  return true;
}

static bool reorder__442(int8_t* __restrict__ __outs_0, int8_t* __restrict__ __ins_0) noexcept{
  generic_val __tempargs284[2UL];
  __tempargs284[0UL] = __ins_0;
  __tempargs284[1UL] = __outs_0;
  sc_parallel_call_cpu_with_env((void*)&reorder__4420_closure_284_0wrapper, __stream, __module_data, 0UL, 64UL, 1UL, __tempargs284);
  return true;
}

static bool reorder__433(int8_t* __restrict__ __outs_0, int8_t* __restrict__ __ins_0) noexcept{
  generic_val __tempargs285[2UL];
  __tempargs285[0UL] = __ins_0;
  __tempargs285[1UL] = __outs_0;
  sc_parallel_call_cpu_with_env((void*)&reorder__4330_closure_285_0wrapper, __stream, __module_data, 0UL, 128UL, 1UL, __tempargs285);
  return true;
}

static bool reorder__426(int8_t* __restrict__ __outs_0, int8_t* __restrict__ __ins_0) noexcept{
  generic_val __tempargs286[2UL];
  __tempargs286[0UL] = __ins_0;
  __tempargs286[1UL] = __outs_0;
  sc_parallel_call_cpu_with_env((void*)&reorder__4260_closure_286_0wrapper, __stream, __module_data, 0UL, 64UL, 1UL, __tempargs286);
  return true;
}

static bool reorder__419(int8_t* __restrict__ __outs_0, int8_t* __restrict__ __ins_0) noexcept{
  generic_val __tempargs287[2UL];
  __tempargs287[0UL] = __ins_0;
  __tempargs287[1UL] = __outs_0;
  sc_parallel_call_cpu_with_env((void*)&reorder__4190_closure_287_0wrapper, __stream, __module_data, 0UL, 256UL, 1UL, __tempargs287);
  return true;
}

static bool mul__656(float* __restrict__ __outs_0, float* __restrict__ __ins_0, float* __restrict__ __ins_1) noexcept{
  generic_val __tempargs288[3UL];
  __tempargs288[0UL] = __ins_0;
  __tempargs288[1UL] = __ins_1;
  __tempargs288[2UL] = __outs_0;
  sc_parallel_call_cpu_with_env((void*)&mul__6560_closure_288_0wrapper, __stream, __module_data, 0UL, 8UL, 1UL, __tempargs288);
  return true;
}

static bool mul__655(float* __restrict__ __outs_0, float* __restrict__ __ins_0, float* __restrict__ __ins_1) noexcept{
  generic_val __tempargs289[3UL];
  __tempargs289[0UL] = __ins_0;
  __tempargs289[1UL] = __ins_1;
  __tempargs289[2UL] = __outs_0;
  sc_parallel_call_cpu_with_env((void*)&mul__6550_closure_289_0wrapper, __stream, __module_data, 0UL, 8UL, 1UL, __tempargs289);
  return true;
}

static bool reorder__534(int8_t* __restrict__ __outs_0, int8_t* __restrict__ __ins_0) noexcept{
  generic_val __tempargs290[2UL];
  __tempargs290[0UL] = __ins_0;
  __tempargs290[1UL] = __outs_0;
  sc_parallel_call_cpu_with_env((void*)&reorder__5340_closure_290_0wrapper, __stream, __module_data, 0UL, 32UL, 1UL, __tempargs290);
  return true;
}

static bool mul__243(float* __restrict__ __outs_0, float* __restrict__ __ins_0, float* __restrict__ __ins_1) noexcept{
  generic_val __tempargs291[3UL];
  __tempargs291[0UL] = __ins_0;
  __tempargs291[1UL] = __ins_1;
  __tempargs291[2UL] = __outs_0;
  sc_parallel_call_cpu_with_env((void*)&mul__2430_closure_291_0wrapper, __stream, __module_data, 0UL, 1048576UL, 1UL, __tempargs291);
  return true;
}

static bool cast__244(int8_t* __restrict__ __outs_0, float* __restrict__ __ins_0) noexcept{
  generic_val __tempargs292[2UL];
  __tempargs292[0UL] = __ins_0;
  __tempargs292[1UL] = __outs_0;
  sc_parallel_call_cpu_with_env((void*)&cast__2440_closure_292_0wrapper, __stream, __module_data, 0UL, 1048576UL, 1UL, __tempargs292);
  return true;
}

static bool mul__252(float* __restrict__ __outs_0, float* __restrict__ __ins_0, float* __restrict__ __ins_1) noexcept{
  generic_val __tempargs293[3UL];
  __tempargs293[0UL] = __ins_0;
  __tempargs293[1UL] = __ins_1;
  __tempargs293[2UL] = __outs_0;
  sc_parallel_call_cpu_with_env((void*)&mul__2520_closure_293_0wrapper, __stream, __module_data, 0UL, 1048576UL, 1UL, __tempargs293);
  return true;
}

static bool cast__253(int8_t* __restrict__ __outs_0, float* __restrict__ __ins_0) noexcept{
  generic_val __tempargs294[2UL];
  __tempargs294[0UL] = __ins_0;
  __tempargs294[1UL] = __outs_0;
  sc_parallel_call_cpu_with_env((void*)&cast__2530_closure_294_0wrapper, __stream, __module_data, 0UL, 1048576UL, 1UL, __tempargs294);
  return true;
}

static bool mul__261(float* __restrict__ __outs_0, float* __restrict__ __ins_0, float* __restrict__ __ins_1) noexcept{
  generic_val __tempargs295[3UL];
  __tempargs295[0UL] = __ins_0;
  __tempargs295[1UL] = __ins_1;
  __tempargs295[2UL] = __outs_0;
  sc_parallel_call_cpu_with_env((void*)&mul__2610_closure_295_0wrapper, __stream, __module_data, 0UL, 1048576UL, 1UL, __tempargs295);
  return true;
}

static bool cast__262(int8_t* __restrict__ __outs_0, float* __restrict__ __ins_0) noexcept{
  generic_val __tempargs296[2UL];
  __tempargs296[0UL] = __ins_0;
  __tempargs296[1UL] = __outs_0;
  sc_parallel_call_cpu_with_env((void*)&cast__2620_closure_296_0wrapper, __stream, __module_data, 0UL, 1048576UL, 1UL, __tempargs296);
  return true;
}

static bool mul__246(float* __restrict__ __outs_0, float* __restrict__ __ins_0, float* __restrict__ __ins_1) noexcept{
  generic_val __tempargs297[3UL];
  __tempargs297[0UL] = __ins_0;
  __tempargs297[1UL] = __ins_1;
  __tempargs297[2UL] = __outs_0;
  sc_parallel_call_cpu_with_env((void*)&mul__2460_closure_297_0wrapper, __stream, __module_data, 0UL, 1048576UL, 1UL, __tempargs297);
  return true;
}

static bool cast__247(int8_t* __restrict__ __outs_0, float* __restrict__ __ins_0) noexcept{
  generic_val __tempargs298[2UL];
  __tempargs298[0UL] = __ins_0;
  __tempargs298[1UL] = __outs_0;
  sc_parallel_call_cpu_with_env((void*)&cast__2470_closure_298_0wrapper, __stream, __module_data, 0UL, 1048576UL, 1UL, __tempargs298);
  return true;
}

static bool mul__255(float* __restrict__ __outs_0, float* __restrict__ __ins_0, float* __restrict__ __ins_1) noexcept{
  generic_val __tempargs299[3UL];
  __tempargs299[0UL] = __ins_0;
  __tempargs299[1UL] = __ins_1;
  __tempargs299[2UL] = __outs_0;
  sc_parallel_call_cpu_with_env((void*)&mul__2550_closure_299_0wrapper, __stream, __module_data, 0UL, 1048576UL, 1UL, __tempargs299);
  return true;
}

static bool cast__256(int8_t* __restrict__ __outs_0, float* __restrict__ __ins_0) noexcept{
  generic_val __tempargs300[2UL];
  __tempargs300[0UL] = __ins_0;
  __tempargs300[1UL] = __outs_0;
  sc_parallel_call_cpu_with_env((void*)&cast__2560_closure_300_0wrapper, __stream, __module_data, 0UL, 1048576UL, 1UL, __tempargs300);
  return true;
}

static bool mul__234(float* __restrict__ __outs_0, float* __restrict__ __ins_0, float* __restrict__ __ins_1) noexcept{
  generic_val __tempargs301[3UL];
  __tempargs301[0UL] = __ins_0;
  __tempargs301[1UL] = __ins_1;
  __tempargs301[2UL] = __outs_0;
  sc_parallel_call_cpu_with_env((void*)&mul__2340_closure_301_0wrapper, __stream, __module_data, 0UL, 2097152UL, 1UL, __tempargs301);
  return true;
}

static bool cast__235(int8_t* __restrict__ __outs_0, float* __restrict__ __ins_0) noexcept{
  generic_val __tempargs302[2UL];
  __tempargs302[0UL] = __ins_0;
  __tempargs302[1UL] = __outs_0;
  sc_parallel_call_cpu_with_env((void*)&cast__2350_closure_302_0wrapper, __stream, __module_data, 0UL, 2097152UL, 1UL, __tempargs302);
  return true;
}

static bool batchwise_4_fused_res2a_conv_b_cast_mul_add_cast_reorder_res2a_conv_0_cast_mul_add_relu_cast_res2a_conv_1_cast_mul_add_relu_cast_res2a_conv_2_cast_mul_add_cast_add_cast_reorder_res2b_conv_0_cast_mul_add_relu_cast_res2b_conv_1_cast_mul_add_relu_cast_reorder_res2b_conv_2_cast_mul_add_cast_add_cast_reorder_res2c_conv_0_cast_mul_add_relu_cast_reorder_res2c_conv_1_cast_mul_add_relu_cast_reorder_res2c_conv_2_cast_mul_add_cast_add_cast_reorder_res3a_conv_b_cast_mul_add_cast_res3a_conv_0_cast_mul_add_relu_cast_reorder_res3a_conv_1_cast_mul_add_relu_cast_res3a_conv_2_cast_mul_add_cast_add_cast_reorder_res3b_conv_0_cast_mul_add_relu_cast_reorder_res3b_conv_1_cast_mul_add_relu_cast_reorder_res3b_conv_2_cast_mul_add_cast_add_cast_reorder_res3c_conv_0_cast_mul_add_relu_cast_reorder_res3c_conv_1_cast_mul_add_relu_cast_reorder_res3c_conv_2_cast_mul_add_cast_add_cast_reorder_res3d_conv_0_cast_mul_add_relu_cast_reorder_res3d_conv_1_cast_mul_add_relu_cast_res3d_conv_2_cast_mul_add_cast_add_cast_res4a_conv_b_cast_mul_add_cast_reorder_res4a_conv_0_cast_mul_add_relu_cast_res4a_conv_1_cast_mul_add_relu_cast_reorder_res4a_conv_2_cast_mul_add_cast_add_cast_reorder_res4b_conv_0_cast_mul_add_relu_cast_reorder_res4b_conv_1_cast_mul_add_relu_cast_reorder_res4b_conv_2_cast_mul_add_cast_add_cast_reorder_res4c_conv_0_cast_mul_add_relu_cast_reorder_res4c_conv_1_cast_mul_add_relu_cast_reorder_res4c_conv_2_cast_mul_add_cast_add_cast_res4d_conv_0_cast_mul_add_relu_cast_res4d_conv_1_cast_mul_add_relu_cast_reorder_res4d_conv_2_cast_mul_add_cast_add_cast_res4e_conv_0_cast_mul_add_relu_cast_reorder_res4e_conv_1_cast_mul_add_relu_cast_reorder_res4e_conv_2_cast_mul_add_cast_add_cast_reorder_reorder_res4f_conv_0_cast_mul_add_relu_cast_reorder_res4f_conv_1_cast_mul_add_relu_cast_reorder_res4f_conv_2_cast_mul_add_cast_add_cast__683(uint8_t* __restrict__ __outs_0, int8_t* __restrict__ __ins_0, int8_t* __restrict__ __ins_1, float* __restrict__ __ins_2, float* __restrict__ __ins_3, int8_t* __restrict__ __ins_4, float* __restrict__ __ins_5, float* __restrict__ __ins_6, int8_t* __restrict__ __ins_7, float* __restrict__ __ins_8, float* __restrict__ __ins_9, int8_t* __restrict__ __ins_10, float* __restrict__ __ins_11, float* __restrict__ __ins_12, int8_t* __restrict__ __ins_13, float* __restrict__ __ins_14, float* __restrict__ __ins_15, int8_t* __restrict__ __ins_16, float* __restrict__ __ins_17, float* __restrict__ __ins_18, int8_t* __restrict__ __ins_19, float* __restrict__ __ins_20, float* __restrict__ __ins_21, int8_t* __restrict__ __ins_22, float* __restrict__ __ins_23, float* __restrict__ __ins_24, int8_t* __restrict__ __ins_25, float* __restrict__ __ins_26, float* __restrict__ __ins_27, int8_t* __restrict__ __ins_28, float* __restrict__ __ins_29, float* __restrict__ __ins_30, int8_t* __restrict__ __ins_31, float* __restrict__ __ins_32, float* __restrict__ __ins_33, int8_t* __restrict__ __ins_34, float* __restrict__ __ins_35, float* __restrict__ __ins_36, int8_t* __restrict__ __ins_37, float* __restrict__ __ins_38, float* __restrict__ __ins_39, int8_t* __restrict__ __ins_40, float* __restrict__ __ins_41, float* __restrict__ __ins_42, int8_t* __restrict__ __ins_43, float* __restrict__ __ins_44, float* __restrict__ __ins_45, int8_t* __restrict__ __ins_46, float* __restrict__ __ins_47, float* __restrict__ __ins_48, int8_t* __restrict__ __ins_49, float* __restrict__ __ins_50, float* __restrict__ __ins_51, int8_t* __restrict__ __ins_52, float* __restrict__ __ins_53, float* __restrict__ __ins_54, int8_t* __restrict__ __ins_55, float* __restrict__ __ins_56, float* __restrict__ __ins_57, int8_t* __restrict__ __ins_58, float* __restrict__ __ins_59, float* __restrict__ __ins_60, int8_t* __restrict__ __ins_61, float* __restrict__ __ins_62, float* __restrict__ __ins_63, int8_t* __restrict__ __ins_64, float* __restrict__ __ins_65, float* __restrict__ __ins_66, int8_t* __restrict__ __ins_67, float* __restrict__ __ins_68, float* __restrict__ __ins_69, int8_t* __restrict__ __ins_70, float* __restrict__ __ins_71, float* __restrict__ __ins_72, int8_t* __restrict__ __ins_73, float* __restrict__ __ins_74, float* __restrict__ __ins_75, int8_t* __restrict__ __ins_76, float* __restrict__ __ins_77, float* __restrict__ __ins_78, int8_t* __restrict__ __ins_79, float* __restrict__ __ins_80, float* __restrict__ __ins_81, int8_t* __restrict__ __ins_82, float* __restrict__ __ins_83, float* __restrict__ __ins_84, int8_t* __restrict__ __ins_85, float* __restrict__ __ins_86, float* __restrict__ __ins_87, int8_t* __restrict__ __ins_88, float* __restrict__ __ins_89, float* __restrict__ __ins_90, int8_t* __restrict__ __ins_91, float* __restrict__ __ins_92, float* __restrict__ __ins_93, int8_t* __restrict__ __ins_94, float* __restrict__ __ins_95, float* __restrict__ __ins_96, int8_t* __restrict__ __ins_97, float* __restrict__ __ins_98, float* __restrict__ __ins_99, int8_t* __restrict__ __ins_100, float* __restrict__ __ins_101, float* __restrict__ __ins_102, int8_t* __restrict__ __ins_103, float* __restrict__ __ins_104, float* __restrict__ __ins_105, int8_t* __restrict__ __ins_106, float* __restrict__ __ins_107, float* __restrict__ __ins_108, int8_t* __restrict__ __ins_109, float* __restrict__ __ins_110, float* __restrict__ __ins_111, int8_t* __restrict__ __ins_112, float* __restrict__ __ins_113, float* __restrict__ __ins_114, int8_t* __restrict__ __ins_115, float* __restrict__ __ins_116, float* __restrict__ __ins_117, int8_t* __restrict__ __ins_118, float* __restrict__ __ins_119, float* __restrict__ __ins_120, int8_t* __restrict__ __ins_121, float* __restrict__ __ins_122, float* __restrict__ __ins_123, int8_t* __restrict__ __ins_124, float* __restrict__ __ins_125, float* __restrict__ __ins_126) noexcept{
  generic_val* __tempargs303 = (generic_val*)sc_aligned_malloc(__stream, 1024UL);
  __tempargs303[0UL] = __ins_0;
  __tempargs303[1UL] = __ins_4;
  __tempargs303[2UL] = __ins_5;
  __tempargs303[3UL] = __ins_6;
  __tempargs303[4UL] = __ins_7;
  __tempargs303[5UL] = __ins_8;
  __tempargs303[6UL] = __ins_9;
  __tempargs303[7UL] = __ins_1;
  __tempargs303[8UL] = __ins_2;
  __tempargs303[9UL] = __ins_3;
  __tempargs303[10UL] = __ins_10;
  __tempargs303[11UL] = __ins_11;
  __tempargs303[12UL] = __ins_12;
  __tempargs303[13UL] = __ins_13;
  __tempargs303[14UL] = __ins_14;
  __tempargs303[15UL] = __ins_15;
  __tempargs303[16UL] = __ins_16;
  __tempargs303[17UL] = __ins_17;
  __tempargs303[18UL] = __ins_18;
  __tempargs303[19UL] = __ins_19;
  __tempargs303[20UL] = __ins_20;
  __tempargs303[21UL] = __ins_21;
  __tempargs303[22UL] = __ins_22;
  __tempargs303[23UL] = __ins_23;
  __tempargs303[24UL] = __ins_24;
  __tempargs303[25UL] = __ins_25;
  __tempargs303[26UL] = __ins_26;
  __tempargs303[27UL] = __ins_27;
  __tempargs303[28UL] = __ins_28;
  __tempargs303[29UL] = __ins_29;
  __tempargs303[30UL] = __ins_30;
  __tempargs303[31UL] = __ins_34;
  __tempargs303[32UL] = __ins_35;
  __tempargs303[33UL] = __ins_36;
  __tempargs303[34UL] = __ins_37;
  __tempargs303[35UL] = __ins_38;
  __tempargs303[36UL] = __ins_39;
  __tempargs303[37UL] = __ins_31;
  __tempargs303[38UL] = __ins_32;
  __tempargs303[39UL] = __ins_33;
  __tempargs303[40UL] = __ins_40;
  __tempargs303[41UL] = __ins_41;
  __tempargs303[42UL] = __ins_42;
  __tempargs303[43UL] = __ins_43;
  __tempargs303[44UL] = __ins_44;
  __tempargs303[45UL] = __ins_45;
  __tempargs303[46UL] = __ins_46;
  __tempargs303[47UL] = __ins_47;
  __tempargs303[48UL] = __ins_48;
  __tempargs303[49UL] = __ins_49;
  __tempargs303[50UL] = __ins_50;
  __tempargs303[51UL] = __ins_51;
  __tempargs303[52UL] = __ins_52;
  __tempargs303[53UL] = __ins_53;
  __tempargs303[54UL] = __ins_54;
  __tempargs303[55UL] = __ins_55;
  __tempargs303[56UL] = __ins_56;
  __tempargs303[57UL] = __ins_57;
  __tempargs303[58UL] = __ins_58;
  __tempargs303[59UL] = __ins_59;
  __tempargs303[60UL] = __ins_60;
  __tempargs303[61UL] = __ins_61;
  __tempargs303[62UL] = __ins_62;
  __tempargs303[63UL] = __ins_63;
  __tempargs303[64UL] = __ins_64;
  __tempargs303[65UL] = __ins_65;
  __tempargs303[66UL] = __ins_66;
  __tempargs303[67UL] = __ins_67;
  __tempargs303[68UL] = __ins_68;
  __tempargs303[69UL] = __ins_69;
  __tempargs303[70UL] = __ins_73;
  __tempargs303[71UL] = __ins_74;
  __tempargs303[72UL] = __ins_75;
  __tempargs303[73UL] = __ins_76;
  __tempargs303[74UL] = __ins_77;
  __tempargs303[75UL] = __ins_78;
  __tempargs303[76UL] = __ins_70;
  __tempargs303[77UL] = __ins_71;
  __tempargs303[78UL] = __ins_72;
  __tempargs303[79UL] = __ins_79;
  __tempargs303[80UL] = __ins_80;
  __tempargs303[81UL] = __ins_81;
  __tempargs303[82UL] = __ins_82;
  __tempargs303[83UL] = __ins_83;
  __tempargs303[84UL] = __ins_84;
  __tempargs303[85UL] = __ins_85;
  __tempargs303[86UL] = __ins_86;
  __tempargs303[87UL] = __ins_87;
  __tempargs303[88UL] = __ins_88;
  __tempargs303[89UL] = __ins_89;
  __tempargs303[90UL] = __ins_90;
  __tempargs303[91UL] = __ins_91;
  __tempargs303[92UL] = __ins_92;
  __tempargs303[93UL] = __ins_93;
  __tempargs303[94UL] = __ins_94;
  __tempargs303[95UL] = __ins_95;
  __tempargs303[96UL] = __ins_96;
  __tempargs303[97UL] = __ins_97;
  __tempargs303[98UL] = __ins_98;
  __tempargs303[99UL] = __ins_99;
  __tempargs303[100UL] = __ins_100;
  __tempargs303[101UL] = __ins_101;
  __tempargs303[102UL] = __ins_102;
  __tempargs303[103UL] = __ins_103;
  __tempargs303[104UL] = __ins_104;
  __tempargs303[105UL] = __ins_105;
  __tempargs303[106UL] = __ins_106;
  __tempargs303[107UL] = __ins_107;
  __tempargs303[108UL] = __ins_108;
  __tempargs303[109UL] = __ins_109;
  __tempargs303[110UL] = __ins_110;
  __tempargs303[111UL] = __ins_111;
  __tempargs303[112UL] = __ins_112;
  __tempargs303[113UL] = __ins_113;
  __tempargs303[114UL] = __ins_114;
  __tempargs303[115UL] = __ins_115;
  __tempargs303[116UL] = __ins_116;
  __tempargs303[117UL] = __ins_117;
  __tempargs303[118UL] = __ins_118;
  __tempargs303[119UL] = __ins_119;
  __tempargs303[120UL] = __ins_120;
  __tempargs303[121UL] = __ins_121;
  __tempargs303[122UL] = __ins_122;
  __tempargs303[123UL] = __ins_123;
  __tempargs303[124UL] = __outs_0;
  __tempargs303[125UL] = __ins_124;
  __tempargs303[126UL] = __ins_125;
  __tempargs303[127UL] = __ins_126;
  sc_parallel_call_cpu_with_env((void*)&batchwise_4_fused_res2a_conv_b_cast_mul_add_cast_reorder_res2a_conv_0_cast_mul_add_relu_cast_res2a_conv_1_cast_mul_add_relu_cast_res2a_conv_2_cast_mul_add_cast_add_cast_reorder_res2b_conv_0_cast_mul_add_relu_cast_res2b_conv_1_cast_mul_add_relu_cast_reorder_res2b_conv_2_cast_mul_add_cast_add_cast_reorder_res2c_conv_0_cast_mul_add_relu_cast_reorder_res2c_conv_1_cast_mul_add_relu_cast_reorder_res2c_conv_2_cast_mul_add_cast_add_cast_reorder_res3a_conv_b_cast_mul_add_cast_res3a_conv_0_cast_mul_add_relu_cast_reorder_res3a_conv_1_cast_mul_add_relu_cast_res3a_conv_2_cast_mul_add_cast_add_cast_reorder_res3b_conv_0_cast_mul_add_relu_cast_reorder_res3b_conv_1_cast_mul_add_relu_cast_reorder_res3b_conv_2_cast_mul_add_cast_add_cast_reorder_res3c_conv_0_cast_mul_add_relu_cast_reorder_res3c_conv_1_cast_mul_add_relu_cast_reorder_res3c_conv_2_cast_mul_add_cast_add_cast_reorder_res3d_conv_0_cast_mul_add_relu_cast_reorder_res3d_conv_1_cast_mul_add_relu_cast_res3d_conv_2_cast_mul_add_cast_add_cast_res4a_conv_b_cast_mul_add_cast_reorder_res4a_conv_0_cast_mul_add_relu_cast_res4a_conv_1_cast_mul_add_relu_cast_reorder_res4a_conv_2_cast_mul_add_cast_add_cast_reorder_res4b_conv_0_cast_mul_add_relu_cast_reorder_res4b_conv_1_cast_mul_add_relu_cast_reorder_res4b_conv_2_cast_mul_add_cast_add_cast_reorder_res4c_conv_0_cast_mul_add_relu_cast_reorder_res4c_conv_1_cast_mul_add_relu_cast_reorder_res4c_conv_2_cast_mul_add_cast_add_cast_res4d_conv_0_cast_mul_add_relu_cast_res4d_conv_1_cast_mul_add_relu_cast_reorder_res4d_conv_2_cast_mul_add_cast_add_cast_res4e_conv_0_cast_mul_add_relu_cast_reorder_res4e_conv_1_cast_mul_add_relu_cast_reorder_res4e_conv_2_cast_mul_add_cast_add_cast_reorder_reorder_res4f_conv_0_cast_mul_add_relu_cast_reorder_res4f_conv_1_cast_mul_add_relu_cast_reorder_res4f_conv_2_cast_mul_add_cast_add_cast__6830_closure_303_0wrapper, __stream, __module_data, 0UL, 4UL, 1UL, __tempargs303);
  sc_aligned_free(__stream, __tempargs303);
  return true;
}


static bool res2a_conv_0_cast_mul_add_relu_cast__8(int8_t* __restrict__ __outs_0, int8_t* __restrict__ __ins_0, int8_t* __restrict__ __ins_1, float* __restrict__ __ins_2, float* __restrict__ __ins_3) noexcept{
  void*& __sc_kernel_cache = *(void**)(__module_data + 8);
  alignas(64) int8_t __rescheduled_0[128UL];
  memset(&__outs_0[0UL], 0, 3712UL);
  for (uint64_t p1 = 0UL; p1 < 56UL; p1 += 1UL) {
    memset(&__outs_0[((p1 + 1UL) * 3712UL)], 0, 64UL);
    memset(&__outs_0[(((p1 + 1UL) * 3712UL) + 3648UL)], 0, 64UL);
  }
  memset(&__outs_0[211584UL], 0, 3712UL);
  for (uint64_t p_o = 0UL; p_o < 28UL; p_o += 1UL) {
    int32_t* __origouts_1560_shr = (int32_t*)sc_aligned_malloc(__stream, 28672UL);
    void** A_list = (void**)&__rescheduled_0[0UL];
    void** B_list = (void**)&__rescheduled_0[64UL];
    void* __cached_0;
    __cached_0 = &__ins_0[(p_o * 7168UL)];
    A_list[0UL] = __cached_0;
    void* __cached_1;
    __cached_1 = &__ins_1[0UL];
    B_list[0UL] = __cached_1;
    void* _arg_cache_0 = &__origouts_1560_shr[0UL];
    dnnl_brgemm_list_call(__sc_kernel_cache, A_list, B_list, &__origouts_1560_shr[0UL], 1, 1, 1, 1, 7, 7, __stream);
    for (uint64_t _fuseiter5550 = 0UL; _fuseiter5550 < 2UL; _fuseiter5550 += 1UL) {
      for (uint64_t _fuseiter5551 = 0UL; _fuseiter5551 < 56UL; _fuseiter5551 += 1UL) {
        for (uint64_t _fuseiter5552 = 0UL; _fuseiter5552 < 64UL; _fuseiter5552 += 16UL) {
          vec_s32x16 __cached_2;
          __cached_2 = vec_s32x16::load(&__origouts_1560_shr[((_fuseiter5550 * 3584UL) + ((_fuseiter5551 * 64UL) + _fuseiter5552))]);
          vec_f32x16 __cached_3;
          __cached_3 = (vec_f32x16)(__cached_2);
          vec_f32x16 __cached_4;
          __cached_4 = vec_f32x16::load(&__ins_2[_fuseiter5552]);
          __cached_3 = (__cached_3 * __cached_4);
          vec_f32x16 __cached_5;
          __cached_5 = vec_f32x16::load(&__ins_3[_fuseiter5552]);
          __cached_3 = (__cached_3 + __cached_5);
          __cached_3 = sc_max(__cached_3, vec_f32x16(0.f));
          vec_s8x16 __cached_6;
          __cached_6 = sc_saturated_cast<vec_s8x16>(sc_round_and_cast<vec_s32x16>(__cached_3));
          vec_s8x16::store(__cached_6, &__outs_0[(((((p_o * 2UL) + 1UL) * 3712UL) + 64UL) + ((_fuseiter5550 * 3712UL) + ((_fuseiter5551 * 64UL) + _fuseiter5552)))]);
        }
      }
    }
    sc_aligned_free(__stream, __origouts_1560_shr);
  }
  return true;
}

static bool res2a_conv_1_cast_mul_add_relu_cast__12(int8_t* __restrict__ __outs_0, int8_t* __restrict__ __ins_0, int8_t* __restrict__ __ins_1, float* __restrict__ __ins_2, float* __restrict__ __ins_3) noexcept{
  void*& __sc_kernel_cache_104 = *(void**)(__module_data + 16);
  int8_t* __rescheduled_0 = (int8_t*)sc_aligned_malloc(__stream, 320UL);
  for (uint64_t fused_0k_o__n_1823 = 0UL; fused_0k_o__n_1823 < 2UL; fused_0k_o__n_1823 += 1UL) {
    int32_t* __origouts_1570_shr = (int32_t*)sc_aligned_malloc(__stream, 401408UL);
    for (uint64_t o_o = 0UL; o_o < 224UL; o_o += 1UL) {
      void** A_list = (void**)&__rescheduled_0[0UL];
      void** B_list = (void**)&__rescheduled_0[128UL];
      memset(&__origouts_1570_shr[((((o_o * 14UL) / 56UL) * 1792UL) + (((o_o * 14UL) % 56UL) * 32UL))], 0, 1792UL);
      for (uint64_t r = 0UL; r < 3UL; r += 1UL) {
        for (uint64_t s = 0UL; s < 3UL; s += 1UL) {
          void* __cached_0;
          __cached_0 = &__ins_0[(((((o_o * 14UL) / 56UL) + r) * 3712UL) + ((((o_o * 14UL) % 56UL) + s) * 64UL))];
          A_list[((r * 3UL) + s)] = __cached_0;
          void* __cached_1;
          __cached_1 = &__ins_1[((fused_0k_o__n_1823 * 18432UL) + ((r * 6144UL) + (s * 2048UL)))];
          B_list[((r * 3UL) + s)] = __cached_1;
        }
      }
      void* _arg_cache_1 = &__origouts_1570_shr[((((o_o * 14UL) / 56UL) * 1792UL) + (((o_o * 14UL) % 56UL) * 32UL))];
      dnnl_brgemm_list_call(__sc_kernel_cache_104, A_list, B_list, &__origouts_1570_shr[((((o_o * 14UL) / 56UL) * 1792UL) + (((o_o * 14UL) % 56UL) * 32UL))], 1, 64, 2048, 9, 7, 7, __stream);
    }
    float* _cast_buf_0_shr = (float*)&__rescheduled_0[256UL];
    for (uint64_t _fuseiter5580 = 0UL; _fuseiter5580 < 56UL; _fuseiter5580 += 1UL) {
      for (uint64_t _fuseiter5581 = 0UL; _fuseiter5581 < 56UL; _fuseiter5581 += 1UL) {
        for (uint64_t _fuseiter5582 = 0UL; _fuseiter5582 < 32UL; _fuseiter5582 += 16UL) {
          vec_s32x16 __cached_2;
          __cached_2 = vec_s32x16::load(&__origouts_1570_shr[((_fuseiter5580 * 1792UL) + ((_fuseiter5581 * 32UL) + _fuseiter5582))]);
          vec_f32x16 __cached_3;
          __cached_3 = (vec_f32x16)(__cached_2);
          vec_f32x16::store(__cached_3, &_cast_buf_0_shr[((((0UL - _fuseiter5580) * 16UL) + (((0UL - _fuseiter5581) * 16UL) + (0UL - _fuseiter5582))) + ((_fuseiter5580 * 16UL) + ((_fuseiter5581 * 16UL) + _fuseiter5582)))]);
          vec_f32x16 __cached_4;
          __cached_4 = vec_f32x16::load(&_cast_buf_0_shr[0UL]);
          vec_f32x16 __cached_5;
          __cached_5 = vec_f32x16::load(&__ins_2[((fused_0k_o__n_1823 * 32UL) + _fuseiter5582)]);
          __cached_4 = (__cached_4 * __cached_5);
          vec_f32x16 __cached_6;
          __cached_6 = vec_f32x16::load(&__ins_3[((fused_0k_o__n_1823 * 32UL) + _fuseiter5582)]);
          __cached_4 = (__cached_4 + __cached_6);
          __cached_4 = sc_max(__cached_4, vec_f32x16(0.f));
          vec_f32x16::store(__cached_4, &_cast_buf_0_shr[0UL]);
          vec_s8x16 __cached_7;
          __cached_7 = sc_saturated_cast<vec_s8x16>(sc_round_and_cast<vec_s32x16>(__cached_4));
          vec_s8x16::store(__cached_7, &__outs_0[((fused_0k_o__n_1823 * 100352UL) + ((_fuseiter5580 * 1792UL) + ((_fuseiter5581 * 32UL) + _fuseiter5582)))]);
        }
      }
    }
    sc_aligned_free(__stream, __origouts_1570_shr);
  }
  sc_aligned_free(__stream, __rescheduled_0);
  return true;
}

static bool res2a_conv_b_cast_mul_add_cast_reorder__4(int8_t* __restrict__ __outs_0, int8_t* __restrict__ __ins_0, int8_t* __restrict__ __ins_1, float* __restrict__ __ins_2, float* __restrict__ __ins_3) noexcept{
  void*& __sc_kernel_cache_106 = *(void**)(__module_data + 24);
  alignas(64) int8_t __rescheduled_0[128UL];
  for (uint64_t fused_0n__k_1824 = 0UL; fused_0n__k_1824 < 16UL; fused_0n__k_1824 += 1UL) {
    for (uint64_t p_o = 0UL; p_o < 28UL; p_o += 1UL) {
      int32_t* __origouts_1580_shr = (int32_t*)sc_aligned_malloc(__stream, 7168UL);
      void** A_list = (void**)&__rescheduled_0[0UL];
      void** B_list = (void**)&__rescheduled_0[64UL];
      void* __cached_0;
      __cached_0 = &__ins_0[(((fused_0n__k_1824 / 16UL) * 200704UL) + (p_o * 7168UL))];
      A_list[0UL] = __cached_0;
      void* __cached_1;
      __cached_1 = &__ins_1[((fused_0n__k_1824 % 16UL) * 1024UL)];
      B_list[0UL] = __cached_1;
      void* _arg_cache_2 = &__origouts_1580_shr[0UL];
      dnnl_brgemm_list_call(__sc_kernel_cache_106, A_list, B_list, &__origouts_1580_shr[0UL], 1, 1, 1, 1, 7, 7, __stream);
      for (uint64_t _fuseiter5610 = 0UL; _fuseiter5610 < 2UL; _fuseiter5610 += 1UL) {
        for (uint64_t _fuseiter5611 = 0UL; _fuseiter5611 < 56UL; _fuseiter5611 += 1UL) {
          vec_s32x16 __cached_2;
          __cached_2 = vec_s32x16::load(&__origouts_1580_shr[((_fuseiter5610 * 896UL) + (_fuseiter5611 * 16UL))]);
          vec_f32x16 __cached_3;
          __cached_3 = (vec_f32x16)(__cached_2);
          vec_f32x16 __cached_4;
          __cached_4 = vec_f32x16::load(&__ins_2[(((fused_0n__k_1824 / 16UL) * 256UL) + ((fused_0n__k_1824 % 16UL) * 16UL))]);
          __cached_3 = (__cached_3 * __cached_4);
          vec_f32x16 __cached_5;
          __cached_5 = vec_f32x16::load(&__ins_3[((fused_0n__k_1824 % 16UL) * 16UL)]);
          __cached_3 = (__cached_3 + __cached_5);
          vec_s8x16 __cached_6;
          __cached_6 = sc_saturated_cast<vec_s8x16>(sc_round_and_cast<vec_s32x16>(__cached_3));
          vec_s8x16 __cached_7;
          __cached_7 = __cached_6;
          vec_s8x16::store(__cached_7, &__outs_0[(((fused_0n__k_1824 / 16UL) * 802816UL) + (((((fused_0n__k_1824 % 16UL) * 16UL) / 64UL) * 200704UL) + (((_fuseiter5610 + (p_o * 2UL)) * 3584UL) + ((_fuseiter5611 * 64UL) + (((fused_0n__k_1824 % 16UL) * 16UL) % 64UL)))))]);
        }
      }
      sc_aligned_free(__stream, __origouts_1580_shr);
    }
  }
  return true;
}

static bool res2a_conv_2_cast_mul_add_cast_add_cast_reorder__16(uint8_t* __restrict__ __outs_0, int8_t* __restrict__ __ins_0, int8_t* __restrict__ __ins_1, float* __restrict__ __ins_2, float* __restrict__ __ins_3, int8_t* __restrict__ __ins_4) noexcept{
  void*& __sc_kernel_cache_108 = *(void**)(__module_data + 32);
  alignas(64) int8_t __rescheduled_0[128UL];
  for (uint64_t fused_0n__k_1825 = 0UL; fused_0n__k_1825 < 4UL; fused_0n__k_1825 += 1UL) {
    for (uint64_t p_o = 0UL; p_o < 56UL; p_o += 1UL) {
      int32_t* __origouts_1590_shr = (int32_t*)sc_aligned_malloc(__stream, 14336UL);
      void** A_list = (void**)&__rescheduled_0[0UL];
      void** B_list = (void**)&__rescheduled_0[64UL];
      for (uint64_t c = 0UL; c < 2UL; c += 1UL) {
        void* __cached_0;
        __cached_0 = &__ins_0[(((fused_0n__k_1825 / 4UL) * 200704UL) + ((c * 100352UL) + (p_o * 1792UL)))];
        A_list[c] = __cached_0;
        void* __cached_1;
        __cached_1 = &__ins_1[(((fused_0n__k_1825 % 4UL) * 4096UL) + (c * 2048UL))];
        B_list[c] = __cached_1;
      }
      void* _arg_cache_3 = &__origouts_1590_shr[0UL];
      dnnl_brgemm_list_call(__sc_kernel_cache_108, A_list, B_list, &__origouts_1590_shr[0UL], 1, 1, 1, 2, 7, 7, __stream);
      for (uint64_t _fuseiter5640 = 0UL; _fuseiter5640 < 56UL; _fuseiter5640 += 1UL) {
        for (uint64_t _fuseiter5641 = 0UL; _fuseiter5641 < 64UL; _fuseiter5641 += 16UL) {
          vec_s32x16 __cached_2;
          __cached_2 = vec_s32x16::load(&__origouts_1590_shr[((_fuseiter5640 * 64UL) + _fuseiter5641)]);
          vec_f32x16 __cached_3;
          __cached_3 = (vec_f32x16)(__cached_2);
          vec_f32x16 __cached_4;
          __cached_4 = vec_f32x16::load(&__ins_2[((((fused_0n__k_1825 / 4UL) * 256UL) + ((fused_0n__k_1825 % 4UL) * 64UL)) + _fuseiter5641)]);
          __cached_3 = (__cached_3 * __cached_4);
          vec_f32x16 __cached_5;
          __cached_5 = vec_f32x16::load(&__ins_3[(((fused_0n__k_1825 % 4UL) * 64UL) + _fuseiter5641)]);
          __cached_3 = (__cached_3 + __cached_5);
          vec_s8x16 __cached_6;
          __cached_6 = sc_saturated_cast<vec_s8x16>(sc_round_and_cast<vec_s32x16>(__cached_3));
          vec_s8x16 __cached_7;
          __cached_7 = vec_s8x16::load(&__ins_4[((((fused_0n__k_1825 / 4UL) * 802816UL) + (((fused_0n__k_1825 % 4UL) * 200704UL) + (p_o * 3584UL))) + ((_fuseiter5640 * 64UL) + _fuseiter5641))]);
          __cached_6 = (__cached_6 + __cached_7);
          vec_u8x16 __cached_8;
          __cached_8 = (vec_u8x16)(sc_max(__cached_6, vec_s8x16(0)));
          vec_u8x16 __cached_9;
          __cached_9 = __cached_8;
          vec_u8x16::store(__cached_9, &__outs_0[(((fused_0n__k_1825 / 4UL) * 802816UL) + ((((_fuseiter5641 + ((fused_0n__k_1825 % 4UL) * 64UL)) / 32UL) * 100352UL) + ((p_o * 1792UL) + ((_fuseiter5640 * 32UL) + ((_fuseiter5641 + ((fused_0n__k_1825 % 4UL) * 64UL)) % 32UL)))))]);
        }
      }
      sc_aligned_free(__stream, __origouts_1590_shr);
    }
  }
  return true;
}

static bool res2b_conv_0_cast_mul_add_relu_cast__20(int8_t* __restrict__ __outs_0, uint8_t* __restrict__ __ins_0, int8_t* __restrict__ __ins_1, float* __restrict__ __ins_2, float* __restrict__ __ins_3) noexcept{
  void*& __sc_kernel_cache_110 = *(void**)(__module_data + 40);
  alignas(64) int8_t __rescheduled_0[128UL];
  memset(&__outs_0[0UL], 0, 3712UL);
  for (uint64_t p1 = 0UL; p1 < 56UL; p1 += 1UL) {
    memset(&__outs_0[((p1 + 1UL) * 3712UL)], 0, 64UL);
    memset(&__outs_0[(((p1 + 1UL) * 3712UL) + 3648UL)], 0, 64UL);
  }
  memset(&__outs_0[211584UL], 0, 3712UL);
  for (uint64_t p_o = 0UL; p_o < 8UL; p_o += 1UL) {
    int32_t* __origouts_1600_shr = (int32_t*)sc_aligned_malloc(__stream, 100352UL);
    void** A_list = (void**)&__rescheduled_0[0UL];
    void** B_list = (void**)&__rescheduled_0[64UL];
    for (uint64_t c = 0UL; c < 8UL; c += 1UL) {
      void* __cached_0;
      __cached_0 = &__ins_0[((c * 100352UL) + (p_o * 12544UL))];
      A_list[c] = __cached_0;
      void* __cached_1;
      __cached_1 = &__ins_1[(c * 2048UL)];
      B_list[c] = __cached_1;
    }
    void* _arg_cache_4 = &__origouts_1600_shr[0UL];
    dnnl_brgemm_list_call(__sc_kernel_cache_110, A_list, B_list, &__origouts_1600_shr[0UL], 1, 1, 1, 8, 8, 7, __stream);
    for (uint64_t _fuseiter5680 = 0UL; _fuseiter5680 < 7UL; _fuseiter5680 += 1UL) {
      for (uint64_t _fuseiter5681 = 0UL; _fuseiter5681 < 56UL; _fuseiter5681 += 1UL) {
        for (uint64_t _fuseiter5682 = 0UL; _fuseiter5682 < 64UL; _fuseiter5682 += 16UL) {
          vec_s32x16 __cached_2;
          __cached_2 = vec_s32x16::load(&__origouts_1600_shr[((_fuseiter5680 * 3584UL) + ((_fuseiter5681 * 64UL) + _fuseiter5682))]);
          vec_f32x16 __cached_3;
          __cached_3 = (vec_f32x16)(__cached_2);
          vec_f32x16 __cached_4;
          __cached_4 = vec_f32x16::load(&__ins_2[_fuseiter5682]);
          __cached_3 = (__cached_3 * __cached_4);
          vec_f32x16 __cached_5;
          __cached_5 = vec_f32x16::load(&__ins_3[_fuseiter5682]);
          __cached_3 = (__cached_3 + __cached_5);
          __cached_3 = sc_max(__cached_3, vec_f32x16(0.f));
          vec_s8x16 __cached_6;
          __cached_6 = sc_saturated_cast<vec_s8x16>(sc_round_and_cast<vec_s32x16>(__cached_3));
          vec_s8x16::store(__cached_6, &__outs_0[(((((p_o * 7UL) + 1UL) * 3712UL) + 64UL) + ((_fuseiter5680 * 3712UL) + ((_fuseiter5681 * 64UL) + _fuseiter5682)))]);
        }
      }
    }
    sc_aligned_free(__stream, __origouts_1600_shr);
  }
  return true;
}

static bool res2b_conv_1_cast_mul_add_relu_cast_reorder__24(int8_t* __restrict__ __outs_0, int8_t* __restrict__ __ins_0, int8_t* __restrict__ __ins_1, float* __restrict__ __ins_2, float* __restrict__ __ins_3) noexcept{
  void*& __sc_kernel_cache_112 = *(void**)(__module_data + 48);
  int8_t* __rescheduled_0 = (int8_t*)sc_aligned_malloc(__stream, 256UL);
  for (uint64_t fused_0n__k_o_1828 = 0UL; fused_0n__k_o_1828 < 4UL; fused_0n__k_o_1828 += 1UL) {
    int32_t* __origouts_1610_shr = (int32_t*)sc_aligned_malloc(__stream, 200704UL);
    for (uint64_t o_o = 0UL; o_o < 112UL; o_o += 1UL) {
      void** A_list = (void**)&__rescheduled_0[0UL];
      void** B_list = (void**)&__rescheduled_0[128UL];
      memset(&__origouts_1610_shr[((((o_o * 28UL) / 56UL) * 896UL) + (((o_o * 28UL) % 56UL) * 16UL))], 0, 1792UL);
      for (uint64_t r = 0UL; r < 3UL; r += 1UL) {
        for (uint64_t s = 0UL; s < 3UL; s += 1UL) {
          void* __cached_0;
          __cached_0 = &__ins_0[(((fused_0n__k_o_1828 / 4UL) * 215296UL) + (((((o_o * 28UL) / 56UL) + r) * 3712UL) + ((((o_o * 28UL) % 56UL) + s) * 64UL)))];
          A_list[((r * 3UL) + s)] = __cached_0;
          void* __cached_1;
          __cached_1 = &__ins_1[(((fused_0n__k_o_1828 % 4UL) * 9216UL) + ((r * 3072UL) + (s * 1024UL)))];
          B_list[((r * 3UL) + s)] = __cached_1;
        }
      }
      void* _arg_cache_5 = &__origouts_1610_shr[((((o_o * 28UL) / 56UL) * 896UL) + (((o_o * 28UL) % 56UL) * 16UL))];
      dnnl_brgemm_list_call(__sc_kernel_cache_112, A_list, B_list, &__origouts_1610_shr[((((o_o * 28UL) / 56UL) * 896UL) + (((o_o * 28UL) % 56UL) * 16UL))], 1, 64, 1024, 9, 7, 7, __stream);
    }
    for (uint64_t _fuseiter5710 = 0UL; _fuseiter5710 < 56UL; _fuseiter5710 += 1UL) {
      for (uint64_t _fuseiter5711 = 0UL; _fuseiter5711 < 56UL; _fuseiter5711 += 1UL) {
        vec_s32x16 __cached_2;
        __cached_2 = vec_s32x16::load(&__origouts_1610_shr[((_fuseiter5710 * 896UL) + (_fuseiter5711 * 16UL))]);
        vec_f32x16 __cached_3;
        __cached_3 = (vec_f32x16)(__cached_2);
        vec_f32x16 __cached_4;
        __cached_4 = vec_f32x16::load(&__ins_2[(((fused_0n__k_o_1828 / 4UL) * 64UL) + ((fused_0n__k_o_1828 % 4UL) * 16UL))]);
        __cached_3 = (__cached_3 * __cached_4);
        vec_f32x16 __cached_5;
        __cached_5 = vec_f32x16::load(&__ins_3[((fused_0n__k_o_1828 % 4UL) * 16UL)]);
        __cached_3 = (__cached_3 + __cached_5);
        __cached_3 = sc_max(__cached_3, vec_f32x16(0.f));
        vec_s8x16 __cached_6;
        __cached_6 = sc_saturated_cast<vec_s8x16>(sc_round_and_cast<vec_s32x16>(__cached_3));
        vec_s8x16 __cached_7;
        __cached_7 = __cached_6;
        vec_s8x16::store(__cached_7, &__outs_0[(((fused_0n__k_o_1828 / 4UL) * 200704UL) + (((((fused_0n__k_o_1828 % 4UL) * 16UL) / 64UL) * 200704UL) + ((_fuseiter5710 * 3584UL) + ((_fuseiter5711 * 64UL) + (((fused_0n__k_o_1828 % 4UL) * 16UL) % 64UL)))))]);
      }
    }
    sc_aligned_free(__stream, __origouts_1610_shr);
  }
  sc_aligned_free(__stream, __rescheduled_0);
  return true;
}

static bool res2b_conv_2_cast_mul_add_cast_add_cast_reorder__28(uint8_t* __restrict__ __outs_0, int8_t* __restrict__ __ins_0, int8_t* __restrict__ __ins_1, float* __restrict__ __ins_2, float* __restrict__ __ins_3, uint8_t* __restrict__ __ins_4) noexcept{
  void*& __sc_kernel_cache_114 = *(void**)(__module_data + 56);
  alignas(64) int8_t __rescheduled_0[128UL];
  for (uint64_t fused_0k__n_1829 = 0UL; fused_0k__n_1829 < 8UL; fused_0k__n_1829 += 1UL) {
    for (uint64_t p_o = 0UL; p_o < 56UL; p_o += 1UL) {
      int32_t* __origouts_1620_shr = (int32_t*)sc_aligned_malloc(__stream, 7168UL);
      void** A_list = (void**)&__rescheduled_0[0UL];
      void** B_list = (void**)&__rescheduled_0[64UL];
      void* __cached_0;
      __cached_0 = &__ins_0[(p_o * 3584UL)];
      A_list[0UL] = __cached_0;
      void* __cached_1;
      __cached_1 = &__ins_1[(fused_0k__n_1829 * 2048UL)];
      B_list[0UL] = __cached_1;
      void* _arg_cache_6 = &__origouts_1620_shr[0UL];
      dnnl_brgemm_list_call(__sc_kernel_cache_114, A_list, B_list, &__origouts_1620_shr[0UL], 1, 1, 1, 1, 7, 7, __stream);
      for (uint64_t _fuseiter5746 = 0UL; _fuseiter5746 < 56UL; _fuseiter5746 += 1UL) {
        for (uint64_t _fuseiter5747 = 0UL; _fuseiter5747 < 32UL; _fuseiter5747 += 16UL) {
          vec_s32x16 __cached_2;
          __cached_2 = vec_s32x16::load(&__origouts_1620_shr[((_fuseiter5746 * 32UL) + _fuseiter5747)]);
          vec_f32x16 __cached_3;
          __cached_3 = (vec_f32x16)(__cached_2);
          vec_f32x16 __cached_4;
          __cached_4 = vec_f32x16::load(&__ins_2[((fused_0k__n_1829 * 32UL) + _fuseiter5747)]);
          __cached_3 = (__cached_3 * __cached_4);
          vec_f32x16 __cached_5;
          __cached_5 = vec_f32x16::load(&__ins_3[((fused_0k__n_1829 * 32UL) + _fuseiter5747)]);
          __cached_3 = (__cached_3 + __cached_5);
          vec_s8x16 __cached_6;
          __cached_6 = sc_saturated_cast<vec_s8x16>(sc_round_and_cast<vec_s32x16>(__cached_3));
          vec_u8x16 __cached_7;
          __cached_7 = vec_u8x16::load(&__ins_4[(((fused_0k__n_1829 * 100352UL) + (p_o * 1792UL)) + ((_fuseiter5746 * 32UL) + _fuseiter5747))]);
          __cached_6 = (__cached_6 + (vec_s8x16)(__cached_7));
          vec_u8x16 __cached_8;
          __cached_8 = (vec_u8x16)(sc_max(__cached_6, vec_s8x16(0)));
          vec_u8x16 __cached_9;
          __cached_9 = __cached_8;
          vec_u8x16::store(__cached_9, &__outs_0[((((_fuseiter5747 + (fused_0k__n_1829 * 32UL)) / 64UL) * 200704UL) + ((p_o * 3584UL) + ((_fuseiter5746 * 64UL) + ((_fuseiter5747 + (fused_0k__n_1829 * 32UL)) % 64UL))))]);
        }
      }
      sc_aligned_free(__stream, __origouts_1620_shr);
    }
  }
  return true;
}

static bool res2c_conv_0_cast_mul_add_relu_cast_reorder__32(int8_t* __restrict__ __outs_0, uint8_t* __restrict__ __ins_0, int8_t* __restrict__ __ins_1, float* __restrict__ __ins_2, float* __restrict__ __ins_3) noexcept{
  void*& __sc_kernel_cache_116 = *(void**)(__module_data + 64);
  alignas(64) int8_t __rescheduled_0[128UL];
  memset(&__outs_0[0UL], 0, 3712UL);
  for (uint64_t p1 = 0UL; p1 < 56UL; p1 += 1UL) {
    memset(&__outs_0[((p1 + 1UL) * 3712UL)], 0, 64UL);
    memset(&__outs_0[(((p1 + 1UL) * 3712UL) + 3648UL)], 0, 64UL);
  }
  memset(&__outs_0[211584UL], 0, 3712UL);
  for (uint64_t fused_0n__k_1831 = 0UL; fused_0n__k_1831 < 2UL; fused_0n__k_1831 += 1UL) {
    for (uint64_t p_o = 0UL; p_o < 56UL; p_o += 1UL) {
      int32_t* __origouts_1630_shr = (int32_t*)sc_aligned_malloc(__stream, 7168UL);
      void** A_list = (void**)&__rescheduled_0[0UL];
      void** B_list = (void**)&__rescheduled_0[64UL];
      for (uint64_t c = 0UL; c < 4UL; c += 1UL) {
        void* __cached_0;
        __cached_0 = &__ins_0[(((fused_0n__k_1831 / 2UL) * 802816UL) + ((c * 200704UL) + (p_o * 3584UL)))];
        A_list[c] = __cached_0;
        void* __cached_1;
        __cached_1 = &__ins_1[(((fused_0n__k_1831 % 2UL) * 8192UL) + (c * 2048UL))];
        B_list[c] = __cached_1;
      }
      void* _arg_cache_7 = &__origouts_1630_shr[0UL];
      dnnl_brgemm_list_call(__sc_kernel_cache_116, A_list, B_list, &__origouts_1630_shr[0UL], 1, 1, 1, 4, 8, 7, __stream);
      for (uint64_t _fuseiter5787 = 0UL; _fuseiter5787 < 56UL; _fuseiter5787 += 1UL) {
        for (uint64_t _fuseiter5788 = 0UL; _fuseiter5788 < 32UL; _fuseiter5788 += 16UL) {
          vec_s32x16 __cached_2;
          __cached_2 = vec_s32x16::load(&__origouts_1630_shr[((_fuseiter5787 * 32UL) + _fuseiter5788)]);
          vec_f32x16 __cached_3;
          __cached_3 = (vec_f32x16)(__cached_2);
          vec_f32x16 __cached_4;
          __cached_4 = vec_f32x16::load(&__ins_2[((((fused_0n__k_1831 / 2UL) * 64UL) + ((fused_0n__k_1831 % 2UL) * 32UL)) + _fuseiter5788)]);
          __cached_3 = (__cached_3 * __cached_4);
          vec_f32x16 __cached_5;
          __cached_5 = vec_f32x16::load(&__ins_3[(((fused_0n__k_1831 % 2UL) * 32UL) + _fuseiter5788)]);
          __cached_3 = (__cached_3 + __cached_5);
          __cached_3 = sc_max(__cached_3, vec_f32x16(0.f));
          vec_s8x16 __cached_6;
          __cached_6 = sc_saturated_cast<vec_s8x16>(sc_round_and_cast<vec_s32x16>(__cached_3));
          vec_s8x16 __cached_7;
          __cached_7 = __cached_6;
          vec_s8x16::store(__cached_7, &__outs_0[(((fused_0n__k_1831 / 2UL) * 215296UL) + ((((_fuseiter5788 + ((fused_0n__k_1831 % 2UL) * 32UL)) / 64UL) * 215296UL) + (((p_o + 1UL) * 3712UL) + (((_fuseiter5787 + 1UL) * 64UL) + ((_fuseiter5788 + ((fused_0n__k_1831 % 2UL) * 32UL)) % 64UL)))))]);
        }
      }
      sc_aligned_free(__stream, __origouts_1630_shr);
    }
  }
  return true;
}

static bool res2c_conv_1_cast_mul_add_relu_cast_reorder__36(int8_t* __restrict__ __outs_0, int8_t* __restrict__ __ins_0, int8_t* __restrict__ __ins_1, float* __restrict__ __ins_2, float* __restrict__ __ins_3) noexcept{
  void*& __sc_kernel_cache_118 = *(void**)(__module_data + 72);
  int8_t* __rescheduled_0 = (int8_t*)sc_aligned_malloc(__stream, 256UL);
  for (uint64_t fused_0k_o__n_1832 = 0UL; fused_0k_o__n_1832 < 2UL; fused_0k_o__n_1832 += 1UL) {
    int32_t* __origouts_1640_shr = (int32_t*)sc_aligned_malloc(__stream, 401408UL);
    for (uint64_t o_o = 0UL; o_o < 112UL; o_o += 1UL) {
      void** A_list = (void**)&__rescheduled_0[0UL];
      void** B_list = (void**)&__rescheduled_0[128UL];
      memset(&__origouts_1640_shr[((((o_o * 28UL) / 56UL) * 1792UL) + (((o_o * 28UL) % 56UL) * 32UL))], 0, 3584UL);
      for (uint64_t r = 0UL; r < 3UL; r += 1UL) {
        for (uint64_t s = 0UL; s < 3UL; s += 1UL) {
          void* __cached_0;
          __cached_0 = &__ins_0[(((((o_o * 28UL) / 56UL) + r) * 3712UL) + ((((o_o * 28UL) % 56UL) + s) * 64UL))];
          A_list[((r * 3UL) + s)] = __cached_0;
          void* __cached_1;
          __cached_1 = &__ins_1[((fused_0k_o__n_1832 * 18432UL) + ((r * 6144UL) + (s * 2048UL)))];
          B_list[((r * 3UL) + s)] = __cached_1;
        }
      }
      void* _arg_cache_8 = &__origouts_1640_shr[((((o_o * 28UL) / 56UL) * 1792UL) + (((o_o * 28UL) % 56UL) * 32UL))];
      dnnl_brgemm_list_call(__sc_kernel_cache_118, A_list, B_list, &__origouts_1640_shr[((((o_o * 28UL) / 56UL) * 1792UL) + (((o_o * 28UL) % 56UL) * 32UL))], 1, 64, 2048, 9, 7, 7, __stream);
    }
    for (uint64_t _fuseiter5821 = 0UL; _fuseiter5821 < 56UL; _fuseiter5821 += 1UL) {
      for (uint64_t _fuseiter5822 = 0UL; _fuseiter5822 < 56UL; _fuseiter5822 += 1UL) {
        for (uint64_t _fuseiter5823 = 0UL; _fuseiter5823 < 32UL; _fuseiter5823 += 16UL) {
          vec_s32x16 __cached_2;
          __cached_2 = vec_s32x16::load(&__origouts_1640_shr[((_fuseiter5821 * 1792UL) + ((_fuseiter5822 * 32UL) + _fuseiter5823))]);
          vec_f32x16 __cached_3;
          __cached_3 = (vec_f32x16)(__cached_2);
          vec_f32x16 __cached_4;
          __cached_4 = vec_f32x16::load(&__ins_2[((fused_0k_o__n_1832 * 32UL) + _fuseiter5823)]);
          __cached_3 = (__cached_3 * __cached_4);
          vec_f32x16 __cached_5;
          __cached_5 = vec_f32x16::load(&__ins_3[((fused_0k_o__n_1832 * 32UL) + _fuseiter5823)]);
          __cached_3 = (__cached_3 + __cached_5);
          __cached_3 = sc_max(__cached_3, vec_f32x16(0.f));
          vec_s8x16 __cached_6;
          __cached_6 = sc_saturated_cast<vec_s8x16>(sc_round_and_cast<vec_s32x16>(__cached_3));
          vec_s8x16 __cached_7;
          __cached_7 = __cached_6;
          vec_s8x16::store(__cached_7, &__outs_0[((((_fuseiter5823 + (fused_0k_o__n_1832 * 32UL)) / 64UL) * 200704UL) + ((_fuseiter5821 * 3584UL) + ((_fuseiter5822 * 64UL) + ((_fuseiter5823 + (fused_0k_o__n_1832 * 32UL)) % 64UL))))]);
        }
      }
    }
    sc_aligned_free(__stream, __origouts_1640_shr);
  }
  sc_aligned_free(__stream, __rescheduled_0);
  return true;
}

static bool res2c_conv_2_cast_mul_add_cast_add_cast_reorder__40(uint8_t* __restrict__ __outs_0, int8_t* __restrict__ __ins_0, int8_t* __restrict__ __ins_1, float* __restrict__ __ins_2, float* __restrict__ __ins_3, uint8_t* __restrict__ __ins_4) noexcept{
  void*& __sc_kernel_cache_120 = *(void**)(__module_data + 80);
  alignas(64) int8_t __rescheduled_0[128UL];
  for (uint64_t fused_0n__k_1833 = 0UL; fused_0n__k_1833 < 4UL; fused_0n__k_1833 += 1UL) {
    for (uint64_t p_o = 0UL; p_o < 56UL; p_o += 1UL) {
      int32_t* __origouts_1650_shr = (int32_t*)sc_aligned_malloc(__stream, 14336UL);
      void** A_list = (void**)&__rescheduled_0[0UL];
      void** B_list = (void**)&__rescheduled_0[64UL];
      void* __cached_0;
      __cached_0 = &__ins_0[(((fused_0n__k_1833 / 4UL) * 200704UL) + (p_o * 3584UL))];
      A_list[0UL] = __cached_0;
      void* __cached_1;
      __cached_1 = &__ins_1[((fused_0n__k_1833 % 4UL) * 4096UL)];
      B_list[0UL] = __cached_1;
      void* _arg_cache_9 = &__origouts_1650_shr[0UL];
      dnnl_brgemm_list_call(__sc_kernel_cache_120, A_list, B_list, &__origouts_1650_shr[0UL], 1, 1, 1, 1, 7, 7, __stream);
      for (uint64_t _fuseiter5857 = 0UL; _fuseiter5857 < 56UL; _fuseiter5857 += 1UL) {
        for (uint64_t _fuseiter5858 = 0UL; _fuseiter5858 < 64UL; _fuseiter5858 += 16UL) {
          vec_s32x16 __cached_2;
          __cached_2 = vec_s32x16::load(&__origouts_1650_shr[((_fuseiter5857 * 64UL) + _fuseiter5858)]);
          vec_f32x16 __cached_3;
          __cached_3 = (vec_f32x16)(__cached_2);
          vec_f32x16 __cached_4;
          __cached_4 = vec_f32x16::load(&__ins_2[((((fused_0n__k_1833 / 4UL) * 256UL) + ((fused_0n__k_1833 % 4UL) * 64UL)) + _fuseiter5858)]);
          __cached_3 = (__cached_3 * __cached_4);
          vec_f32x16 __cached_5;
          __cached_5 = vec_f32x16::load(&__ins_3[(((fused_0n__k_1833 % 4UL) * 64UL) + _fuseiter5858)]);
          __cached_3 = (__cached_3 + __cached_5);
          vec_s8x16 __cached_6;
          __cached_6 = sc_saturated_cast<vec_s8x16>(sc_round_and_cast<vec_s32x16>(__cached_3));
          vec_u8x16 __cached_7;
          __cached_7 = vec_u8x16::load(&__ins_4[((((fused_0n__k_1833 / 4UL) * 802816UL) + (((fused_0n__k_1833 % 4UL) * 200704UL) + (p_o * 3584UL))) + ((_fuseiter5857 * 64UL) + _fuseiter5858))]);
          __cached_6 = (__cached_6 + (vec_s8x16)(__cached_7));
          vec_u8x16 __cached_8;
          __cached_8 = (vec_u8x16)(sc_max(__cached_6, vec_s8x16(0)));
          vec_u8x16 __cached_9;
          __cached_9 = __cached_8;
          vec_u8x16::store(__cached_9, &__outs_0[(((fused_0n__k_1833 / 4UL) * 802816UL) + ((((_fuseiter5858 + ((fused_0n__k_1833 % 4UL) * 64UL)) / 128UL) * 401408UL) + ((p_o * 7168UL) + ((_fuseiter5857 * 128UL) + ((_fuseiter5858 + ((fused_0n__k_1833 % 4UL) * 64UL)) % 128UL)))))]);
        }
      }
      sc_aligned_free(__stream, __origouts_1650_shr);
    }
  }
  return true;
}

static bool res3a_conv_0_cast_mul_add_relu_cast_reorder__48(int8_t* __restrict__ __outs_0, uint8_t* __restrict__ __ins_0, int8_t* __restrict__ __ins_1, float* __restrict__ __ins_2, float* __restrict__ __ins_3) noexcept{
  void*& __sc_kernel_cache_122 = *(void**)(__module_data + 88);
  alignas(64) int8_t __rescheduled_0[128UL];
  memset(&__outs_0[0UL], 0, 7424UL);
  for (uint64_t p1 = 0UL; p1 < 56UL; p1 += 1UL) {
    memset(&__outs_0[((p1 + 1UL) * 7424UL)], 0, 128UL);
    memset(&__outs_0[(((p1 + 1UL) * 7424UL) + 7296UL)], 0, 128UL);
  }
  memset(&__outs_0[423168UL], 0, 7424UL);
  for (uint64_t fused_0k__n_1835 = 0UL; fused_0k__n_1835 < 2UL; fused_0k__n_1835 += 1UL) {
    for (uint64_t p_o = 0UL; p_o < 28UL; p_o += 1UL) {
      int32_t* __origouts_1660_shr = (int32_t*)sc_aligned_malloc(__stream, 28672UL);
      void** A_list = (void**)&__rescheduled_0[0UL];
      void** B_list = (void**)&__rescheduled_0[64UL];
      for (uint64_t c = 0UL; c < 2UL; c += 1UL) {
        void* __cached_0;
        __cached_0 = &__ins_0[((c * 401408UL) + (p_o * 14336UL))];
        A_list[c] = __cached_0;
        void* __cached_1;
        __cached_1 = &__ins_1[((fused_0k__n_1835 * 16384UL) + (c * 8192UL))];
        B_list[c] = __cached_1;
      }
      void* _arg_cache_10 = &__origouts_1660_shr[0UL];
      dnnl_brgemm_list_call(__sc_kernel_cache_122, A_list, B_list, &__origouts_1660_shr[0UL], 1, 1, 1, 2, 8, 7, __stream);
      for (uint64_t _fuseiter5897 = 0UL; _fuseiter5897 < 2UL; _fuseiter5897 += 1UL) {
        for (uint64_t _fuseiter5898 = 0UL; _fuseiter5898 < 56UL; _fuseiter5898 += 1UL) {
          for (uint64_t _fuseiter5899 = 0UL; _fuseiter5899 < 64UL; _fuseiter5899 += 16UL) {
            vec_s32x16 __cached_2;
            __cached_2 = vec_s32x16::load(&__origouts_1660_shr[((_fuseiter5897 * 3584UL) + ((_fuseiter5898 * 64UL) + _fuseiter5899))]);
            vec_f32x16 __cached_3;
            __cached_3 = (vec_f32x16)(__cached_2);
            vec_f32x16 __cached_4;
            __cached_4 = vec_f32x16::load(&__ins_2[((fused_0k__n_1835 * 64UL) + _fuseiter5899)]);
            __cached_3 = (__cached_3 * __cached_4);
            vec_f32x16 __cached_5;
            __cached_5 = vec_f32x16::load(&__ins_3[((fused_0k__n_1835 * 64UL) + _fuseiter5899)]);
            __cached_3 = (__cached_3 + __cached_5);
            __cached_3 = sc_max(__cached_3, vec_f32x16(0.f));
            vec_s8x16 __cached_6;
            __cached_6 = sc_saturated_cast<vec_s8x16>(sc_round_and_cast<vec_s32x16>(__cached_3));
            vec_s8x16 __cached_7;
            __cached_7 = __cached_6;
            vec_s8x16::store(__cached_7, &__outs_0[((((_fuseiter5899 + (fused_0k__n_1835 * 64UL)) / 128UL) * 430592UL) + ((((_fuseiter5897 + (p_o * 2UL)) + 1UL) * 7424UL) + (((_fuseiter5898 + 1UL) * 128UL) + ((_fuseiter5899 + (fused_0k__n_1835 * 64UL)) % 128UL))))]);
          }
        }
      }
      sc_aligned_free(__stream, __origouts_1660_shr);
    }
  }
  return true;
}

static bool res3a_conv_1_cast_mul_add_relu_cast__52(int8_t* __restrict__ __outs_0, int8_t* __restrict__ __ins_0, int8_t* __restrict__ __ins_1, float* __restrict__ __ins_2, float* __restrict__ __ins_3) noexcept{
  void*& __sc_kernel_cache_124 = *(void**)(__module_data + 96);
  int8_t* __rescheduled_0 = (int8_t*)sc_aligned_malloc(__stream, 320UL);
  for (uint64_t fused_0n__k_o_1836 = 0UL; fused_0n__k_o_1836 < 4UL; fused_0n__k_o_1836 += 1UL) {
    int32_t* __origouts_1670_shr = (int32_t*)sc_aligned_malloc(__stream, 100352UL);
    for (uint64_t o_o = 0UL; o_o < 28UL; o_o += 1UL) {
      void** A_list = (void**)&__rescheduled_0[0UL];
      void** B_list = (void**)&__rescheduled_0[128UL];
      memset(&__origouts_1670_shr[(((o_o * 28UL) / 28UL) * 896UL)], 0, 3584UL);
      for (uint64_t r = 0UL; r < 3UL; r += 1UL) {
        for (uint64_t s = 0UL; s < 3UL; s += 1UL) {
          void* __cached_0;
          __cached_0 = &__ins_0[(((fused_0n__k_o_1836 / 4UL) * 430592UL) + ((((((o_o * 28UL) / 28UL) * 2UL) + r) * 7424UL) + (s * 128UL)))];
          A_list[((r * 3UL) + s)] = __cached_0;
          void* __cached_1;
          __cached_1 = &__ins_1[(((fused_0n__k_o_1836 % 4UL) * 36864UL) + ((r * 12288UL) + (s * 4096UL)))];
          B_list[((r * 3UL) + s)] = __cached_1;
        }
      }
      void* _arg_cache_11 = &__origouts_1670_shr[(((o_o * 28UL) / 28UL) * 896UL)];
      dnnl_brgemm_list_call(__sc_kernel_cache_124, A_list, B_list, &__origouts_1670_shr[(((o_o * 28UL) / 28UL) * 896UL)], 1, 128, 4096, 9, 7, 7, __stream);
    }
    float* _cast_buf_0_shr = (float*)&__rescheduled_0[256UL];
    for (uint64_t _fuseiter5932 = 0UL; _fuseiter5932 < 28UL; _fuseiter5932 += 1UL) {
      for (uint64_t _fuseiter5933 = 0UL; _fuseiter5933 < 28UL; _fuseiter5933 += 1UL) {
        for (uint64_t _fuseiter5934 = 0UL; _fuseiter5934 < 32UL; _fuseiter5934 += 16UL) {
          vec_s32x16 __cached_2;
          __cached_2 = vec_s32x16::load(&__origouts_1670_shr[((_fuseiter5932 * 896UL) + ((_fuseiter5933 * 32UL) + _fuseiter5934))]);
          vec_f32x16 __cached_3;
          __cached_3 = (vec_f32x16)(__cached_2);
          vec_f32x16::store(__cached_3, &_cast_buf_0_shr[((((0UL - _fuseiter5932) * 16UL) + (((0UL - _fuseiter5933) * 16UL) + (0UL - _fuseiter5934))) + ((_fuseiter5932 * 16UL) + ((_fuseiter5933 * 16UL) + _fuseiter5934)))]);
          vec_f32x16 __cached_4;
          __cached_4 = vec_f32x16::load(&_cast_buf_0_shr[0UL]);
          vec_f32x16 __cached_5;
          __cached_5 = vec_f32x16::load(&__ins_2[(((fused_0n__k_o_1836 / 4UL) * 128UL) + (((fused_0n__k_o_1836 % 4UL) * 32UL) + _fuseiter5934))]);
          __cached_4 = (__cached_4 * __cached_5);
          vec_f32x16 __cached_6;
          __cached_6 = vec_f32x16::load(&__ins_3[(((fused_0n__k_o_1836 % 4UL) * 32UL) + _fuseiter5934)]);
          __cached_4 = (__cached_4 + __cached_6);
          __cached_4 = sc_max(__cached_4, vec_f32x16(0.f));
          vec_f32x16::store(__cached_4, &_cast_buf_0_shr[0UL]);
          vec_s8x16 __cached_7;
          __cached_7 = sc_saturated_cast<vec_s8x16>(sc_round_and_cast<vec_s32x16>(__cached_4));
          vec_s8x16::store(__cached_7, &__outs_0[(((fused_0n__k_o_1836 / 4UL) * 100352UL) + (((fused_0n__k_o_1836 % 4UL) * 25088UL) + ((_fuseiter5932 * 896UL) + ((_fuseiter5933 * 32UL) + _fuseiter5934))))]);
        }
      }
    }
    sc_aligned_free(__stream, __origouts_1670_shr);
  }
  sc_aligned_free(__stream, __rescheduled_0);
  return true;
}

static bool res3a_conv_b_cast_mul_add_cast__44(int8_t* __restrict__ __outs_0, uint8_t* __restrict__ __ins_0, int8_t* __restrict__ __ins_1, float* __restrict__ __ins_2, float* __restrict__ __ins_3) noexcept{
  void*& __sc_kernel_cache_126 = *(void**)(__module_data + 104);
  alignas(64) int8_t __rescheduled_0[128UL];
  uint8_t* input_tmp = (uint8_t*)sc_aligned_malloc(__stream, 200704UL);
  for (uint64_t fused_0fused_0n__c_o_1837__p_1838 = 0UL; fused_0fused_0n__c_o_1837__p_1838 < 56UL; fused_0fused_0n__c_o_1837__p_1838 += 1UL) {
    for (uint64_t q = 0UL; q < 28UL; q += 1UL) {
      for (uint64_t c_i = 0UL; c_i < 128UL; c_i += 64UL) {
        vec_u8x64 __cached_0;
        __cached_0 = vec_u8x64::load(&__ins_0[(((fused_0fused_0n__c_o_1837__p_1838 / 56UL) * 802816UL) + ((((fused_0fused_0n__c_o_1837__p_1838 / 28UL) % 2UL) * 401408UL) + (((fused_0fused_0n__c_o_1837__p_1838 % 28UL) * 14336UL) + ((q * 256UL) + c_i))))]);
        vec_u8x64 __cached_1;
        __cached_1 = __cached_0;
        vec_u8x64::store(__cached_1, &input_tmp[(((fused_0fused_0n__c_o_1837__p_1838 / 56UL) * 200704UL) + ((((fused_0fused_0n__c_o_1837__p_1838 / 28UL) % 2UL) * 100352UL) + (((fused_0fused_0n__c_o_1837__p_1838 % 28UL) * 3584UL) + ((q * 128UL) + c_i))))]);
      }
    }
  }
  for (uint64_t fused_0k__n_1839 = 0UL; fused_0k__n_1839 < 16UL; fused_0k__n_1839 += 1UL) {
    for (uint64_t p_o = 0UL; p_o < 4UL; p_o += 1UL) {
      int32_t* __origouts_1680_shr = (int32_t*)sc_aligned_malloc(__stream, 25088UL);
      void** A_list = (void**)&__rescheduled_0[0UL];
      void** B_list = (void**)&__rescheduled_0[64UL];
      for (uint64_t c = 0UL; c < 2UL; c += 1UL) {
        void* __cached_2;
        __cached_2 = &input_tmp[((c * 100352UL) + (p_o * 25088UL))];
        A_list[c] = __cached_2;
        void* __cached_3;
        __cached_3 = &__ins_1[((fused_0k__n_1839 * 8192UL) + (c * 4096UL))];
        B_list[c] = __cached_3;
      }
      void* _arg_cache_12 = &__origouts_1680_shr[0UL];
      dnnl_brgemm_list_call(__sc_kernel_cache_126, A_list, B_list, &__origouts_1680_shr[0UL], 1, 1, 1, 2, 8, 7, __stream);
      for (uint64_t _fuseiter5962 = 0UL; _fuseiter5962 < 7UL; _fuseiter5962 += 1UL) {
        for (uint64_t _fuseiter5963 = 0UL; _fuseiter5963 < 28UL; _fuseiter5963 += 1UL) {
          for (uint64_t _fuseiter5964 = 0UL; _fuseiter5964 < 32UL; _fuseiter5964 += 16UL) {
            vec_s32x16 __cached_4;
            __cached_4 = vec_s32x16::load(&__origouts_1680_shr[((_fuseiter5962 * 896UL) + ((_fuseiter5963 * 32UL) + _fuseiter5964))]);
            vec_f32x16 __cached_5;
            __cached_5 = (vec_f32x16)(__cached_4);
            vec_f32x16 __cached_6;
            __cached_6 = vec_f32x16::load(&__ins_2[((fused_0k__n_1839 * 32UL) + _fuseiter5964)]);
            __cached_5 = (__cached_5 * __cached_6);
            vec_f32x16 __cached_7;
            __cached_7 = vec_f32x16::load(&__ins_3[((fused_0k__n_1839 * 32UL) + _fuseiter5964)]);
            __cached_5 = (__cached_5 + __cached_7);
            vec_s8x16 __cached_8;
            __cached_8 = sc_saturated_cast<vec_s8x16>(sc_round_and_cast<vec_s32x16>(__cached_5));
            vec_s8x16::store(__cached_8, &__outs_0[(((fused_0k__n_1839 * 25088UL) + (p_o * 6272UL)) + ((_fuseiter5962 * 896UL) + ((_fuseiter5963 * 32UL) + _fuseiter5964)))]);
          }
        }
      }
      sc_aligned_free(__stream, __origouts_1680_shr);
    }
  }
  sc_aligned_free(__stream, input_tmp);
  return true;
}

static bool res3a_conv_2_cast_mul_add_cast_add_cast_reorder__56(uint8_t* __restrict__ __outs_0, int8_t* __restrict__ __ins_0, int8_t* __restrict__ __ins_1, float* __restrict__ __ins_2, float* __restrict__ __ins_3, int8_t* __restrict__ __ins_4) noexcept{
  void*& __sc_kernel_cache_128 = *(void**)(__module_data + 112);
  alignas(64) int8_t __rescheduled_0[128UL];
  for (uint64_t fused_0k__n_1840 = 0UL; fused_0k__n_1840 < 16UL; fused_0k__n_1840 += 1UL) {
    for (uint64_t p_o = 0UL; p_o < 2UL; p_o += 1UL) {
      int32_t* __origouts_1690_shr = (int32_t*)sc_aligned_malloc(__stream, 50176UL);
      void** A_list = (void**)&__rescheduled_0[0UL];
      void** B_list = (void**)&__rescheduled_0[64UL];
      for (uint64_t c = 0UL; c < 4UL; c += 1UL) {
        void* __cached_0;
        __cached_0 = &__ins_0[((c * 25088UL) + (p_o * 12544UL))];
        A_list[c] = __cached_0;
        void* __cached_1;
        __cached_1 = &__ins_1[((fused_0k__n_1840 * 4096UL) + (c * 1024UL))];
        B_list[c] = __cached_1;
      }
      void* _arg_cache_13 = &__origouts_1690_shr[0UL];
      dnnl_brgemm_list_call(__sc_kernel_cache_128, A_list, B_list, &__origouts_1690_shr[0UL], 1, 1, 1, 4, 7, 7, __stream);
      for (uint64_t _fuseiter5986 = 0UL; _fuseiter5986 < 14UL; _fuseiter5986 += 1UL) {
        for (uint64_t _fuseiter5987 = 0UL; _fuseiter5987 < 28UL; _fuseiter5987 += 1UL) {
          for (uint64_t _fuseiter5988 = 0UL; _fuseiter5988 < 32UL; _fuseiter5988 += 16UL) {
            vec_s32x16 __cached_2;
            __cached_2 = vec_s32x16::load(&__origouts_1690_shr[((_fuseiter5986 * 896UL) + ((_fuseiter5987 * 32UL) + _fuseiter5988))]);
            vec_f32x16 __cached_3;
            __cached_3 = (vec_f32x16)(__cached_2);
            vec_f32x16 __cached_4;
            __cached_4 = vec_f32x16::load(&__ins_2[((fused_0k__n_1840 * 32UL) + _fuseiter5988)]);
            __cached_3 = (__cached_3 * __cached_4);
            vec_f32x16 __cached_5;
            __cached_5 = vec_f32x16::load(&__ins_3[((fused_0k__n_1840 * 32UL) + _fuseiter5988)]);
            __cached_3 = (__cached_3 + __cached_5);
            vec_s8x16 __cached_6;
            __cached_6 = sc_saturated_cast<vec_s8x16>(sc_round_and_cast<vec_s32x16>(__cached_3));
            vec_s8x16 __cached_7;
            __cached_7 = vec_s8x16::load(&__ins_4[(((fused_0k__n_1840 * 25088UL) + (p_o * 12544UL)) + ((_fuseiter5986 * 896UL) + ((_fuseiter5987 * 32UL) + _fuseiter5988)))]);
            __cached_6 = (__cached_6 + __cached_7);
            vec_u8x16 __cached_8;
            __cached_8 = (vec_u8x16)(sc_max(__cached_6, vec_s8x16(0)));
            vec_u8x16 __cached_9;
            __cached_9 = __cached_8;
            vec_u8x16::store(__cached_9, &__outs_0[((((_fuseiter5988 + (fused_0k__n_1840 * 32UL)) / 128UL) * 100352UL) + (((_fuseiter5986 + (p_o * 14UL)) * 3584UL) + ((_fuseiter5987 * 128UL) + ((_fuseiter5988 + (fused_0k__n_1840 * 32UL)) % 128UL))))]);
          }
        }
      }
      sc_aligned_free(__stream, __origouts_1690_shr);
    }
  }
  return true;
}

static bool res3b_conv_0_cast_mul_add_relu_cast_reorder__60(int8_t* __restrict__ __outs_0, uint8_t* __restrict__ __ins_0, int8_t* __restrict__ __ins_1, float* __restrict__ __ins_2, float* __restrict__ __ins_3) noexcept{
  void*& __sc_kernel_cache_130 = *(void**)(__module_data + 120);
  alignas(64) int8_t __rescheduled_0[128UL];
  for (uint64_t fused_0n__k_1841 = 0UL; fused_0n__k_1841 < 2UL; fused_0n__k_1841 += 1UL) {
    memset(&__outs_0[(((fused_0n__k_1841 / 2UL) * 115200UL) + ((fused_0n__k_1841 % 2UL) * 57600UL))], 0, 1920UL);
    for (uint64_t p1 = 0UL; p1 < 28UL; p1 += 1UL) {
      memset(&__outs_0[(((fused_0n__k_1841 / 2UL) * 115200UL) + (((fused_0n__k_1841 % 2UL) * 57600UL) + ((p1 + 1UL) * 1920UL)))], 0, 64UL);
      memset(&__outs_0[((((fused_0n__k_1841 / 2UL) * 115200UL) + (((fused_0n__k_1841 % 2UL) * 57600UL) + ((p1 + 1UL) * 1920UL))) + 1856UL)], 0, 64UL);
    }
    memset(&__outs_0[((((fused_0n__k_1841 / 2UL) * 115200UL) + ((fused_0n__k_1841 % 2UL) * 57600UL)) + 55680UL)], 0, 1920UL);
  }
  for (uint64_t fused_0k__n_1842 = 0UL; fused_0k__n_1842 < 4UL; fused_0k__n_1842 += 1UL) {
    for (uint64_t p_o = 0UL; p_o < 7UL; p_o += 1UL) {
      int32_t* __origouts_1700_shr = (int32_t*)sc_aligned_malloc(__stream, 14336UL);
      void** A_list = (void**)&__rescheduled_0[0UL];
      void** B_list = (void**)&__rescheduled_0[64UL];
      for (uint64_t c = 0UL; c < 4UL; c += 1UL) {
        void* __cached_0;
        __cached_0 = &__ins_0[((c * 100352UL) + (p_o * 14336UL))];
        A_list[c] = __cached_0;
        void* __cached_1;
        __cached_1 = &__ins_1[((fused_0k__n_1842 * 16384UL) + (c * 4096UL))];
        B_list[c] = __cached_1;
      }
      void* _arg_cache_14 = &__origouts_1700_shr[0UL];
      dnnl_brgemm_list_call(__sc_kernel_cache_130, A_list, B_list, &__origouts_1700_shr[0UL], 1, 1, 1, 4, 8, 7, __stream);
      for (uint64_t _fuseiter6027 = 0UL; _fuseiter6027 < 4UL; _fuseiter6027 += 1UL) {
        for (uint64_t _fuseiter6028 = 0UL; _fuseiter6028 < 28UL; _fuseiter6028 += 1UL) {
          for (uint64_t _fuseiter6029 = 0UL; _fuseiter6029 < 32UL; _fuseiter6029 += 16UL) {
            vec_s32x16 __cached_2;
            __cached_2 = vec_s32x16::load(&__origouts_1700_shr[((_fuseiter6027 * 896UL) + ((_fuseiter6028 * 32UL) + _fuseiter6029))]);
            vec_f32x16 __cached_3;
            __cached_3 = (vec_f32x16)(__cached_2);
            vec_f32x16 __cached_4;
            __cached_4 = vec_f32x16::load(&__ins_2[((fused_0k__n_1842 * 32UL) + _fuseiter6029)]);
            __cached_3 = (__cached_3 * __cached_4);
            vec_f32x16 __cached_5;
            __cached_5 = vec_f32x16::load(&__ins_3[((fused_0k__n_1842 * 32UL) + _fuseiter6029)]);
            __cached_3 = (__cached_3 + __cached_5);
            __cached_3 = sc_max(__cached_3, vec_f32x16(0.f));
            vec_s8x16 __cached_6;
            __cached_6 = sc_saturated_cast<vec_s8x16>(sc_round_and_cast<vec_s32x16>(__cached_3));
            vec_s8x16 __cached_7;
            __cached_7 = __cached_6;
            vec_s8x16::store(__cached_7, &__outs_0[((((_fuseiter6029 + (fused_0k__n_1842 * 32UL)) / 64UL) * 57600UL) + ((((_fuseiter6027 + (p_o * 4UL)) + 1UL) * 1920UL) + (((_fuseiter6028 + 1UL) * 64UL) + ((_fuseiter6029 + (fused_0k__n_1842 * 32UL)) % 64UL))))]);
          }
        }
      }
      sc_aligned_free(__stream, __origouts_1700_shr);
    }
  }
  return true;
}

static bool res3b_conv_1_cast_mul_add_relu_cast_reorder__64(int8_t* __restrict__ __outs_0, int8_t* __restrict__ __ins_0, int8_t* __restrict__ __ins_1, float* __restrict__ __ins_2, float* __restrict__ __ins_3) noexcept{
  void** __sc_kernel_cache_arr = (void**)&__uninitialized_data[23657488UL];
  int8_t* __rescheduled_0 = (int8_t*)sc_aligned_malloc(__stream, 512UL);
  int32_t* conv_os_blk_size = (int32_t*)&__rescheduled_0[0UL];
  int32_t* conv_os_acc_size = (int32_t*)&__rescheduled_0[64UL];
  int32_t __cached_0;
  __cached_0 = 392;
  conv_os_blk_size[0] = __cached_0;
  int32_t __cached_1;
  __cached_1 = 0;
  conv_os_acc_size[0] = __cached_1;
  int32_t __cached_2;
  __cached_2 = 392;
  conv_os_blk_size[1] = __cached_2;
  int32_t __cached_3;
  __cached_3 = 392;
  conv_os_acc_size[1] = __cached_3;
  int32_t* __origouts_1710_shr = (int32_t*)sc_aligned_malloc(__stream, 401408UL);
  for (uint64_t o_o = 0UL; o_o < 2UL; o_o += 1UL) {
    void** A_list = (void**)&__rescheduled_0[128UL];
    void** B_list = (void**)&__rescheduled_0[320UL];
    int32_t __cached_4;
    __cached_4 = conv_os_acc_size[o_o];
    int32_t __cached_5;
    __cached_5 = conv_os_blk_size[o_o];
    memset(&__origouts_1710_shr[(uint64_t)(((__cached_4 / 28) * 3584) + ((__cached_4 % 28) * 128))], 0, ((uint64_t)(__cached_5 * 128) * 4UL));
    for (uint64_t c_o = 0UL; c_o < 2UL; c_o += 1UL) {
      for (uint64_t r = 0UL; r < 3UL; r += 1UL) {
        for (uint64_t s = 0UL; s < 3UL; s += 1UL) {
          void* __cached_6;
          __cached_6 = &__ins_0[((c_o * 57600UL) + (((((o_o * 419UL) / 30UL) + r) * 1920UL) + ((((o_o * 419UL) % 30UL) + s) * 64UL)))];
          A_list[(((c_o * 9UL) + (r * 3UL)) + s)] = __cached_6;
          void* __cached_7;
          __cached_7 = &__ins_1[((c_o * 73728UL) + ((r * 24576UL) + (s * 8192UL)))];
          B_list[(((c_o * 9UL) + (r * 3UL)) + s)] = __cached_7;
        }
      }
    }
    void* _arg_cache_15 = &__origouts_1710_shr[(uint64_t)(((__cached_4 / 28) * 3584) + ((__cached_4 % 28) * 128))];
    dnnl_brgemm_list_call(__sc_kernel_cache_arr[o_o], A_list, B_list, &__origouts_1710_shr[(uint64_t)(((__cached_4 / 28) * 3584) + ((__cached_4 % 28) * 128))], 1, 64, 8192, 18, 7, 7, __stream);
  }
  for (uint64_t _fuseiter6062 = 0UL; _fuseiter6062 < 28UL; _fuseiter6062 += 1UL) {
    for (uint64_t _fuseiter6063 = 0UL; _fuseiter6063 < 28UL; _fuseiter6063 += 1UL) {
      for (uint64_t _fuseiter6064 = 0UL; _fuseiter6064 < 128UL; _fuseiter6064 += 16UL) {
        vec_s32x16 __cached_8;
        __cached_8 = vec_s32x16::load(&__origouts_1710_shr[((_fuseiter6062 * 3584UL) + ((_fuseiter6063 * 128UL) + _fuseiter6064))]);
        vec_f32x16 __cached_9;
        __cached_9 = (vec_f32x16)(__cached_8);
        vec_f32x16 __cached_10;
        __cached_10 = vec_f32x16::load(&__ins_2[_fuseiter6064]);
        __cached_9 = (__cached_9 * __cached_10);
        vec_f32x16 __cached_11;
        __cached_11 = vec_f32x16::load(&__ins_3[_fuseiter6064]);
        __cached_9 = (__cached_9 + __cached_11);
        __cached_9 = sc_max(__cached_9, vec_f32x16(0.f));
        vec_s8x16 __cached_12;
        __cached_12 = sc_saturated_cast<vec_s8x16>(sc_round_and_cast<vec_s32x16>(__cached_9));
        vec_s8x16 __cached_13;
        __cached_13 = __cached_12;
        vec_s8x16::store(__cached_13, &__outs_0[(((_fuseiter6064 / 64UL) * 50176UL) + ((_fuseiter6062 * 1792UL) + ((_fuseiter6063 * 64UL) + (_fuseiter6064 % 64UL))))]);
      }
    }
  }
  sc_aligned_free(__stream, __origouts_1710_shr);
  sc_aligned_free(__stream, __rescheduled_0);
  return true;
}

static bool res3b_conv_2_cast_mul_add_cast_add_cast_reorder__68(uint8_t* __restrict__ __outs_0, int8_t* __restrict__ __ins_0, int8_t* __restrict__ __ins_1, float* __restrict__ __ins_2, float* __restrict__ __ins_3, uint8_t* __restrict__ __ins_4) noexcept{
  void*& __sc_kernel_cache_133 = *(void**)(__module_data + 128);
  alignas(64) int8_t __rescheduled_0[128UL];
  for (uint64_t fused_0n__k_1844 = 0UL; fused_0n__k_1844 < 4UL; fused_0n__k_1844 += 1UL) {
    for (uint64_t p_o = 0UL; p_o < 28UL; p_o += 1UL) {
      int32_t* __origouts_1720_shr = (int32_t*)sc_aligned_malloc(__stream, 14336UL);
      void** A_list = (void**)&__rescheduled_0[0UL];
      void** B_list = (void**)&__rescheduled_0[64UL];
      for (uint64_t c = 0UL; c < 2UL; c += 1UL) {
        void* __cached_0;
        __cached_0 = &__ins_0[(((fused_0n__k_1844 / 4UL) * 100352UL) + ((c * 50176UL) + (p_o * 1792UL)))];
        A_list[c] = __cached_0;
        void* __cached_1;
        __cached_1 = &__ins_1[(((fused_0n__k_1844 % 4UL) * 16384UL) + (c * 8192UL))];
        B_list[c] = __cached_1;
      }
      void* _arg_cache_16 = &__origouts_1720_shr[0UL];
      dnnl_brgemm_list_call(__sc_kernel_cache_133, A_list, B_list, &__origouts_1720_shr[0UL], 1, 1, 1, 2, 7, 7, __stream);
      for (uint64_t _fuseiter6098 = 0UL; _fuseiter6098 < 28UL; _fuseiter6098 += 1UL) {
        for (uint64_t _fuseiter6099 = 0UL; _fuseiter6099 < 128UL; _fuseiter6099 += 16UL) {
          vec_s32x16 __cached_2;
          __cached_2 = vec_s32x16::load(&__origouts_1720_shr[((_fuseiter6098 * 128UL) + _fuseiter6099)]);
          vec_f32x16 __cached_3;
          __cached_3 = (vec_f32x16)(__cached_2);
          vec_f32x16 __cached_4;
          __cached_4 = vec_f32x16::load(&__ins_2[((((fused_0n__k_1844 / 4UL) * 512UL) + ((fused_0n__k_1844 % 4UL) * 128UL)) + _fuseiter6099)]);
          __cached_3 = (__cached_3 * __cached_4);
          vec_f32x16 __cached_5;
          __cached_5 = vec_f32x16::load(&__ins_3[(((fused_0n__k_1844 % 4UL) * 128UL) + _fuseiter6099)]);
          __cached_3 = (__cached_3 + __cached_5);
          vec_s8x16 __cached_6;
          __cached_6 = sc_saturated_cast<vec_s8x16>(sc_round_and_cast<vec_s32x16>(__cached_3));
          vec_u8x16 __cached_7;
          __cached_7 = vec_u8x16::load(&__ins_4[((((fused_0n__k_1844 / 4UL) * 401408UL) + (((fused_0n__k_1844 % 4UL) * 100352UL) + (p_o * 3584UL))) + ((_fuseiter6098 * 128UL) + _fuseiter6099))]);
          __cached_6 = (__cached_6 + (vec_s8x16)(__cached_7));
          vec_u8x16 __cached_8;
          __cached_8 = (vec_u8x16)(sc_max(__cached_6, vec_s8x16(0)));
          vec_u8x16 __cached_9;
          __cached_9 = __cached_8;
          vec_u8x16::store(__cached_9, &__outs_0[(((fused_0n__k_1844 / 4UL) * 401408UL) + ((((_fuseiter6099 + ((fused_0n__k_1844 % 4UL) * 128UL)) / 64UL) * 50176UL) + ((p_o * 1792UL) + ((_fuseiter6098 * 64UL) + ((_fuseiter6099 + ((fused_0n__k_1844 % 4UL) * 128UL)) % 64UL)))))]);
        }
      }
      sc_aligned_free(__stream, __origouts_1720_shr);
    }
  }
  return true;
}

static bool res3c_conv_0_cast_mul_add_relu_cast_reorder__72(int8_t* __restrict__ __outs_0, uint8_t* __restrict__ __ins_0, int8_t* __restrict__ __ins_1, float* __restrict__ __ins_2, float* __restrict__ __ins_3) noexcept{
  void*& __sc_kernel_cache_135 = *(void**)(__module_data + 136);
  alignas(64) int8_t __rescheduled_0[128UL];
  for (uint64_t fused_0n__k_1845 = 0UL; fused_0n__k_1845 < 2UL; fused_0n__k_1845 += 1UL) {
    memset(&__outs_0[(((fused_0n__k_1845 / 2UL) * 115200UL) + ((fused_0n__k_1845 % 2UL) * 57600UL))], 0, 1920UL);
    for (uint64_t p1 = 0UL; p1 < 28UL; p1 += 1UL) {
      memset(&__outs_0[(((fused_0n__k_1845 / 2UL) * 115200UL) + (((fused_0n__k_1845 % 2UL) * 57600UL) + ((p1 + 1UL) * 1920UL)))], 0, 64UL);
      memset(&__outs_0[((((fused_0n__k_1845 / 2UL) * 115200UL) + (((fused_0n__k_1845 % 2UL) * 57600UL) + ((p1 + 1UL) * 1920UL))) + 1856UL)], 0, 64UL);
    }
    memset(&__outs_0[((((fused_0n__k_1845 / 2UL) * 115200UL) + ((fused_0n__k_1845 % 2UL) * 57600UL)) + 55680UL)], 0, 1920UL);
  }
  int32_t* __origouts_1730_shr = (int32_t*)sc_aligned_malloc(__stream, 401408UL);
  void** A_list = (void**)&__rescheduled_0[0UL];
  void** B_list = (void**)&__rescheduled_0[64UL];
  for (uint64_t c = 0UL; c < 8UL; c += 1UL) {
    void* __cached_0;
    __cached_0 = &__ins_0[(c * 50176UL)];
    A_list[c] = __cached_0;
    void* __cached_1;
    __cached_1 = &__ins_1[(c * 8192UL)];
    B_list[c] = __cached_1;
  }
  void* _arg_cache_17 = &__origouts_1730_shr[0UL];
  dnnl_brgemm_list_call(__sc_kernel_cache_135, A_list, B_list, &__origouts_1730_shr[0UL], 1, 1, 1, 8, 8, 7, __stream);
  for (uint64_t _fuseiter6138 = 0UL; _fuseiter6138 < 28UL; _fuseiter6138 += 1UL) {
    for (uint64_t _fuseiter6139 = 0UL; _fuseiter6139 < 28UL; _fuseiter6139 += 1UL) {
      for (uint64_t _fuseiter6140 = 0UL; _fuseiter6140 < 128UL; _fuseiter6140 += 16UL) {
        vec_s32x16 __cached_2;
        __cached_2 = vec_s32x16::load(&__origouts_1730_shr[((_fuseiter6138 * 3584UL) + ((_fuseiter6139 * 128UL) + _fuseiter6140))]);
        vec_f32x16 __cached_3;
        __cached_3 = (vec_f32x16)(__cached_2);
        vec_f32x16 __cached_4;
        __cached_4 = vec_f32x16::load(&__ins_2[_fuseiter6140]);
        __cached_3 = (__cached_3 * __cached_4);
        vec_f32x16 __cached_5;
        __cached_5 = vec_f32x16::load(&__ins_3[_fuseiter6140]);
        __cached_3 = (__cached_3 + __cached_5);
        __cached_3 = sc_max(__cached_3, vec_f32x16(0.f));
        vec_s8x16 __cached_6;
        __cached_6 = sc_saturated_cast<vec_s8x16>(sc_round_and_cast<vec_s32x16>(__cached_3));
        vec_s8x16 __cached_7;
        __cached_7 = __cached_6;
        vec_s8x16::store(__cached_7, &__outs_0[(((_fuseiter6140 / 64UL) * 57600UL) + (((_fuseiter6138 + 1UL) * 1920UL) + (((_fuseiter6139 + 1UL) * 64UL) + (_fuseiter6140 % 64UL))))]);
      }
    }
  }
  sc_aligned_free(__stream, __origouts_1730_shr);
  return true;
}

static bool res3c_conv_1_cast_mul_add_relu_cast_reorder__76(int8_t* __restrict__ __outs_0, int8_t* __restrict__ __ins_0, int8_t* __restrict__ __ins_1, float* __restrict__ __ins_2, float* __restrict__ __ins_3) noexcept{
  void*& __sc_kernel_cache_137 = *(void**)(__module_data + 144);
  int8_t* __rescheduled_0 = (int8_t*)sc_aligned_malloc(__stream, 384UL);
  for (uint64_t fused_0k_o__n_1847 = 0UL; fused_0k_o__n_1847 < 4UL; fused_0k_o__n_1847 += 1UL) {
    int32_t* __origouts_1740_shr = (int32_t*)sc_aligned_malloc(__stream, 100352UL);
    for (uint64_t o_o = 0UL; o_o < 28UL; o_o += 1UL) {
      void** A_list = (void**)&__rescheduled_0[0UL];
      void** B_list = (void**)&__rescheduled_0[192UL];
      memset(&__origouts_1740_shr[(((o_o * 28UL) / 28UL) * 896UL)], 0, 3584UL);
      for (uint64_t c_o = 0UL; c_o < 2UL; c_o += 1UL) {
        for (uint64_t r = 0UL; r < 3UL; r += 1UL) {
          for (uint64_t s = 0UL; s < 3UL; s += 1UL) {
            void* __cached_0;
            __cached_0 = &__ins_0[((c_o * 57600UL) + (((((o_o * 28UL) / 28UL) + r) * 1920UL) + (s * 64UL)))];
            A_list[(((c_o * 9UL) + (r * 3UL)) + s)] = __cached_0;
            void* __cached_1;
            __cached_1 = &__ins_1[((fused_0k_o__n_1847 * 36864UL) + ((c_o * 18432UL) + ((r * 6144UL) + (s * 2048UL))))];
            B_list[(((c_o * 9UL) + (r * 3UL)) + s)] = __cached_1;
          }
        }
      }
      void* _arg_cache_18 = &__origouts_1740_shr[(((o_o * 28UL) / 28UL) * 896UL)];
      dnnl_brgemm_list_call(__sc_kernel_cache_137, A_list, B_list, &__origouts_1740_shr[(((o_o * 28UL) / 28UL) * 896UL)], 1, 64, 2048, 18, 7, 7, __stream);
    }
    for (uint64_t _fuseiter6173 = 0UL; _fuseiter6173 < 28UL; _fuseiter6173 += 1UL) {
      for (uint64_t _fuseiter6174 = 0UL; _fuseiter6174 < 28UL; _fuseiter6174 += 1UL) {
        for (uint64_t _fuseiter6175 = 0UL; _fuseiter6175 < 32UL; _fuseiter6175 += 16UL) {
          vec_s32x16 __cached_2;
          __cached_2 = vec_s32x16::load(&__origouts_1740_shr[((_fuseiter6173 * 896UL) + ((_fuseiter6174 * 32UL) + _fuseiter6175))]);
          vec_f32x16 __cached_3;
          __cached_3 = (vec_f32x16)(__cached_2);
          vec_f32x16 __cached_4;
          __cached_4 = vec_f32x16::load(&__ins_2[((fused_0k_o__n_1847 * 32UL) + _fuseiter6175)]);
          __cached_3 = (__cached_3 * __cached_4);
          vec_f32x16 __cached_5;
          __cached_5 = vec_f32x16::load(&__ins_3[((fused_0k_o__n_1847 * 32UL) + _fuseiter6175)]);
          __cached_3 = (__cached_3 + __cached_5);
          __cached_3 = sc_max(__cached_3, vec_f32x16(0.f));
          vec_s8x16 __cached_6;
          __cached_6 = sc_saturated_cast<vec_s8x16>(sc_round_and_cast<vec_s32x16>(__cached_3));
          vec_s8x16 __cached_7;
          __cached_7 = __cached_6;
          vec_s8x16::store(__cached_7, &__outs_0[((((_fuseiter6175 + (fused_0k_o__n_1847 * 32UL)) / 128UL) * 100352UL) + ((_fuseiter6173 * 3584UL) + ((_fuseiter6174 * 128UL) + ((_fuseiter6175 + (fused_0k_o__n_1847 * 32UL)) % 128UL))))]);
        }
      }
    }
    sc_aligned_free(__stream, __origouts_1740_shr);
  }
  sc_aligned_free(__stream, __rescheduled_0);
  return true;
}

static bool res3c_conv_2_cast_mul_add_cast_add_cast_reorder__80(uint8_t* __restrict__ __outs_0, int8_t* __restrict__ __ins_0, int8_t* __restrict__ __ins_1, float* __restrict__ __ins_2, float* __restrict__ __ins_3, uint8_t* __restrict__ __ins_4) noexcept{
  void*& __sc_kernel_cache_139 = *(void**)(__module_data + 152);
  alignas(64) int8_t __rescheduled_0[128UL];
  for (uint64_t fused_0k__n_1848 = 0UL; fused_0k__n_1848 < 8UL; fused_0k__n_1848 += 1UL) {
    for (uint64_t p_o = 0UL; p_o < 14UL; p_o += 1UL) {
      int32_t* __origouts_1750_shr = (int32_t*)sc_aligned_malloc(__stream, 14336UL);
      void** A_list = (void**)&__rescheduled_0[0UL];
      void** B_list = (void**)&__rescheduled_0[64UL];
      void* __cached_0;
      __cached_0 = &__ins_0[(p_o * 7168UL)];
      A_list[0UL] = __cached_0;
      void* __cached_1;
      __cached_1 = &__ins_1[(fused_0k__n_1848 * 8192UL)];
      B_list[0UL] = __cached_1;
      void* _arg_cache_19 = &__origouts_1750_shr[0UL];
      dnnl_brgemm_list_call(__sc_kernel_cache_139, A_list, B_list, &__origouts_1750_shr[0UL], 1, 1, 1, 1, 7, 7, __stream);
      for (uint64_t _fuseiter6208 = 0UL; _fuseiter6208 < 2UL; _fuseiter6208 += 1UL) {
        for (uint64_t _fuseiter6209 = 0UL; _fuseiter6209 < 28UL; _fuseiter6209 += 1UL) {
          for (uint64_t _fuseiter6210 = 0UL; _fuseiter6210 < 64UL; _fuseiter6210 += 16UL) {
            vec_s32x16 __cached_2;
            __cached_2 = vec_s32x16::load(&__origouts_1750_shr[((_fuseiter6208 * 1792UL) + ((_fuseiter6209 * 64UL) + _fuseiter6210))]);
            vec_f32x16 __cached_3;
            __cached_3 = (vec_f32x16)(__cached_2);
            vec_f32x16 __cached_4;
            __cached_4 = vec_f32x16::load(&__ins_2[((fused_0k__n_1848 * 64UL) + _fuseiter6210)]);
            __cached_3 = (__cached_3 * __cached_4);
            vec_f32x16 __cached_5;
            __cached_5 = vec_f32x16::load(&__ins_3[((fused_0k__n_1848 * 64UL) + _fuseiter6210)]);
            __cached_3 = (__cached_3 + __cached_5);
            vec_s8x16 __cached_6;
            __cached_6 = sc_saturated_cast<vec_s8x16>(sc_round_and_cast<vec_s32x16>(__cached_3));
            vec_u8x16 __cached_7;
            __cached_7 = vec_u8x16::load(&__ins_4[(((fused_0k__n_1848 * 50176UL) + (p_o * 3584UL)) + ((_fuseiter6208 * 1792UL) + ((_fuseiter6209 * 64UL) + _fuseiter6210)))]);
            __cached_6 = (__cached_6 + (vec_s8x16)(__cached_7));
            vec_u8x16 __cached_8;
            __cached_8 = (vec_u8x16)(sc_max(__cached_6, vec_s8x16(0)));
            vec_u8x16 __cached_9;
            __cached_9 = __cached_8;
            vec_u8x16::store(__cached_9, &__outs_0[((((_fuseiter6210 + (fused_0k__n_1848 * 64UL)) / 128UL) * 100352UL) + (((_fuseiter6208 + (p_o * 2UL)) * 3584UL) + ((_fuseiter6209 * 128UL) + ((_fuseiter6210 + (fused_0k__n_1848 * 64UL)) % 128UL))))]);
          }
        }
      }
      sc_aligned_free(__stream, __origouts_1750_shr);
    }
  }
  return true;
}

static bool res3d_conv_0_cast_mul_add_relu_cast_reorder__84(int8_t* __restrict__ __outs_0, uint8_t* __restrict__ __ins_0, int8_t* __restrict__ __ins_1, float* __restrict__ __ins_2, float* __restrict__ __ins_3) noexcept{
  void*& __sc_kernel_cache_141 = *(void**)(__module_data + 160);
  alignas(64) int8_t __rescheduled_0[128UL];
  memset(&__outs_0[0UL], 0, 3840UL);
  for (uint64_t p1 = 0UL; p1 < 28UL; p1 += 1UL) {
    memset(&__outs_0[((p1 + 1UL) * 3840UL)], 0, 128UL);
    memset(&__outs_0[(((p1 + 1UL) * 3840UL) + 3712UL)], 0, 128UL);
  }
  memset(&__outs_0[111360UL], 0, 3840UL);
  for (uint64_t fused_0k__n_1850 = 0UL; fused_0k__n_1850 < 2UL; fused_0k__n_1850 += 1UL) {
    for (uint64_t p_o = 0UL; p_o < 14UL; p_o += 1UL) {
      int32_t* __origouts_1760_shr = (int32_t*)sc_aligned_malloc(__stream, 14336UL);
      void** A_list = (void**)&__rescheduled_0[0UL];
      void** B_list = (void**)&__rescheduled_0[64UL];
      for (uint64_t c = 0UL; c < 4UL; c += 1UL) {
        void* __cached_0;
        __cached_0 = &__ins_0[((c * 100352UL) + (p_o * 7168UL))];
        A_list[c] = __cached_0;
        void* __cached_1;
        __cached_1 = &__ins_1[((fused_0k__n_1850 * 32768UL) + (c * 8192UL))];
        B_list[c] = __cached_1;
      }
      void* _arg_cache_20 = &__origouts_1760_shr[0UL];
      dnnl_brgemm_list_call(__sc_kernel_cache_141, A_list, B_list, &__origouts_1760_shr[0UL], 1, 1, 1, 4, 8, 7, __stream);
      for (uint64_t _fuseiter6249 = 0UL; _fuseiter6249 < 2UL; _fuseiter6249 += 1UL) {
        for (uint64_t _fuseiter6250 = 0UL; _fuseiter6250 < 28UL; _fuseiter6250 += 1UL) {
          for (uint64_t _fuseiter6251 = 0UL; _fuseiter6251 < 64UL; _fuseiter6251 += 16UL) {
            vec_s32x16 __cached_2;
            __cached_2 = vec_s32x16::load(&__origouts_1760_shr[((_fuseiter6249 * 1792UL) + ((_fuseiter6250 * 64UL) + _fuseiter6251))]);
            vec_f32x16 __cached_3;
            __cached_3 = (vec_f32x16)(__cached_2);
            vec_f32x16 __cached_4;
            __cached_4 = vec_f32x16::load(&__ins_2[((fused_0k__n_1850 * 64UL) + _fuseiter6251)]);
            __cached_3 = (__cached_3 * __cached_4);
            vec_f32x16 __cached_5;
            __cached_5 = vec_f32x16::load(&__ins_3[((fused_0k__n_1850 * 64UL) + _fuseiter6251)]);
            __cached_3 = (__cached_3 + __cached_5);
            __cached_3 = sc_max(__cached_3, vec_f32x16(0.f));
            vec_s8x16 __cached_6;
            __cached_6 = sc_saturated_cast<vec_s8x16>(sc_round_and_cast<vec_s32x16>(__cached_3));
            vec_s8x16 __cached_7;
            __cached_7 = __cached_6;
            vec_s8x16::store(__cached_7, &__outs_0[((((_fuseiter6251 + (fused_0k__n_1850 * 64UL)) / 128UL) * 115200UL) + ((((_fuseiter6249 + (p_o * 2UL)) + 1UL) * 3840UL) + (((_fuseiter6250 + 1UL) * 128UL) + ((_fuseiter6251 + (fused_0k__n_1850 * 64UL)) % 128UL))))]);
          }
        }
      }
      sc_aligned_free(__stream, __origouts_1760_shr);
    }
  }
  return true;
}

static bool res3d_conv_1_cast_mul_add_relu_cast__88(int8_t* __restrict__ __outs_0, int8_t* __restrict__ __ins_0, int8_t* __restrict__ __ins_1, float* __restrict__ __ins_2, float* __restrict__ __ins_3) noexcept{
  void*& __sc_kernel_cache_142 = *(void**)(__module_data + 168);
  int8_t* __rescheduled_0 = (int8_t*)sc_aligned_malloc(__stream, 320UL);
  for (uint64_t fused_0k_o__n_1851 = 0UL; fused_0k_o__n_1851 < 4UL; fused_0k_o__n_1851 += 1UL) {
    int32_t* __origouts_1770_shr = (int32_t*)sc_aligned_malloc(__stream, 100352UL);
    for (uint64_t o_o = 0UL; o_o < 28UL; o_o += 1UL) {
      void** A_list = (void**)&__rescheduled_0[0UL];
      void** B_list = (void**)&__rescheduled_0[128UL];
      memset(&__origouts_1770_shr[(((o_o * 28UL) / 28UL) * 896UL)], 0, 3584UL);
      for (uint64_t r = 0UL; r < 3UL; r += 1UL) {
        for (uint64_t s = 0UL; s < 3UL; s += 1UL) {
          void* __cached_0;
          __cached_0 = &__ins_0[(((((o_o * 28UL) / 28UL) + r) * 3840UL) + (s * 128UL))];
          A_list[((r * 3UL) + s)] = __cached_0;
          void* __cached_1;
          __cached_1 = &__ins_1[((fused_0k_o__n_1851 * 36864UL) + ((r * 12288UL) + (s * 4096UL)))];
          B_list[((r * 3UL) + s)] = __cached_1;
        }
      }
      void* _arg_cache_21 = &__origouts_1770_shr[(((o_o * 28UL) / 28UL) * 896UL)];
      dnnl_brgemm_list_call(__sc_kernel_cache_142, A_list, B_list, &__origouts_1770_shr[(((o_o * 28UL) / 28UL) * 896UL)], 1, 128, 4096, 9, 7, 7, __stream);
    }
    float* _cast_buf_0_shr = (float*)&__rescheduled_0[256UL];
    for (uint64_t _fuseiter6284 = 0UL; _fuseiter6284 < 28UL; _fuseiter6284 += 1UL) {
      for (uint64_t _fuseiter6285 = 0UL; _fuseiter6285 < 28UL; _fuseiter6285 += 1UL) {
        for (uint64_t _fuseiter6286 = 0UL; _fuseiter6286 < 32UL; _fuseiter6286 += 16UL) {
          vec_s32x16 __cached_2;
          __cached_2 = vec_s32x16::load(&__origouts_1770_shr[((_fuseiter6284 * 896UL) + ((_fuseiter6285 * 32UL) + _fuseiter6286))]);
          vec_f32x16 __cached_3;
          __cached_3 = (vec_f32x16)(__cached_2);
          vec_f32x16::store(__cached_3, &_cast_buf_0_shr[((((0UL - _fuseiter6284) * 16UL) + (((0UL - _fuseiter6285) * 16UL) + (0UL - _fuseiter6286))) + ((_fuseiter6284 * 16UL) + ((_fuseiter6285 * 16UL) + _fuseiter6286)))]);
          vec_f32x16 __cached_4;
          __cached_4 = vec_f32x16::load(&_cast_buf_0_shr[0UL]);
          vec_f32x16 __cached_5;
          __cached_5 = vec_f32x16::load(&__ins_2[((fused_0k_o__n_1851 * 32UL) + _fuseiter6286)]);
          __cached_4 = (__cached_4 * __cached_5);
          vec_f32x16 __cached_6;
          __cached_6 = vec_f32x16::load(&__ins_3[((fused_0k_o__n_1851 * 32UL) + _fuseiter6286)]);
          __cached_4 = (__cached_4 + __cached_6);
          __cached_4 = sc_max(__cached_4, vec_f32x16(0.f));
          vec_f32x16::store(__cached_4, &_cast_buf_0_shr[0UL]);
          vec_s8x16 __cached_7;
          __cached_7 = sc_saturated_cast<vec_s8x16>(sc_round_and_cast<vec_s32x16>(__cached_4));
          vec_s8x16::store(__cached_7, &__outs_0[((fused_0k_o__n_1851 * 25088UL) + ((_fuseiter6284 * 896UL) + ((_fuseiter6285 * 32UL) + _fuseiter6286)))]);
        }
      }
    }
    sc_aligned_free(__stream, __origouts_1770_shr);
  }
  sc_aligned_free(__stream, __rescheduled_0);
  return true;
}

static bool res3d_conv_2_cast_mul_add_cast_add_cast__92(uint8_t* __restrict__ __outs_0, int8_t* __restrict__ __ins_0, int8_t* __restrict__ __ins_1, float* __restrict__ __ins_2, float* __restrict__ __ins_3, uint8_t* __restrict__ __ins_4) noexcept{
  void*& __sc_kernel_cache_144 = *(void**)(__module_data + 176);
  alignas(64) int8_t __rescheduled_0[128UL];
  for (uint64_t fused_0k__n_1852 = 0UL; fused_0k__n_1852 < 4UL; fused_0k__n_1852 += 1UL) {
    for (uint64_t p_o = 0UL; p_o < 4UL; p_o += 1UL) {
      int32_t* __origouts_1780_shr = (int32_t*)sc_aligned_malloc(__stream, 100352UL);
      void** A_list = (void**)&__rescheduled_0[0UL];
      void** B_list = (void**)&__rescheduled_0[64UL];
      for (uint64_t c = 0UL; c < 4UL; c += 1UL) {
        void* __cached_0;
        __cached_0 = &__ins_0[((c * 25088UL) + (p_o * 6272UL))];
        A_list[c] = __cached_0;
        void* __cached_1;
        __cached_1 = &__ins_1[((fused_0k__n_1852 * 16384UL) + (c * 4096UL))];
        B_list[c] = __cached_1;
      }
      void* _arg_cache_22 = &__origouts_1780_shr[0UL];
      dnnl_brgemm_list_call(__sc_kernel_cache_144, A_list, B_list, &__origouts_1780_shr[0UL], 1, 1, 1, 4, 7, 7, __stream);
      for (uint64_t _fuseiter6314 = 0UL; _fuseiter6314 < 7UL; _fuseiter6314 += 1UL) {
        for (uint64_t _fuseiter6315 = 0UL; _fuseiter6315 < 28UL; _fuseiter6315 += 1UL) {
          for (uint64_t _fuseiter6316 = 0UL; _fuseiter6316 < 128UL; _fuseiter6316 += 16UL) {
            vec_s32x16 __cached_2;
            __cached_2 = vec_s32x16::load(&__origouts_1780_shr[((_fuseiter6314 * 3584UL) + ((_fuseiter6315 * 128UL) + _fuseiter6316))]);
            vec_f32x16 __cached_3;
            __cached_3 = (vec_f32x16)(__cached_2);
            vec_f32x16 __cached_4;
            __cached_4 = vec_f32x16::load(&__ins_2[((fused_0k__n_1852 * 128UL) + _fuseiter6316)]);
            __cached_3 = (__cached_3 * __cached_4);
            vec_f32x16 __cached_5;
            __cached_5 = vec_f32x16::load(&__ins_3[((fused_0k__n_1852 * 128UL) + _fuseiter6316)]);
            __cached_3 = (__cached_3 + __cached_5);
            vec_s8x16 __cached_6;
            __cached_6 = sc_saturated_cast<vec_s8x16>(sc_round_and_cast<vec_s32x16>(__cached_3));
            vec_u8x16 __cached_7;
            __cached_7 = vec_u8x16::load(&__ins_4[(((fused_0k__n_1852 * 100352UL) + (p_o * 25088UL)) + ((_fuseiter6314 * 3584UL) + ((_fuseiter6315 * 128UL) + _fuseiter6316)))]);
            __cached_6 = (__cached_6 + (vec_s8x16)(__cached_7));
            vec_u8x16 __cached_8;
            __cached_8 = (vec_u8x16)(sc_max(__cached_6, vec_s8x16(0)));
            vec_u8x16::store(__cached_8, &__outs_0[(((fused_0k__n_1852 * 100352UL) + (p_o * 25088UL)) + ((_fuseiter6314 * 3584UL) + ((_fuseiter6315 * 128UL) + _fuseiter6316)))]);
          }
        }
      }
      sc_aligned_free(__stream, __origouts_1780_shr);
    }
  }
  return true;
}

static bool res4a_conv_0_cast_mul_add_relu_cast__100(int8_t* __restrict__ __outs_0, uint8_t* __restrict__ __ins_0, int8_t* __restrict__ __ins_1, float* __restrict__ __ins_2, float* __restrict__ __ins_3) noexcept{
  void*& __sc_kernel_cache_146 = *(void**)(__module_data + 184);
  alignas(64) int8_t __rescheduled_0[128UL];
  memset(&__outs_0[0UL], 0, 7680UL);
  for (uint64_t p1 = 0UL; p1 < 28UL; p1 += 1UL) {
    memset(&__outs_0[((p1 + 1UL) * 7680UL)], 0, 256UL);
    memset(&__outs_0[(((p1 + 1UL) * 7680UL) + 7424UL)], 0, 256UL);
  }
  memset(&__outs_0[222720UL], 0, 7680UL);
  for (uint64_t p_o = 0UL; p_o < 14UL; p_o += 1UL) {
    int32_t* __origouts_1790_shr = (int32_t*)sc_aligned_malloc(__stream, 57344UL);
    void** A_list = (void**)&__rescheduled_0[0UL];
    void** B_list = (void**)&__rescheduled_0[64UL];
    for (uint64_t c = 0UL; c < 4UL; c += 1UL) {
      void* __cached_0;
      __cached_0 = &__ins_0[((c * 100352UL) + (p_o * 7168UL))];
      A_list[c] = __cached_0;
      void* __cached_1;
      __cached_1 = &__ins_1[(c * 32768UL)];
      B_list[c] = __cached_1;
    }
    void* _arg_cache_23 = &__origouts_1790_shr[0UL];
    dnnl_brgemm_list_call(__sc_kernel_cache_146, A_list, B_list, &__origouts_1790_shr[0UL], 1, 1, 1, 4, 8, 7, __stream);
    for (uint64_t _fuseiter6350 = 0UL; _fuseiter6350 < 2UL; _fuseiter6350 += 1UL) {
      for (uint64_t _fuseiter6351 = 0UL; _fuseiter6351 < 28UL; _fuseiter6351 += 1UL) {
        for (uint64_t _fuseiter6352 = 0UL; _fuseiter6352 < 256UL; _fuseiter6352 += 16UL) {
          vec_s32x16 __cached_2;
          __cached_2 = vec_s32x16::load(&__origouts_1790_shr[((_fuseiter6350 * 7168UL) + ((_fuseiter6351 * 256UL) + _fuseiter6352))]);
          vec_f32x16 __cached_3;
          __cached_3 = (vec_f32x16)(__cached_2);
          vec_f32x16 __cached_4;
          __cached_4 = vec_f32x16::load(&__ins_2[_fuseiter6352]);
          __cached_3 = (__cached_3 * __cached_4);
          vec_f32x16 __cached_5;
          __cached_5 = vec_f32x16::load(&__ins_3[_fuseiter6352]);
          __cached_3 = (__cached_3 + __cached_5);
          __cached_3 = sc_max(__cached_3, vec_f32x16(0.f));
          vec_s8x16 __cached_6;
          __cached_6 = sc_saturated_cast<vec_s8x16>(sc_round_and_cast<vec_s32x16>(__cached_3));
          vec_s8x16::store(__cached_6, &__outs_0[(((((p_o * 2UL) + 1UL) * 7680UL) + 256UL) + ((_fuseiter6350 * 7680UL) + ((_fuseiter6351 * 256UL) + _fuseiter6352)))]);
        }
      }
    }
    sc_aligned_free(__stream, __origouts_1790_shr);
  }
  return true;
}

static bool res4a_conv_1_cast_mul_add_relu_cast_reorder__104(int8_t* __restrict__ __outs_0, int8_t* __restrict__ __ins_0, int8_t* __restrict__ __ins_1, float* __restrict__ __ins_2, float* __restrict__ __ins_3) noexcept{
  void** __sc_kernel_cache_arr_150 = (void**)&__uninitialized_data[23657512UL];
  int8_t* __rescheduled_0 = (int8_t*)sc_aligned_malloc(__stream, 384UL);
  int32_t* conv_os_blk_size = (int32_t*)&__rescheduled_0[0UL];
  int32_t* conv_os_acc_size = (int32_t*)&__rescheduled_0[64UL];
  int32_t __cached_0;
  __cached_0 = 196;
  conv_os_blk_size[0] = __cached_0;
  int32_t __cached_1;
  __cached_1 = 0;
  conv_os_acc_size[0] = __cached_1;
  for (uint64_t fused_0k_o__n_1855 = 0UL; fused_0k_o__n_1855 < 2UL; fused_0k_o__n_1855 += 1UL) {
    int32_t* __origouts_1800_shr = (int32_t*)sc_aligned_malloc(__stream, 100352UL);
    void** A_list = (void**)&__rescheduled_0[128UL];
    void** B_list = (void**)&__rescheduled_0[256UL];
    int32_t __cached_2;
    __cached_2 = conv_os_acc_size[0UL];
    int32_t __cached_3;
    __cached_3 = conv_os_blk_size[0UL];
    memset(&__origouts_1800_shr[(uint64_t)(((__cached_2 / 14) * 1792) + ((__cached_2 % 14) * 128))], 0, ((uint64_t)(__cached_3 * 128) * 4UL));
    for (uint64_t r = 0UL; r < 3UL; r += 1UL) {
      for (uint64_t s = 0UL; s < 3UL; s += 1UL) {
        void* __cached_4;
        __cached_4 = &__ins_0[((r * 7680UL) + (s * 256UL))];
        A_list[((r * 3UL) + s)] = __cached_4;
        void* __cached_5;
        __cached_5 = &__ins_1[((fused_0k_o__n_1855 * 294912UL) + ((r * 98304UL) + (s * 32768UL)))];
        B_list[((r * 3UL) + s)] = __cached_5;
      }
    }
    void* _arg_cache_24 = &__origouts_1800_shr[(uint64_t)(((__cached_2 / 14) * 1792) + ((__cached_2 % 14) * 128))];
    dnnl_brgemm_list_call(__sc_kernel_cache_arr_150[0UL], A_list, B_list, &__origouts_1800_shr[(uint64_t)(((__cached_2 / 14) * 1792) + ((__cached_2 % 14) * 128))], 1, 256, 32768, 9, 7, 7, __stream);
    for (uint64_t _fuseiter6380 = 0UL; _fuseiter6380 < 14UL; _fuseiter6380 += 1UL) {
      for (uint64_t _fuseiter6381 = 0UL; _fuseiter6381 < 14UL; _fuseiter6381 += 1UL) {
        for (uint64_t _fuseiter6382 = 0UL; _fuseiter6382 < 128UL; _fuseiter6382 += 16UL) {
          vec_s32x16 __cached_6;
          __cached_6 = vec_s32x16::load(&__origouts_1800_shr[((_fuseiter6380 * 1792UL) + ((_fuseiter6381 * 128UL) + _fuseiter6382))]);
          vec_f32x16 __cached_7;
          __cached_7 = (vec_f32x16)(__cached_6);
          vec_f32x16 __cached_8;
          __cached_8 = vec_f32x16::load(&__ins_2[((fused_0k_o__n_1855 * 128UL) + _fuseiter6382)]);
          __cached_7 = (__cached_7 * __cached_8);
          vec_f32x16 __cached_9;
          __cached_9 = vec_f32x16::load(&__ins_3[((fused_0k_o__n_1855 * 128UL) + _fuseiter6382)]);
          __cached_7 = (__cached_7 + __cached_9);
          __cached_7 = sc_max(__cached_7, vec_f32x16(0.f));
          vec_s8x16 __cached_10;
          __cached_10 = sc_saturated_cast<vec_s8x16>(sc_round_and_cast<vec_s32x16>(__cached_7));
          vec_s8x16 __cached_11;
          __cached_11 = __cached_10;
          vec_s8x16::store(__cached_11, &__outs_0[((((_fuseiter6382 + (fused_0k_o__n_1855 * 128UL)) / 256UL) * 50176UL) + ((_fuseiter6380 * 3584UL) + ((_fuseiter6381 * 256UL) + ((_fuseiter6382 + (fused_0k_o__n_1855 * 128UL)) % 256UL))))]);
        }
      }
    }
    sc_aligned_free(__stream, __origouts_1800_shr);
  }
  sc_aligned_free(__stream, __rescheduled_0);
  return true;
}

static bool res4a_conv_b_cast_mul_add_cast_reorder__96(int8_t* __restrict__ __outs_0, uint8_t* __restrict__ __ins_0, int8_t* __restrict__ __ins_1, float* __restrict__ __ins_2, float* __restrict__ __ins_3) noexcept{
  void*& __sc_kernel_cache_152 = *(void**)(__module_data + 192);
  alignas(64) int8_t __rescheduled_0[128UL];
  uint8_t* input_tmp = (uint8_t*)sc_aligned_malloc(__stream, 100352UL);
  for (uint64_t fused_0n__c_o_1856 = 0UL; fused_0n__c_o_1856 < 4UL; fused_0n__c_o_1856 += 1UL) {
    for (uint64_t p = 0UL; p < 14UL; p += 1UL) {
      for (uint64_t q = 0UL; q < 14UL; q += 1UL) {
        for (uint64_t c_i = 0UL; c_i < 128UL; c_i += 64UL) {
          vec_u8x64 __cached_0;
          __cached_0 = vec_u8x64::load(&__ins_0[(((fused_0n__c_o_1856 / 4UL) * 401408UL) + (((fused_0n__c_o_1856 % 4UL) * 100352UL) + ((p * 7168UL) + ((q * 256UL) + c_i))))]);
          vec_u8x64 __cached_1;
          __cached_1 = __cached_0;
          vec_u8x64::store(__cached_1, &input_tmp[(((fused_0n__c_o_1856 / 4UL) * 100352UL) + (((fused_0n__c_o_1856 % 4UL) * 25088UL) + ((p * 1792UL) + ((q * 128UL) + c_i))))]);
        }
      }
    }
  }
  for (uint64_t fused_0k__n_1857 = 0UL; fused_0k__n_1857 < 32UL; fused_0k__n_1857 += 1UL) {
    for (uint64_t p_o = 0UL; p_o < 2UL; p_o += 1UL) {
      int32_t* __origouts_1810_shr = (int32_t*)sc_aligned_malloc(__stream, 12544UL);
      void** A_list = (void**)&__rescheduled_0[0UL];
      void** B_list = (void**)&__rescheduled_0[64UL];
      for (uint64_t c = 0UL; c < 4UL; c += 1UL) {
        void* __cached_2;
        __cached_2 = &input_tmp[((c * 25088UL) + (p_o * 12544UL))];
        A_list[c] = __cached_2;
        void* __cached_3;
        __cached_3 = &__ins_1[((fused_0k__n_1857 * 16384UL) + (c * 4096UL))];
        B_list[c] = __cached_3;
      }
      void* _arg_cache_25 = &__origouts_1810_shr[0UL];
      dnnl_brgemm_list_call(__sc_kernel_cache_152, A_list, B_list, &__origouts_1810_shr[0UL], 1, 1, 1, 4, 8, 7, __stream);
      for (uint64_t _fuseiter6415 = 0UL; _fuseiter6415 < 7UL; _fuseiter6415 += 1UL) {
        for (uint64_t _fuseiter6416 = 0UL; _fuseiter6416 < 14UL; _fuseiter6416 += 1UL) {
          for (uint64_t _fuseiter6417 = 0UL; _fuseiter6417 < 32UL; _fuseiter6417 += 16UL) {
            vec_s32x16 __cached_4;
            __cached_4 = vec_s32x16::load(&__origouts_1810_shr[((_fuseiter6415 * 448UL) + ((_fuseiter6416 * 32UL) + _fuseiter6417))]);
            vec_f32x16 __cached_5;
            __cached_5 = (vec_f32x16)(__cached_4);
            vec_f32x16 __cached_6;
            __cached_6 = vec_f32x16::load(&__ins_2[((fused_0k__n_1857 * 32UL) + _fuseiter6417)]);
            __cached_5 = (__cached_5 * __cached_6);
            vec_f32x16 __cached_7;
            __cached_7 = vec_f32x16::load(&__ins_3[((fused_0k__n_1857 * 32UL) + _fuseiter6417)]);
            __cached_5 = (__cached_5 + __cached_7);
            vec_s8x16 __cached_8;
            __cached_8 = sc_saturated_cast<vec_s8x16>(sc_round_and_cast<vec_s32x16>(__cached_5));
            vec_s8x16 __cached_9;
            __cached_9 = __cached_8;
            vec_s8x16::store(__cached_9, &__outs_0[((((_fuseiter6417 + (fused_0k__n_1857 * 32UL)) / 128UL) * 25088UL) + (((_fuseiter6415 + (p_o * 7UL)) * 1792UL) + ((_fuseiter6416 * 128UL) + ((_fuseiter6417 + (fused_0k__n_1857 * 32UL)) % 128UL))))]);
          }
        }
      }
      sc_aligned_free(__stream, __origouts_1810_shr);
    }
  }
  sc_aligned_free(__stream, input_tmp);
  return true;
}

static bool res4a_conv_2_cast_mul_add_cast_add_cast_reorder__108(uint8_t* __restrict__ __outs_0, int8_t* __restrict__ __ins_0, int8_t* __restrict__ __ins_1, float* __restrict__ __ins_2, float* __restrict__ __ins_3, int8_t* __restrict__ __ins_4) noexcept{
  void*& __sc_kernel_cache_154 = *(void**)(__module_data + 200);
  alignas(64) int8_t __rescheduled_0[128UL];
  for (uint64_t fused_0k__n_1858 = 0UL; fused_0k__n_1858 < 8UL; fused_0k__n_1858 += 1UL) {
    for (uint64_t p_o = 0UL; p_o < 14UL; p_o += 1UL) {
      int32_t* __origouts_1820_shr = (int32_t*)sc_aligned_malloc(__stream, 7168UL);
      void** A_list = (void**)&__rescheduled_0[0UL];
      void** B_list = (void**)&__rescheduled_0[64UL];
      void* __cached_0;
      __cached_0 = &__ins_0[(p_o * 3584UL)];
      A_list[0UL] = __cached_0;
      void* __cached_1;
      __cached_1 = &__ins_1[(fused_0k__n_1858 * 32768UL)];
      B_list[0UL] = __cached_1;
      void* _arg_cache_26 = &__origouts_1820_shr[0UL];
      dnnl_brgemm_list_call(__sc_kernel_cache_154, A_list, B_list, &__origouts_1820_shr[0UL], 1, 1, 1, 1, 7, 7, __stream);
      for (uint64_t _fuseiter6445 = 0UL; _fuseiter6445 < 14UL; _fuseiter6445 += 1UL) {
        for (uint64_t _fuseiter6446 = 0UL; _fuseiter6446 < 128UL; _fuseiter6446 += 16UL) {
          vec_s32x16 __cached_2;
          __cached_2 = vec_s32x16::load(&__origouts_1820_shr[((_fuseiter6445 * 128UL) + _fuseiter6446)]);
          vec_f32x16 __cached_3;
          __cached_3 = (vec_f32x16)(__cached_2);
          vec_f32x16 __cached_4;
          __cached_4 = vec_f32x16::load(&__ins_2[((fused_0k__n_1858 * 128UL) + _fuseiter6446)]);
          __cached_3 = (__cached_3 * __cached_4);
          vec_f32x16 __cached_5;
          __cached_5 = vec_f32x16::load(&__ins_3[((fused_0k__n_1858 * 128UL) + _fuseiter6446)]);
          __cached_3 = (__cached_3 + __cached_5);
          vec_s8x16 __cached_6;
          __cached_6 = sc_saturated_cast<vec_s8x16>(sc_round_and_cast<vec_s32x16>(__cached_3));
          vec_s8x16 __cached_7;
          __cached_7 = vec_s8x16::load(&__ins_4[(((fused_0k__n_1858 * 25088UL) + (p_o * 1792UL)) + ((_fuseiter6445 * 128UL) + _fuseiter6446))]);
          __cached_6 = (__cached_6 + __cached_7);
          vec_u8x16 __cached_8;
          __cached_8 = (vec_u8x16)(sc_max(__cached_6, vec_s8x16(0)));
          vec_u8x16 __cached_9;
          __cached_9 = __cached_8;
          vec_u8x16::store(__cached_9, &__outs_0[((((_fuseiter6446 + (fused_0k__n_1858 * 128UL)) / 1024UL) * 200704UL) + ((p_o * 14336UL) + ((_fuseiter6445 * 1024UL) + ((_fuseiter6446 + (fused_0k__n_1858 * 128UL)) % 1024UL))))]);
        }
      }
      sc_aligned_free(__stream, __origouts_1820_shr);
    }
  }
  return true;
}

static bool res4b_conv_0_cast_mul_add_relu_cast_reorder__112(int8_t* __restrict__ __outs_0, uint8_t* __restrict__ __ins_0, int8_t* __restrict__ __ins_1, float* __restrict__ __ins_2, float* __restrict__ __ins_3) noexcept{
  void*& __sc_kernel_cache_156 = *(void**)(__module_data + 208);
  alignas(64) int8_t __rescheduled_0[128UL];
  memset(&__outs_0[0UL], 0, 4096UL);
  for (uint64_t p1 = 0UL; p1 < 14UL; p1 += 1UL) {
    memset(&__outs_0[((p1 + 1UL) * 4096UL)], 0, 256UL);
    memset(&__outs_0[(((p1 + 1UL) * 4096UL) + 3840UL)], 0, 256UL);
  }
  memset(&__outs_0[61440UL], 0, 4096UL);
  for (uint64_t fused_0n__k_1860 = 0UL; fused_0n__k_1860 < 4UL; fused_0n__k_1860 += 1UL) {
    for (uint64_t p_o = 0UL; p_o < 7UL; p_o += 1UL) {
      int32_t* __origouts_1830_shr = (int32_t*)sc_aligned_malloc(__stream, 7168UL);
      void** A_list = (void**)&__rescheduled_0[0UL];
      void** B_list = (void**)&__rescheduled_0[64UL];
      void* __cached_0;
      __cached_0 = &__ins_0[(((fused_0n__k_1860 / 4UL) * 200704UL) + (p_o * 28672UL))];
      A_list[0UL] = __cached_0;
      void* __cached_1;
      __cached_1 = &__ins_1[((fused_0n__k_1860 % 4UL) * 65536UL)];
      B_list[0UL] = __cached_1;
      void* _arg_cache_27 = &__origouts_1830_shr[0UL];
      dnnl_brgemm_list_call(__sc_kernel_cache_156, A_list, B_list, &__origouts_1830_shr[0UL], 1, 1, 1, 1, 8, 7, __stream);
      for (uint64_t _fuseiter6485 = 0UL; _fuseiter6485 < 2UL; _fuseiter6485 += 1UL) {
        for (uint64_t _fuseiter6486 = 0UL; _fuseiter6486 < 14UL; _fuseiter6486 += 1UL) {
          for (uint64_t _fuseiter6487 = 0UL; _fuseiter6487 < 64UL; _fuseiter6487 += 16UL) {
            vec_s32x16 __cached_2;
            __cached_2 = vec_s32x16::load(&__origouts_1830_shr[((_fuseiter6485 * 896UL) + ((_fuseiter6486 * 64UL) + _fuseiter6487))]);
            vec_f32x16 __cached_3;
            __cached_3 = (vec_f32x16)(__cached_2);
            vec_f32x16 __cached_4;
            __cached_4 = vec_f32x16::load(&__ins_2[((((fused_0n__k_1860 / 4UL) * 256UL) + ((fused_0n__k_1860 % 4UL) * 64UL)) + _fuseiter6487)]);
            __cached_3 = (__cached_3 * __cached_4);
            vec_f32x16 __cached_5;
            __cached_5 = vec_f32x16::load(&__ins_3[(((fused_0n__k_1860 % 4UL) * 64UL) + _fuseiter6487)]);
            __cached_3 = (__cached_3 + __cached_5);
            __cached_3 = sc_max(__cached_3, vec_f32x16(0.f));
            vec_s8x16 __cached_6;
            __cached_6 = sc_saturated_cast<vec_s8x16>(sc_round_and_cast<vec_s32x16>(__cached_3));
            vec_s8x16 __cached_7;
            __cached_7 = __cached_6;
            vec_s8x16::store(__cached_7, &__outs_0[(((fused_0n__k_1860 / 4UL) * 65536UL) + ((((_fuseiter6487 + ((fused_0n__k_1860 % 4UL) * 64UL)) / 256UL) * 65536UL) + ((((_fuseiter6485 + (p_o * 2UL)) + 1UL) * 4096UL) + (((_fuseiter6486 + 1UL) * 256UL) + ((_fuseiter6487 + ((fused_0n__k_1860 % 4UL) * 64UL)) % 256UL)))))]);
          }
        }
      }
      sc_aligned_free(__stream, __origouts_1830_shr);
    }
  }
  return true;
}

static bool res4b_conv_1_cast_mul_add_relu_cast_reorder__116(int8_t* __restrict__ __outs_0, int8_t* __restrict__ __ins_0, int8_t* __restrict__ __ins_1, float* __restrict__ __ins_2, float* __restrict__ __ins_3) noexcept{
  void** __sc_kernel_cache_arr_160 = (void**)&__uninitialized_data[23657528UL];
  int8_t* __rescheduled_0 = (int8_t*)sc_aligned_malloc(__stream, 384UL);
  int32_t* conv_os_blk_size = (int32_t*)&__rescheduled_0[0UL];
  int32_t* conv_os_acc_size = (int32_t*)&__rescheduled_0[64UL];
  int32_t __cached_0;
  __cached_0 = 196;
  conv_os_blk_size[0] = __cached_0;
  int32_t __cached_1;
  __cached_1 = 0;
  conv_os_acc_size[0] = __cached_1;
  for (uint64_t fused_0n__k_o_1861 = 0UL; fused_0n__k_o_1861 < 16UL; fused_0n__k_o_1861 += 1UL) {
    int32_t* __origouts_1840_shr = (int32_t*)sc_aligned_malloc(__stream, 12544UL);
    void** A_list = (void**)&__rescheduled_0[128UL];
    void** B_list = (void**)&__rescheduled_0[256UL];
    int32_t __cached_2;
    __cached_2 = conv_os_acc_size[0UL];
    int32_t __cached_3;
    __cached_3 = conv_os_blk_size[0UL];
    memset(&__origouts_1840_shr[(uint64_t)(((__cached_2 / 14) * 224) + ((__cached_2 % 14) * 16))], 0, ((uint64_t)(__cached_3 * 16) * 4UL));
    for (uint64_t r = 0UL; r < 3UL; r += 1UL) {
      for (uint64_t s = 0UL; s < 3UL; s += 1UL) {
        void* __cached_4;
        __cached_4 = &__ins_0[(((fused_0n__k_o_1861 / 16UL) * 65536UL) + ((r * 4096UL) + (s * 256UL)))];
        A_list[((r * 3UL) + s)] = __cached_4;
        void* __cached_5;
        __cached_5 = &__ins_1[(((fused_0n__k_o_1861 % 16UL) * 36864UL) + ((r * 12288UL) + (s * 4096UL)))];
        B_list[((r * 3UL) + s)] = __cached_5;
      }
    }
    void* _arg_cache_28 = &__origouts_1840_shr[(uint64_t)(((__cached_2 / 14) * 224) + ((__cached_2 % 14) * 16))];
    dnnl_brgemm_list_call(__sc_kernel_cache_arr_160[0UL], A_list, B_list, &__origouts_1840_shr[(uint64_t)(((__cached_2 / 14) * 224) + ((__cached_2 % 14) * 16))], 1, 256, 4096, 9, 7, 7, __stream);
    for (uint64_t _fuseiter6520 = 0UL; _fuseiter6520 < 14UL; _fuseiter6520 += 1UL) {
      for (uint64_t _fuseiter6521 = 0UL; _fuseiter6521 < 14UL; _fuseiter6521 += 1UL) {
        vec_s32x16 __cached_6;
        __cached_6 = vec_s32x16::load(&__origouts_1840_shr[((_fuseiter6520 * 224UL) + (_fuseiter6521 * 16UL))]);
        vec_f32x16 __cached_7;
        __cached_7 = (vec_f32x16)(__cached_6);
        vec_f32x16 __cached_8;
        __cached_8 = vec_f32x16::load(&__ins_2[(((fused_0n__k_o_1861 / 16UL) * 256UL) + ((fused_0n__k_o_1861 % 16UL) * 16UL))]);
        __cached_7 = (__cached_7 * __cached_8);
        vec_f32x16 __cached_9;
        __cached_9 = vec_f32x16::load(&__ins_3[((fused_0n__k_o_1861 % 16UL) * 16UL)]);
        __cached_7 = (__cached_7 + __cached_9);
        __cached_7 = sc_max(__cached_7, vec_f32x16(0.f));
        vec_s8x16 __cached_10;
        __cached_10 = sc_saturated_cast<vec_s8x16>(sc_round_and_cast<vec_s32x16>(__cached_7));
        vec_s8x16 __cached_11;
        __cached_11 = __cached_10;
        vec_s8x16::store(__cached_11, &__outs_0[(((fused_0n__k_o_1861 / 16UL) * 50176UL) + (((((fused_0n__k_o_1861 % 16UL) * 16UL) / 64UL) * 12544UL) + ((_fuseiter6520 * 896UL) + ((_fuseiter6521 * 64UL) + (((fused_0n__k_o_1861 % 16UL) * 16UL) % 64UL)))))]);
      }
    }
    sc_aligned_free(__stream, __origouts_1840_shr);
  }
  sc_aligned_free(__stream, __rescheduled_0);
  return true;
}

static bool res4b_conv_2_cast_mul_add_cast_add_cast_reorder__120(uint8_t* __restrict__ __outs_0, int8_t* __restrict__ __ins_0, int8_t* __restrict__ __ins_1, float* __restrict__ __ins_2, float* __restrict__ __ins_3, uint8_t* __restrict__ __ins_4) noexcept{
  void*& __sc_kernel_cache_162 = *(void**)(__module_data + 216);
  alignas(64) int8_t __rescheduled_0[128UL];
  for (uint64_t p_o = 0UL; p_o < 2UL; p_o += 1UL) {
    int32_t* __origouts_1850_shr = (int32_t*)sc_aligned_malloc(__stream, 401408UL);
    void** A_list = (void**)&__rescheduled_0[0UL];
    void** B_list = (void**)&__rescheduled_0[64UL];
    for (uint64_t c = 0UL; c < 4UL; c += 1UL) {
      void* __cached_0;
      __cached_0 = &__ins_0[((c * 12544UL) + (p_o * 6272UL))];
      A_list[c] = __cached_0;
      void* __cached_1;
      __cached_1 = &__ins_1[(c * 65536UL)];
      B_list[c] = __cached_1;
    }
    void* _arg_cache_29 = &__origouts_1850_shr[0UL];
    dnnl_brgemm_list_call(__sc_kernel_cache_162, A_list, B_list, &__origouts_1850_shr[0UL], 1, 1, 1, 4, 7, 7, __stream);
    for (uint64_t _fuseiter6555 = 0UL; _fuseiter6555 < 7UL; _fuseiter6555 += 1UL) {
      for (uint64_t _fuseiter6556 = 0UL; _fuseiter6556 < 14UL; _fuseiter6556 += 1UL) {
        for (uint64_t _fuseiter6557 = 0UL; _fuseiter6557 < 1024UL; _fuseiter6557 += 16UL) {
          vec_s32x16 __cached_2;
          __cached_2 = vec_s32x16::load(&__origouts_1850_shr[((_fuseiter6555 * 14336UL) + ((_fuseiter6556 * 1024UL) + _fuseiter6557))]);
          vec_f32x16 __cached_3;
          __cached_3 = (vec_f32x16)(__cached_2);
          vec_f32x16 __cached_4;
          __cached_4 = vec_f32x16::load(&__ins_2[_fuseiter6557]);
          __cached_3 = (__cached_3 * __cached_4);
          vec_f32x16 __cached_5;
          __cached_5 = vec_f32x16::load(&__ins_3[_fuseiter6557]);
          __cached_3 = (__cached_3 + __cached_5);
          vec_s8x16 __cached_6;
          __cached_6 = sc_saturated_cast<vec_s8x16>(sc_round_and_cast<vec_s32x16>(__cached_3));
          vec_u8x16 __cached_7;
          __cached_7 = vec_u8x16::load(&__ins_4[((p_o * 100352UL) + ((_fuseiter6555 * 14336UL) + ((_fuseiter6556 * 1024UL) + _fuseiter6557)))]);
          __cached_6 = (__cached_6 + (vec_s8x16)(__cached_7));
          vec_u8x16 __cached_8;
          __cached_8 = (vec_u8x16)(sc_max(__cached_6, vec_s8x16(0)));
          vec_u8x16 __cached_9;
          __cached_9 = __cached_8;
          vec_u8x16::store(__cached_9, &__outs_0[(((_fuseiter6557 / 128UL) * 25088UL) + (((_fuseiter6555 + (p_o * 7UL)) * 1792UL) + ((_fuseiter6556 * 128UL) + (_fuseiter6557 % 128UL))))]);
        }
      }
    }
    sc_aligned_free(__stream, __origouts_1850_shr);
  }
  return true;
}

static bool res4c_conv_0_cast_mul_add_relu_cast_reorder__124(int8_t* __restrict__ __outs_0, uint8_t* __restrict__ __ins_0, int8_t* __restrict__ __ins_1, float* __restrict__ __ins_2, float* __restrict__ __ins_3) noexcept{
  void*& __sc_kernel_cache_164 = *(void**)(__module_data + 224);
  alignas(64) int8_t __rescheduled_0[128UL];
  for (uint64_t fused_0n__k_1863 = 0UL; fused_0n__k_1863 < 2UL; fused_0n__k_1863 += 1UL) {
    memset(&__outs_0[(((fused_0n__k_1863 / 2UL) * 65536UL) + ((fused_0n__k_1863 % 2UL) * 32768UL))], 0, 2048UL);
    for (uint64_t p1 = 0UL; p1 < 14UL; p1 += 1UL) {
      memset(&__outs_0[(((fused_0n__k_1863 / 2UL) * 65536UL) + (((fused_0n__k_1863 % 2UL) * 32768UL) + ((p1 + 1UL) * 2048UL)))], 0, 128UL);
      memset(&__outs_0[((((fused_0n__k_1863 / 2UL) * 65536UL) + (((fused_0n__k_1863 % 2UL) * 32768UL) + ((p1 + 1UL) * 2048UL))) + 1920UL)], 0, 128UL);
    }
    memset(&__outs_0[((((fused_0n__k_1863 / 2UL) * 65536UL) + ((fused_0n__k_1863 % 2UL) * 32768UL)) + 30720UL)], 0, 2048UL);
  }
  for (uint64_t fused_0k__n_1864 = 0UL; fused_0k__n_1864 < 8UL; fused_0k__n_1864 += 1UL) {
    int32_t* __origouts_1860_shr = (int32_t*)sc_aligned_malloc(__stream, 25088UL);
    void** A_list = (void**)&__rescheduled_0[0UL];
    void** B_list = (void**)&__rescheduled_0[64UL];
    for (uint64_t c = 0UL; c < 8UL; c += 1UL) {
      void* __cached_0;
      __cached_0 = &__ins_0[(c * 25088UL)];
      A_list[c] = __cached_0;
      void* __cached_1;
      __cached_1 = &__ins_1[((fused_0k__n_1864 * 32768UL) + (c * 4096UL))];
      B_list[c] = __cached_1;
    }
    void* _arg_cache_30 = &__origouts_1860_shr[0UL];
    dnnl_brgemm_list_call(__sc_kernel_cache_164, A_list, B_list, &__origouts_1860_shr[0UL], 1, 1, 1, 8, 8, 7, __stream);
    for (uint64_t _fuseiter6596 = 0UL; _fuseiter6596 < 14UL; _fuseiter6596 += 1UL) {
      for (uint64_t _fuseiter6597 = 0UL; _fuseiter6597 < 14UL; _fuseiter6597 += 1UL) {
        for (uint64_t _fuseiter6598 = 0UL; _fuseiter6598 < 32UL; _fuseiter6598 += 16UL) {
          vec_s32x16 __cached_2;
          __cached_2 = vec_s32x16::load(&__origouts_1860_shr[((_fuseiter6596 * 448UL) + ((_fuseiter6597 * 32UL) + _fuseiter6598))]);
          vec_f32x16 __cached_3;
          __cached_3 = (vec_f32x16)(__cached_2);
          vec_f32x16 __cached_4;
          __cached_4 = vec_f32x16::load(&__ins_2[((fused_0k__n_1864 * 32UL) + _fuseiter6598)]);
          __cached_3 = (__cached_3 * __cached_4);
          vec_f32x16 __cached_5;
          __cached_5 = vec_f32x16::load(&__ins_3[((fused_0k__n_1864 * 32UL) + _fuseiter6598)]);
          __cached_3 = (__cached_3 + __cached_5);
          __cached_3 = sc_max(__cached_3, vec_f32x16(0.f));
          vec_s8x16 __cached_6;
          __cached_6 = sc_saturated_cast<vec_s8x16>(sc_round_and_cast<vec_s32x16>(__cached_3));
          vec_s8x16 __cached_7;
          __cached_7 = __cached_6;
          vec_s8x16::store(__cached_7, &__outs_0[((((_fuseiter6598 + (fused_0k__n_1864 * 32UL)) / 128UL) * 32768UL) + (((_fuseiter6596 + 1UL) * 2048UL) + (((_fuseiter6597 + 1UL) * 128UL) + ((_fuseiter6598 + (fused_0k__n_1864 * 32UL)) % 128UL))))]);
        }
      }
    }
    sc_aligned_free(__stream, __origouts_1860_shr);
  }
  return true;
}

static bool res4c_conv_1_cast_mul_add_relu_cast_reorder__128(int8_t* __restrict__ __outs_0, int8_t* __restrict__ __ins_0, int8_t* __restrict__ __ins_1, float* __restrict__ __ins_2, float* __restrict__ __ins_3) noexcept{
  void** __sc_kernel_cache_arr_166 = (void**)&__uninitialized_data[23657536UL];
  int8_t* __rescheduled_0 = (int8_t*)sc_aligned_malloc(__stream, 512UL);
  int32_t* conv_os_blk_size = (int32_t*)&__rescheduled_0[0UL];
  int32_t* conv_os_acc_size = (int32_t*)&__rescheduled_0[64UL];
  int32_t __cached_0;
  __cached_0 = 196;
  conv_os_blk_size[0] = __cached_0;
  int32_t __cached_1;
  __cached_1 = 0;
  conv_os_acc_size[0] = __cached_1;
  for (uint64_t fused_0k_o__n_1865 = 0UL; fused_0k_o__n_1865 < 8UL; fused_0k_o__n_1865 += 1UL) {
    int32_t* __origouts_1870_shr = (int32_t*)sc_aligned_malloc(__stream, 25088UL);
    void** A_list = (void**)&__rescheduled_0[128UL];
    void** B_list = (void**)&__rescheduled_0[320UL];
    int32_t __cached_2;
    __cached_2 = conv_os_acc_size[0UL];
    int32_t __cached_3;
    __cached_3 = conv_os_blk_size[0UL];
    memset(&__origouts_1870_shr[(uint64_t)(((__cached_2 / 14) * 448) + ((__cached_2 % 14) * 32))], 0, ((uint64_t)(__cached_3 * 32) * 4UL));
    for (uint64_t c_o = 0UL; c_o < 2UL; c_o += 1UL) {
      for (uint64_t r = 0UL; r < 3UL; r += 1UL) {
        for (uint64_t s = 0UL; s < 3UL; s += 1UL) {
          void* __cached_4;
          __cached_4 = &__ins_0[((c_o * 32768UL) + ((r * 2048UL) + (s * 128UL)))];
          A_list[(((c_o * 9UL) + (r * 3UL)) + s)] = __cached_4;
          void* __cached_5;
          __cached_5 = &__ins_1[((fused_0k_o__n_1865 * 73728UL) + ((c_o * 36864UL) + ((r * 12288UL) + (s * 4096UL))))];
          B_list[(((c_o * 9UL) + (r * 3UL)) + s)] = __cached_5;
        }
      }
    }
    void* _arg_cache_31 = &__origouts_1870_shr[(uint64_t)(((__cached_2 / 14) * 448) + ((__cached_2 % 14) * 32))];
    dnnl_brgemm_list_call(__sc_kernel_cache_arr_166[0UL], A_list, B_list, &__origouts_1870_shr[(uint64_t)(((__cached_2 / 14) * 448) + ((__cached_2 % 14) * 32))], 1, 128, 4096, 18, 7, 7, __stream);
    for (uint64_t _fuseiter6631 = 0UL; _fuseiter6631 < 14UL; _fuseiter6631 += 1UL) {
      for (uint64_t _fuseiter6632 = 0UL; _fuseiter6632 < 14UL; _fuseiter6632 += 1UL) {
        for (uint64_t _fuseiter6633 = 0UL; _fuseiter6633 < 32UL; _fuseiter6633 += 16UL) {
          vec_s32x16 __cached_6;
          __cached_6 = vec_s32x16::load(&__origouts_1870_shr[((_fuseiter6631 * 448UL) + ((_fuseiter6632 * 32UL) + _fuseiter6633))]);
          vec_f32x16 __cached_7;
          __cached_7 = (vec_f32x16)(__cached_6);
          vec_f32x16 __cached_8;
          __cached_8 = vec_f32x16::load(&__ins_2[((fused_0k_o__n_1865 * 32UL) + _fuseiter6633)]);
          __cached_7 = (__cached_7 * __cached_8);
          vec_f32x16 __cached_9;
          __cached_9 = vec_f32x16::load(&__ins_3[((fused_0k_o__n_1865 * 32UL) + _fuseiter6633)]);
          __cached_7 = (__cached_7 + __cached_9);
          __cached_7 = sc_max(__cached_7, vec_f32x16(0.f));
          vec_s8x16 __cached_10;
          __cached_10 = sc_saturated_cast<vec_s8x16>(sc_round_and_cast<vec_s32x16>(__cached_7));
          vec_s8x16 __cached_11;
          __cached_11 = __cached_10;
          vec_s8x16::store(__cached_11, &__outs_0[((((_fuseiter6633 + (fused_0k_o__n_1865 * 32UL)) / 256UL) * 50176UL) + ((_fuseiter6631 * 3584UL) + ((_fuseiter6632 * 256UL) + ((_fuseiter6633 + (fused_0k_o__n_1865 * 32UL)) % 256UL))))]);
        }
      }
    }
    sc_aligned_free(__stream, __origouts_1870_shr);
  }
  sc_aligned_free(__stream, __rescheduled_0);
  return true;
}

static bool res4c_conv_2_cast_mul_add_cast_add_cast__132(uint8_t* __restrict__ __outs_0, int8_t* __restrict__ __ins_0, int8_t* __restrict__ __ins_1, float* __restrict__ __ins_2, float* __restrict__ __ins_3, uint8_t* __restrict__ __ins_4) noexcept{
  void*& __sc_kernel_cache_168 = *(void**)(__module_data + 232);
  alignas(64) int8_t __rescheduled_0[128UL];
  for (uint64_t fused_0n__k_1866 = 0UL; fused_0n__k_1866 < 8UL; fused_0n__k_1866 += 1UL) {
    int32_t* __origouts_1880_shr = (int32_t*)sc_aligned_malloc(__stream, 100352UL);
    void** A_list = (void**)&__rescheduled_0[0UL];
    void** B_list = (void**)&__rescheduled_0[64UL];
    void* __cached_0;
    __cached_0 = &__ins_0[((fused_0n__k_1866 / 8UL) * 50176UL)];
    A_list[0UL] = __cached_0;
    void* __cached_1;
    __cached_1 = &__ins_1[((fused_0n__k_1866 % 8UL) * 32768UL)];
    B_list[0UL] = __cached_1;
    void* _arg_cache_32 = &__origouts_1880_shr[0UL];
    dnnl_brgemm_list_call(__sc_kernel_cache_168, A_list, B_list, &__origouts_1880_shr[0UL], 1, 1, 1, 1, 7, 7, __stream);
    for (uint64_t _fuseiter6666 = 0UL; _fuseiter6666 < 14UL; _fuseiter6666 += 1UL) {
      for (uint64_t _fuseiter6667 = 0UL; _fuseiter6667 < 14UL; _fuseiter6667 += 1UL) {
        for (uint64_t _fuseiter6668 = 0UL; _fuseiter6668 < 128UL; _fuseiter6668 += 16UL) {
          vec_s32x16 __cached_2;
          __cached_2 = vec_s32x16::load(&__origouts_1880_shr[((_fuseiter6666 * 1792UL) + ((_fuseiter6667 * 128UL) + _fuseiter6668))]);
          vec_f32x16 __cached_3;
          __cached_3 = (vec_f32x16)(__cached_2);
          vec_f32x16 __cached_4;
          __cached_4 = vec_f32x16::load(&__ins_2[((((fused_0n__k_1866 / 8UL) * 1024UL) + ((fused_0n__k_1866 % 8UL) * 128UL)) + _fuseiter6668)]);
          __cached_3 = (__cached_3 * __cached_4);
          vec_f32x16 __cached_5;
          __cached_5 = vec_f32x16::load(&__ins_3[(((fused_0n__k_1866 % 8UL) * 128UL) + _fuseiter6668)]);
          __cached_3 = (__cached_3 + __cached_5);
          vec_s8x16 __cached_6;
          __cached_6 = sc_saturated_cast<vec_s8x16>(sc_round_and_cast<vec_s32x16>(__cached_3));
          vec_u8x16 __cached_7;
          __cached_7 = vec_u8x16::load(&__ins_4[((((fused_0n__k_1866 / 8UL) * 200704UL) + ((fused_0n__k_1866 % 8UL) * 25088UL)) + ((_fuseiter6666 * 1792UL) + ((_fuseiter6667 * 128UL) + _fuseiter6668)))]);
          __cached_6 = (__cached_6 + (vec_s8x16)(__cached_7));
          vec_u8x16 __cached_8;
          __cached_8 = (vec_u8x16)(sc_max(__cached_6, vec_s8x16(0)));
          vec_u8x16::store(__cached_8, &__outs_0[((((fused_0n__k_1866 / 8UL) * 200704UL) + ((fused_0n__k_1866 % 8UL) * 25088UL)) + ((_fuseiter6666 * 1792UL) + ((_fuseiter6667 * 128UL) + _fuseiter6668)))]);
        }
      }
    }
    sc_aligned_free(__stream, __origouts_1880_shr);
  }
  return true;
}

static bool res4d_conv_0_cast_mul_add_relu_cast__136(int8_t* __restrict__ __outs_0, uint8_t* __restrict__ __ins_0, int8_t* __restrict__ __ins_1, float* __restrict__ __ins_2, float* __restrict__ __ins_3) noexcept{
  void*& __sc_kernel_cache_170 = *(void**)(__module_data + 240);
  alignas(64) int8_t __rescheduled_0[128UL];
  for (uint64_t fused_0n__k_1867 = 0UL; fused_0n__k_1867 < 4UL; fused_0n__k_1867 += 1UL) {
    memset(&__outs_0[(((fused_0n__k_1867 / 4UL) * 65536UL) + ((fused_0n__k_1867 % 4UL) * 16384UL))], 0, 1024UL);
    for (uint64_t p1 = 0UL; p1 < 14UL; p1 += 1UL) {
      memset(&__outs_0[(((fused_0n__k_1867 / 4UL) * 65536UL) + (((fused_0n__k_1867 % 4UL) * 16384UL) + ((p1 + 1UL) * 1024UL)))], 0, 64UL);
      memset(&__outs_0[((((fused_0n__k_1867 / 4UL) * 65536UL) + (((fused_0n__k_1867 % 4UL) * 16384UL) + ((p1 + 1UL) * 1024UL))) + 960UL)], 0, 64UL);
    }
    memset(&__outs_0[((((fused_0n__k_1867 / 4UL) * 65536UL) + ((fused_0n__k_1867 % 4UL) * 16384UL)) + 15360UL)], 0, 1024UL);
  }
  for (uint64_t fused_0k__n_1868 = 0UL; fused_0k__n_1868 < 4UL; fused_0k__n_1868 += 1UL) {
    int32_t* __origouts_1890_shr = (int32_t*)sc_aligned_malloc(__stream, 50176UL);
    void** A_list = (void**)&__rescheduled_0[0UL];
    void** B_list = (void**)&__rescheduled_0[64UL];
    for (uint64_t c = 0UL; c < 8UL; c += 1UL) {
      void* __cached_0;
      __cached_0 = &__ins_0[(c * 25088UL)];
      A_list[c] = __cached_0;
      void* __cached_1;
      __cached_1 = &__ins_1[((fused_0k__n_1868 * 65536UL) + (c * 8192UL))];
      B_list[c] = __cached_1;
    }
    void* _arg_cache_33 = &__origouts_1890_shr[0UL];
    dnnl_brgemm_list_call(__sc_kernel_cache_170, A_list, B_list, &__origouts_1890_shr[0UL], 1, 1, 1, 8, 8, 7, __stream);
    for (uint64_t _fuseiter6702 = 0UL; _fuseiter6702 < 14UL; _fuseiter6702 += 1UL) {
      for (uint64_t _fuseiter6703 = 0UL; _fuseiter6703 < 14UL; _fuseiter6703 += 1UL) {
        for (uint64_t _fuseiter6704 = 0UL; _fuseiter6704 < 64UL; _fuseiter6704 += 16UL) {
          vec_s32x16 __cached_2;
          __cached_2 = vec_s32x16::load(&__origouts_1890_shr[((_fuseiter6702 * 896UL) + ((_fuseiter6703 * 64UL) + _fuseiter6704))]);
          vec_f32x16 __cached_3;
          __cached_3 = (vec_f32x16)(__cached_2);
          vec_f32x16 __cached_4;
          __cached_4 = vec_f32x16::load(&__ins_2[((fused_0k__n_1868 * 64UL) + _fuseiter6704)]);
          __cached_3 = (__cached_3 * __cached_4);
          vec_f32x16 __cached_5;
          __cached_5 = vec_f32x16::load(&__ins_3[((fused_0k__n_1868 * 64UL) + _fuseiter6704)]);
          __cached_3 = (__cached_3 + __cached_5);
          __cached_3 = sc_max(__cached_3, vec_f32x16(0.f));
          vec_s8x16 __cached_6;
          __cached_6 = sc_saturated_cast<vec_s8x16>(sc_round_and_cast<vec_s32x16>(__cached_3));
          vec_s8x16::store(__cached_6, &__outs_0[(((fused_0k__n_1868 * 16384UL) + 1088UL) + ((_fuseiter6702 * 1024UL) + ((_fuseiter6703 * 64UL) + _fuseiter6704)))]);
        }
      }
    }
    sc_aligned_free(__stream, __origouts_1890_shr);
  }
  return true;
}

static bool res4d_conv_1_cast_mul_add_relu_cast_reorder__140(int8_t* __restrict__ __outs_0, int8_t* __restrict__ __ins_0, int8_t* __restrict__ __ins_1, float* __restrict__ __ins_2, float* __restrict__ __ins_3) noexcept{
  void** __sc_kernel_cache_arr_172 = (void**)&__uninitialized_data[23657544UL];
  int8_t* __rescheduled_0 = (int8_t*)sc_aligned_malloc(__stream, 768UL);
  int32_t* conv_os_blk_size = (int32_t*)&__rescheduled_0[0UL];
  int32_t* conv_os_acc_size = (int32_t*)&__rescheduled_0[64UL];
  int32_t __cached_0;
  __cached_0 = 196;
  conv_os_blk_size[0] = __cached_0;
  int32_t __cached_1;
  __cached_1 = 0;
  conv_os_acc_size[0] = __cached_1;
  for (uint64_t fused_0k_o__n_1869 = 0UL; fused_0k_o__n_1869 < 4UL; fused_0k_o__n_1869 += 1UL) {
    int32_t* __origouts_1900_shr = (int32_t*)sc_aligned_malloc(__stream, 50176UL);
    void** A_list = (void**)&__rescheduled_0[128UL];
    void** B_list = (void**)&__rescheduled_0[448UL];
    int32_t __cached_2;
    __cached_2 = conv_os_acc_size[0UL];
    int32_t __cached_3;
    __cached_3 = conv_os_blk_size[0UL];
    memset(&__origouts_1900_shr[(uint64_t)(((__cached_2 / 14) * 896) + ((__cached_2 % 14) * 64))], 0, ((uint64_t)(__cached_3 * 64) * 4UL));
    for (uint64_t c_o = 0UL; c_o < 4UL; c_o += 1UL) {
      for (uint64_t r = 0UL; r < 3UL; r += 1UL) {
        for (uint64_t s = 0UL; s < 3UL; s += 1UL) {
          void* __cached_4;
          __cached_4 = &__ins_0[((c_o * 16384UL) + ((r * 1024UL) + (s * 64UL)))];
          A_list[(((c_o * 9UL) + (r * 3UL)) + s)] = __cached_4;
          void* __cached_5;
          __cached_5 = &__ins_1[((fused_0k_o__n_1869 * 147456UL) + ((c_o * 36864UL) + ((r * 12288UL) + (s * 4096UL))))];
          B_list[(((c_o * 9UL) + (r * 3UL)) + s)] = __cached_5;
        }
      }
    }
    void* _arg_cache_34 = &__origouts_1900_shr[(uint64_t)(((__cached_2 / 14) * 896) + ((__cached_2 % 14) * 64))];
    dnnl_brgemm_list_call(__sc_kernel_cache_arr_172[0UL], A_list, B_list, &__origouts_1900_shr[(uint64_t)(((__cached_2 / 14) * 896) + ((__cached_2 % 14) * 64))], 1, 64, 4096, 36, 7, 7, __stream);
    for (uint64_t _fuseiter6732 = 0UL; _fuseiter6732 < 14UL; _fuseiter6732 += 1UL) {
      for (uint64_t _fuseiter6733 = 0UL; _fuseiter6733 < 14UL; _fuseiter6733 += 1UL) {
        for (uint64_t _fuseiter6734 = 0UL; _fuseiter6734 < 64UL; _fuseiter6734 += 16UL) {
          vec_s32x16 __cached_6;
          __cached_6 = vec_s32x16::load(&__origouts_1900_shr[((_fuseiter6732 * 896UL) + ((_fuseiter6733 * 64UL) + _fuseiter6734))]);
          vec_f32x16 __cached_7;
          __cached_7 = (vec_f32x16)(__cached_6);
          vec_f32x16 __cached_8;
          __cached_8 = vec_f32x16::load(&__ins_2[((fused_0k_o__n_1869 * 64UL) + _fuseiter6734)]);
          __cached_7 = (__cached_7 * __cached_8);
          vec_f32x16 __cached_9;
          __cached_9 = vec_f32x16::load(&__ins_3[((fused_0k_o__n_1869 * 64UL) + _fuseiter6734)]);
          __cached_7 = (__cached_7 + __cached_9);
          __cached_7 = sc_max(__cached_7, vec_f32x16(0.f));
          vec_s8x16 __cached_10;
          __cached_10 = sc_saturated_cast<vec_s8x16>(sc_round_and_cast<vec_s32x16>(__cached_7));
          vec_s8x16 __cached_11;
          __cached_11 = __cached_10;
          vec_s8x16::store(__cached_11, &__outs_0[((((_fuseiter6734 + (fused_0k_o__n_1869 * 64UL)) / 128UL) * 25088UL) + ((_fuseiter6732 * 1792UL) + ((_fuseiter6733 * 128UL) + ((_fuseiter6734 + (fused_0k_o__n_1869 * 64UL)) % 128UL))))]);
        }
      }
    }
    sc_aligned_free(__stream, __origouts_1900_shr);
  }
  sc_aligned_free(__stream, __rescheduled_0);
  return true;
}

static bool res4d_conv_2_cast_mul_add_cast_add_cast__144(uint8_t* __restrict__ __outs_0, int8_t* __restrict__ __ins_0, int8_t* __restrict__ __ins_1, float* __restrict__ __ins_2, float* __restrict__ __ins_3, uint8_t* __restrict__ __ins_4) noexcept{
  void*& __sc_kernel_cache_174 = *(void**)(__module_data + 248);
  alignas(64) int8_t __rescheduled_0[128UL];
  for (uint64_t fused_0k__n_1870 = 0UL; fused_0k__n_1870 < 8UL; fused_0k__n_1870 += 1UL) {
    int32_t* __origouts_1910_shr = (int32_t*)sc_aligned_malloc(__stream, 100352UL);
    void** A_list = (void**)&__rescheduled_0[0UL];
    void** B_list = (void**)&__rescheduled_0[64UL];
    for (uint64_t c = 0UL; c < 2UL; c += 1UL) {
      void* __cached_0;
      __cached_0 = &__ins_0[(c * 25088UL)];
      A_list[c] = __cached_0;
      void* __cached_1;
      __cached_1 = &__ins_1[((fused_0k__n_1870 * 32768UL) + (c * 16384UL))];
      B_list[c] = __cached_1;
    }
    void* _arg_cache_35 = &__origouts_1910_shr[0UL];
    dnnl_brgemm_list_call(__sc_kernel_cache_174, A_list, B_list, &__origouts_1910_shr[0UL], 1, 1, 1, 2, 7, 7, __stream);
    for (uint64_t _fuseiter6767 = 0UL; _fuseiter6767 < 14UL; _fuseiter6767 += 1UL) {
      for (uint64_t _fuseiter6768 = 0UL; _fuseiter6768 < 14UL; _fuseiter6768 += 1UL) {
        for (uint64_t _fuseiter6769 = 0UL; _fuseiter6769 < 128UL; _fuseiter6769 += 16UL) {
          vec_s32x16 __cached_2;
          __cached_2 = vec_s32x16::load(&__origouts_1910_shr[((_fuseiter6767 * 1792UL) + ((_fuseiter6768 * 128UL) + _fuseiter6769))]);
          vec_f32x16 __cached_3;
          __cached_3 = (vec_f32x16)(__cached_2);
          vec_f32x16 __cached_4;
          __cached_4 = vec_f32x16::load(&__ins_2[((fused_0k__n_1870 * 128UL) + _fuseiter6769)]);
          __cached_3 = (__cached_3 * __cached_4);
          vec_f32x16 __cached_5;
          __cached_5 = vec_f32x16::load(&__ins_3[((fused_0k__n_1870 * 128UL) + _fuseiter6769)]);
          __cached_3 = (__cached_3 + __cached_5);
          vec_s8x16 __cached_6;
          __cached_6 = sc_saturated_cast<vec_s8x16>(sc_round_and_cast<vec_s32x16>(__cached_3));
          vec_u8x16 __cached_7;
          __cached_7 = vec_u8x16::load(&__ins_4[((fused_0k__n_1870 * 25088UL) + ((_fuseiter6767 * 1792UL) + ((_fuseiter6768 * 128UL) + _fuseiter6769)))]);
          __cached_6 = (__cached_6 + (vec_s8x16)(__cached_7));
          vec_u8x16 __cached_8;
          __cached_8 = (vec_u8x16)(sc_max(__cached_6, vec_s8x16(0)));
          vec_u8x16::store(__cached_8, &__outs_0[((fused_0k__n_1870 * 25088UL) + ((_fuseiter6767 * 1792UL) + ((_fuseiter6768 * 128UL) + _fuseiter6769)))]);
        }
      }
    }
    sc_aligned_free(__stream, __origouts_1910_shr);
  }
  return true;
}

static bool res4e_conv_0_cast_mul_add_relu_cast_reorder__148(int8_t* __restrict__ __outs_0, uint8_t* __restrict__ __ins_0, int8_t* __restrict__ __ins_1, float* __restrict__ __ins_2, float* __restrict__ __ins_3) noexcept{
  void*& __sc_kernel_cache_176 = *(void**)(__module_data + 256);
  alignas(64) int8_t __rescheduled_0[128UL];
  for (uint64_t fused_0n__k_1871 = 0UL; fused_0n__k_1871 < 2UL; fused_0n__k_1871 += 1UL) {
    memset(&__outs_0[(((fused_0n__k_1871 / 2UL) * 65536UL) + ((fused_0n__k_1871 % 2UL) * 32768UL))], 0, 2048UL);
    for (uint64_t p1 = 0UL; p1 < 14UL; p1 += 1UL) {
      memset(&__outs_0[(((fused_0n__k_1871 / 2UL) * 65536UL) + (((fused_0n__k_1871 % 2UL) * 32768UL) + ((p1 + 1UL) * 2048UL)))], 0, 128UL);
      memset(&__outs_0[((((fused_0n__k_1871 / 2UL) * 65536UL) + (((fused_0n__k_1871 % 2UL) * 32768UL) + ((p1 + 1UL) * 2048UL))) + 1920UL)], 0, 128UL);
    }
    memset(&__outs_0[((((fused_0n__k_1871 / 2UL) * 65536UL) + ((fused_0n__k_1871 % 2UL) * 32768UL)) + 30720UL)], 0, 2048UL);
  }
  for (uint64_t p_o = 0UL; p_o < 2UL; p_o += 1UL) {
    int32_t* __origouts_1920_shr = (int32_t*)sc_aligned_malloc(__stream, 100352UL);
    void** A_list = (void**)&__rescheduled_0[0UL];
    void** B_list = (void**)&__rescheduled_0[64UL];
    for (uint64_t c = 0UL; c < 8UL; c += 1UL) {
      void* __cached_0;
      __cached_0 = &__ins_0[((c * 25088UL) + (p_o * 12544UL))];
      A_list[c] = __cached_0;
      void* __cached_1;
      __cached_1 = &__ins_1[(c * 32768UL)];
      B_list[c] = __cached_1;
    }
    void* _arg_cache_36 = &__origouts_1920_shr[0UL];
    dnnl_brgemm_list_call(__sc_kernel_cache_176, A_list, B_list, &__origouts_1920_shr[0UL], 1, 1, 1, 8, 8, 7, __stream);
    for (uint64_t _fuseiter6803 = 0UL; _fuseiter6803 < 7UL; _fuseiter6803 += 1UL) {
      for (uint64_t _fuseiter6804 = 0UL; _fuseiter6804 < 14UL; _fuseiter6804 += 1UL) {
        for (uint64_t _fuseiter6805 = 0UL; _fuseiter6805 < 256UL; _fuseiter6805 += 16UL) {
          vec_s32x16 __cached_2;
          __cached_2 = vec_s32x16::load(&__origouts_1920_shr[((_fuseiter6803 * 3584UL) + ((_fuseiter6804 * 256UL) + _fuseiter6805))]);
          vec_f32x16 __cached_3;
          __cached_3 = (vec_f32x16)(__cached_2);
          vec_f32x16 __cached_4;
          __cached_4 = vec_f32x16::load(&__ins_2[_fuseiter6805]);
          __cached_3 = (__cached_3 * __cached_4);
          vec_f32x16 __cached_5;
          __cached_5 = vec_f32x16::load(&__ins_3[_fuseiter6805]);
          __cached_3 = (__cached_3 + __cached_5);
          __cached_3 = sc_max(__cached_3, vec_f32x16(0.f));
          vec_s8x16 __cached_6;
          __cached_6 = sc_saturated_cast<vec_s8x16>(sc_round_and_cast<vec_s32x16>(__cached_3));
          vec_s8x16 __cached_7;
          __cached_7 = __cached_6;
          vec_s8x16::store(__cached_7, &__outs_0[(((_fuseiter6805 / 128UL) * 32768UL) + ((((_fuseiter6803 + (p_o * 7UL)) + 1UL) * 2048UL) + (((_fuseiter6804 + 1UL) * 128UL) + (_fuseiter6805 % 128UL))))]);
        }
      }
    }
    sc_aligned_free(__stream, __origouts_1920_shr);
  }
  return true;
}

static bool res4e_conv_1_cast_mul_add_relu_cast_reorder__152(int8_t* __restrict__ __outs_0, int8_t* __restrict__ __ins_0, int8_t* __restrict__ __ins_1, float* __restrict__ __ins_2, float* __restrict__ __ins_3) noexcept{
  void** __sc_kernel_cache_arr_166 = (void**)&__uninitialized_data[23657536UL];
  int8_t* __rescheduled_0 = (int8_t*)sc_aligned_malloc(__stream, 512UL);
  int32_t* conv_os_blk_size = (int32_t*)&__rescheduled_0[0UL];
  int32_t* conv_os_acc_size = (int32_t*)&__rescheduled_0[64UL];
  int32_t __cached_0;
  __cached_0 = 196;
  conv_os_blk_size[0] = __cached_0;
  int32_t __cached_1;
  __cached_1 = 0;
  conv_os_acc_size[0] = __cached_1;
  for (uint64_t fused_0n__k_o_1873 = 0UL; fused_0n__k_o_1873 < 8UL; fused_0n__k_o_1873 += 1UL) {
    int32_t* __origouts_1930_shr = (int32_t*)sc_aligned_malloc(__stream, 25088UL);
    void** A_list = (void**)&__rescheduled_0[128UL];
    void** B_list = (void**)&__rescheduled_0[320UL];
    int32_t __cached_2;
    __cached_2 = conv_os_acc_size[0UL];
    int32_t __cached_3;
    __cached_3 = conv_os_blk_size[0UL];
    memset(&__origouts_1930_shr[(uint64_t)(((__cached_2 / 14) * 448) + ((__cached_2 % 14) * 32))], 0, ((uint64_t)(__cached_3 * 32) * 4UL));
    for (uint64_t c_o = 0UL; c_o < 2UL; c_o += 1UL) {
      for (uint64_t r = 0UL; r < 3UL; r += 1UL) {
        for (uint64_t s = 0UL; s < 3UL; s += 1UL) {
          void* __cached_4;
          __cached_4 = &__ins_0[(((fused_0n__k_o_1873 / 8UL) * 65536UL) + ((c_o * 32768UL) + ((r * 2048UL) + (s * 128UL))))];
          A_list[(((c_o * 9UL) + (r * 3UL)) + s)] = __cached_4;
          void* __cached_5;
          __cached_5 = &__ins_1[(((fused_0n__k_o_1873 % 8UL) * 73728UL) + ((c_o * 36864UL) + ((r * 12288UL) + (s * 4096UL))))];
          B_list[(((c_o * 9UL) + (r * 3UL)) + s)] = __cached_5;
        }
      }
    }
    void* _arg_cache_37 = &__origouts_1930_shr[(uint64_t)(((__cached_2 / 14) * 448) + ((__cached_2 % 14) * 32))];
    dnnl_brgemm_list_call(__sc_kernel_cache_arr_166[0UL], A_list, B_list, &__origouts_1930_shr[(uint64_t)(((__cached_2 / 14) * 448) + ((__cached_2 % 14) * 32))], 1, 128, 4096, 18, 7, 7, __stream);
    for (uint64_t _fuseiter6838 = 0UL; _fuseiter6838 < 14UL; _fuseiter6838 += 1UL) {
      for (uint64_t _fuseiter6839 = 0UL; _fuseiter6839 < 14UL; _fuseiter6839 += 1UL) {
        for (uint64_t _fuseiter6840 = 0UL; _fuseiter6840 < 32UL; _fuseiter6840 += 16UL) {
          vec_s32x16 __cached_6;
          __cached_6 = vec_s32x16::load(&__origouts_1930_shr[((_fuseiter6838 * 448UL) + ((_fuseiter6839 * 32UL) + _fuseiter6840))]);
          vec_f32x16 __cached_7;
          __cached_7 = (vec_f32x16)(__cached_6);
          vec_f32x16 __cached_8;
          __cached_8 = vec_f32x16::load(&__ins_2[((((fused_0n__k_o_1873 / 8UL) * 256UL) + ((fused_0n__k_o_1873 % 8UL) * 32UL)) + _fuseiter6840)]);
          __cached_7 = (__cached_7 * __cached_8);
          vec_f32x16 __cached_9;
          __cached_9 = vec_f32x16::load(&__ins_3[(((fused_0n__k_o_1873 % 8UL) * 32UL) + _fuseiter6840)]);
          __cached_7 = (__cached_7 + __cached_9);
          __cached_7 = sc_max(__cached_7, vec_f32x16(0.f));
          vec_s8x16 __cached_10;
          __cached_10 = sc_saturated_cast<vec_s8x16>(sc_round_and_cast<vec_s32x16>(__cached_7));
          vec_s8x16 __cached_11;
          __cached_11 = __cached_10;
          vec_s8x16::store(__cached_11, &__outs_0[(((fused_0n__k_o_1873 / 8UL) * 50176UL) + ((((_fuseiter6840 + ((fused_0n__k_o_1873 % 8UL) * 32UL)) / 256UL) * 50176UL) + ((_fuseiter6838 * 3584UL) + ((_fuseiter6839 * 256UL) + ((_fuseiter6840 + ((fused_0n__k_o_1873 % 8UL) * 32UL)) % 256UL)))))]);
        }
      }
    }
    sc_aligned_free(__stream, __origouts_1930_shr);
  }
  sc_aligned_free(__stream, __rescheduled_0);
  return true;
}

static bool res4e_conv_2_cast_mul_add_cast_add_cast_reorder__156(uint8_t* __restrict__ __outs_0, uint8_t* __restrict__ __outs_1, int8_t* __restrict__ __ins_0, int8_t* __restrict__ __ins_1, float* __restrict__ __ins_2, float* __restrict__ __ins_3, uint8_t* __restrict__ __ins_4) noexcept{
  void*& __sc_kernel_cache_154 = *(void**)(__module_data + 200);
  alignas(64) int8_t __rescheduled_0[128UL];
  for (uint64_t fused_0n__k_1874 = 0UL; fused_0n__k_1874 < 8UL; fused_0n__k_1874 += 1UL) {
    for (uint64_t p_o = 0UL; p_o < 14UL; p_o += 1UL) {
      int32_t* __origouts_1940_shr = (int32_t*)sc_aligned_malloc(__stream, 7168UL);
      void** A_list = (void**)&__rescheduled_0[0UL];
      void** B_list = (void**)&__rescheduled_0[64UL];
      void* __cached_0;
      __cached_0 = &__ins_0[(((fused_0n__k_1874 / 8UL) * 50176UL) + (p_o * 3584UL))];
      A_list[0UL] = __cached_0;
      void* __cached_1;
      __cached_1 = &__ins_1[((fused_0n__k_1874 % 8UL) * 32768UL)];
      B_list[0UL] = __cached_1;
      void* _arg_cache_38 = &__origouts_1940_shr[0UL];
      dnnl_brgemm_list_call(__sc_kernel_cache_154, A_list, B_list, &__origouts_1940_shr[0UL], 1, 1, 1, 1, 7, 7, __stream);
      for (uint64_t _fuseiter6874 = 0UL; _fuseiter6874 < 14UL; _fuseiter6874 += 1UL) {
        for (uint64_t _fuseiter6875 = 0UL; _fuseiter6875 < 128UL; _fuseiter6875 += 16UL) {
          vec_s32x16 __cached_2;
          __cached_2 = vec_s32x16::load(&__origouts_1940_shr[((_fuseiter6874 * 128UL) + _fuseiter6875)]);
          vec_f32x16 __cached_3;
          __cached_3 = (vec_f32x16)(__cached_2);
          vec_f32x16 __cached_4;
          __cached_4 = vec_f32x16::load(&__ins_2[((((fused_0n__k_1874 / 8UL) * 1024UL) + ((fused_0n__k_1874 % 8UL) * 128UL)) + _fuseiter6875)]);
          __cached_3 = (__cached_3 * __cached_4);
          vec_f32x16 __cached_5;
          __cached_5 = vec_f32x16::load(&__ins_3[(((fused_0n__k_1874 % 8UL) * 128UL) + _fuseiter6875)]);
          __cached_3 = (__cached_3 + __cached_5);
          vec_s8x16 __cached_6;
          __cached_6 = sc_saturated_cast<vec_s8x16>(sc_round_and_cast<vec_s32x16>(__cached_3));
          vec_u8x16 __cached_7;
          __cached_7 = vec_u8x16::load(&__ins_4[((((fused_0n__k_1874 / 8UL) * 200704UL) + (((fused_0n__k_1874 % 8UL) * 25088UL) + (p_o * 1792UL))) + ((_fuseiter6874 * 128UL) + _fuseiter6875))]);
          __cached_6 = (__cached_6 + (vec_s8x16)(__cached_7));
          vec_u8x16 __cached_8;
          __cached_8 = (vec_u8x16)(sc_max(__cached_6, vec_s8x16(0)));
          vec_u8x16::store(__cached_8, &__outs_0[((((fused_0n__k_1874 / 8UL) * 200704UL) + (((fused_0n__k_1874 % 8UL) * 25088UL) + (p_o * 1792UL))) + ((_fuseiter6874 * 128UL) + _fuseiter6875))]);
          vec_u8x16 __cached_9;
          __cached_9 = __cached_8;
          vec_u8x16::store(__cached_9, &__outs_1[(((fused_0n__k_1874 / 8UL) * 200704UL) + ((((_fuseiter6875 + ((fused_0n__k_1874 % 8UL) * 128UL)) / 256UL) * 50176UL) + ((p_o * 3584UL) + ((_fuseiter6874 * 256UL) + ((_fuseiter6875 + ((fused_0n__k_1874 % 8UL) * 128UL)) % 256UL)))))]);
        }
      }
      sc_aligned_free(__stream, __origouts_1940_shr);
    }
  }
  return true;
}

static bool reorder__157(uint8_t* __restrict__ __outs_0, uint8_t* __restrict__ __ins_0) noexcept{
  for (uint64_t fused_0fused_0_fuseiter_6912___fuseiter_6913_1875___fuseiter_6914_1876 = 0UL; fused_0fused_0_fuseiter_6912___fuseiter_6913_1875___fuseiter_6914_1876 < 28UL; fused_0fused_0_fuseiter_6912___fuseiter_6913_1875___fuseiter_6914_1876 += 1UL) {
    for (uint64_t _fuseiter_6915 = 0UL; _fuseiter_6915 < 14UL; _fuseiter_6915 += 1UL) {
      for (uint64_t _fuseiter_6916 = 0UL; _fuseiter_6916 < 512UL; _fuseiter_6916 += 16UL) {
        vec_u8x16 __cached_0;
        __cached_0 = vec_u8x16::load(&__ins_0[(((fused_0fused_0_fuseiter_6912___fuseiter_6913_1875___fuseiter_6914_1876 / 28UL) * 200704UL) + ((((_fuseiter_6916 + (((fused_0fused_0_fuseiter_6912___fuseiter_6913_1875___fuseiter_6914_1876 / 14UL) % 2UL) * 512UL)) / 128UL) * 25088UL) + (((fused_0fused_0_fuseiter_6912___fuseiter_6913_1875___fuseiter_6914_1876 % 14UL) * 1792UL) + ((_fuseiter_6915 * 128UL) + ((_fuseiter_6916 + (((fused_0fused_0_fuseiter_6912___fuseiter_6913_1875___fuseiter_6914_1876 / 14UL) % 2UL) * 512UL)) % 128UL)))))]);
        vec_u8x16 __cached_1;
        __cached_1 = __cached_0;
        vec_u8x16::store(__cached_1, &__outs_0[(((fused_0fused_0_fuseiter_6912___fuseiter_6913_1875___fuseiter_6914_1876 / 28UL) * 200704UL) + ((((fused_0fused_0_fuseiter_6912___fuseiter_6913_1875___fuseiter_6914_1876 / 14UL) % 2UL) * 100352UL) + (((fused_0fused_0_fuseiter_6912___fuseiter_6913_1875___fuseiter_6914_1876 % 14UL) * 7168UL) + ((_fuseiter_6915 * 512UL) + _fuseiter_6916))))]);
      }
    }
  }
  return true;
}

static bool res4f_conv_0_cast_mul_add_relu_cast_reorder__161(int8_t* __restrict__ __outs_0, uint8_t* __restrict__ __ins_0, int8_t* __restrict__ __ins_1, float* __restrict__ __ins_2, float* __restrict__ __ins_3) noexcept{
  void*& __sc_kernel_cache_178 = *(void**)(__module_data + 264);
  alignas(64) int8_t __rescheduled_0[128UL];
  for (uint64_t fused_0n__k_1877 = 0UL; fused_0n__k_1877 < 4UL; fused_0n__k_1877 += 1UL) {
    memset(&__outs_0[(((fused_0n__k_1877 / 4UL) * 65536UL) + ((fused_0n__k_1877 % 4UL) * 16384UL))], 0, 1024UL);
    for (uint64_t p1 = 0UL; p1 < 14UL; p1 += 1UL) {
      memset(&__outs_0[(((fused_0n__k_1877 / 4UL) * 65536UL) + (((fused_0n__k_1877 % 4UL) * 16384UL) + ((p1 + 1UL) * 1024UL)))], 0, 64UL);
      memset(&__outs_0[((((fused_0n__k_1877 / 4UL) * 65536UL) + (((fused_0n__k_1877 % 4UL) * 16384UL) + ((p1 + 1UL) * 1024UL))) + 960UL)], 0, 64UL);
    }
    memset(&__outs_0[((((fused_0n__k_1877 / 4UL) * 65536UL) + ((fused_0n__k_1877 % 4UL) * 16384UL)) + 15360UL)], 0, 1024UL);
  }
  for (uint64_t fused_0k__n_1878 = 0UL; fused_0k__n_1878 < 8UL; fused_0k__n_1878 += 1UL) {
    for (uint64_t p_o = 0UL; p_o < 7UL; p_o += 1UL) {
      int32_t* __origouts_1950_shr = (int32_t*)sc_aligned_malloc(__stream, 3584UL);
      void** A_list = (void**)&__rescheduled_0[0UL];
      void** B_list = (void**)&__rescheduled_0[64UL];
      for (uint64_t c = 0UL; c < 2UL; c += 1UL) {
        void* __cached_0;
        __cached_0 = &__ins_0[((c * 100352UL) + (p_o * 14336UL))];
        A_list[c] = __cached_0;
        void* __cached_1;
        __cached_1 = &__ins_1[((fused_0k__n_1878 * 32768UL) + (c * 16384UL))];
        B_list[c] = __cached_1;
      }
      void* _arg_cache_39 = &__origouts_1950_shr[0UL];
      dnnl_brgemm_list_call(__sc_kernel_cache_178, A_list, B_list, &__origouts_1950_shr[0UL], 1, 1, 1, 2, 8, 7, __stream);
      for (uint64_t _fuseiter6919 = 0UL; _fuseiter6919 < 2UL; _fuseiter6919 += 1UL) {
        for (uint64_t _fuseiter6920 = 0UL; _fuseiter6920 < 14UL; _fuseiter6920 += 1UL) {
          for (uint64_t _fuseiter6921 = 0UL; _fuseiter6921 < 32UL; _fuseiter6921 += 16UL) {
            vec_s32x16 __cached_2;
            __cached_2 = vec_s32x16::load(&__origouts_1950_shr[((_fuseiter6919 * 448UL) + ((_fuseiter6920 * 32UL) + _fuseiter6921))]);
            vec_f32x16 __cached_3;
            __cached_3 = (vec_f32x16)(__cached_2);
            vec_f32x16 __cached_4;
            __cached_4 = vec_f32x16::load(&__ins_2[((fused_0k__n_1878 * 32UL) + _fuseiter6921)]);
            __cached_3 = (__cached_3 * __cached_4);
            vec_f32x16 __cached_5;
            __cached_5 = vec_f32x16::load(&__ins_3[((fused_0k__n_1878 * 32UL) + _fuseiter6921)]);
            __cached_3 = (__cached_3 + __cached_5);
            __cached_3 = sc_max(__cached_3, vec_f32x16(0.f));
            vec_s8x16 __cached_6;
            __cached_6 = sc_saturated_cast<vec_s8x16>(sc_round_and_cast<vec_s32x16>(__cached_3));
            vec_s8x16 __cached_7;
            __cached_7 = __cached_6;
            vec_s8x16::store(__cached_7, &__outs_0[((((_fuseiter6921 + (fused_0k__n_1878 * 32UL)) / 64UL) * 16384UL) + ((((_fuseiter6919 + (p_o * 2UL)) + 1UL) * 1024UL) + (((_fuseiter6920 + 1UL) * 64UL) + ((_fuseiter6921 + (fused_0k__n_1878 * 32UL)) % 64UL))))]);
          }
        }
      }
      sc_aligned_free(__stream, __origouts_1950_shr);
    }
  }
  return true;
}

static bool res4f_conv_1_cast_mul_add_relu_cast_reorder__165(int8_t* __restrict__ __outs_0, int8_t* __restrict__ __ins_0, int8_t* __restrict__ __ins_1, float* __restrict__ __ins_2, float* __restrict__ __ins_3) noexcept{
  void** __sc_kernel_cache_arr_172 = (void**)&__uninitialized_data[23657544UL];
  int8_t* __rescheduled_0 = (int8_t*)sc_aligned_malloc(__stream, 768UL);
  int32_t* conv_os_blk_size = (int32_t*)&__rescheduled_0[0UL];
  int32_t* conv_os_acc_size = (int32_t*)&__rescheduled_0[64UL];
  int32_t __cached_0;
  __cached_0 = 196;
  conv_os_blk_size[0] = __cached_0;
  int32_t __cached_1;
  __cached_1 = 0;
  conv_os_acc_size[0] = __cached_1;
  for (uint64_t fused_0k_o__n_1879 = 0UL; fused_0k_o__n_1879 < 4UL; fused_0k_o__n_1879 += 1UL) {
    int32_t* __origouts_1960_shr = (int32_t*)sc_aligned_malloc(__stream, 50176UL);
    void** A_list = (void**)&__rescheduled_0[128UL];
    void** B_list = (void**)&__rescheduled_0[448UL];
    int32_t __cached_2;
    __cached_2 = conv_os_acc_size[0UL];
    int32_t __cached_3;
    __cached_3 = conv_os_blk_size[0UL];
    memset(&__origouts_1960_shr[(uint64_t)(((__cached_2 / 14) * 896) + ((__cached_2 % 14) * 64))], 0, ((uint64_t)(__cached_3 * 64) * 4UL));
    for (uint64_t c_o = 0UL; c_o < 4UL; c_o += 1UL) {
      for (uint64_t r = 0UL; r < 3UL; r += 1UL) {
        for (uint64_t s = 0UL; s < 3UL; s += 1UL) {
          void* __cached_4;
          __cached_4 = &__ins_0[((c_o * 16384UL) + ((r * 1024UL) + (s * 64UL)))];
          A_list[(((c_o * 9UL) + (r * 3UL)) + s)] = __cached_4;
          void* __cached_5;
          __cached_5 = &__ins_1[((fused_0k_o__n_1879 * 147456UL) + ((c_o * 36864UL) + ((r * 12288UL) + (s * 4096UL))))];
          B_list[(((c_o * 9UL) + (r * 3UL)) + s)] = __cached_5;
        }
      }
    }
    void* _arg_cache_40 = &__origouts_1960_shr[(uint64_t)(((__cached_2 / 14) * 896) + ((__cached_2 % 14) * 64))];
    dnnl_brgemm_list_call(__sc_kernel_cache_arr_172[0UL], A_list, B_list, &__origouts_1960_shr[(uint64_t)(((__cached_2 / 14) * 896) + ((__cached_2 % 14) * 64))], 1, 64, 4096, 36, 7, 7, __stream);
    for (uint64_t _fuseiter6954 = 0UL; _fuseiter6954 < 14UL; _fuseiter6954 += 1UL) {
      for (uint64_t _fuseiter6955 = 0UL; _fuseiter6955 < 14UL; _fuseiter6955 += 1UL) {
        for (uint64_t _fuseiter6956 = 0UL; _fuseiter6956 < 64UL; _fuseiter6956 += 16UL) {
          vec_s32x16 __cached_6;
          __cached_6 = vec_s32x16::load(&__origouts_1960_shr[((_fuseiter6954 * 896UL) + ((_fuseiter6955 * 64UL) + _fuseiter6956))]);
          vec_f32x16 __cached_7;
          __cached_7 = (vec_f32x16)(__cached_6);
          vec_f32x16 __cached_8;
          __cached_8 = vec_f32x16::load(&__ins_2[((fused_0k_o__n_1879 * 64UL) + _fuseiter6956)]);
          __cached_7 = (__cached_7 * __cached_8);
          vec_f32x16 __cached_9;
          __cached_9 = vec_f32x16::load(&__ins_3[((fused_0k_o__n_1879 * 64UL) + _fuseiter6956)]);
          __cached_7 = (__cached_7 + __cached_9);
          __cached_7 = sc_max(__cached_7, vec_f32x16(0.f));
          vec_s8x16 __cached_10;
          __cached_10 = sc_saturated_cast<vec_s8x16>(sc_round_and_cast<vec_s32x16>(__cached_7));
          vec_s8x16 __cached_11;
          __cached_11 = __cached_10;
          vec_s8x16::store(__cached_11, &__outs_0[((((_fuseiter6956 + (fused_0k_o__n_1879 * 64UL)) / 128UL) * 25088UL) + ((_fuseiter6954 * 1792UL) + ((_fuseiter6955 * 128UL) + ((_fuseiter6956 + (fused_0k_o__n_1879 * 64UL)) % 128UL))))]);
        }
      }
    }
    sc_aligned_free(__stream, __origouts_1960_shr);
  }
  sc_aligned_free(__stream, __rescheduled_0);
  return true;
}

static bool res4f_conv_2_cast_mul_add_cast_add_cast__170(uint8_t* __restrict__ __outs_0, int8_t* __restrict__ __ins_0, int8_t* __restrict__ __ins_1, float* __restrict__ __ins_2, float* __restrict__ __ins_3, uint8_t* __restrict__ __ins_4) noexcept{
  void*& __sc_kernel_cache_180 = *(void**)(__module_data + 272);
  alignas(64) int8_t __rescheduled_0[128UL];
  for (uint64_t fused_0k__n_1880 = 0UL; fused_0k__n_1880 < 4UL; fused_0k__n_1880 += 1UL) {
    int32_t* __origouts_1970_shr = (int32_t*)sc_aligned_malloc(__stream, 200704UL);
    void** A_list = (void**)&__rescheduled_0[0UL];
    void** B_list = (void**)&__rescheduled_0[64UL];
    for (uint64_t c = 0UL; c < 2UL; c += 1UL) {
      void* __cached_0;
      __cached_0 = &__ins_0[(c * 25088UL)];
      A_list[c] = __cached_0;
      void* __cached_1;
      __cached_1 = &__ins_1[((fused_0k__n_1880 * 65536UL) + (c * 32768UL))];
      B_list[c] = __cached_1;
    }
    void* _arg_cache_41 = &__origouts_1970_shr[0UL];
    dnnl_brgemm_list_call(__sc_kernel_cache_180, A_list, B_list, &__origouts_1970_shr[0UL], 1, 1, 1, 2, 7, 7, __stream);
    for (uint64_t _fuseiter6989 = 0UL; _fuseiter6989 < 14UL; _fuseiter6989 += 1UL) {
      for (uint64_t _fuseiter6990 = 0UL; _fuseiter6990 < 14UL; _fuseiter6990 += 1UL) {
        for (uint64_t _fuseiter6991 = 0UL; _fuseiter6991 < 256UL; _fuseiter6991 += 16UL) {
          vec_s32x16 __cached_2;
          __cached_2 = vec_s32x16::load(&__origouts_1970_shr[((_fuseiter6989 * 3584UL) + ((_fuseiter6990 * 256UL) + _fuseiter6991))]);
          vec_f32x16 __cached_3;
          __cached_3 = (vec_f32x16)(__cached_2);
          vec_f32x16 __cached_4;
          __cached_4 = vec_f32x16::load(&__ins_2[((fused_0k__n_1880 * 256UL) + _fuseiter6991)]);
          __cached_3 = (__cached_3 * __cached_4);
          vec_f32x16 __cached_5;
          __cached_5 = vec_f32x16::load(&__ins_3[((fused_0k__n_1880 * 256UL) + _fuseiter6991)]);
          __cached_3 = (__cached_3 + __cached_5);
          vec_s8x16 __cached_6;
          __cached_6 = sc_saturated_cast<vec_s8x16>(sc_round_and_cast<vec_s32x16>(__cached_3));
          vec_u8x16 __cached_7;
          __cached_7 = vec_u8x16::load(&__ins_4[((fused_0k__n_1880 * 50176UL) + ((_fuseiter6989 * 3584UL) + ((_fuseiter6990 * 256UL) + _fuseiter6991)))]);
          __cached_6 = (__cached_6 + (vec_s8x16)(__cached_7));
          vec_u8x16 __cached_8;
          __cached_8 = (vec_u8x16)(sc_max(__cached_6, vec_s8x16(0)));
          vec_u8x16::store(__cached_8, &__outs_0[((fused_0k__n_1880 * 50176UL) + ((_fuseiter6989 * 3584UL) + ((_fuseiter6990 * 256UL) + _fuseiter6991)))]);
        }
      }
    }
    sc_aligned_free(__stream, __origouts_1970_shr);
  }
  return true;
}

static bool reorder__531(int8_t* __restrict__ __outs_0, int8_t* __restrict__ __ins_0) noexcept{
  generic_val __tempargs304[2UL];
  __tempargs304[0UL] = __ins_0;
  __tempargs304[1UL] = __outs_0;
  sc_parallel_call_cpu_with_env((void*)&reorder__5310_closure_304_0wrapper, __stream, __module_data, 0UL, 128UL, 1UL, __tempargs304);
  return true;
}

static bool mul__654(float* __restrict__ __outs_0, float* __restrict__ __ins_0, float* __restrict__ __ins_1) noexcept{
  generic_val __tempargs305[3UL];
  __tempargs305[0UL] = __ins_0;
  __tempargs305[1UL] = __ins_1;
  __tempargs305[2UL] = __outs_0;
  sc_parallel_call_cpu_with_env((void*)&mul__6540_closure_305_0wrapper, __stream, __module_data, 0UL, 128UL, 1UL, __tempargs305);
  return true;
}

static bool mul__653(float* __restrict__ __outs_0, float* __restrict__ __ins_0, float* __restrict__ __ins_1) noexcept{
  generic_val __tempargs306[3UL];
  __tempargs306[0UL] = __ins_0;
  __tempargs306[1UL] = __ins_1;
  __tempargs306[2UL] = __outs_0;
  sc_parallel_call_cpu_with_env((void*)&mul__6530_closure_306_0wrapper, __stream, __module_data, 0UL, 128UL, 1UL, __tempargs306);
  return true;
}

static bool mul__240(float* __restrict__ __outs_0, float* __restrict__ __ins_0, float* __restrict__ __ins_1) noexcept{
  generic_val __tempargs307[3UL];
  __tempargs307[0UL] = __ins_0;
  __tempargs307[1UL] = __ins_1;
  __tempargs307[2UL] = __outs_0;
  sc_parallel_call_cpu_with_env((void*)&mul__2400_closure_307_0wrapper, __stream, __module_data, 0UL, 786432UL, 1UL, __tempargs307);
  return true;
}

static bool cast__241(int8_t* __restrict__ __outs_0, float* __restrict__ __ins_0) noexcept{
  generic_val __tempargs308[2UL];
  __tempargs308[0UL] = __ins_0;
  __tempargs308[1UL] = __outs_0;
  sc_parallel_call_cpu_with_env((void*)&cast__2410_closure_308_0wrapper, __stream, __module_data, 0UL, 786432UL, 1UL, __tempargs308);
  return true;
}

static bool reorder__537(int8_t* __restrict__ __outs_0, int8_t* __restrict__ __ins_0) noexcept{
  generic_val __tempargs309[2UL];
  __tempargs309[0UL] = __ins_0;
  __tempargs309[1UL] = __outs_0;
  sc_parallel_call_cpu_with_env((void*)&reorder__5370_closure_309_0wrapper, __stream, __module_data, 0UL, 24UL, 1UL, __tempargs309);
  return true;
}

static bool res5a_conv_0_cast_mul_add_relu_cast_reorder__681(int8_t* __restrict__ __outs_0, uint8_t* __restrict__ __ins_0, int8_t* __restrict__ __ins_1, float* __restrict__ __ins_2, float* __restrict__ __ins_3) noexcept{
  generic_val __tempargs310[1UL];
  __tempargs310[0UL] = __outs_0;
  sc_parallel_call_cpu_with_env((void*)&res5a_conv_0_cast_mul_add_relu_cast_reorder__6810_closure_310_0wrapper, __stream, __module_data, 0UL, 8UL, 1UL, __tempargs310);
  generic_val __tempargs311[5UL];
  __tempargs311[0UL] = __ins_0;
  __tempargs311[1UL] = __ins_1;
  __tempargs311[2UL] = __ins_2;
  __tempargs311[3UL] = __ins_3;
  __tempargs311[4UL] = __outs_0;
  sc_parallel_call_cpu_with_env((void*)&res5a_conv_0_cast_mul_add_relu_cast_reorder__6810_closure_311_0wrapper, __stream, __module_data, 0UL, 32UL, 1UL, __tempargs311);
  return true;
}

static bool mul__658(float* __restrict__ __outs_0, float* __restrict__ __ins_0, float* __restrict__ __ins_1) noexcept{
  generic_val __tempargs312[3UL];
  __tempargs312[0UL] = __ins_0;
  __tempargs312[1UL] = __ins_1;
  __tempargs312[2UL] = __outs_0;
  sc_parallel_call_cpu_with_env((void*)&mul__6580_closure_312_0wrapper, __stream, __module_data, 0UL, 4UL, 1UL, __tempargs312);
  return true;
}

static bool mul__657(float* __restrict__ __outs_0, float* __restrict__ __ins_0, float* __restrict__ __ins_1) noexcept{
  generic_val __tempargs313[3UL];
  __tempargs313[0UL] = __ins_0;
  __tempargs313[1UL] = __ins_1;
  __tempargs313[2UL] = __outs_0;
  sc_parallel_call_cpu_with_env((void*)&mul__6570_closure_313_0wrapper, __stream, __module_data, 0UL, 4UL, 1UL, __tempargs313);
  return true;
}

static bool res5a_conv_1_cast_mul_add_relu_cast_reorder__680(int8_t* __restrict__ __outs_0, int8_t* __restrict__ __ins_0, int8_t* __restrict__ __ins_1, float* __restrict__ __ins_2, float* __restrict__ __ins_3) noexcept{
  alignas(64) int8_t __rescheduled_0[128UL];
  int32_t* conv_os_blk_size = (int32_t*)&__rescheduled_0[0UL];
  int32_t* conv_os_acc_size = (int32_t*)&__rescheduled_0[64UL];
  int32_t __cached_0;
  __cached_0 = 49;
  conv_os_blk_size[0] = __cached_0;
  int32_t __cached_1;
  __cached_1 = 0;
  conv_os_acc_size[0] = __cached_1;
  generic_val __tempargs314[7UL];
  __tempargs314[0UL] = conv_os_acc_size;
  __tempargs314[1UL] = conv_os_blk_size;
  __tempargs314[2UL] = __ins_0;
  __tempargs314[3UL] = __ins_1;
  __tempargs314[4UL] = __ins_2;
  __tempargs314[5UL] = __ins_3;
  __tempargs314[6UL] = __outs_0;
  sc_parallel_call_cpu_with_env((void*)&res5a_conv_1_cast_mul_add_relu_cast_reorder__6800_closure_314_0wrapper, __stream, __module_data, 0UL, 16UL, 1UL, __tempargs314);
  return true;
}

static bool res5a_conv_b_cast_mul_add_cast_reorder__682(int8_t* __restrict__ __outs_0, uint8_t* __restrict__ __ins_0, int8_t* __restrict__ __ins_1, float* __restrict__ __ins_2, float* __restrict__ __ins_3) noexcept{
  uint8_t* input_tmp = (uint8_t*)sc_aligned_malloc(__stream, 200704UL);
  generic_val __tempargs315[2UL];
  __tempargs315[0UL] = __ins_0;
  __tempargs315[1UL] = input_tmp;
  sc_parallel_call_cpu_with_env((void*)&res5a_conv_b_cast_mul_add_cast_reorder__6820_closure_315_0wrapper, __stream, __module_data, 0UL, 16UL, 1UL, __tempargs315);
  generic_val __tempargs316[5UL];
  __tempargs316[0UL] = input_tmp;
  __tempargs316[1UL] = __ins_1;
  __tempargs316[2UL] = __ins_2;
  __tempargs316[3UL] = __ins_3;
  __tempargs316[4UL] = __outs_0;
  sc_parallel_call_cpu_with_env((void*)&res5a_conv_b_cast_mul_add_cast_reorder__6820_closure_316_0wrapper, __stream, __module_data, 0UL, 512UL, 1UL, __tempargs316);
  sc_aligned_free(__stream, input_tmp);
  return true;
}

static bool mul__660(float* __restrict__ __outs_0, float* __restrict__ __ins_0, float* __restrict__ __ins_1) noexcept{
  generic_val __tempargs317[3UL];
  __tempargs317[0UL] = __ins_0;
  __tempargs317[1UL] = __ins_1;
  __tempargs317[2UL] = __outs_0;
  sc_parallel_call_cpu_with_env((void*)&mul__6600_closure_317_0wrapper, __stream, __module_data, 0UL, 32UL, 1UL, __tempargs317);
  return true;
}

static bool mul__659(float* __restrict__ __outs_0, float* __restrict__ __ins_0, float* __restrict__ __ins_1) noexcept{
  generic_val __tempargs318[3UL];
  __tempargs318[0UL] = __ins_0;
  __tempargs318[1UL] = __ins_1;
  __tempargs318[2UL] = __outs_0;
  sc_parallel_call_cpu_with_env((void*)&mul__6590_closure_318_0wrapper, __stream, __module_data, 0UL, 32UL, 1UL, __tempargs318);
  return true;
}

static bool reorder__540(int8_t* __restrict__ __outs_0, int8_t* __restrict__ __ins_0) noexcept{
  generic_val __tempargs319[2UL];
  __tempargs319[0UL] = __ins_0;
  __tempargs319[1UL] = __outs_0;
  sc_parallel_call_cpu_with_env((void*)&reorder__5400_closure_319_0wrapper, __stream, __module_data, 0UL, 32UL, 1UL, __tempargs319);
  return true;
}

static bool mul__662(float* __restrict__ __outs_0, float* __restrict__ __ins_0, float* __restrict__ __ins_1) noexcept{
  generic_val __tempargs320[3UL];
  __tempargs320[0UL] = __ins_0;
  __tempargs320[1UL] = __ins_1;
  __tempargs320[2UL] = __outs_0;
  sc_parallel_call_cpu_with_env((void*)&mul__6620_closure_320_0wrapper, __stream, __module_data, 0UL, 4UL, 1UL, __tempargs320);
  return true;
}

static bool mul__661(float* __restrict__ __outs_0, float* __restrict__ __ins_0, float* __restrict__ __ins_1) noexcept{
  generic_val __tempargs321[3UL];
  __tempargs321[0UL] = __ins_0;
  __tempargs321[1UL] = __ins_1;
  __tempargs321[2UL] = __outs_0;
  sc_parallel_call_cpu_with_env((void*)&mul__6610_closure_321_0wrapper, __stream, __module_data, 0UL, 4UL, 1UL, __tempargs321);
  return true;
}

static bool reorder__543(int8_t* __restrict__ __outs_0, int8_t* __restrict__ __ins_0) noexcept{
  generic_val __tempargs322[2UL];
  __tempargs322[0UL] = __ins_0;
  __tempargs322[1UL] = __outs_0;
  sc_parallel_call_cpu_with_env((void*)&reorder__5430_closure_322_0wrapper, __stream, __module_data, 0UL, 128UL, 1UL, __tempargs322);
  return true;
}

static bool mul__249(float* __restrict__ __outs_0, float* __restrict__ __ins_0, float* __restrict__ __ins_1) noexcept{
  generic_val __tempargs323[3UL];
  __tempargs323[0UL] = __ins_0;
  __tempargs323[1UL] = __ins_1;
  __tempargs323[2UL] = __outs_0;
  sc_parallel_call_cpu_with_env((void*)&mul__2490_closure_323_0wrapper, __stream, __module_data, 0UL, 786432UL, 1UL, __tempargs323);
  return true;
}

static bool cast__250(int8_t* __restrict__ __outs_0, float* __restrict__ __ins_0) noexcept{
  generic_val __tempargs324[2UL];
  __tempargs324[0UL] = __ins_0;
  __tempargs324[1UL] = __outs_0;
  sc_parallel_call_cpu_with_env((void*)&cast__2500_closure_324_0wrapper, __stream, __module_data, 0UL, 786432UL, 1UL, __tempargs324);
  return true;
}

static bool mul__258(float* __restrict__ __outs_0, float* __restrict__ __ins_0, float* __restrict__ __ins_1) noexcept{
  generic_val __tempargs325[3UL];
  __tempargs325[0UL] = __ins_0;
  __tempargs325[1UL] = __ins_1;
  __tempargs325[2UL] = __outs_0;
  sc_parallel_call_cpu_with_env((void*)&mul__2580_closure_325_0wrapper, __stream, __module_data, 0UL, 786432UL, 1UL, __tempargs325);
  return true;
}

static bool cast__259(int8_t* __restrict__ __outs_0, float* __restrict__ __ins_0) noexcept{
  generic_val __tempargs326[2UL];
  __tempargs326[0UL] = __ins_0;
  __tempargs326[1UL] = __outs_0;
  sc_parallel_call_cpu_with_env((void*)&cast__2590_closure_326_0wrapper, __stream, __module_data, 0UL, 786432UL, 1UL, __tempargs326);
  return true;
}

static bool mul__664(float* __restrict__ __outs_0, float* __restrict__ __ins_0, float* __restrict__ __ins_1) noexcept{
  generic_val __tempargs327[3UL];
  __tempargs327[0UL] = __ins_0;
  __tempargs327[1UL] = __ins_1;
  __tempargs327[2UL] = __outs_0;
  sc_parallel_call_cpu_with_env((void*)&mul__6640_closure_327_0wrapper, __stream, __module_data, 0UL, 32UL, 1UL, __tempargs327);
  return true;
}

static bool mul__663(float* __restrict__ __outs_0, float* __restrict__ __ins_0, float* __restrict__ __ins_1) noexcept{
  generic_val __tempargs328[3UL];
  __tempargs328[0UL] = __ins_0;
  __tempargs328[1UL] = __ins_1;
  __tempargs328[2UL] = __outs_0;
  sc_parallel_call_cpu_with_env((void*)&mul__6630_closure_328_0wrapper, __stream, __module_data, 0UL, 32UL, 1UL, __tempargs328);
  return true;
}

static bool reorder__546(int8_t* __restrict__ __outs_0, int8_t* __restrict__ __ins_0) noexcept{
  generic_val __tempargs329[2UL];
  __tempargs329[0UL] = __ins_0;
  __tempargs329[1UL] = __outs_0;
  sc_parallel_call_cpu_with_env((void*)&reorder__5460_closure_329_0wrapper, __stream, __module_data, 0UL, 32UL, 1UL, __tempargs329);
  return true;
}

static bool res5a_conv_2_cast_mul_add_cast_add_cast__679(uint8_t* __restrict__ __outs_0, int8_t* __restrict__ __ins_0, int8_t* __restrict__ __ins_1, float* __restrict__ __ins_2, float* __restrict__ __ins_3, int8_t* __restrict__ __ins_4) noexcept{
  generic_val __tempargs330[6UL];
  __tempargs330[0UL] = __ins_0;
  __tempargs330[1UL] = __ins_1;
  __tempargs330[2UL] = __ins_2;
  __tempargs330[3UL] = __ins_3;
  __tempargs330[4UL] = __ins_4;
  __tempargs330[5UL] = __outs_0;
  sc_parallel_call_cpu_with_env((void*)&res5a_conv_2_cast_mul_add_cast_add_cast__6790_closure_330_0wrapper, __stream, __module_data, 0UL, 128UL, 1UL, __tempargs330);
  return true;
}

static bool res5b_conv_0_cast_mul_add_relu_cast_reorder__678(int8_t* __restrict__ __outs_0, uint8_t* __restrict__ __ins_0, int8_t* __restrict__ __ins_1, float* __restrict__ __ins_2, float* __restrict__ __ins_3) noexcept{
  generic_val __tempargs331[1UL];
  __tempargs331[0UL] = __outs_0;
  sc_parallel_call_cpu_with_env((void*)&res5b_conv_0_cast_mul_add_relu_cast_reorder__6780_closure_331_0wrapper, __stream, __module_data, 0UL, 4UL, 1UL, __tempargs331);
  generic_val __tempargs332[5UL];
  __tempargs332[0UL] = __ins_0;
  __tempargs332[1UL] = __ins_1;
  __tempargs332[2UL] = __ins_2;
  __tempargs332[3UL] = __ins_3;
  __tempargs332[4UL] = __outs_0;
  sc_parallel_call_cpu_with_env((void*)&res5b_conv_0_cast_mul_add_relu_cast_reorder__6780_closure_332_0wrapper, __stream, __module_data, 0UL, 16UL, 1UL, __tempargs332);
  return true;
}

static bool res5b_conv_1_cast_mul_add_relu_cast_reorder__677(int8_t* __restrict__ __outs_0, int8_t* __restrict__ __ins_0, int8_t* __restrict__ __ins_1, float* __restrict__ __ins_2, float* __restrict__ __ins_3) noexcept{
  alignas(64) int8_t __rescheduled_0[128UL];
  int32_t* conv_os_blk_size = (int32_t*)&__rescheduled_0[0UL];
  int32_t* conv_os_acc_size = (int32_t*)&__rescheduled_0[64UL];
  int32_t __cached_0;
  __cached_0 = 49;
  conv_os_blk_size[0] = __cached_0;
  int32_t __cached_1;
  __cached_1 = 0;
  conv_os_acc_size[0] = __cached_1;
  generic_val __tempargs333[7UL];
  __tempargs333[0UL] = conv_os_acc_size;
  __tempargs333[1UL] = conv_os_blk_size;
  __tempargs333[2UL] = __ins_0;
  __tempargs333[3UL] = __ins_1;
  __tempargs333[4UL] = __ins_2;
  __tempargs333[5UL] = __ins_3;
  __tempargs333[6UL] = __outs_0;
  sc_parallel_call_cpu_with_env((void*)&res5b_conv_1_cast_mul_add_relu_cast_reorder__6770_closure_333_0wrapper, __stream, __module_data, 0UL, 128UL, 1UL, __tempargs333);
  return true;
}

static bool mul__666(float* __restrict__ __outs_0, float* __restrict__ __ins_0, float* __restrict__ __ins_1) noexcept{
  generic_val __tempargs334[3UL];
  __tempargs334[0UL] = __ins_0;
  __tempargs334[1UL] = __ins_1;
  __tempargs334[2UL] = __outs_0;
  sc_parallel_call_cpu_with_env((void*)&mul__6660_closure_334_0wrapper, __stream, __module_data, 0UL, 32UL, 1UL, __tempargs334);
  return true;
}

static bool mul__665(float* __restrict__ __outs_0, float* __restrict__ __ins_0, float* __restrict__ __ins_1) noexcept{
  generic_val __tempargs335[3UL];
  __tempargs335[0UL] = __ins_0;
  __tempargs335[1UL] = __ins_1;
  __tempargs335[2UL] = __outs_0;
  sc_parallel_call_cpu_with_env((void*)&mul__6650_closure_335_0wrapper, __stream, __module_data, 0UL, 32UL, 1UL, __tempargs335);
  return true;
}

static bool reorder__549(int8_t* __restrict__ __outs_0, int8_t* __restrict__ __ins_0) noexcept{
  generic_val __tempargs336[2UL];
  __tempargs336[0UL] = __ins_0;
  __tempargs336[1UL] = __outs_0;
  sc_parallel_call_cpu_with_env((void*)&reorder__5490_closure_336_0wrapper, __stream, __module_data, 0UL, 32UL, 1UL, __tempargs336);
  return true;
}

static bool mul__668(float* __restrict__ __outs_0, float* __restrict__ __ins_0, float* __restrict__ __ins_1) noexcept{
  generic_val __tempargs337[3UL];
  __tempargs337[0UL] = __ins_0;
  __tempargs337[1UL] = __ins_1;
  __tempargs337[2UL] = __outs_0;
  sc_parallel_call_cpu_with_env((void*)&mul__6680_closure_337_0wrapper, __stream, __module_data, 0UL, 16UL, 1UL, __tempargs337);
  return true;
}

static bool mul__667(float* __restrict__ __outs_0, float* __restrict__ __ins_0, float* __restrict__ __ins_1) noexcept{
  generic_val __tempargs338[3UL];
  __tempargs338[0UL] = __ins_0;
  __tempargs338[1UL] = __ins_1;
  __tempargs338[2UL] = __outs_0;
  sc_parallel_call_cpu_with_env((void*)&mul__6670_closure_338_0wrapper, __stream, __module_data, 0UL, 16UL, 1UL, __tempargs338);
  return true;
}

static bool reorder__552(int8_t* __restrict__ __outs_0, int8_t* __restrict__ __ins_0) noexcept{
  generic_val __tempargs339[2UL];
  __tempargs339[0UL] = __ins_0;
  __tempargs339[1UL] = __outs_0;
  sc_parallel_call_cpu_with_env((void*)&reorder__5520_closure_339_0wrapper, __stream, __module_data, 0UL, 64UL, 1UL, __tempargs339);
  return true;
}

static bool mul__670(float* __restrict__ __outs_0, float* __restrict__ __ins_0, float* __restrict__ __ins_1) noexcept{
  generic_val __tempargs340[3UL];
  __tempargs340[0UL] = __ins_0;
  __tempargs340[1UL] = __ins_1;
  __tempargs340[2UL] = __outs_0;
  sc_parallel_call_cpu_with_env((void*)&mul__6700_closure_340_0wrapper, __stream, __module_data, 0UL, 4UL, 1UL, __tempargs340);
  return true;
}

static bool mul__669(float* __restrict__ __outs_0, float* __restrict__ __ins_0, float* __restrict__ __ins_1) noexcept{
  generic_val __tempargs341[3UL];
  __tempargs341[0UL] = __ins_0;
  __tempargs341[1UL] = __ins_1;
  __tempargs341[2UL] = __outs_0;
  sc_parallel_call_cpu_with_env((void*)&mul__6690_closure_341_0wrapper, __stream, __module_data, 0UL, 4UL, 1UL, __tempargs341);
  return true;
}

static bool reorder__555(int8_t* __restrict__ __outs_0, int8_t* __restrict__ __ins_0) noexcept{
  generic_val __tempargs342[2UL];
  __tempargs342[0UL] = __ins_0;
  __tempargs342[1UL] = __outs_0;
  sc_parallel_call_cpu_with_env((void*)&reorder__5550_closure_342_0wrapper, __stream, __module_data, 0UL, 24UL, 1UL, __tempargs342);
  return true;
}

static bool res5b_conv_2_cast_mul_add_cast_add_cast_reorder__676(uint8_t* __restrict__ __outs_0, int8_t* __restrict__ __ins_0, int8_t* __restrict__ __ins_1, float* __restrict__ __ins_2, float* __restrict__ __ins_3, uint8_t* __restrict__ __ins_4) noexcept{
  generic_val __tempargs343[6UL];
  __tempargs343[0UL] = __ins_0;
  __tempargs343[1UL] = __ins_1;
  __tempargs343[2UL] = __ins_2;
  __tempargs343[3UL] = __ins_3;
  __tempargs343[4UL] = __ins_4;
  __tempargs343[5UL] = __outs_0;
  sc_parallel_call_cpu_with_env((void*)&res5b_conv_2_cast_mul_add_cast_add_cast_reorder__6760_closure_343_0wrapper, __stream, __module_data, 0UL, 128UL, 1UL, __tempargs343);
  return true;
}

static bool res5c_conv_0_cast_mul_add_relu_cast_reorder__675(int8_t* __restrict__ __outs_0, uint8_t* __restrict__ __ins_0, int8_t* __restrict__ __ins_1, float* __restrict__ __ins_2, float* __restrict__ __ins_3) noexcept{
  generic_val __tempargs344[1UL];
  __tempargs344[0UL] = __outs_0;
  sc_parallel_call_cpu_with_env((void*)&res5c_conv_0_cast_mul_add_relu_cast_reorder__6750_closure_344_0wrapper, __stream, __module_data, 0UL, 8UL, 1UL, __tempargs344);
  generic_val __tempargs345[5UL];
  __tempargs345[0UL] = __ins_0;
  __tempargs345[1UL] = __ins_1;
  __tempargs345[2UL] = __ins_2;
  __tempargs345[3UL] = __ins_3;
  __tempargs345[4UL] = __outs_0;
  sc_parallel_call_cpu_with_env((void*)&res5c_conv_0_cast_mul_add_relu_cast_reorder__6750_closure_345_0wrapper, __stream, __module_data, 0UL, 64UL, 1UL, __tempargs345);
  return true;
}

static bool res5c_conv_1_cast_mul_add_relu_cast_reorder__674(int8_t* __restrict__ __outs_0, int8_t* __restrict__ __ins_0, int8_t* __restrict__ __ins_1, float* __restrict__ __ins_2, float* __restrict__ __ins_3) noexcept{
  alignas(64) int8_t __rescheduled_0[128UL];
  int32_t* conv_os_blk_size = (int32_t*)&__rescheduled_0[0UL];
  int32_t* conv_os_acc_size = (int32_t*)&__rescheduled_0[64UL];
  int32_t __cached_0;
  __cached_0 = 49;
  conv_os_blk_size[0] = __cached_0;
  int32_t __cached_1;
  __cached_1 = 0;
  conv_os_acc_size[0] = __cached_1;
  generic_val __tempargs346[7UL];
  __tempargs346[0UL] = conv_os_acc_size;
  __tempargs346[1UL] = conv_os_blk_size;
  __tempargs346[2UL] = __ins_0;
  __tempargs346[3UL] = __ins_1;
  __tempargs346[4UL] = __ins_2;
  __tempargs346[5UL] = __ins_3;
  __tempargs346[6UL] = __outs_0;
  sc_parallel_call_cpu_with_env((void*)&res5c_conv_1_cast_mul_add_relu_cast_reorder__6740_closure_346_0wrapper, __stream, __module_data, 0UL, 16UL, 1UL, __tempargs346);
  return true;
}

static bool mul__672(float* __restrict__ __outs_0, float* __restrict__ __ins_0, float* __restrict__ __ins_1) noexcept{
  generic_val __tempargs347[3UL];
  __tempargs347[0UL] = __ins_0;
  __tempargs347[1UL] = __ins_1;
  __tempargs347[2UL] = __outs_0;
  sc_parallel_call_cpu_with_env((void*)&mul__6720_closure_347_0wrapper, __stream, __module_data, 0UL, 4UL, 1UL, __tempargs347);
  return true;
}

static bool mul__671(float* __restrict__ __outs_0, float* __restrict__ __ins_0, float* __restrict__ __ins_1) noexcept{
  generic_val __tempargs348[3UL];
  __tempargs348[0UL] = __ins_0;
  __tempargs348[1UL] = __ins_1;
  __tempargs348[2UL] = __outs_0;
  sc_parallel_call_cpu_with_env((void*)&mul__6710_closure_348_0wrapper, __stream, __module_data, 0UL, 4UL, 1UL, __tempargs348);
  return true;
}

static bool reorder__558(int8_t* __restrict__ __outs_0, int8_t* __restrict__ __ins_0) noexcept{
  generic_val __tempargs349[2UL];
  __tempargs349[0UL] = __ins_0;
  __tempargs349[1UL] = __outs_0;
  sc_parallel_call_cpu_with_env((void*)&reorder__5580_closure_349_0wrapper, __stream, __module_data, 0UL, 512UL, 1UL, __tempargs349);
  return true;
}

static bool res5c_conv_2_cast_mul_add_cast_add_cast_cast__673(int8_t* __restrict__ __outs_0, int8_t* __restrict__ __ins_0, int8_t* __restrict__ __ins_1, float* __restrict__ __ins_2, float* __restrict__ __ins_3, uint8_t* __restrict__ __ins_4) noexcept{
  generic_val __tempargs350[6UL];
  __tempargs350[0UL] = __ins_0;
  __tempargs350[1UL] = __ins_1;
  __tempargs350[2UL] = __ins_2;
  __tempargs350[3UL] = __ins_3;
  __tempargs350[4UL] = __ins_4;
  __tempargs350[5UL] = __outs_0;
  sc_parallel_call_cpu_with_env((void*)&res5c_conv_2_cast_mul_add_cast_add_cast_cast__6730_closure_350_0wrapper, __stream, __module_data, 0UL, 16UL, 1UL, __tempargs350);
  return true;
}

static bool reorder__105(int8_t* __restrict__ __outs_0, int8_t* __restrict__ __ins_0) noexcept{
  generic_val __tempargs351[2UL];
  __tempargs351[0UL] = __ins_0;
  __tempargs351[1UL] = __outs_0;
  sc_parallel_call_cpu_with_env((void*)&reorder__1050_closure_351_0wrapper, __stream, __module_data, 0UL, 28UL, 1UL, __tempargs351);
  return true;
}

static void __init_const_globals(int8_t* __restrict__ backbone_output, int8_t* __restrict__ backbone_input, float* __restrict__ res2a_weight_b, float* __restrict__ res2a_bias_b, float* __restrict__ res2a_weight_0, float* __restrict__ res2a_bias_0, float* __restrict__ res2a_weight_1, float* __restrict__ res2a_bias_1, float* __restrict__ res2a_weight_2, float* __restrict__ res2a_bias_2, float* __restrict__ res2b_weight_0, float* __restrict__ res2b_bias_0, float* __restrict__ res2b_weight_1, float* __restrict__ res2b_bias_1, float* __restrict__ res2b_weight_2, float* __restrict__ res2b_bias_2, float* __restrict__ res2c_weight_0, float* __restrict__ res2c_bias_0, float* __restrict__ res2c_weight_1, float* __restrict__ res2c_bias_1, float* __restrict__ res2c_weight_2, float* __restrict__ res2c_bias_2, float* __restrict__ res3a_weight_b, float* __restrict__ res3a_bias_b, float* __restrict__ res3a_weight_0, float* __restrict__ res3a_bias_0, float* __restrict__ res3a_weight_1, float* __restrict__ res3a_bias_1, float* __restrict__ res3a_weight_2, float* __restrict__ res3a_bias_2, float* __restrict__ res3b_weight_0, float* __restrict__ res3b_bias_0, float* __restrict__ res3b_weight_1, float* __restrict__ res3b_bias_1, float* __restrict__ res3b_weight_2, float* __restrict__ res3b_bias_2, float* __restrict__ res3c_weight_0, float* __restrict__ res3c_bias_0, float* __restrict__ res3c_weight_1, float* __restrict__ res3c_bias_1, float* __restrict__ res3c_weight_2, float* __restrict__ res3c_bias_2, float* __restrict__ res3d_weight_0, float* __restrict__ res3d_bias_0, float* __restrict__ res3d_weight_1, float* __restrict__ res3d_bias_1, float* __restrict__ res3d_weight_2, float* __restrict__ res3d_bias_2, float* __restrict__ res4a_weight_b, float* __restrict__ res4a_bias_b, float* __restrict__ res4a_weight_0, float* __restrict__ res4a_bias_0, float* __restrict__ res4a_weight_1, float* __restrict__ res4a_bias_1, float* __restrict__ res4a_weight_2, float* __restrict__ res4a_bias_2, float* __restrict__ res4b_weight_0, float* __restrict__ res4b_bias_0, float* __restrict__ res4b_weight_1, float* __restrict__ res4b_bias_1, float* __restrict__ res4b_weight_2, float* __restrict__ res4b_bias_2, float* __restrict__ res4c_weight_0, float* __restrict__ res4c_bias_0, float* __restrict__ res4c_weight_1, float* __restrict__ res4c_bias_1, float* __restrict__ res4c_weight_2, float* __restrict__ res4c_bias_2, float* __restrict__ res4d_weight_0, float* __restrict__ res4d_bias_0, float* __restrict__ res4d_weight_1, float* __restrict__ res4d_bias_1, float* __restrict__ res4d_weight_2, float* __restrict__ res4d_bias_2, float* __restrict__ res4e_weight_0, float* __restrict__ res4e_bias_0, float* __restrict__ res4e_weight_1, float* __restrict__ res4e_bias_1, float* __restrict__ res4e_weight_2, float* __restrict__ res4e_bias_2, float* __restrict__ res4f_weight_0, float* __restrict__ res4f_bias_0, float* __restrict__ res4f_weight_1, float* __restrict__ res4f_bias_1, float* __restrict__ res4f_weight_2, float* __restrict__ res4f_bias_2, float* __restrict__ res5a_weight_b, float* __restrict__ res5a_bias_b, float* __restrict__ res5a_weight_0, float* __restrict__ res5a_bias_0, float* __restrict__ res5a_weight_1, float* __restrict__ res5a_bias_1, float* __restrict__ res5a_weight_2, float* __restrict__ res5a_bias_2, float* __restrict__ res5b_weight_0, float* __restrict__ res5b_bias_0, float* __restrict__ res5b_weight_1, float* __restrict__ res5b_bias_1, float* __restrict__ res5b_weight_2, float* __restrict__ res5b_bias_2, float* __restrict__ res5c_weight_0, float* __restrict__ res5c_bias_0, float* __restrict__ res5c_weight_1, float* __restrict__ res5c_bias_1, float* __restrict__ res5c_weight_2, float* __restrict__ res5c_bias_2) noexcept{
  float* folded_const_46 = (float*)&__module_data[83456UL];
  float* folded_const_41 = (float*)&__module_data[77184UL];
  float* folded_const_31 = (float*)&__module_data[64640UL];
  float* folded_const_26 = (float*)&__module_data[58368UL];
  float* folded_const_21 = (float*)&__module_data[52096UL];
  float* folded_const_16 = (float*)&__module_data[45824UL];
  float* folded_const_103 = (float*)&__module_data[107456UL];
  float* folded_const_80 = (float*)&__module_data[106816UL];
  float* folded_const_78 = (float*)&__module_data[105728UL];
  float* folded_const_75 = (float*)&__module_data[105152UL];
  float* folded_const_73 = (float*)&__module_data[104064UL];
  float* folded_const_72 = (float*)&__module_data[103808UL];
  float* folded_const_70 = (float*)&__module_data[103488UL];
  float* folded_const_68 = (float*)&__module_data[102400UL];
  float* folded_const_67 = (float*)&__module_data[100352UL];
  float* folded_const_66 = (float*)&__module_data[99840UL];
  float* folded_const_64 = (float*)&__module_data[99264UL];
  float* folded_const_62 = (float*)&__module_data[97152UL];
  float* folded_const_61 = (float*)&__module_data[96640UL];
  float* folded_const_57 = (float*)&__module_data[93952UL];
  float* folded_const_54 = (float*)&__module_data[92864UL];
  float* folded_const_52 = (float*)&__module_data[90752UL];
  float* folded_const_51 = (float*)&__module_data[90240UL];
  float* folded_const_49 = (float*)&__module_data[89664UL];
  float* folded_const_47 = (float*)&__module_data[87552UL];
  float* folded_const_43 = (float*)&__module_data[81344UL];
  float* folded_const_40 = (float*)&__module_data[76160UL];
  float* folded_const_38 = (float*)&__module_data[75072UL];
  float* folded_const_35 = (float*)&__module_data[69888UL];
  float* folded_const_33 = (float*)&__module_data[68800UL];
  float* folded_const_30 = (float*)&__module_data[63616UL];
  float* folded_const_28 = (float*)&__module_data[62528UL];
  float* folded_const_23 = (float*)&__module_data[56256UL];
  float* folded_const_20 = (float*)&__module_data[51072UL];
  float* folded_const_18 = (float*)&__module_data[49984UL];
  float* folded_const_15 = (float*)&__module_data[37632UL];
  float* folded_const_14 = (float*)&__module_data[35584UL];
  float* folded_const_12 = (float*)&__module_data[33472UL];
  float* folded_const_10 = (float*)&__module_data[25216UL];
  float* folded_const_9 = (float*)&__module_data[23168UL];
  float* folded_const_7 = (float*)&__module_data[21056UL];
  float* folded_const_5 = (float*)&__module_data[12800UL];
  float* folded_const_4 = (float*)&__module_data[10752UL];
  float* folded_const_2 = (float*)&__module_data[8640UL];
  float* folded_const_0 = (float*)&__module_data[384UL];
  float* folded_const_154 = (float*)&__module_data[213184UL];
  float* folded_const_155 = (float*)&__module_data[213440UL];
  float* folded_const_152 = (float*)&__module_data[211904UL];
  float* folded_const_149 = (float*)&__module_data[210368UL];
  float* folded_const_146 = (float*)&__module_data[208832UL];
  float* folded_const_151 = (float*)&__module_data[211648UL];
  float* folded_const_148 = (float*)&__module_data[210112UL];
  float* folded_const_144 = (float*)&__module_data[206272UL];
  float* folded_const_153 = (float*)&__module_data[212928UL];
  float* folded_const_150 = (float*)&__module_data[211392UL];
  float* folded_const_147 = (float*)&__module_data[209856UL];
  float* folded_const_142 = (float*)&__module_data[203712UL];
  float* folded_const_139 = (float*)&__module_data[200640UL];
  float* folded_const_136 = (float*)&__module_data[197568UL];
  float* folded_const_133 = (float*)&__module_data[194496UL];
  float* folded_const_141 = (float*)&__module_data[203200UL];
  float* folded_const_138 = (float*)&__module_data[200128UL];
  float* folded_const_135 = (float*)&__module_data[197056UL];
  float* folded_const_145 = (float*)&__module_data[206784UL];
  float* folded_const_131 = (float*)&__module_data[189376UL];
  float* folded_const_143 = (float*)&__module_data[205760UL];
  float* folded_const_140 = (float*)&__module_data[202688UL];
  float* folded_const_137 = (float*)&__module_data[199616UL];
  float* folded_const_134 = (float*)&__module_data[196544UL];
  float* folded_const_129 = (float*)&__module_data[184256UL];
  float* folded_const_126 = (float*)&__module_data[178112UL];
  float* folded_const_123 = (float*)&__module_data[171968UL];
  float* folded_const_120 = (float*)&__module_data[165824UL];
  float* folded_const_117 = (float*)&__module_data[159680UL];
  float* folded_const_114 = (float*)&__module_data[153536UL];
  float* folded_const_128 = (float*)&__module_data[183232UL];
  float* folded_const_125 = (float*)&__module_data[177088UL];
  float* folded_const_122 = (float*)&__module_data[170944UL];
  float* folded_const_119 = (float*)&__module_data[164800UL];
  float* folded_const_116 = (float*)&__module_data[158656UL];
  float* folded_const_132 = (float*)&__module_data[190400UL];
  float* folded_const_112 = (float*)&__module_data[143296UL];
  float* folded_const_130 = (float*)&__module_data[188352UL];
  float* folded_const_127 = (float*)&__module_data[182208UL];
  float* folded_const_124 = (float*)&__module_data[176064UL];
  float* folded_const_121 = (float*)&__module_data[169920UL];
  float* folded_const_118 = (float*)&__module_data[163776UL];
  float* folded_const_115 = (float*)&__module_data[157632UL];
  int8_t* folded_const_156 = (int8_t*)&__uninitialized_data[0UL];
  float* folded_const_157 = (float*)&__uninitialized_data[589824UL];
  float* folded_const_86 = (float*)&__module_data[107092UL];
  float* folded_const_158 = (float*)&__uninitialized_data[593920UL];
  float* folded_const_159 = (float*)&__uninitialized_data[598016UL];
  float* folded_const_87 = (float*)&__module_data[107096UL];
  float* folded_const_160 = (float*)&__uninitialized_data[602112UL];
  float* folded_const_161 = (float*)&__uninitialized_data[606208UL];
  float* folded_const_88 = (float*)&__module_data[107100UL];
  float* folded_const_162 = (float*)&__uninitialized_data[610304UL];
  float* folded_const_163 = (float*)&__uninitialized_data[614400UL];
  float* folded_const_89 = (float*)&__module_data[107104UL];
  float* folded_const_164 = (float*)&__uninitialized_data[618496UL];
  float* folded_const_165 = (float*)&__uninitialized_data[622592UL];
  float* folded_const_90 = (float*)&__module_data[107108UL];
  float* folded_const_166 = (float*)&__uninitialized_data[626688UL];
  float* folded_const_36 = (float*)&__module_data[70912UL];
  float* folded_const_167 = (float*)&__uninitialized_data[630784UL];
  float* folded_const_91 = (float*)&__module_data[107112UL];
  float* folded_const_168 = (float*)&__uninitialized_data[634880UL];
  float* folded_const_169 = (float*)&__uninitialized_data[638976UL];
  float* folded_const_92 = (float*)&__module_data[107116UL];
  float* folded_const_170 = (float*)&__uninitialized_data[643072UL];
  float* folded_const_171 = (float*)&__uninitialized_data[647168UL];
  float* folded_const_93 = (float*)&__module_data[107120UL];
  float* folded_const_172 = (float*)&__uninitialized_data[649216UL];
  float* folded_const_173 = (float*)&__uninitialized_data[651264UL];
  float* folded_const_94 = (float*)&__module_data[107124UL];
  float* folded_const_174 = (float*)&__uninitialized_data[653312UL];
  float* folded_const_175 = (float*)&__uninitialized_data[655360UL];
  float* folded_const_95 = (float*)&__module_data[107128UL];
  float* folded_const_176 = (float*)&__uninitialized_data[657408UL];
  float* folded_const_177 = (float*)&__uninitialized_data[659456UL];
  float* folded_const_96 = (float*)&__module_data[107132UL];
  float* folded_const_178 = (float*)&__uninitialized_data[661504UL];
  float* folded_const_179 = (float*)&__uninitialized_data[663552UL];
  float* folded_const_97 = (float*)&__module_data[107136UL];
  float* folded_const_180 = (float*)&__uninitialized_data[665600UL];
  float* folded_const_181 = (float*)&__uninitialized_data[667648UL];
  float* folded_const_17 = (float*)&__module_data[49920UL];
  float* folded_const_182 = (float*)&__uninitialized_data[668672UL];
  float* folded_const_183 = (float*)&__uninitialized_data[669696UL];
  float* folded_const_19 = (float*)&__module_data[51008UL];
  float* folded_const_184 = (float*)&__uninitialized_data[670720UL];
  float* folded_const_185 = (float*)&__uninitialized_data[671744UL];
  float* folded_const_22 = (float*)&__module_data[56192UL];
  float* folded_const_186 = (float*)&__uninitialized_data[672768UL];
  float* folded_const_187 = (float*)&__uninitialized_data[673792UL];
  float* folded_const_24 = (float*)&__module_data[57280UL];
  float* folded_const_188 = (float*)&__uninitialized_data[674816UL];
  float* folded_const_25 = (float*)&__module_data[57344UL];
  float* folded_const_189 = (float*)&__uninitialized_data[675840UL];
  float* folded_const_27 = (float*)&__module_data[62464UL];
  float* folded_const_190 = (float*)&__uninitialized_data[676864UL];
  float* folded_const_191 = (float*)&__uninitialized_data[677888UL];
  float* folded_const_29 = (float*)&__module_data[63552UL];
  float* folded_const_192 = (float*)&__uninitialized_data[678912UL];
  float* folded_const_193 = (float*)&__uninitialized_data[679936UL];
  float* folded_const_32 = (float*)&__module_data[68736UL];
  float* folded_const_194 = (float*)&__uninitialized_data[680960UL];
  float* folded_const_195 = (float*)&__uninitialized_data[681984UL];
  float* folded_const_34 = (float*)&__module_data[69824UL];
  float* folded_const_196 = (float*)&__uninitialized_data[683008UL];
  float* folded_const_197 = (float*)&__uninitialized_data[684032UL];
  float* folded_const_37 = (float*)&__module_data[75008UL];
  float* folded_const_198 = (float*)&__uninitialized_data[685056UL];
  float* folded_const_199 = (float*)&__uninitialized_data[686080UL];
  float* folded_const_39 = (float*)&__module_data[76096UL];
  float* folded_const_200 = (float*)&__uninitialized_data[687104UL];
  float* folded_const_201 = (float*)&__uninitialized_data[688128UL];
  float* folded_const_42 = (float*)&__module_data[81280UL];
  float* folded_const_202 = (float*)&__uninitialized_data[689152UL];
  float* folded_const_203 = (float*)&__uninitialized_data[690176UL];
  float* folded_const_44 = (float*)&__module_data[82368UL];
  float* folded_const_204 = (float*)&__uninitialized_data[691200UL];
  float* folded_const_45 = (float*)&__module_data[82432UL];
  float* folded_const_205 = (float*)&__uninitialized_data[692224UL];
  float* folded_const_98 = (float*)&__module_data[107140UL];
  float* folded_const_206 = (float*)&__uninitialized_data[693248UL];
  float* folded_const_207 = (float*)&__uninitialized_data[694272UL];
  float* folded_const_99 = (float*)&__module_data[107144UL];
  float* folded_const_208 = (float*)&__uninitialized_data[695296UL];
  float* folded_const_209 = (float*)&__uninitialized_data[696320UL];
  float* folded_const_100 = (float*)&__module_data[107148UL];
  float* folded_const_210 = (float*)&__uninitialized_data[697344UL];
  float* folded_const_211 = (float*)&__uninitialized_data[698368UL];
  float* folded_const_101 = (float*)&__module_data[107152UL];
  float* folded_const_212 = (float*)&__uninitialized_data[699392UL];
  int8_t* folded_const_213 = (int8_t*)&__uninitialized_data[700416UL];
  int8_t* folded_const_214 = (int8_t*)&__uninitialized_data[962560UL];
  int8_t* folded_const_215 = (int8_t*)&__uninitialized_data[1224704UL];
  int8_t* folded_const_216 = (int8_t*)&__uninitialized_data[1486848UL];
  int8_t* folded_const_217 = (int8_t*)&__uninitialized_data[1748992UL];
  int8_t* folded_const_218 = (int8_t*)&__uninitialized_data[2011136UL];
  int8_t* folded_const_219 = (int8_t*)&__uninitialized_data[2273280UL];
  int8_t* folded_const_220 = (int8_t*)&__uninitialized_data[2535424UL];
  int8_t* folded_const_221 = (int8_t*)&__uninitialized_data[2797568UL];
  int8_t* folded_const_222 = (int8_t*)&__uninitialized_data[3059712UL];
  int8_t* folded_const_223 = (int8_t*)&__uninitialized_data[3321856UL];
  int8_t* folded_const_224 = (int8_t*)&__uninitialized_data[3584000UL];
  float* folded_const_225 = (float*)&__uninitialized_data[3588096UL];
  float* folded_const_48 = (float*)&__module_data[89600UL];
  float* folded_const_226 = (float*)&__uninitialized_data[3588608UL];
  float* folded_const_227 = (float*)&__uninitialized_data[3589120UL];
  float* folded_const_50 = (float*)&__module_data[90176UL];
  float* folded_const_228 = (float*)&__uninitialized_data[3589632UL];
  float* folded_const_229 = (float*)&__uninitialized_data[3590144UL];
  float* folded_const_53 = (float*)&__module_data[92800UL];
  float* folded_const_230 = (float*)&__uninitialized_data[3590656UL];
  float* folded_const_231 = (float*)&__uninitialized_data[3591168UL];
  float* folded_const_55 = (float*)&__module_data[93376UL];
  float* folded_const_232 = (float*)&__uninitialized_data[3591680UL];
  float* folded_const_56 = (float*)&__module_data[93440UL];
  float* folded_const_233 = (float*)&__uninitialized_data[3592192UL];
  float* folded_const_58 = (float*)&__module_data[96000UL];
  float* folded_const_234 = (float*)&__uninitialized_data[3592704UL];
  float* folded_const_59 = (float*)&__module_data[96064UL];
  float* folded_const_235 = (float*)&__uninitialized_data[3593216UL];
  float* folded_const_60 = (float*)&__module_data[96576UL];
  float* folded_const_236 = (float*)&__uninitialized_data[3593728UL];
  float* folded_const_237 = (float*)&__uninitialized_data[3594240UL];
  float* folded_const_63 = (float*)&__module_data[99200UL];
  float* folded_const_238 = (float*)&__uninitialized_data[3594752UL];
  float* folded_const_239 = (float*)&__uninitialized_data[3595264UL];
  float* folded_const_65 = (float*)&__module_data[99776UL];
  float* folded_const_240 = (float*)&__uninitialized_data[3595776UL];
  float* folded_const_241 = (float*)&__uninitialized_data[3596288UL];
  float* folded_const_69 = (float*)&__module_data[103424UL];
  float* folded_const_242 = (float*)&__uninitialized_data[3596544UL];
  float* folded_const_243 = (float*)&__uninitialized_data[3596800UL];
  float* folded_const_71 = (float*)&__module_data[103744UL];
  float* folded_const_244 = (float*)&__uninitialized_data[3597056UL];
  float* folded_const_245 = (float*)&__uninitialized_data[3597312UL];
  float* folded_const_74 = (float*)&__module_data[105088UL];
  float* folded_const_246 = (float*)&__uninitialized_data[3597568UL];
  float* folded_const_247 = (float*)&__uninitialized_data[3597824UL];
  float* folded_const_76 = (float*)&__module_data[105408UL];
  float* folded_const_248 = (float*)&__uninitialized_data[3598080UL];
  float* folded_const_77 = (float*)&__module_data[105472UL];
  float* folded_const_249 = (float*)&__uninitialized_data[3598336UL];
  float* folded_const_79 = (float*)&__module_data[106752UL];
  float* folded_const_250 = (float*)&__uninitialized_data[3598592UL];
  float* folded_const_251 = (float*)&__uninitialized_data[3598848UL];
  float* folded_const_81 = (float*)&__module_data[107072UL];
  float* folded_const_252 = (float*)&__uninitialized_data[3599104UL];
  float* folded_const_102 = (float*)&__module_data[107200UL];
  int8_t* folded_const_253 = (int8_t*)&__uninitialized_data[3599360UL];
  int8_t* folded_const_254 = (int8_t*)&__uninitialized_data[4189184UL];
  int8_t* folded_const_255 = (int8_t*)&__uninitialized_data[4779008UL];
  int8_t* folded_const_256 = (int8_t*)&__uninitialized_data[5368832UL];
  int8_t* folded_const_257 = (int8_t*)&__uninitialized_data[5958656UL];
  int8_t* folded_const_258 = (int8_t*)&__uninitialized_data[6548480UL];
  int8_t* folded_const_259 = (int8_t*)&__uninitialized_data[7072768UL];
  int8_t* folded_const_260 = (int8_t*)&__uninitialized_data[7220224UL];
  int8_t* folded_const_261 = (int8_t*)&__uninitialized_data[7367680UL];
  int8_t* folded_const_262 = (int8_t*)&__uninitialized_data[7515136UL];
  int8_t* folded_const_263 = (int8_t*)&__uninitialized_data[7662592UL];
  int8_t* folded_const_264 = (int8_t*)&__uninitialized_data[7793664UL];
  int8_t* folded_const_265 = (int8_t*)&__uninitialized_data[7924736UL];
  int8_t* folded_const_266 = (int8_t*)&__uninitialized_data[7990272UL];
  int8_t* folded_const_267 = (int8_t*)&__uninitialized_data[8055808UL];
  int8_t* folded_const_268 = (int8_t*)&__uninitialized_data[8121344UL];
  int8_t* folded_const_269 = (int8_t*)&__uninitialized_data[8186880UL];
  int8_t* folded_const_270 = (int8_t*)&__uninitialized_data[8252416UL];
  int8_t* folded_const_271 = (int8_t*)&__uninitialized_data[8317952UL];
  int8_t* folded_const_272 = (int8_t*)&__uninitialized_data[8383488UL];
  int8_t* folded_const_273 = (int8_t*)&__uninitialized_data[8420352UL];
  int8_t* folded_const_274 = (int8_t*)&__uninitialized_data[8457216UL];
  int8_t* folded_const_275 = (int8_t*)&__uninitialized_data[8494080UL];
  int8_t* folded_const_276 = (int8_t*)&__uninitialized_data[8526848UL];
  int8_t* folded_const_277 = (int8_t*)&__uninitialized_data[8543232UL];
  int8_t* folded_const_278 = (int8_t*)&__uninitialized_data[8559616UL];
  int8_t* folded_const_279 = (int8_t*)&__uninitialized_data[8576000UL];
  int8_t* folded_const_280 = (int8_t*)&__uninitialized_data[8592384UL];
  int8_t* folded_const_281 = (int8_t*)&__uninitialized_data[8608768UL];
  float* folded_const_282 = (float*)&__uninitialized_data[8625152UL];
  float* folded_const_13 = (float*)&__module_data[35520UL];
  float* folded_const_283 = (float*)&__uninitialized_data[8627200UL];
  int8_t* folded_const_284 = (int8_t*)&__uninitialized_data[8629248UL];
  float* folded_const_110 = (float*)&__module_data[133056UL];
  float* folded_const_107 = (float*)&__module_data[120768UL];
  float* folded_const_104 = (float*)&__module_data[108480UL];
  float* folded_const_109 = (float*)&__module_data[131008UL];
  float* folded_const_106 = (float*)&__module_data[118720UL];
  float* folded_const_113 = (float*)&__module_data[145344UL];
  int8_t* folded_const_285 = (int8_t*)&__uninitialized_data[9153536UL];
  float* folded_const_286 = (float*)&__uninitialized_data[11250688UL];
  float* folded_const_85 = (float*)&__module_data[107088UL];
  float* folded_const_287 = (float*)&__uninitialized_data[11258880UL];
  float* folded_const_111 = (float*)&__module_data[141248UL];
  int8_t* folded_const_288 = (int8_t*)&__uninitialized_data[11267072UL];
  float* folded_const_289 = (float*)&__uninitialized_data[13626368UL];
  float* folded_const_11 = (float*)&__module_data[33408UL];
  float* folded_const_290 = (float*)&__uninitialized_data[13628416UL];
  float* folded_const_291 = (float*)&__uninitialized_data[13630464UL];
  float* folded_const_84 = (float*)&__module_data[107084UL];
  float* folded_const_292 = (float*)&__uninitialized_data[13638656UL];
  int8_t* folded_const_293 = (int8_t*)&__uninitialized_data[13646848UL];
  float* folded_const_294 = (float*)&__uninitialized_data[14695424UL];
  float* folded_const_8 = (float*)&__module_data[23104UL];
  float* folded_const_295 = (float*)&__uninitialized_data[14697472UL];
  int8_t* folded_const_296 = (int8_t*)&__uninitialized_data[14699520UL];
  float* folded_const_108 = (float*)&__module_data[128960UL];
  float* folded_const_105 = (float*)&__module_data[116672UL];
  float* folded_const_297 = (float*)&__uninitialized_data[15748096UL];
  float* folded_const_6 = (float*)&__module_data[20992UL];
  float* folded_const_298 = (float*)&__uninitialized_data[15750144UL];
  int8_t* folded_const_299 = (int8_t*)&__uninitialized_data[15752192UL];
  float* folded_const_300 = (float*)&__uninitialized_data[18111488UL];
  float* folded_const_83 = (float*)&__module_data[107080UL];
  float* folded_const_301 = (float*)&__uninitialized_data[18119680UL];
  int8_t* folded_const_302 = (int8_t*)&__uninitialized_data[18127872UL];
  float* folded_const_303 = (float*)&__uninitialized_data[19176448UL];
  float* folded_const_3 = (float*)&__module_data[10688UL];
  float* folded_const_304 = (float*)&__uninitialized_data[19178496UL];
  int8_t* folded_const_305 = (int8_t*)&__uninitialized_data[19180544UL];
  float* folded_const_306 = (float*)&__uninitialized_data[20229120UL];
  float* folded_const_1 = (float*)&__module_data[8576UL];
  float* folded_const_307 = (float*)&__uninitialized_data[20231168UL];
  int8_t* folded_const_308 = (int8_t*)&__uninitialized_data[20233216UL];
  float* folded_const_309 = (float*)&__uninitialized_data[22592512UL];
  float* folded_const_82 = (float*)&__module_data[107076UL];
  float* folded_const_310 = (float*)&__uninitialized_data[22600704UL];
  int8_t* folded_const_311 = (int8_t*)&__uninitialized_data[22608896UL];
  bool& is_init = *(bool*)(__module_data + 0);
  int8_t* __rescheduled_0 = (int8_t*)sc_aligned_malloc(__stream, 18546688UL);
  // [f32 [1, 32, 1, 1, 32] @ ABCD32b]
  float* buffer_261 = (float*)&__rescheduled_0[0UL];
  reorder__481(buffer_261, folded_const_46);
  // [f32 [1, 8, 1, 1, 128] @ ABCD128b]
  float* buffer_262 = (float*)&__rescheduled_0[4096UL];
  reorder__488(buffer_262, folded_const_41);
  // [f32 [1, 8, 1, 1, 128] @ ABCD128b]
  float* buffer_263 = (float*)&__rescheduled_0[8192UL];
  reorder__504(buffer_263, folded_const_31);
  // [f32 [1, 8, 1, 1, 128] @ ABCD128b]
  float* buffer_264 = (float*)&__rescheduled_0[12288UL];
  reorder__513(buffer_264, folded_const_26);
  // [f32 [1, 8, 1, 1, 128] @ ABCD128b]
  float* buffer_265 = (float*)&__rescheduled_0[16384UL];
  reorder__520(buffer_265, folded_const_21);
  // [f32 [1, 4, 1, 1, 256] @ ABCD256b]
  float* buffer_266 = (float*)&__rescheduled_0[20480UL];
  reorder__529(buffer_266, folded_const_16);
  // [f32 [1, 16, 1, 1, 16] @ ABCD16b]
  float* buffer_267 = (float*)&__rescheduled_0[24576UL];
  reorder__420(buffer_267, folded_const_103);
  // [f32 [1, 2, 1, 1, 32] @ ABCD32b]
  float* buffer_268 = (float*)&__rescheduled_0[25600UL];
  reorder__424(buffer_268, folded_const_80);
  // [f32 [1, 4, 1, 1, 64] @ ABCD64b]
  float* buffer_269 = (float*)&__rescheduled_0[25856UL];
  reorder__427(buffer_269, folded_const_78);
  // [f32 [1, 4, 1, 1, 16] @ ABCD16b]
  float* buffer_270 = (float*)&__rescheduled_0[26880UL];
  reorder__431(buffer_270, folded_const_75);
  // [f32 [1, 8, 1, 1, 32] @ ABCD32b]
  float* buffer_271 = (float*)&__rescheduled_0[27136UL];
  reorder__434(buffer_271, folded_const_73);
  // [f32 [1, 2, 1, 1, 32] @ ABCD32b]
  float* buffer_272 = (float*)&__rescheduled_0[28160UL];
  reorder__437(buffer_272, folded_const_72);
  // [f32 [1, 2, 1, 1, 32] @ ABCD32b]
  float* buffer_273 = (float*)&__rescheduled_0[28416UL];
  reorder__440(buffer_273, folded_const_70);
  // [f32 [1, 4, 1, 1, 64] @ ABCD64b]
  float* buffer_274 = (float*)&__rescheduled_0[28672UL];
  reorder__443(buffer_274, folded_const_68);
  // [f32 [1, 16, 1, 1, 32] @ ABCD32b]
  float* buffer_275 = (float*)&__rescheduled_0[29696UL];
  reorder__446(buffer_275, folded_const_67);
  // [f32 [1, 2, 1, 1, 64] @ ABCD64b]
  float* buffer_276 = (float*)&__rescheduled_0[31744UL];
  reorder__449(buffer_276, folded_const_66);
  // [f32 [1, 4, 1, 1, 32] @ ABCD32b]
  float* buffer_277 = (float*)&__rescheduled_0[32256UL];
  reorder__452(buffer_277, folded_const_64);
  // [f32 [1, 16, 1, 1, 32] @ ABCD32b]
  float* buffer_278 = (float*)&__rescheduled_0[32768UL];
  reorder__455(buffer_278, folded_const_62);
  // [f32 [1, 4, 1, 1, 32] @ ABCD32b]
  float* buffer_279 = (float*)&__rescheduled_0[34816UL];
  reorder__458(buffer_279, folded_const_61);
  // [f32 [1, 4, 1, 1, 128] @ ABCD128b]
  float* buffer_280 = (float*)&__rescheduled_0[35328UL];
  reorder__462(buffer_280, folded_const_57);
  // [f32 [1, 4, 1, 1, 32] @ ABCD32b]
  float* buffer_281 = (float*)&__rescheduled_0[37376UL];
  reorder__466(buffer_281, folded_const_54);
  // [f32 [1, 8, 1, 1, 64] @ ABCD64b]
  float* buffer_282 = (float*)&__rescheduled_0[37888UL];
  reorder__469(buffer_282, folded_const_52);
  // [f32 [1, 2, 1, 1, 64] @ ABCD64b]
  float* buffer_283 = (float*)&__rescheduled_0[39936UL];
  reorder__472(buffer_283, folded_const_51);
  // [f32 [1, 4, 1, 1, 32] @ ABCD32b]
  float* buffer_284 = (float*)&__rescheduled_0[40448UL];
  reorder__475(buffer_284, folded_const_49);
  // [f32 [1, 4, 1, 1, 128] @ ABCD128b]
  float* buffer_285 = (float*)&__rescheduled_0[40960UL];
  reorder__478(buffer_285, folded_const_47);
  // [f32 [1, 2, 1, 1, 128] @ ABCD128b]
  float* buffer_286 = (float*)&__rescheduled_0[43008UL];
  reorder__485(buffer_286, folded_const_43);
  // [f32 [1, 4, 1, 1, 64] @ ABCD64b]
  float* buffer_287 = (float*)&__rescheduled_0[44032UL];
  reorder__491(buffer_287, folded_const_40);
  // [f32 [1, 16, 1, 1, 16] @ ABCD16b]
  float* buffer_288 = (float*)&__rescheduled_0[45056UL];
  reorder__494(buffer_288, folded_const_38);
  // [f32 [1, 8, 1, 1, 32] @ ABCD32b]
  float* buffer_289 = (float*)&__rescheduled_0[46080UL];
  reorder__498(buffer_289, folded_const_35);
  // [f32 [1, 8, 1, 1, 32] @ ABCD32b]
  float* buffer_290 = (float*)&__rescheduled_0[47104UL];
  reorder__501(buffer_290, folded_const_33);
  // [f32 [1, 4, 1, 1, 64] @ ABCD64b]
  float* buffer_291 = (float*)&__rescheduled_0[48128UL];
  reorder__507(buffer_291, folded_const_30);
  // [f32 [1, 4, 1, 1, 64] @ ABCD64b]
  float* buffer_292 = (float*)&__rescheduled_0[49152UL];
  reorder__510(buffer_292, folded_const_28);
  // [f32 [1, 8, 1, 1, 32] @ ABCD32b]
  float* buffer_293 = (float*)&__rescheduled_0[50176UL];
  reorder__517(buffer_293, folded_const_23);
  // [f32 [1, 8, 1, 1, 32] @ ABCD32b]
  float* buffer_294 = (float*)&__rescheduled_0[51200UL];
  reorder__523(buffer_294, folded_const_20);
  // [f32 [1, 4, 1, 1, 64] @ ABCD64b]
  float* buffer_295 = (float*)&__rescheduled_0[52224UL];
  reorder__526(buffer_295, folded_const_18);
  // [f32 [1, 128, 1, 1, 16] @ ABCD16b]
  float* buffer_296 = (float*)&__rescheduled_0[53248UL];
  reorder__532(buffer_296, folded_const_15);
  // [f32 [1, 8, 1, 1, 64] @ ABCD64b]
  float* buffer_297 = (float*)&__rescheduled_0[61440UL];
  reorder__535(buffer_297, folded_const_14);
  // [f32 [1, 4, 1, 1, 128] @ ABCD128b]
  float* buffer_298 = (float*)&__rescheduled_0[63488UL];
  reorder__538(buffer_298, folded_const_12);
  // [f32 [1, 32, 1, 1, 64] @ ABCD64b]
  float* buffer_299 = (float*)&__rescheduled_0[65536UL];
  reorder__541(buffer_299, folded_const_10);
  // [f32 [1, 4, 1, 1, 128] @ ABCD128b]
  float* buffer_300 = (float*)&__rescheduled_0[73728UL];
  reorder__544(buffer_300, folded_const_9);
  // [f32 [1, 32, 1, 1, 16] @ ABCD16b]
  float* buffer_301 = (float*)&__rescheduled_0[75776UL];
  reorder__547(buffer_301, folded_const_7);
  // [f32 [1, 32, 1, 1, 64] @ ABCD64b]
  float* buffer_302 = (float*)&__rescheduled_0[77824UL];
  reorder__550(buffer_302, folded_const_5);
  // [f32 [1, 16, 1, 1, 32] @ ABCD32b]
  float* buffer_303 = (float*)&__rescheduled_0[86016UL];
  reorder__553(buffer_303, folded_const_4);
  // [f32 [1, 4, 1, 1, 128] @ ABCD128b]
  float* buffer_304 = (float*)&__rescheduled_0[88064UL];
  reorder__556(buffer_304, folded_const_2);
  // [f32 [1, 4, 1, 1, 512] @ ABCD512b]
  float* buffer_305 = (float*)&__rescheduled_0[90112UL];
  reorder__559(buffer_305, folded_const_0);
  // [f32 [1, 2, 1, 1, 32] @ ABCD32b]
  float* buffer_306 = (float*)&__rescheduled_0[98304UL];
  reorder__425(buffer_306, &res2a_bias_1[0]);
  // [f32 [1, 4, 1, 1, 16] @ ABCD16b]
  float* buffer_307 = (float*)&__rescheduled_0[98560UL];
  reorder__432(buffer_307, &res2b_bias_1[0]);
  // [f32 [1, 2, 1, 1, 32] @ ABCD32b]
  float* buffer_308 = (float*)&__rescheduled_0[98816UL];
  reorder__438(buffer_308, &res2c_bias_0[0]);
  // [f32 [1, 2, 1, 1, 32] @ ABCD32b]
  float* buffer_309 = (float*)&__rescheduled_0[99072UL];
  reorder__441(buffer_309, &res2c_bias_1[0]);
  // [f32 [1, 2, 1, 1, 64] @ ABCD64b]
  float* buffer_310 = (float*)&__rescheduled_0[99328UL];
  reorder__450(buffer_310, &res3a_bias_0[0]);
  // [f32 [1, 4, 1, 1, 32] @ ABCD32b]
  float* buffer_311 = (float*)&__rescheduled_0[99840UL];
  reorder__453(buffer_311, &res3a_bias_1[0]);
  // [f32 [1, 4, 1, 1, 32] @ ABCD32b]
  float* buffer_312 = (float*)&__rescheduled_0[100352UL];
  reorder__459(buffer_312, &res3b_bias_0[0]);
  // [f32 [1, 4, 1, 1, 32] @ ABCD32b]
  float* buffer_313 = (float*)&__rescheduled_0[100864UL];
  reorder__467(buffer_313, &res3c_bias_1[0]);
  // [f32 [1, 2, 1, 1, 64] @ ABCD64b]
  float* buffer_314 = (float*)&__rescheduled_0[101376UL];
  reorder__473(buffer_314, &res3d_bias_0[0]);
  // [f32 [1, 4, 1, 1, 32] @ ABCD32b]
  float* buffer_315 = (float*)&__rescheduled_0[101888UL];
  reorder__476(buffer_315, &res3d_bias_1[0]);
  // [f32 [1, 16, 1, 1, 16] @ ABCD16b]
  float* buffer_316 = (float*)&__rescheduled_0[102400UL];
  reorder__421(buffer_316, &res2a_bias_b[0]);
  // [f32 [1, 4, 1, 1, 64] @ ABCD64b]
  float* buffer_317 = (float*)&__rescheduled_0[103424UL];
  reorder__428(buffer_317, &res2a_bias_2[0]);
  // [f32 [1, 8, 1, 1, 32] @ ABCD32b]
  float* buffer_318 = (float*)&__rescheduled_0[104448UL];
  reorder__435(buffer_318, &res2b_bias_2[0]);
  // [f32 [1, 4, 1, 1, 64] @ ABCD64b]
  float* buffer_319 = (float*)&__rescheduled_0[105472UL];
  reorder__444(buffer_319, &res2c_bias_2[0]);
  // [f32 [1, 2, 1, 1, 128] @ ABCD128b]
  float* buffer_320 = (float*)&__rescheduled_0[106496UL];
  reorder__486(buffer_320, &res4a_bias_1[0]);
  // [f32 [1, 4, 1, 1, 64] @ ABCD64b]
  float* buffer_321 = (float*)&__rescheduled_0[107520UL];
  reorder__492(buffer_321, &res4b_bias_0[0]);
  // [f32 [1, 16, 1, 1, 16] @ ABCD16b]
  float* buffer_322 = (float*)&__rescheduled_0[108544UL];
  reorder__495(buffer_322, &res4b_bias_1[0]);
  // [f32 [1, 8, 1, 1, 32] @ ABCD32b]
  float* buffer_323 = (float*)&__rescheduled_0[109568UL];
  reorder__499(buffer_323, &res4c_bias_0[0]);
  // [f32 [1, 8, 1, 1, 32] @ ABCD32b]
  float* buffer_324 = (float*)&__rescheduled_0[110592UL];
  reorder__502(buffer_324, &res4c_bias_1[0]);
  // [f32 [1, 4, 1, 1, 64] @ ABCD64b]
  float* buffer_325 = (float*)&__rescheduled_0[111616UL];
  reorder__508(buffer_325, &res4d_bias_0[0]);
  // [f32 [1, 4, 1, 1, 64] @ ABCD64b]
  float* buffer_326 = (float*)&__rescheduled_0[112640UL];
  reorder__511(buffer_326, &res4d_bias_1[0]);
  // [f32 [1, 8, 1, 1, 32] @ ABCD32b]
  float* buffer_327 = (float*)&__rescheduled_0[113664UL];
  reorder__518(buffer_327, &res4e_bias_1[0]);
  // [f32 [1, 8, 1, 1, 32] @ ABCD32b]
  float* buffer_328 = (float*)&__rescheduled_0[114688UL];
  reorder__524(buffer_328, &res4f_bias_0[0]);
  // [f32 [1, 4, 1, 1, 64] @ ABCD64b]
  float* buffer_329 = (float*)&__rescheduled_0[115712UL];
  reorder__527(buffer_329, &res4f_bias_1[0]);
  // [f32 [1, 16, 1, 1, 32] @ ABCD32b]
  float* buffer_330 = (float*)&__rescheduled_0[116736UL];
  reorder__447(buffer_330, &res3a_bias_b[0]);
  // [f32 [1, 16, 1, 1, 32] @ ABCD32b]
  float* buffer_331 = (float*)&__rescheduled_0[118784UL];
  reorder__456(buffer_331, &res3a_bias_2[0]);
  // [f32 [1, 4, 1, 1, 128] @ ABCD128b]
  float* buffer_332 = (float*)&__rescheduled_0[120832UL];
  reorder__463(buffer_332, &res3b_bias_2[0]);
  // [f32 [1, 8, 1, 1, 64] @ ABCD64b]
  float* buffer_333 = (float*)&__rescheduled_0[122880UL];
  reorder__470(buffer_333, &res3c_bias_2[0]);
  // [f32 [1, 4, 1, 1, 128] @ ABCD128b]
  float* buffer_334 = (float*)&__rescheduled_0[124928UL];
  reorder__479(buffer_334, &res3d_bias_2[0]);
  // [f32 [1, 8, 1, 1, 64] @ ABCD64b]
  float* buffer_335 = (float*)&__rescheduled_0[126976UL];
  reorder__536(buffer_335, &res5a_bias_0[0]);
  // [f32 [1, 4, 1, 1, 128] @ ABCD128b]
  float* buffer_336 = (float*)&__rescheduled_0[129024UL];
  reorder__539(buffer_336, &res5a_bias_1[0]);
  // [f32 [1, 4, 1, 1, 128] @ ABCD128b]
  float* buffer_337 = (float*)&__rescheduled_0[131072UL];
  reorder__545(buffer_337, &res5b_bias_0[0]);
  // [f32 [1, 32, 1, 1, 16] @ ABCD16b]
  float* buffer_338 = (float*)&__rescheduled_0[133120UL];
  reorder__548(buffer_338, &res5b_bias_1[0]);
  // [f32 [1, 16, 1, 1, 32] @ ABCD32b]
  float* buffer_339 = (float*)&__rescheduled_0[135168UL];
  reorder__554(buffer_339, &res5c_bias_0[0]);
  // [f32 [1, 4, 1, 1, 128] @ ABCD128b]
  float* buffer_340 = (float*)&__rescheduled_0[137216UL];
  reorder__557(buffer_340, &res5c_bias_1[0]);
  // [f32 [1, 32, 1, 1, 32] @ ABCD32b]
  float* buffer_341 = (float*)&__rescheduled_0[139264UL];
  reorder__482(buffer_341, &res4a_bias_b[0]);
  // [f32 [1, 8, 1, 1, 128] @ ABCD128b]
  float* buffer_342 = (float*)&__rescheduled_0[143360UL];
  reorder__489(buffer_342, &res4a_bias_2[0]);
  // [f32 [1, 8, 1, 1, 128] @ ABCD128b]
  float* buffer_343 = (float*)&__rescheduled_0[147456UL];
  reorder__505(buffer_343, &res4c_bias_2[0]);
  // [f32 [1, 8, 1, 1, 128] @ ABCD128b]
  float* buffer_344 = (float*)&__rescheduled_0[151552UL];
  reorder__514(buffer_344, &res4d_bias_2[0]);
  // [f32 [1, 8, 1, 1, 128] @ ABCD128b]
  float* buffer_345 = (float*)&__rescheduled_0[155648UL];
  reorder__521(buffer_345, &res4e_bias_2[0]);
  // [f32 [1, 4, 1, 1, 256] @ ABCD256b]
  float* buffer_346 = (float*)&__rescheduled_0[159744UL];
  reorder__530(buffer_346, &res4f_bias_2[0]);
  // [f32 [1, 128, 1, 1, 16] @ ABCD16b]
  float* buffer_347 = (float*)&__rescheduled_0[163840UL];
  reorder__533(buffer_347, &res5a_bias_b[0]);
  // [f32 [1, 32, 1, 1, 64] @ ABCD64b]
  float* buffer_348 = (float*)&__rescheduled_0[172032UL];
  reorder__542(buffer_348, &res5a_bias_2[0]);
  // [f32 [1, 32, 1, 1, 64] @ ABCD64b]
  float* buffer_349 = (float*)&__rescheduled_0[180224UL];
  reorder__551(buffer_349, &res5b_bias_2[0]);
  // [f32 [1, 4, 1, 1, 512] @ ABCD512b]
  float* buffer_350 = (float*)&__rescheduled_0[188416UL];
  reorder__560(buffer_350, &res5c_bias_2[0]);
  // [f32 [64, 64, 1, 1] @ ABCD]
  float* buffer_351 = (float*)&__rescheduled_0[196608UL];
  mul__111(buffer_351, res2a_weight_0, folded_const_154);
  // [s8 [64, 64, 1, 1] @ ABCD]
  int8_t* buffer_352 = (int8_t*)&__rescheduled_0[2555904UL];
  cast__112(buffer_352, buffer_351);
  // [f32 [256, 64, 1, 1] @ ABCD]
  float* buffer_353 = (float*)&__rescheduled_0[196608UL];
  mul__108(buffer_353, res2a_weight_b, folded_const_155);
  // [s8 [256, 64, 1, 1] @ ABCD]
  int8_t* buffer_354 = (int8_t*)&__rescheduled_0[2560000UL];
  cast__109(buffer_354, buffer_353);
  // [f32 [256, 64, 1, 1] @ ABCD]
  float* buffer_355 = (float*)&__rescheduled_0[196608UL];
  mul__117(buffer_355, res2a_weight_2, folded_const_152);
  // [s8 [256, 64, 1, 1] @ ABCD]
  int8_t* buffer_356 = (int8_t*)&__rescheduled_0[2576384UL];
  cast__118(buffer_356, buffer_355);
  // [f32 [256, 64, 1, 1] @ ABCD]
  float* buffer_357 = (float*)&__rescheduled_0[196608UL];
  mul__126(buffer_357, res2b_weight_2, folded_const_149);
  // [s8 [256, 64, 1, 1] @ ABCD]
  int8_t* buffer_358 = (int8_t*)&__rescheduled_0[2592768UL];
  cast__127(buffer_358, buffer_357);
  // [f32 [256, 64, 1, 1] @ ABCD]
  float* buffer_359 = (float*)&__rescheduled_0[196608UL];
  mul__135(buffer_359, res2c_weight_2, folded_const_146);
  // [s8 [256, 64, 1, 1] @ ABCD]
  int8_t* buffer_360 = (int8_t*)&__rescheduled_0[2609152UL];
  cast__136(buffer_360, buffer_359);
  // [f32 [64, 256, 1, 1] @ ABCD]
  float* buffer_361 = (float*)&__rescheduled_0[196608UL];
  mul__120(buffer_361, res2b_weight_0, folded_const_151);
  // [s8 [64, 256, 1, 1] @ ABCD]
  int8_t* buffer_362 = (int8_t*)&__rescheduled_0[2625536UL];
  cast__121(buffer_362, buffer_361);
  // [f32 [64, 256, 1, 1] @ ABCD]
  float* buffer_363 = (float*)&__rescheduled_0[196608UL];
  mul__129(buffer_363, res2c_weight_0, folded_const_148);
  // [s8 [64, 256, 1, 1] @ ABCD]
  int8_t* buffer_364 = (int8_t*)&__rescheduled_0[2641920UL];
  cast__130(buffer_364, buffer_363);
  // [f32 [128, 256, 1, 1] @ ABCD]
  float* buffer_365 = (float*)&__rescheduled_0[196608UL];
  mul__141(buffer_365, res3a_weight_0, folded_const_144);
  // [s8 [128, 256, 1, 1] @ ABCD]
  int8_t* buffer_366 = (int8_t*)&__rescheduled_0[2658304UL];
  cast__142(buffer_366, buffer_365);
  // [f32 [64, 64, 3, 3] @ ABCD]
  float* buffer_367 = (float*)&__rescheduled_0[196608UL];
  mul__114(buffer_367, res2a_weight_1, folded_const_153);
  // [s8 [64, 64, 3, 3] @ ABCD]
  int8_t* buffer_368 = (int8_t*)&__rescheduled_0[2691072UL];
  cast__115(buffer_368, buffer_367);
  // [f32 [64, 64, 3, 3] @ ABCD]
  float* buffer_369 = (float*)&__rescheduled_0[196608UL];
  mul__123(buffer_369, res2b_weight_1, folded_const_150);
  // [s8 [64, 64, 3, 3] @ ABCD]
  int8_t* buffer_370 = (int8_t*)&__rescheduled_0[2727936UL];
  cast__124(buffer_370, buffer_369);
  // [f32 [64, 64, 3, 3] @ ABCD]
  float* buffer_371 = (float*)&__rescheduled_0[196608UL];
  mul__132(buffer_371, res2c_weight_1, folded_const_147);
  // [s8 [64, 64, 3, 3] @ ABCD]
  int8_t* buffer_372 = (int8_t*)&__rescheduled_0[2764800UL];
  cast__133(buffer_372, buffer_371);
  // [f32 [512, 128, 1, 1] @ ABCD]
  float* buffer_373 = (float*)&__rescheduled_0[196608UL];
  mul__147(buffer_373, res3a_weight_2, folded_const_142);
  // [s8 [512, 128, 1, 1] @ ABCD]
  int8_t* buffer_374 = (int8_t*)&__rescheduled_0[2801664UL];
  cast__148(buffer_374, buffer_373);
  // [f32 [512, 128, 1, 1] @ ABCD]
  float* buffer_375 = (float*)&__rescheduled_0[196608UL];
  mul__156(buffer_375, res3b_weight_2, folded_const_139);
  // [s8 [512, 128, 1, 1] @ ABCD]
  int8_t* buffer_376 = (int8_t*)&__rescheduled_0[2867200UL];
  cast__157(buffer_376, buffer_375);
  // [f32 [512, 128, 1, 1] @ ABCD]
  float* buffer_377 = (float*)&__rescheduled_0[196608UL];
  mul__165(buffer_377, res3c_weight_2, folded_const_136);
  // [s8 [512, 128, 1, 1] @ ABCD]
  int8_t* buffer_378 = (int8_t*)&__rescheduled_0[2932736UL];
  cast__166(buffer_378, buffer_377);
  // [f32 [512, 128, 1, 1] @ ABCD]
  float* buffer_379 = (float*)&__rescheduled_0[196608UL];
  mul__174(buffer_379, res3d_weight_2, folded_const_133);
  // [s8 [512, 128, 1, 1] @ ABCD]
  int8_t* buffer_380 = (int8_t*)&__rescheduled_0[2998272UL];
  cast__175(buffer_380, buffer_379);
  // [f32 [128, 512, 1, 1] @ ABCD]
  float* buffer_381 = (float*)&__rescheduled_0[196608UL];
  mul__150(buffer_381, res3b_weight_0, folded_const_141);
  // [s8 [128, 512, 1, 1] @ ABCD]
  int8_t* buffer_382 = (int8_t*)&__rescheduled_0[3063808UL];
  cast__151(buffer_382, buffer_381);
  // [f32 [128, 512, 1, 1] @ ABCD]
  float* buffer_383 = (float*)&__rescheduled_0[196608UL];
  mul__159(buffer_383, res3c_weight_0, folded_const_138);
  // [s8 [128, 512, 1, 1] @ ABCD]
  int8_t* buffer_384 = (int8_t*)&__rescheduled_0[3129344UL];
  cast__160(buffer_384, buffer_383);
  // [f32 [128, 512, 1, 1] @ ABCD]
  float* buffer_385 = (float*)&__rescheduled_0[196608UL];
  mul__168(buffer_385, res3d_weight_0, folded_const_135);
  // [s8 [128, 512, 1, 1] @ ABCD]
  int8_t* buffer_386 = (int8_t*)&__rescheduled_0[3194880UL];
  cast__169(buffer_386, buffer_385);
  // [f32 [512, 256, 1, 1] @ ABCD]
  float* buffer_387 = (float*)&__rescheduled_0[196608UL];
  mul__138(buffer_387, res3a_weight_b, folded_const_145);
  // [s8 [512, 256, 1, 1] @ ABCD]
  int8_t* buffer_388 = (int8_t*)&__rescheduled_0[3260416UL];
  cast__139(buffer_388, buffer_387);
  // [f32 [256, 512, 1, 1] @ ABCD]
  float* buffer_389 = (float*)&__rescheduled_0[196608UL];
  mul__180(buffer_389, res4a_weight_0, folded_const_131);
  // [s8 [256, 512, 1, 1] @ ABCD]
  int8_t* buffer_390 = (int8_t*)&__rescheduled_0[3391488UL];
  cast__181(buffer_390, buffer_389);
  // [f32 [128, 128, 3, 3] @ ABCD]
  float* buffer_391 = (float*)&__rescheduled_0[196608UL];
  mul__144(buffer_391, res3a_weight_1, folded_const_143);
  // [s8 [128, 128, 3, 3] @ ABCD]
  int8_t* buffer_392 = (int8_t*)&__rescheduled_0[3522560UL];
  cast__145(buffer_392, buffer_391);
  // [f32 [128, 128, 3, 3] @ ABCD]
  float* buffer_393 = (float*)&__rescheduled_0[196608UL];
  mul__153(buffer_393, res3b_weight_1, folded_const_140);
  // [s8 [128, 128, 3, 3] @ ABCD]
  int8_t* buffer_394 = (int8_t*)&__rescheduled_0[3670016UL];
  cast__154(buffer_394, buffer_393);
  // [f32 [128, 128, 3, 3] @ ABCD]
  float* buffer_395 = (float*)&__rescheduled_0[196608UL];
  mul__162(buffer_395, res3c_weight_1, folded_const_137);
  // [s8 [128, 128, 3, 3] @ ABCD]
  int8_t* buffer_396 = (int8_t*)&__rescheduled_0[3817472UL];
  cast__163(buffer_396, buffer_395);
  // [f32 [128, 128, 3, 3] @ ABCD]
  float* buffer_397 = (float*)&__rescheduled_0[196608UL];
  mul__171(buffer_397, res3d_weight_1, folded_const_134);
  // [s8 [128, 128, 3, 3] @ ABCD]
  int8_t* buffer_398 = (int8_t*)&__rescheduled_0[3964928UL];
  cast__172(buffer_398, buffer_397);
  // [f32 [1024, 256, 1, 1] @ ABCD]
  float* buffer_399 = (float*)&__rescheduled_0[196608UL];
  mul__186(buffer_399, res4a_weight_2, folded_const_129);
  // [s8 [1024, 256, 1, 1] @ ABCD]
  int8_t* buffer_400 = (int8_t*)&__rescheduled_0[4112384UL];
  cast__187(buffer_400, buffer_399);
  // [f32 [1024, 256, 1, 1] @ ABCD]
  float* buffer_401 = (float*)&__rescheduled_0[196608UL];
  mul__195(buffer_401, res4b_weight_2, folded_const_126);
  // [s8 [1024, 256, 1, 1] @ ABCD]
  int8_t* buffer_402 = (int8_t*)&__rescheduled_0[4374528UL];
  cast__196(buffer_402, buffer_401);
  // [f32 [1024, 256, 1, 1] @ ABCD]
  float* buffer_403 = (float*)&__rescheduled_0[196608UL];
  mul__204(buffer_403, res4c_weight_2, folded_const_123);
  // [s8 [1024, 256, 1, 1] @ ABCD]
  int8_t* buffer_404 = (int8_t*)&__rescheduled_0[4636672UL];
  cast__205(buffer_404, buffer_403);
  // [f32 [1024, 256, 1, 1] @ ABCD]
  float* buffer_405 = (float*)&__rescheduled_0[196608UL];
  mul__213(buffer_405, res4d_weight_2, folded_const_120);
  // [s8 [1024, 256, 1, 1] @ ABCD]
  int8_t* buffer_406 = (int8_t*)&__rescheduled_0[4898816UL];
  cast__214(buffer_406, buffer_405);
  // [f32 [1024, 256, 1, 1] @ ABCD]
  float* buffer_407 = (float*)&__rescheduled_0[196608UL];
  mul__222(buffer_407, res4e_weight_2, folded_const_117);
  // [s8 [1024, 256, 1, 1] @ ABCD]
  int8_t* buffer_408 = (int8_t*)&__rescheduled_0[5160960UL];
  cast__223(buffer_408, buffer_407);
  // [f32 [1024, 256, 1, 1] @ ABCD]
  float* buffer_409 = (float*)&__rescheduled_0[196608UL];
  mul__231(buffer_409, res4f_weight_2, folded_const_114);
  // [s8 [1024, 256, 1, 1] @ ABCD]
  int8_t* buffer_410 = (int8_t*)&__rescheduled_0[5423104UL];
  cast__232(buffer_410, buffer_409);
  // [f32 [256, 1024, 1, 1] @ ABCD]
  float* buffer_411 = (float*)&__rescheduled_0[196608UL];
  mul__189(buffer_411, res4b_weight_0, folded_const_128);
  // [s8 [256, 1024, 1, 1] @ ABCD]
  int8_t* buffer_412 = (int8_t*)&__rescheduled_0[5685248UL];
  cast__190(buffer_412, buffer_411);
  // [f32 [256, 1024, 1, 1] @ ABCD]
  float* buffer_413 = (float*)&__rescheduled_0[196608UL];
  mul__198(buffer_413, res4c_weight_0, folded_const_125);
  // [s8 [256, 1024, 1, 1] @ ABCD]
  int8_t* buffer_414 = (int8_t*)&__rescheduled_0[5947392UL];
  cast__199(buffer_414, buffer_413);
  // [f32 [256, 1024, 1, 1] @ ABCD]
  float* buffer_415 = (float*)&__rescheduled_0[196608UL];
  mul__207(buffer_415, res4d_weight_0, folded_const_122);
  // [s8 [256, 1024, 1, 1] @ ABCD]
  int8_t* buffer_416 = (int8_t*)&__rescheduled_0[6209536UL];
  cast__208(buffer_416, buffer_415);
  // [f32 [256, 1024, 1, 1] @ ABCD]
  float* buffer_417 = (float*)&__rescheduled_0[196608UL];
  mul__216(buffer_417, res4e_weight_0, folded_const_119);
  // [s8 [256, 1024, 1, 1] @ ABCD]
  int8_t* buffer_418 = (int8_t*)&__rescheduled_0[6471680UL];
  cast__217(buffer_418, buffer_417);
  // [f32 [256, 1024, 1, 1] @ ABCD]
  float* buffer_419 = (float*)&__rescheduled_0[196608UL];
  mul__225(buffer_419, res4f_weight_0, folded_const_116);
  // [s8 [256, 1024, 1, 1] @ ABCD]
  int8_t* buffer_420 = (int8_t*)&__rescheduled_0[6733824UL];
  cast__226(buffer_420, buffer_419);
  // [f32 [1024, 512, 1, 1] @ ABCD]
  float* buffer_421 = (float*)&__rescheduled_0[196608UL];
  mul__177(buffer_421, res4a_weight_b, folded_const_132);
  // [s8 [1024, 512, 1, 1] @ ABCD]
  int8_t* buffer_422 = (int8_t*)&__rescheduled_0[6995968UL];
  cast__178(buffer_422, buffer_421);
  // [f32 [512, 1024, 1, 1] @ ABCD]
  float* buffer_423 = (float*)&__rescheduled_0[196608UL];
  mul__237(buffer_423, res5a_weight_0, folded_const_112);
  // [s8 [512, 1024, 1, 1] @ ABCD]
  int8_t* buffer_424 = (int8_t*)&__rescheduled_0[7520256UL];
  cast__238(buffer_424, buffer_423);
  // [f32 [256, 256, 3, 3] @ ABCD]
  float* buffer_425 = (float*)&__rescheduled_0[196608UL];
  mul__183(buffer_425, res4a_weight_1, folded_const_130);
  // [s8 [256, 256, 3, 3] @ ABCD]
  int8_t* buffer_426 = (int8_t*)&__rescheduled_0[8044544UL];
  cast__184(buffer_426, buffer_425);
  // [f32 [256, 256, 3, 3] @ ABCD]
  float* buffer_427 = (float*)&__rescheduled_0[196608UL];
  mul__192(buffer_427, res4b_weight_1, folded_const_127);
  // [s8 [256, 256, 3, 3] @ ABCD]
  int8_t* buffer_428 = (int8_t*)&__rescheduled_0[8634368UL];
  cast__193(buffer_428, buffer_427);
  // [f32 [256, 256, 3, 3] @ ABCD]
  float* buffer_429 = (float*)&__rescheduled_0[196608UL];
  mul__201(buffer_429, res4c_weight_1, folded_const_124);
  // [s8 [256, 256, 3, 3] @ ABCD]
  int8_t* buffer_430 = (int8_t*)&__rescheduled_0[9224192UL];
  cast__202(buffer_430, buffer_429);
  // [f32 [256, 256, 3, 3] @ ABCD]
  float* buffer_431 = (float*)&__rescheduled_0[196608UL];
  mul__210(buffer_431, res4d_weight_1, folded_const_121);
  // [s8 [256, 256, 3, 3] @ ABCD]
  int8_t* buffer_432 = (int8_t*)&__rescheduled_0[9814016UL];
  cast__211(buffer_432, buffer_431);
  // [f32 [256, 256, 3, 3] @ ABCD]
  float* buffer_433 = (float*)&__rescheduled_0[196608UL];
  mul__219(buffer_433, res4e_weight_1, folded_const_118);
  // [s8 [256, 256, 3, 3] @ ABCD]
  int8_t* buffer_434 = (int8_t*)&__rescheduled_0[10403840UL];
  cast__220(buffer_434, buffer_433);
  // [f32 [256, 256, 3, 3] @ ABCD]
  float* buffer_435 = (float*)&__rescheduled_0[196608UL];
  mul__228(buffer_435, res4f_weight_1, folded_const_115);
  // [s8 [256, 256, 3, 3] @ ABCD]
  int8_t* buffer_436 = (int8_t*)&__rescheduled_0[10993664UL];
  cast__229(buffer_436, buffer_435);
  reorder__525(folded_const_156, buffer_436);
  mul__652(folded_const_157, buffer_346, folded_const_86);
  mul__651(folded_const_158, buffer_266, folded_const_86);
  mul__646(folded_const_159, buffer_345, folded_const_87);
  mul__645(folded_const_160, buffer_265, folded_const_87);
  mul__640(folded_const_161, buffer_344, folded_const_88);
  mul__639(folded_const_162, buffer_264, folded_const_88);
  mul__634(folded_const_163, buffer_343, folded_const_89);
  mul__633(folded_const_164, buffer_263, folded_const_89);
  mul__628(folded_const_165, &res4b_bias_2[0], folded_const_90);
  mul__627(folded_const_166, &folded_const_36[0UL], folded_const_90);
  mul__622(folded_const_167, buffer_342, folded_const_91);
  mul__621(folded_const_168, buffer_262, folded_const_91);
  mul__616(folded_const_169, buffer_341, folded_const_92);
  mul__615(folded_const_170, buffer_261, folded_const_92);
  mul__614(folded_const_171, buffer_334, folded_const_93);
  mul__613(folded_const_172, buffer_285, folded_const_93);
  mul__608(folded_const_173, buffer_333, folded_const_94);
  mul__607(folded_const_174, buffer_282, folded_const_94);
  mul__602(folded_const_175, buffer_332, folded_const_95);
  mul__601(folded_const_176, buffer_280, folded_const_95);
  mul__596(folded_const_177, buffer_331, folded_const_96);
  mul__595(folded_const_178, buffer_278, folded_const_96);
  mul__590(folded_const_179, buffer_330, folded_const_97);
  mul__589(folded_const_180, buffer_275, folded_const_97);
  mul__650(folded_const_181, buffer_329, folded_const_17);
  mul__649(folded_const_182, buffer_295, folded_const_17);
  mul__648(folded_const_183, buffer_328, folded_const_19);
  mul__647(folded_const_184, buffer_294, folded_const_19);
  mul__644(folded_const_185, buffer_327, folded_const_22);
  mul__643(folded_const_186, buffer_293, folded_const_22);
  mul__642(folded_const_187, &res4e_bias_0[0], folded_const_24);
  mul__641(folded_const_188, &folded_const_25[0UL], folded_const_24);
  mul__638(folded_const_189, buffer_326, folded_const_27);
  mul__637(folded_const_190, buffer_292, folded_const_27);
  mul__636(folded_const_191, buffer_325, folded_const_29);
  mul__635(folded_const_192, buffer_291, folded_const_29);
  mul__632(folded_const_193, buffer_324, folded_const_32);
  mul__631(folded_const_194, buffer_290, folded_const_32);
  mul__630(folded_const_195, buffer_323, folded_const_34);
  mul__629(folded_const_196, buffer_289, folded_const_34);
  mul__626(folded_const_197, buffer_322, folded_const_37);
  mul__625(folded_const_198, buffer_288, folded_const_37);
  mul__624(folded_const_199, buffer_321, folded_const_39);
  mul__623(folded_const_200, buffer_287, folded_const_39);
  mul__620(folded_const_201, buffer_320, folded_const_42);
  mul__619(folded_const_202, buffer_286, folded_const_42);
  mul__618(folded_const_203, &res4a_bias_0[0], folded_const_44);
  mul__617(folded_const_204, &folded_const_45[0UL], folded_const_44);
  mul__588(folded_const_205, buffer_319, folded_const_98);
  mul__587(folded_const_206, buffer_274, folded_const_98);
  mul__582(folded_const_207, buffer_318, folded_const_99);
  mul__581(folded_const_208, buffer_271, folded_const_99);
  mul__576(folded_const_209, buffer_317, folded_const_100);
  mul__575(folded_const_210, buffer_269, folded_const_100);
  mul__570(folded_const_211, buffer_316, folded_const_101);
  mul__569(folded_const_212, buffer_267, folded_const_101);
  reorder__522(folded_const_213, buffer_420);
  reorder__515(folded_const_214, buffer_418);
  reorder__506(folded_const_215, buffer_416);
  reorder__497(folded_const_216, buffer_414);
  reorder__490(folded_const_217, buffer_412);
  reorder__528(folded_const_218, buffer_410);
  reorder__519(folded_const_219, buffer_408);
  reorder__512(folded_const_220, buffer_406);
  reorder__503(folded_const_221, buffer_404);
  reorder__496(folded_const_222, buffer_402);
  reorder__487(folded_const_223, buffer_400);
  reorder__422(folded_const_224, buffer_352);
  mul__612(folded_const_225, buffer_315, folded_const_48);
  mul__611(folded_const_226, buffer_284, folded_const_48);
  mul__610(folded_const_227, buffer_314, folded_const_50);
  mul__609(folded_const_228, buffer_283, folded_const_50);
  mul__606(folded_const_229, buffer_313, folded_const_53);
  mul__605(folded_const_230, buffer_281, folded_const_53);
  mul__604(folded_const_231, &res3c_bias_0[0], folded_const_55);
  mul__603(folded_const_232, &folded_const_56[0UL], folded_const_55);
  mul__600(folded_const_233, &res3b_bias_1[0], folded_const_58);
  mul__599(folded_const_234, &folded_const_59[0UL], folded_const_58);
  mul__598(folded_const_235, buffer_312, folded_const_60);
  mul__597(folded_const_236, buffer_279, folded_const_60);
  mul__594(folded_const_237, buffer_311, folded_const_63);
  mul__593(folded_const_238, buffer_277, folded_const_63);
  mul__592(folded_const_239, buffer_310, folded_const_65);
  mul__591(folded_const_240, buffer_276, folded_const_65);
  mul__586(folded_const_241, buffer_309, folded_const_69);
  mul__585(folded_const_242, buffer_273, folded_const_69);
  mul__584(folded_const_243, buffer_308, folded_const_71);
  mul__583(folded_const_244, buffer_272, folded_const_71);
  mul__580(folded_const_245, buffer_307, folded_const_74);
  mul__579(folded_const_246, buffer_270, folded_const_74);
  mul__578(folded_const_247, &res2b_bias_0[0], folded_const_76);
  mul__577(folded_const_248, &folded_const_77[0UL], folded_const_76);
  mul__574(folded_const_249, buffer_306, folded_const_79);
  mul__573(folded_const_250, buffer_268, folded_const_79);
  mul__572(folded_const_251, &res2a_bias_0[0], folded_const_81);
  mul__571(folded_const_252, &folded_const_102[0UL], folded_const_81);
  reorder__516(folded_const_253, buffer_434);
  reorder__509(folded_const_254, buffer_432);
  reorder__500(folded_const_255, buffer_430);
  reorder__493(folded_const_256, buffer_428);
  reorder__484(folded_const_257, buffer_426);
  reorder__480(folded_const_258, buffer_422);
  reorder__474(folded_const_259, buffer_398);
  reorder__465(folded_const_260, buffer_396);
  reorder__460(folded_const_261, buffer_394);
  reorder__451(folded_const_262, buffer_392);
  reorder__483(folded_const_263, buffer_390);
  reorder__445(folded_const_264, buffer_388);
  reorder__471(folded_const_265, buffer_386);
  reorder__464(folded_const_266, buffer_384);
  reorder__457(folded_const_267, buffer_382);
  reorder__477(folded_const_268, buffer_380);
  reorder__468(folded_const_269, buffer_378);
  reorder__461(folded_const_270, buffer_376);
  reorder__454(folded_const_271, buffer_374);
  reorder__439(folded_const_272, buffer_372);
  reorder__430(folded_const_273, buffer_370);
  reorder__423(folded_const_274, buffer_368);
  reorder__448(folded_const_275, buffer_366);
  reorder__436(folded_const_276, buffer_364);
  reorder__429(folded_const_277, buffer_362);
  reorder__442(folded_const_278, buffer_360);
  reorder__433(folded_const_279, buffer_358);
  reorder__426(folded_const_280, buffer_356);
  reorder__419(folded_const_281, buffer_354);
  mul__656(folded_const_282, buffer_335, folded_const_13);
  mul__655(folded_const_283, buffer_297, folded_const_13);
  reorder__534(folded_const_284, buffer_424);
  // [f32 [2048, 512, 1, 1] @ ABCD]
  float* buffer_566 = (float*)&__rescheduled_0[196608UL];
  mul__243(buffer_566, res5a_weight_2, folded_const_110);
  // [s8 [2048, 512, 1, 1] @ ABCD]
  int8_t* buffer_567 = (int8_t*)&__rescheduled_0[9633792UL];
  cast__244(buffer_567, buffer_566);
  // [f32 [2048, 512, 1, 1] @ ABCD]
  float* buffer_568 = (float*)&__rescheduled_0[196608UL];
  mul__252(buffer_568, res5b_weight_2, folded_const_107);
  // [s8 [2048, 512, 1, 1] @ ABCD]
  int8_t* buffer_569 = (int8_t*)&__rescheduled_0[11993088UL];
  cast__253(buffer_569, buffer_568);
  // [f32 [2048, 512, 1, 1] @ ABCD]
  float* buffer_570 = (float*)&__rescheduled_0[196608UL];
  mul__261(buffer_570, res5c_weight_2, folded_const_104);
  // [s8 [2048, 512, 1, 1] @ ABCD]
  int8_t* buffer_571 = (int8_t*)&__rescheduled_0[13041664UL];
  cast__262(buffer_571, buffer_570);
  // [f32 [512, 2048, 1, 1] @ ABCD]
  float* buffer_572 = (float*)&__rescheduled_0[196608UL];
  mul__246(buffer_572, res5b_weight_0, folded_const_109);
  // [s8 [512, 2048, 1, 1] @ ABCD]
  int8_t* buffer_573 = (int8_t*)&__rescheduled_0[14090240UL];
  cast__247(buffer_573, buffer_572);
  // [f32 [512, 2048, 1, 1] @ ABCD]
  float* buffer_574 = (float*)&__rescheduled_0[196608UL];
  mul__255(buffer_574, res5c_weight_0, folded_const_106);
  // [s8 [512, 2048, 1, 1] @ ABCD]
  int8_t* buffer_575 = (int8_t*)&__rescheduled_0[15138816UL];
  cast__256(buffer_575, buffer_574);
  // [f32 [2048, 1024, 1, 1] @ ABCD]
  float* buffer_576 = (float*)&__rescheduled_0[196608UL];
  mul__234(buffer_576, res5a_weight_b, folded_const_113);
  // [s8 [2048, 1024, 1, 1] @ ABCD]
  int8_t* buffer_577 = (int8_t*)&__rescheduled_0[16187392UL];
  cast__235(buffer_577, buffer_576);
  reorder__531(folded_const_285, buffer_577);
  mul__654(folded_const_286, buffer_347, folded_const_85);
  mul__653(folded_const_287, buffer_296, folded_const_85);
  // [f32 [512, 512, 3, 3] @ ABCD]
  float* buffer_582 = (float*)&__rescheduled_0[196608UL];
  mul__240(buffer_582, res5a_weight_1, folded_const_111);
  // [s8 [512, 512, 3, 3] @ ABCD]
  int8_t* buffer_583 = (int8_t*)&__rescheduled_0[16187392UL];
  cast__241(buffer_583, buffer_582);
  reorder__537(folded_const_288, buffer_583);
  mul__658(folded_const_289, buffer_336, folded_const_11);
  mul__657(folded_const_290, buffer_298, folded_const_11);
  mul__660(folded_const_291, buffer_348, folded_const_84);
  mul__659(folded_const_292, buffer_299, folded_const_84);
  reorder__540(folded_const_293, buffer_567);
  mul__662(folded_const_294, buffer_337, folded_const_8);
  mul__661(folded_const_295, buffer_300, folded_const_8);
  reorder__543(folded_const_296, buffer_573);
  // [f32 [512, 512, 3, 3] @ ABCD]
  float* buffer_596 = (float*)&__rescheduled_0[196608UL];
  mul__249(buffer_596, res5b_weight_1, folded_const_108);
  // [s8 [512, 512, 3, 3] @ ABCD]
  int8_t* buffer_597 = (int8_t*)&__rescheduled_0[16187392UL];
  cast__250(buffer_597, buffer_596);
  // [f32 [512, 512, 3, 3] @ ABCD]
  float* buffer_598 = (float*)&__rescheduled_0[196608UL];
  mul__258(buffer_598, res5c_weight_1, folded_const_105);
  // [s8 [512, 512, 3, 3] @ ABCD]
  int8_t* buffer_599 = (int8_t*)&__rescheduled_0[9633792UL];
  cast__259(buffer_599, buffer_598);
  mul__664(folded_const_297, buffer_338, folded_const_6);
  mul__663(folded_const_298, buffer_301, folded_const_6);
  reorder__546(folded_const_299, buffer_597);
  mul__666(folded_const_300, buffer_349, folded_const_83);
  mul__665(folded_const_301, buffer_302, folded_const_83);
  reorder__549(folded_const_302, buffer_569);
  mul__668(folded_const_303, buffer_339, folded_const_3);
  mul__667(folded_const_304, buffer_303, folded_const_3);
  reorder__552(folded_const_305, buffer_575);
  mul__670(folded_const_306, buffer_340, folded_const_1);
  mul__669(folded_const_307, buffer_304, folded_const_1);
  reorder__555(folded_const_308, buffer_599);
  mul__672(folded_const_309, buffer_350, folded_const_82);
  mul__671(folded_const_310, buffer_305, folded_const_82);
  reorder__558(folded_const_311, buffer_571);
  is_init = true;
  sc_aligned_free(__stream, __rescheduled_0);
}

extern "C" void sc_init_rn50_backbone_bs4() {
  bool& is_init = *(bool*)(__module_data + 0);
  void*& __sc_kernel_cache = *(void**)(__module_data + 8);
  uint8_t* __brgemm_attrs = (uint8_t*)&__module_data[214464UL];
  void*& __sc_kernel_cache_104 = *(void**)(__module_data + 16);
  uint8_t* __brgemm_attrs_103 = (uint8_t*)&__module_data[214592UL];
  void*& __sc_kernel_cache_106 = *(void**)(__module_data + 24);
  uint8_t* __brgemm_attrs_105 = (uint8_t*)&__module_data[214720UL];
  void*& __sc_kernel_cache_108 = *(void**)(__module_data + 32);
  uint8_t* __brgemm_attrs_107 = (uint8_t*)&__module_data[214848UL];
  void*& __sc_kernel_cache_110 = *(void**)(__module_data + 40);
  uint8_t* __brgemm_attrs_109 = (uint8_t*)&__module_data[214976UL];
  void*& __sc_kernel_cache_112 = *(void**)(__module_data + 48);
  uint8_t* __brgemm_attrs_111 = (uint8_t*)&__module_data[215104UL];
  void*& __sc_kernel_cache_114 = *(void**)(__module_data + 56);
  uint8_t* __brgemm_attrs_113 = (uint8_t*)&__module_data[215232UL];
  void*& __sc_kernel_cache_116 = *(void**)(__module_data + 64);
  uint8_t* __brgemm_attrs_115 = (uint8_t*)&__module_data[215360UL];
  void*& __sc_kernel_cache_118 = *(void**)(__module_data + 72);
  uint8_t* __brgemm_attrs_117 = (uint8_t*)&__module_data[215488UL];
  void*& __sc_kernel_cache_120 = *(void**)(__module_data + 80);
  uint8_t* __brgemm_attrs_119 = (uint8_t*)&__module_data[215616UL];
  void*& __sc_kernel_cache_122 = *(void**)(__module_data + 88);
  uint8_t* __brgemm_attrs_121 = (uint8_t*)&__module_data[215744UL];
  void*& __sc_kernel_cache_124 = *(void**)(__module_data + 96);
  uint8_t* __brgemm_attrs_123 = (uint8_t*)&__module_data[215872UL];
  void*& __sc_kernel_cache_126 = *(void**)(__module_data + 104);
  uint8_t* __brgemm_attrs_125 = (uint8_t*)&__module_data[216000UL];
  void*& __sc_kernel_cache_128 = *(void**)(__module_data + 112);
  uint8_t* __brgemm_attrs_127 = (uint8_t*)&__module_data[216128UL];
  void*& __sc_kernel_cache_130 = *(void**)(__module_data + 120);
  uint8_t* __brgemm_attrs_129 = (uint8_t*)&__module_data[216256UL];
  void*& __sc_kernel_cache_133 = *(void**)(__module_data + 128);
  uint8_t* __brgemm_attrs_132 = (uint8_t*)&__module_data[217408UL];
  void*& __sc_kernel_cache_135 = *(void**)(__module_data + 136);
  uint8_t* __brgemm_attrs_134 = (uint8_t*)&__module_data[217536UL];
  void*& __sc_kernel_cache_137 = *(void**)(__module_data + 144);
  uint8_t* __brgemm_attrs_136 = (uint8_t*)&__module_data[217664UL];
  void*& __sc_kernel_cache_139 = *(void**)(__module_data + 152);
  uint8_t* __brgemm_attrs_138 = (uint8_t*)&__module_data[217792UL];
  void*& __sc_kernel_cache_141 = *(void**)(__module_data + 160);
  uint8_t* __brgemm_attrs_140 = (uint8_t*)&__module_data[217920UL];
  void*& __sc_kernel_cache_142 = *(void**)(__module_data + 168);
  void*& __sc_kernel_cache_144 = *(void**)(__module_data + 176);
  uint8_t* __brgemm_attrs_143 = (uint8_t*)&__module_data[218048UL];
  void*& __sc_kernel_cache_146 = *(void**)(__module_data + 184);
  uint8_t* __brgemm_attrs_145 = (uint8_t*)&__module_data[218176UL];
  void*& __sc_kernel_cache_152 = *(void**)(__module_data + 192);
  uint8_t* __brgemm_attrs_151 = (uint8_t*)&__module_data[218880UL];
  void*& __sc_kernel_cache_154 = *(void**)(__module_data + 200);
  uint8_t* __brgemm_attrs_153 = (uint8_t*)&__module_data[219008UL];
  void*& __sc_kernel_cache_156 = *(void**)(__module_data + 208);
  uint8_t* __brgemm_attrs_155 = (uint8_t*)&__module_data[219136UL];
  void*& __sc_kernel_cache_162 = *(void**)(__module_data + 216);
  uint8_t* __brgemm_attrs_161 = (uint8_t*)&__module_data[219648UL];
  void*& __sc_kernel_cache_164 = *(void**)(__module_data + 224);
  uint8_t* __brgemm_attrs_163 = (uint8_t*)&__module_data[219776UL];
  void*& __sc_kernel_cache_168 = *(void**)(__module_data + 232);
  uint8_t* __brgemm_attrs_167 = (uint8_t*)&__module_data[220032UL];
  void*& __sc_kernel_cache_170 = *(void**)(__module_data + 240);
  uint8_t* __brgemm_attrs_169 = (uint8_t*)&__module_data[220160UL];
  void*& __sc_kernel_cache_174 = *(void**)(__module_data + 248);
  uint8_t* __brgemm_attrs_173 = (uint8_t*)&__module_data[220416UL];
  void*& __sc_kernel_cache_176 = *(void**)(__module_data + 256);
  uint8_t* __brgemm_attrs_175 = (uint8_t*)&__module_data[220544UL];
  void*& __sc_kernel_cache_178 = *(void**)(__module_data + 264);
  uint8_t* __brgemm_attrs_177 = (uint8_t*)&__module_data[220672UL];
  void*& __sc_kernel_cache_180 = *(void**)(__module_data + 272);
  uint8_t* __brgemm_attrs_179 = (uint8_t*)&__module_data[220800UL];
  void*& __sc_kernel_cache_182 = *(void**)(__module_data + 280);
  uint8_t* __brgemm_attrs_181 = (uint8_t*)&__module_data[220928UL];
  void*& __sc_kernel_cache_188 = *(void**)(__module_data + 288);
  uint8_t* __brgemm_attrs_187 = (uint8_t*)&__module_data[221312UL];
  void*& __sc_kernel_cache_190 = *(void**)(__module_data + 296);
  uint8_t* __brgemm_attrs_189 = (uint8_t*)&__module_data[221440UL];
  void*& __sc_kernel_cache_192 = *(void**)(__module_data + 304);
  uint8_t* __brgemm_attrs_191 = (uint8_t*)&__module_data[221568UL];
  void*& __sc_kernel_cache_198 = *(void**)(__module_data + 312);
  uint8_t* __brgemm_attrs_197 = (uint8_t*)&__module_data[221888UL];
  void*& __sc_kernel_cache_200 = *(void**)(__module_data + 320);
  uint8_t* __brgemm_attrs_199 = (uint8_t*)&__module_data[222016UL];
  void*& __sc_kernel_cache_204 = *(void**)(__module_data + 328);
  uint8_t* __brgemm_attrs_203 = (uint8_t*)&__module_data[222272UL];
  void** __brgemm_bd_mask_arr = (void**)&__uninitialized_data[23657472UL];
  uint8_t* __brgemm_full_bd_mask = (uint8_t*)&__module_data[216512UL];
  void** __brgemm_bd_mask_arr_149 = (void**)&__uninitialized_data[23657504UL];
  uint8_t* __brgemm_full_bd_mask_148 = (uint8_t*)&__module_data[218432UL];
  void** __brgemm_bd_mask_arr_159 = (void**)&__uninitialized_data[23657520UL];
  uint8_t* __brgemm_full_bd_mask_158 = (uint8_t*)&__module_data[219392UL];
  void** __brgemm_bd_mask_arr_185 = (void**)&__uninitialized_data[23657552UL];
  uint8_t* __brgemm_full_bd_mask_184 = (uint8_t*)&__module_data[221184UL];
  void** __brgemm_bd_mask_arr_195 = (void**)&__uninitialized_data[23657568UL];
  uint8_t* __brgemm_full_bd_mask_194 = (uint8_t*)&__module_data[221816UL];
  void** __sc_kernel_cache_arr = (void**)&__uninitialized_data[23657488UL];
  uint8_t* __brgemm_attrs_131 = (uint8_t*)&__module_data[216384UL];
  void** __sc_kernel_cache_arr_150 = (void**)&__uninitialized_data[23657512UL];
  uint8_t* __brgemm_attrs_147 = (uint8_t*)&__module_data[218304UL];
  void** __sc_kernel_cache_arr_160 = (void**)&__uninitialized_data[23657528UL];
  uint8_t* __brgemm_attrs_157 = (uint8_t*)&__module_data[219264UL];
  void** __sc_kernel_cache_arr_166 = (void**)&__uninitialized_data[23657536UL];
  uint8_t* __brgemm_attrs_165 = (uint8_t*)&__module_data[219904UL];
  void** __sc_kernel_cache_arr_172 = (void**)&__uninitialized_data[23657544UL];
  uint8_t* __brgemm_attrs_171 = (uint8_t*)&__module_data[220288UL];
  void** __sc_kernel_cache_arr_186 = (void**)&__uninitialized_data[23657560UL];
  uint8_t* __brgemm_attrs_183 = (uint8_t*)&__module_data[221056UL];
  void** __sc_kernel_cache_arr_196 = (void**)&__uninitialized_data[23657576UL];
  uint8_t* __brgemm_attrs_193 = (uint8_t*)&__module_data[221696UL];
  void** __sc_kernel_cache_arr_202 = (void**)&__uninitialized_data[23657584UL];
  uint8_t* __brgemm_attrs_201 = (uint8_t*)&__module_data[222144UL];
  is_init = false;
  __sc_kernel_cache = dnnl_brgemm_list_func(112, 64, 64, 64, 64, 64, 0.f, 7, 7, __brgemm_attrs, ((void*)0), ((void*)0));
  __sc_kernel_cache_104 = dnnl_brgemm_list_func(14, 32, 64, 64, 32, 32, 0.f, 7, 7, __brgemm_attrs_103, ((void*)0), ((void*)0));
  __sc_kernel_cache_106 = dnnl_brgemm_list_func(112, 16, 64, 64, 16, 16, 0.f, 7, 7, __brgemm_attrs_105, ((void*)0), ((void*)0));
  __sc_kernel_cache_108 = dnnl_brgemm_list_func(56, 64, 32, 32, 64, 64, 0.f, 7, 7, __brgemm_attrs_107, ((void*)0), ((void*)0));
  __sc_kernel_cache_110 = dnnl_brgemm_list_func(392, 64, 32, 32, 64, 64, 0.f, 8, 7, __brgemm_attrs_109, ((void*)0), ((void*)0));
  __sc_kernel_cache_112 = dnnl_brgemm_list_func(28, 16, 64, 64, 16, 16, 0.f, 7, 7, __brgemm_attrs_111, ((void*)0), ((void*)0));
  __sc_kernel_cache_114 = dnnl_brgemm_list_func(56, 32, 64, 64, 32, 32, 0.f, 7, 7, __brgemm_attrs_113, ((void*)0), ((void*)0));
  __sc_kernel_cache_116 = dnnl_brgemm_list_func(56, 32, 64, 64, 32, 32, 0.f, 8, 7, __brgemm_attrs_115, ((void*)0), ((void*)0));
  __sc_kernel_cache_118 = dnnl_brgemm_list_func(28, 32, 64, 64, 32, 32, 0.f, 7, 7, __brgemm_attrs_117, ((void*)0), ((void*)0));
  __sc_kernel_cache_120 = dnnl_brgemm_list_func(56, 64, 64, 64, 64, 64, 0.f, 7, 7, __brgemm_attrs_119, ((void*)0), ((void*)0));
  __sc_kernel_cache_122 = dnnl_brgemm_list_func(112, 64, 128, 128, 64, 64, 0.f, 8, 7, __brgemm_attrs_121, ((void*)0), ((void*)0));
  __sc_kernel_cache_124 = dnnl_brgemm_list_func(28, 32, 128, 256, 32, 32, 0.f, 7, 7, __brgemm_attrs_123, ((void*)0), ((void*)0));
  __sc_kernel_cache_126 = dnnl_brgemm_list_func(196, 32, 128, 128, 32, 32, 0.f, 8, 7, __brgemm_attrs_125, ((void*)0), ((void*)0));
  __sc_kernel_cache_128 = dnnl_brgemm_list_func(392, 32, 32, 32, 32, 32, 0.f, 7, 7, __brgemm_attrs_127, ((void*)0), ((void*)0));
  __sc_kernel_cache_130 = dnnl_brgemm_list_func(112, 32, 128, 128, 32, 32, 0.f, 8, 7, __brgemm_attrs_129, ((void*)0), ((void*)0));
  __sc_kernel_cache_133 = dnnl_brgemm_list_func(28, 128, 64, 64, 128, 128, 0.f, 7, 7, __brgemm_attrs_132, ((void*)0), ((void*)0));
  __sc_kernel_cache_135 = dnnl_brgemm_list_func(784, 128, 64, 64, 128, 128, 0.f, 8, 7, __brgemm_attrs_134, ((void*)0), ((void*)0));
  __sc_kernel_cache_137 = dnnl_brgemm_list_func(28, 32, 64, 64, 32, 32, 0.f, 7, 7, __brgemm_attrs_136, ((void*)0), ((void*)0));
  __sc_kernel_cache_139 = dnnl_brgemm_list_func(56, 64, 128, 128, 64, 64, 0.f, 7, 7, __brgemm_attrs_138, ((void*)0), ((void*)0));
  __sc_kernel_cache_141 = dnnl_brgemm_list_func(56, 64, 128, 128, 64, 64, 0.f, 8, 7, __brgemm_attrs_140, ((void*)0), ((void*)0));
  __sc_kernel_cache_142 = dnnl_brgemm_list_func(28, 32, 128, 128, 32, 32, 0.f, 7, 7, __brgemm_attrs_123, ((void*)0), ((void*)0));
  __sc_kernel_cache_144 = dnnl_brgemm_list_func(196, 128, 32, 32, 128, 128, 0.f, 7, 7, __brgemm_attrs_143, ((void*)0), ((void*)0));
  __sc_kernel_cache_146 = dnnl_brgemm_list_func(56, 256, 128, 128, 256, 256, 0.f, 8, 7, __brgemm_attrs_145, ((void*)0), ((void*)0));
  __sc_kernel_cache_152 = dnnl_brgemm_list_func(98, 32, 128, 128, 32, 32, 0.f, 8, 7, __brgemm_attrs_151, ((void*)0), ((void*)0));
  __sc_kernel_cache_154 = dnnl_brgemm_list_func(14, 128, 256, 256, 128, 128, 0.f, 7, 7, __brgemm_attrs_153, ((void*)0), ((void*)0));
  __sc_kernel_cache_156 = dnnl_brgemm_list_func(28, 64, 1024, 1024, 64, 64, 0.f, 8, 7, __brgemm_attrs_155, ((void*)0), ((void*)0));
  __sc_kernel_cache_162 = dnnl_brgemm_list_func(98, 1024, 64, 64, 1024, 1024, 0.f, 7, 7, __brgemm_attrs_161, ((void*)0), ((void*)0));
  __sc_kernel_cache_164 = dnnl_brgemm_list_func(196, 32, 128, 128, 32, 32, 0.f, 8, 7, __brgemm_attrs_163, ((void*)0), ((void*)0));
  __sc_kernel_cache_168 = dnnl_brgemm_list_func(196, 128, 256, 256, 128, 128, 0.f, 7, 7, __brgemm_attrs_167, ((void*)0), ((void*)0));
  __sc_kernel_cache_170 = dnnl_brgemm_list_func(196, 64, 128, 128, 64, 64, 0.f, 8, 7, __brgemm_attrs_169, ((void*)0), ((void*)0));
  __sc_kernel_cache_174 = dnnl_brgemm_list_func(196, 128, 128, 128, 128, 128, 0.f, 7, 7, __brgemm_attrs_173, ((void*)0), ((void*)0));
  __sc_kernel_cache_176 = dnnl_brgemm_list_func(98, 256, 128, 128, 256, 256, 0.f, 8, 7, __brgemm_attrs_175, ((void*)0), ((void*)0));
  __sc_kernel_cache_178 = dnnl_brgemm_list_func(28, 32, 512, 512, 32, 32, 0.f, 8, 7, __brgemm_attrs_177, ((void*)0), ((void*)0));
  __sc_kernel_cache_180 = dnnl_brgemm_list_func(196, 256, 128, 128, 256, 256, 0.f, 7, 7, __brgemm_attrs_179, ((void*)0), ((void*)0));
  __sc_kernel_cache_182 = dnnl_brgemm_list_func(196, 64, 256, 256, 64, 64, 0.f, 8, 7, __brgemm_attrs_181, ((void*)0), ((void*)0));
  __sc_kernel_cache_188 = dnnl_brgemm_list_func(49, 16, 256, 256, 16, 16, 0.f, 8, 7, __brgemm_attrs_187, ((void*)0), ((void*)0));
  __sc_kernel_cache_190 = dnnl_brgemm_list_func(49, 64, 256, 256, 64, 64, 0.f, 7, 7, __brgemm_attrs_189, ((void*)0), ((void*)0));
  __sc_kernel_cache_192 = dnnl_brgemm_list_func(49, 128, 64, 64, 128, 128, 0.f, 8, 7, __brgemm_attrs_191, ((void*)0), ((void*)0));
  __sc_kernel_cache_198 = dnnl_brgemm_list_func(7, 64, 256, 256, 64, 64, 0.f, 7, 7, __brgemm_attrs_197, ((void*)0), ((void*)0));
  __sc_kernel_cache_200 = dnnl_brgemm_list_func(49, 32, 512, 512, 32, 32, 0.f, 8, 7, __brgemm_attrs_199, ((void*)0), ((void*)0));
  __sc_kernel_cache_204 = dnnl_brgemm_list_func(49, 512, 512, 512, 512, 512, 0.f, 7, 7, __brgemm_attrs_203, ((void*)0), ((void*)0));
  __brgemm_bd_mask_arr[0] = &__brgemm_full_bd_mask[(0 * 419)];
  __brgemm_bd_mask_arr[1] = &__brgemm_full_bd_mask[(1 * 419)];
  __brgemm_bd_mask_arr_149[0] = &__brgemm_full_bd_mask_148[(0 * 404)];
  __brgemm_bd_mask_arr_159[0] = &__brgemm_full_bd_mask_158[(0 * 222)];
  __brgemm_bd_mask_arr_185[0] = &__brgemm_full_bd_mask_184[(0 * 103)];
  __brgemm_bd_mask_arr_195[0] = &__brgemm_full_bd_mask_194[(0 * 61)];
  __sc_kernel_cache_arr[0] = dnnl_brgemm_list_func(419, 128, 64, 64, 128, 128, 0.f, 7, 7, __brgemm_attrs_131, __brgemm_bd_mask_arr[0], ((void*)0));
  __sc_kernel_cache_arr[1] = dnnl_brgemm_list_func(419, 128, 64, 64, 128, 128, 0.f, 7, 7, __brgemm_attrs_131, __brgemm_bd_mask_arr[1], ((void*)0));
  __sc_kernel_cache_arr_150[0] = dnnl_brgemm_list_func(404, 128, 256, 512, 128, 128, 0.f, 7, 7, __brgemm_attrs_147, __brgemm_bd_mask_arr_149[0], ((void*)0));
  __sc_kernel_cache_arr_160[0] = dnnl_brgemm_list_func(222, 16, 256, 256, 16, 16, 0.f, 7, 7, __brgemm_attrs_157, __brgemm_bd_mask_arr_159[0], ((void*)0));
  __sc_kernel_cache_arr_166[0] = dnnl_brgemm_list_func(222, 32, 128, 128, 32, 32, 0.f, 7, 7, __brgemm_attrs_165, __brgemm_bd_mask_arr_159[0], ((void*)0));
  __sc_kernel_cache_arr_172[0] = dnnl_brgemm_list_func(222, 64, 64, 64, 64, 64, 0.f, 7, 7, __brgemm_attrs_171, __brgemm_bd_mask_arr_159[0], ((void*)0));
  __sc_kernel_cache_arr_186[0] = dnnl_brgemm_list_func(103, 128, 256, 512, 128, 128, 0.f, 7, 7, __brgemm_attrs_183, __brgemm_bd_mask_arr_185[0], ((void*)0));
  __sc_kernel_cache_arr_196[0] = dnnl_brgemm_list_func(61, 16, 512, 512, 16, 16, 0.f, 7, 7, __brgemm_attrs_193, __brgemm_bd_mask_arr_195[0], ((void*)0));
  __sc_kernel_cache_arr_202[0] = dnnl_brgemm_list_func(61, 128, 256, 256, 128, 128, 0.f, 7, 7, __brgemm_attrs_201, __brgemm_bd_mask_arr_195[0], ((void*)0));
}

static void reorder__4810_closure_0(uint64_t fused_0_fuseiter_3791___fuseiter_3792_978, float* __restrict__ __ins_0, float* __restrict__ __outs_0) noexcept{
  for (uint64_t _fuseiter_3795 = 0UL; _fuseiter_3795 < 32UL; _fuseiter_3795 += 1UL) {
    float __cached_0;
    __cached_0 = __ins_0[(((fused_0_fuseiter_3791___fuseiter_3792_978 / 32UL) * 1024UL) + (_fuseiter_3795 + ((fused_0_fuseiter_3791___fuseiter_3792_978 % 32UL) * 32UL)))];
    float __cached_1;
    __cached_1 = __cached_0;
    __outs_0[(((fused_0_fuseiter_3791___fuseiter_3792_978 / 32UL) * 1024UL) + (((fused_0_fuseiter_3791___fuseiter_3792_978 % 32UL) * 32UL) + _fuseiter_3795))] = __cached_1;
  }
}

static void reorder__4810_closure_0_0wrapper(void* __stream, int8_t* __restrict__ __module_data, uint64_t i, generic_val* __restrict__ args) noexcept{
  reorder__4810_closure_0(i, (float*)(args[0UL].v_ptr), (float*)(args[1UL].v_ptr));
}

static void reorder__4880_closure_1(uint64_t fused_0fused_0fused_0_fuseiter_3796___fuseiter_3797_979___fuseiter_3798_980___fuseiter_3799_981, float* __restrict__ __ins_0, float* __restrict__ __outs_0) noexcept{
  for (uint64_t _fuseiter_3800 = 0UL; _fuseiter_3800 < 128UL; _fuseiter_3800 += 1UL) {
    float __cached_0;
    __cached_0 = __ins_0[(((fused_0fused_0fused_0_fuseiter_3796___fuseiter_3797_979___fuseiter_3798_980___fuseiter_3799_981 / 8UL) * 1024UL) + (_fuseiter_3800 + ((fused_0fused_0fused_0_fuseiter_3796___fuseiter_3797_979___fuseiter_3798_980___fuseiter_3799_981 % 8UL) * 128UL)))];
    float __cached_1;
    __cached_1 = __cached_0;
    __outs_0[(((fused_0fused_0fused_0_fuseiter_3796___fuseiter_3797_979___fuseiter_3798_980___fuseiter_3799_981 / 8UL) * 1024UL) + (((fused_0fused_0fused_0_fuseiter_3796___fuseiter_3797_979___fuseiter_3798_980___fuseiter_3799_981 % 8UL) * 128UL) + _fuseiter_3800))] = __cached_1;
  }
}

static void reorder__4880_closure_1_0wrapper(void* __stream, int8_t* __restrict__ __module_data, uint64_t i, generic_val* __restrict__ args) noexcept{
  reorder__4880_closure_1(i, (float*)(args[0UL].v_ptr), (float*)(args[1UL].v_ptr));
}

static void reorder__5040_closure_2(uint64_t fused_0fused_0fused_0_fuseiter_3801___fuseiter_3802_982___fuseiter_3803_983___fuseiter_3804_984, float* __restrict__ __ins_0, float* __restrict__ __outs_0) noexcept{
  for (uint64_t _fuseiter_3805 = 0UL; _fuseiter_3805 < 128UL; _fuseiter_3805 += 1UL) {
    float __cached_0;
    __cached_0 = __ins_0[(((fused_0fused_0fused_0_fuseiter_3801___fuseiter_3802_982___fuseiter_3803_983___fuseiter_3804_984 / 8UL) * 1024UL) + (_fuseiter_3805 + ((fused_0fused_0fused_0_fuseiter_3801___fuseiter_3802_982___fuseiter_3803_983___fuseiter_3804_984 % 8UL) * 128UL)))];
    float __cached_1;
    __cached_1 = __cached_0;
    __outs_0[(((fused_0fused_0fused_0_fuseiter_3801___fuseiter_3802_982___fuseiter_3803_983___fuseiter_3804_984 / 8UL) * 1024UL) + (((fused_0fused_0fused_0_fuseiter_3801___fuseiter_3802_982___fuseiter_3803_983___fuseiter_3804_984 % 8UL) * 128UL) + _fuseiter_3805))] = __cached_1;
  }
}

static void reorder__5040_closure_2_0wrapper(void* __stream, int8_t* __restrict__ __module_data, uint64_t i, generic_val* __restrict__ args) noexcept{
  reorder__5040_closure_2(i, (float*)(args[0UL].v_ptr), (float*)(args[1UL].v_ptr));
}

static void reorder__5130_closure_3(uint64_t fused_0fused_0fused_0_fuseiter_3806___fuseiter_3807_985___fuseiter_3808_986___fuseiter_3809_987, float* __restrict__ __ins_0, float* __restrict__ __outs_0) noexcept{
  for (uint64_t _fuseiter_3810 = 0UL; _fuseiter_3810 < 128UL; _fuseiter_3810 += 1UL) {
    float __cached_0;
    __cached_0 = __ins_0[(((fused_0fused_0fused_0_fuseiter_3806___fuseiter_3807_985___fuseiter_3808_986___fuseiter_3809_987 / 8UL) * 1024UL) + (_fuseiter_3810 + ((fused_0fused_0fused_0_fuseiter_3806___fuseiter_3807_985___fuseiter_3808_986___fuseiter_3809_987 % 8UL) * 128UL)))];
    float __cached_1;
    __cached_1 = __cached_0;
    __outs_0[(((fused_0fused_0fused_0_fuseiter_3806___fuseiter_3807_985___fuseiter_3808_986___fuseiter_3809_987 / 8UL) * 1024UL) + (((fused_0fused_0fused_0_fuseiter_3806___fuseiter_3807_985___fuseiter_3808_986___fuseiter_3809_987 % 8UL) * 128UL) + _fuseiter_3810))] = __cached_1;
  }
}

static void reorder__5130_closure_3_0wrapper(void* __stream, int8_t* __restrict__ __module_data, uint64_t i, generic_val* __restrict__ args) noexcept{
  reorder__5130_closure_3(i, (float*)(args[0UL].v_ptr), (float*)(args[1UL].v_ptr));
}

static void reorder__5200_closure_4(uint64_t fused_0fused_0fused_0_fuseiter_3811___fuseiter_3812_988___fuseiter_3813_989___fuseiter_3814_990, float* __restrict__ __ins_0, float* __restrict__ __outs_0) noexcept{
  for (uint64_t _fuseiter_3815 = 0UL; _fuseiter_3815 < 128UL; _fuseiter_3815 += 1UL) {
    float __cached_0;
    __cached_0 = __ins_0[(((fused_0fused_0fused_0_fuseiter_3811___fuseiter_3812_988___fuseiter_3813_989___fuseiter_3814_990 / 8UL) * 1024UL) + (_fuseiter_3815 + ((fused_0fused_0fused_0_fuseiter_3811___fuseiter_3812_988___fuseiter_3813_989___fuseiter_3814_990 % 8UL) * 128UL)))];
    float __cached_1;
    __cached_1 = __cached_0;
    __outs_0[(((fused_0fused_0fused_0_fuseiter_3811___fuseiter_3812_988___fuseiter_3813_989___fuseiter_3814_990 / 8UL) * 1024UL) + (((fused_0fused_0fused_0_fuseiter_3811___fuseiter_3812_988___fuseiter_3813_989___fuseiter_3814_990 % 8UL) * 128UL) + _fuseiter_3815))] = __cached_1;
  }
}

static void reorder__5200_closure_4_0wrapper(void* __stream, int8_t* __restrict__ __module_data, uint64_t i, generic_val* __restrict__ args) noexcept{
  reorder__5200_closure_4(i, (float*)(args[0UL].v_ptr), (float*)(args[1UL].v_ptr));
}

static void reorder__5290_closure_5(uint64_t fused_0fused_0fused_0_fuseiter_3816___fuseiter_3817_991___fuseiter_3818_992___fuseiter_3819_993, float* __restrict__ __ins_0, float* __restrict__ __outs_0) noexcept{
  for (uint64_t _fuseiter_3820 = 0UL; _fuseiter_3820 < 256UL; _fuseiter_3820 += 1UL) {
    float __cached_0;
    __cached_0 = __ins_0[(((fused_0fused_0fused_0_fuseiter_3816___fuseiter_3817_991___fuseiter_3818_992___fuseiter_3819_993 / 4UL) * 1024UL) + (_fuseiter_3820 + ((fused_0fused_0fused_0_fuseiter_3816___fuseiter_3817_991___fuseiter_3818_992___fuseiter_3819_993 % 4UL) * 256UL)))];
    float __cached_1;
    __cached_1 = __cached_0;
    __outs_0[(((fused_0fused_0fused_0_fuseiter_3816___fuseiter_3817_991___fuseiter_3818_992___fuseiter_3819_993 / 4UL) * 1024UL) + (((fused_0fused_0fused_0_fuseiter_3816___fuseiter_3817_991___fuseiter_3818_992___fuseiter_3819_993 % 4UL) * 256UL) + _fuseiter_3820))] = __cached_1;
  }
}

static void reorder__5290_closure_5_0wrapper(void* __stream, int8_t* __restrict__ __module_data, uint64_t i, generic_val* __restrict__ args) noexcept{
  reorder__5290_closure_5(i, (float*)(args[0UL].v_ptr), (float*)(args[1UL].v_ptr));
}

static void reorder__4200_closure_6(uint64_t fused_0fused_0fused_0_fuseiter_3821___fuseiter_3822_994___fuseiter_3823_995___fuseiter_3824_996, float* __restrict__ __ins_0, float* __restrict__ __outs_0) noexcept{
  for (uint64_t _fuseiter_3825 = 0UL; _fuseiter_3825 < 16UL; _fuseiter_3825 += 1UL) {
    float __cached_0;
    __cached_0 = __ins_0[(((fused_0fused_0fused_0_fuseiter_3821___fuseiter_3822_994___fuseiter_3823_995___fuseiter_3824_996 / 16UL) * 256UL) + (_fuseiter_3825 + ((fused_0fused_0fused_0_fuseiter_3821___fuseiter_3822_994___fuseiter_3823_995___fuseiter_3824_996 % 16UL) * 16UL)))];
    float __cached_1;
    __cached_1 = __cached_0;
    __outs_0[(((fused_0fused_0fused_0_fuseiter_3821___fuseiter_3822_994___fuseiter_3823_995___fuseiter_3824_996 / 16UL) * 256UL) + (((fused_0fused_0fused_0_fuseiter_3821___fuseiter_3822_994___fuseiter_3823_995___fuseiter_3824_996 % 16UL) * 16UL) + _fuseiter_3825))] = __cached_1;
  }
}

static void reorder__4200_closure_6_0wrapper(void* __stream, int8_t* __restrict__ __module_data, uint64_t i, generic_val* __restrict__ args) noexcept{
  reorder__4200_closure_6(i, (float*)(args[0UL].v_ptr), (float*)(args[1UL].v_ptr));
}

static void reorder__4240_closure_7(uint64_t fused_0fused_0fused_0_fuseiter_3826___fuseiter_3827_997___fuseiter_3828_998___fuseiter_3829_999, float* __restrict__ __ins_0, float* __restrict__ __outs_0) noexcept{
  for (uint64_t _fuseiter_3830 = 0UL; _fuseiter_3830 < 32UL; _fuseiter_3830 += 1UL) {
    float __cached_0;
    __cached_0 = __ins_0[(((fused_0fused_0fused_0_fuseiter_3826___fuseiter_3827_997___fuseiter_3828_998___fuseiter_3829_999 / 2UL) * 64UL) + (_fuseiter_3830 + ((fused_0fused_0fused_0_fuseiter_3826___fuseiter_3827_997___fuseiter_3828_998___fuseiter_3829_999 % 2UL) * 32UL)))];
    float __cached_1;
    __cached_1 = __cached_0;
    __outs_0[(((fused_0fused_0fused_0_fuseiter_3826___fuseiter_3827_997___fuseiter_3828_998___fuseiter_3829_999 / 2UL) * 64UL) + (((fused_0fused_0fused_0_fuseiter_3826___fuseiter_3827_997___fuseiter_3828_998___fuseiter_3829_999 % 2UL) * 32UL) + _fuseiter_3830))] = __cached_1;
  }
}

static void reorder__4240_closure_7_0wrapper(void* __stream, int8_t* __restrict__ __module_data, uint64_t i, generic_val* __restrict__ args) noexcept{
  reorder__4240_closure_7(i, (float*)(args[0UL].v_ptr), (float*)(args[1UL].v_ptr));
}

static void reorder__4270_closure_8(uint64_t fused_0fused_0fused_0_fuseiter_3831___fuseiter_3832_1000___fuseiter_3833_1001___fuseiter_3834_1002, float* __restrict__ __ins_0, float* __restrict__ __outs_0) noexcept{
  for (uint64_t _fuseiter_3835 = 0UL; _fuseiter_3835 < 64UL; _fuseiter_3835 += 1UL) {
    float __cached_0;
    __cached_0 = __ins_0[(((fused_0fused_0fused_0_fuseiter_3831___fuseiter_3832_1000___fuseiter_3833_1001___fuseiter_3834_1002 / 4UL) * 256UL) + (_fuseiter_3835 + ((fused_0fused_0fused_0_fuseiter_3831___fuseiter_3832_1000___fuseiter_3833_1001___fuseiter_3834_1002 % 4UL) * 64UL)))];
    float __cached_1;
    __cached_1 = __cached_0;
    __outs_0[(((fused_0fused_0fused_0_fuseiter_3831___fuseiter_3832_1000___fuseiter_3833_1001___fuseiter_3834_1002 / 4UL) * 256UL) + (((fused_0fused_0fused_0_fuseiter_3831___fuseiter_3832_1000___fuseiter_3833_1001___fuseiter_3834_1002 % 4UL) * 64UL) + _fuseiter_3835))] = __cached_1;
  }
}

static void reorder__4270_closure_8_0wrapper(void* __stream, int8_t* __restrict__ __module_data, uint64_t i, generic_val* __restrict__ args) noexcept{
  reorder__4270_closure_8(i, (float*)(args[0UL].v_ptr), (float*)(args[1UL].v_ptr));
}

static void reorder__4310_closure_9(uint64_t fused_0fused_0fused_0_fuseiter_3836___fuseiter_3837_1003___fuseiter_3838_1004___fuseiter_3839_1005, float* __restrict__ __ins_0, float* __restrict__ __outs_0) noexcept{
  for (uint64_t _fuseiter_3840 = 0UL; _fuseiter_3840 < 16UL; _fuseiter_3840 += 1UL) {
    float __cached_0;
    __cached_0 = __ins_0[(((fused_0fused_0fused_0_fuseiter_3836___fuseiter_3837_1003___fuseiter_3838_1004___fuseiter_3839_1005 / 4UL) * 64UL) + (_fuseiter_3840 + ((fused_0fused_0fused_0_fuseiter_3836___fuseiter_3837_1003___fuseiter_3838_1004___fuseiter_3839_1005 % 4UL) * 16UL)))];
    float __cached_1;
    __cached_1 = __cached_0;
    __outs_0[(((fused_0fused_0fused_0_fuseiter_3836___fuseiter_3837_1003___fuseiter_3838_1004___fuseiter_3839_1005 / 4UL) * 64UL) + (((fused_0fused_0fused_0_fuseiter_3836___fuseiter_3837_1003___fuseiter_3838_1004___fuseiter_3839_1005 % 4UL) * 16UL) + _fuseiter_3840))] = __cached_1;
  }
}

static void reorder__4310_closure_9_0wrapper(void* __stream, int8_t* __restrict__ __module_data, uint64_t i, generic_val* __restrict__ args) noexcept{
  reorder__4310_closure_9(i, (float*)(args[0UL].v_ptr), (float*)(args[1UL].v_ptr));
}

static void reorder__4340_closure_10(uint64_t fused_0fused_0fused_0_fuseiter_3841___fuseiter_3842_1006___fuseiter_3843_1007___fuseiter_3844_1008, float* __restrict__ __ins_0, float* __restrict__ __outs_0) noexcept{
  for (uint64_t _fuseiter_3845 = 0UL; _fuseiter_3845 < 32UL; _fuseiter_3845 += 1UL) {
    float __cached_0;
    __cached_0 = __ins_0[(((fused_0fused_0fused_0_fuseiter_3841___fuseiter_3842_1006___fuseiter_3843_1007___fuseiter_3844_1008 / 8UL) * 256UL) + (_fuseiter_3845 + ((fused_0fused_0fused_0_fuseiter_3841___fuseiter_3842_1006___fuseiter_3843_1007___fuseiter_3844_1008 % 8UL) * 32UL)))];
    float __cached_1;
    __cached_1 = __cached_0;
    __outs_0[(((fused_0fused_0fused_0_fuseiter_3841___fuseiter_3842_1006___fuseiter_3843_1007___fuseiter_3844_1008 / 8UL) * 256UL) + (((fused_0fused_0fused_0_fuseiter_3841___fuseiter_3842_1006___fuseiter_3843_1007___fuseiter_3844_1008 % 8UL) * 32UL) + _fuseiter_3845))] = __cached_1;
  }
}

static void reorder__4340_closure_10_0wrapper(void* __stream, int8_t* __restrict__ __module_data, uint64_t i, generic_val* __restrict__ args) noexcept{
  reorder__4340_closure_10(i, (float*)(args[0UL].v_ptr), (float*)(args[1UL].v_ptr));
}

static void reorder__4370_closure_11(uint64_t fused_0fused_0fused_0_fuseiter_3846___fuseiter_3847_1009___fuseiter_3848_1010___fuseiter_3849_1011, float* __restrict__ __ins_0, float* __restrict__ __outs_0) noexcept{
  for (uint64_t _fuseiter_3850 = 0UL; _fuseiter_3850 < 32UL; _fuseiter_3850 += 1UL) {
    float __cached_0;
    __cached_0 = __ins_0[(((fused_0fused_0fused_0_fuseiter_3846___fuseiter_3847_1009___fuseiter_3848_1010___fuseiter_3849_1011 / 2UL) * 64UL) + (_fuseiter_3850 + ((fused_0fused_0fused_0_fuseiter_3846___fuseiter_3847_1009___fuseiter_3848_1010___fuseiter_3849_1011 % 2UL) * 32UL)))];
    float __cached_1;
    __cached_1 = __cached_0;
    __outs_0[(((fused_0fused_0fused_0_fuseiter_3846___fuseiter_3847_1009___fuseiter_3848_1010___fuseiter_3849_1011 / 2UL) * 64UL) + (((fused_0fused_0fused_0_fuseiter_3846___fuseiter_3847_1009___fuseiter_3848_1010___fuseiter_3849_1011 % 2UL) * 32UL) + _fuseiter_3850))] = __cached_1;
  }
}

static void reorder__4370_closure_11_0wrapper(void* __stream, int8_t* __restrict__ __module_data, uint64_t i, generic_val* __restrict__ args) noexcept{
  reorder__4370_closure_11(i, (float*)(args[0UL].v_ptr), (float*)(args[1UL].v_ptr));
}

static void reorder__4400_closure_12(uint64_t fused_0fused_0fused_0_fuseiter_3851___fuseiter_3852_1012___fuseiter_3853_1013___fuseiter_3854_1014, float* __restrict__ __ins_0, float* __restrict__ __outs_0) noexcept{
  for (uint64_t _fuseiter_3855 = 0UL; _fuseiter_3855 < 32UL; _fuseiter_3855 += 1UL) {
    float __cached_0;
    __cached_0 = __ins_0[(((fused_0fused_0fused_0_fuseiter_3851___fuseiter_3852_1012___fuseiter_3853_1013___fuseiter_3854_1014 / 2UL) * 64UL) + (_fuseiter_3855 + ((fused_0fused_0fused_0_fuseiter_3851___fuseiter_3852_1012___fuseiter_3853_1013___fuseiter_3854_1014 % 2UL) * 32UL)))];
    float __cached_1;
    __cached_1 = __cached_0;
    __outs_0[(((fused_0fused_0fused_0_fuseiter_3851___fuseiter_3852_1012___fuseiter_3853_1013___fuseiter_3854_1014 / 2UL) * 64UL) + (((fused_0fused_0fused_0_fuseiter_3851___fuseiter_3852_1012___fuseiter_3853_1013___fuseiter_3854_1014 % 2UL) * 32UL) + _fuseiter_3855))] = __cached_1;
  }
}

static void reorder__4400_closure_12_0wrapper(void* __stream, int8_t* __restrict__ __module_data, uint64_t i, generic_val* __restrict__ args) noexcept{
  reorder__4400_closure_12(i, (float*)(args[0UL].v_ptr), (float*)(args[1UL].v_ptr));
}

static void reorder__4430_closure_13(uint64_t fused_0fused_0fused_0_fuseiter_3856___fuseiter_3857_1015___fuseiter_3858_1016___fuseiter_3859_1017, float* __restrict__ __ins_0, float* __restrict__ __outs_0) noexcept{
  for (uint64_t _fuseiter_3860 = 0UL; _fuseiter_3860 < 64UL; _fuseiter_3860 += 1UL) {
    float __cached_0;
    __cached_0 = __ins_0[(((fused_0fused_0fused_0_fuseiter_3856___fuseiter_3857_1015___fuseiter_3858_1016___fuseiter_3859_1017 / 4UL) * 256UL) + (_fuseiter_3860 + ((fused_0fused_0fused_0_fuseiter_3856___fuseiter_3857_1015___fuseiter_3858_1016___fuseiter_3859_1017 % 4UL) * 64UL)))];
    float __cached_1;
    __cached_1 = __cached_0;
    __outs_0[(((fused_0fused_0fused_0_fuseiter_3856___fuseiter_3857_1015___fuseiter_3858_1016___fuseiter_3859_1017 / 4UL) * 256UL) + (((fused_0fused_0fused_0_fuseiter_3856___fuseiter_3857_1015___fuseiter_3858_1016___fuseiter_3859_1017 % 4UL) * 64UL) + _fuseiter_3860))] = __cached_1;
  }
}

static void reorder__4430_closure_13_0wrapper(void* __stream, int8_t* __restrict__ __module_data, uint64_t i, generic_val* __restrict__ args) noexcept{
  reorder__4430_closure_13(i, (float*)(args[0UL].v_ptr), (float*)(args[1UL].v_ptr));
}

static void reorder__4460_closure_14(uint64_t fused_0fused_0fused_0_fuseiter_3861___fuseiter_3862_1018___fuseiter_3863_1019___fuseiter_3864_1020, float* __restrict__ __ins_0, float* __restrict__ __outs_0) noexcept{
  for (uint64_t _fuseiter_3865 = 0UL; _fuseiter_3865 < 32UL; _fuseiter_3865 += 1UL) {
    float __cached_0;
    __cached_0 = __ins_0[(((fused_0fused_0fused_0_fuseiter_3861___fuseiter_3862_1018___fuseiter_3863_1019___fuseiter_3864_1020 / 16UL) * 512UL) + (_fuseiter_3865 + ((fused_0fused_0fused_0_fuseiter_3861___fuseiter_3862_1018___fuseiter_3863_1019___fuseiter_3864_1020 % 16UL) * 32UL)))];
    float __cached_1;
    __cached_1 = __cached_0;
    __outs_0[(((fused_0fused_0fused_0_fuseiter_3861___fuseiter_3862_1018___fuseiter_3863_1019___fuseiter_3864_1020 / 16UL) * 512UL) + (((fused_0fused_0fused_0_fuseiter_3861___fuseiter_3862_1018___fuseiter_3863_1019___fuseiter_3864_1020 % 16UL) * 32UL) + _fuseiter_3865))] = __cached_1;
  }
}

static void reorder__4460_closure_14_0wrapper(void* __stream, int8_t* __restrict__ __module_data, uint64_t i, generic_val* __restrict__ args) noexcept{
  reorder__4460_closure_14(i, (float*)(args[0UL].v_ptr), (float*)(args[1UL].v_ptr));
}

static void reorder__4490_closure_15(uint64_t fused_0fused_0fused_0_fuseiter_3866___fuseiter_3867_1021___fuseiter_3868_1022___fuseiter_3869_1023, float* __restrict__ __ins_0, float* __restrict__ __outs_0) noexcept{
  for (uint64_t _fuseiter_3870 = 0UL; _fuseiter_3870 < 64UL; _fuseiter_3870 += 1UL) {
    float __cached_0;
    __cached_0 = __ins_0[(((fused_0fused_0fused_0_fuseiter_3866___fuseiter_3867_1021___fuseiter_3868_1022___fuseiter_3869_1023 / 2UL) * 128UL) + (_fuseiter_3870 + ((fused_0fused_0fused_0_fuseiter_3866___fuseiter_3867_1021___fuseiter_3868_1022___fuseiter_3869_1023 % 2UL) * 64UL)))];
    float __cached_1;
    __cached_1 = __cached_0;
    __outs_0[(((fused_0fused_0fused_0_fuseiter_3866___fuseiter_3867_1021___fuseiter_3868_1022___fuseiter_3869_1023 / 2UL) * 128UL) + (((fused_0fused_0fused_0_fuseiter_3866___fuseiter_3867_1021___fuseiter_3868_1022___fuseiter_3869_1023 % 2UL) * 64UL) + _fuseiter_3870))] = __cached_1;
  }
}

static void reorder__4490_closure_15_0wrapper(void* __stream, int8_t* __restrict__ __module_data, uint64_t i, generic_val* __restrict__ args) noexcept{
  reorder__4490_closure_15(i, (float*)(args[0UL].v_ptr), (float*)(args[1UL].v_ptr));
}

static void reorder__4520_closure_16(uint64_t fused_0fused_0fused_0_fuseiter_3871___fuseiter_3872_1024___fuseiter_3873_1025___fuseiter_3874_1026, float* __restrict__ __ins_0, float* __restrict__ __outs_0) noexcept{
  for (uint64_t _fuseiter_3875 = 0UL; _fuseiter_3875 < 32UL; _fuseiter_3875 += 1UL) {
    float __cached_0;
    __cached_0 = __ins_0[(((fused_0fused_0fused_0_fuseiter_3871___fuseiter_3872_1024___fuseiter_3873_1025___fuseiter_3874_1026 / 4UL) * 128UL) + (_fuseiter_3875 + ((fused_0fused_0fused_0_fuseiter_3871___fuseiter_3872_1024___fuseiter_3873_1025___fuseiter_3874_1026 % 4UL) * 32UL)))];
    float __cached_1;
    __cached_1 = __cached_0;
    __outs_0[(((fused_0fused_0fused_0_fuseiter_3871___fuseiter_3872_1024___fuseiter_3873_1025___fuseiter_3874_1026 / 4UL) * 128UL) + (((fused_0fused_0fused_0_fuseiter_3871___fuseiter_3872_1024___fuseiter_3873_1025___fuseiter_3874_1026 % 4UL) * 32UL) + _fuseiter_3875))] = __cached_1;
  }
}

static void reorder__4520_closure_16_0wrapper(void* __stream, int8_t* __restrict__ __module_data, uint64_t i, generic_val* __restrict__ args) noexcept{
  reorder__4520_closure_16(i, (float*)(args[0UL].v_ptr), (float*)(args[1UL].v_ptr));
}

static void reorder__4550_closure_17(uint64_t fused_0fused_0fused_0_fuseiter_3876___fuseiter_3877_1027___fuseiter_3878_1028___fuseiter_3879_1029, float* __restrict__ __ins_0, float* __restrict__ __outs_0) noexcept{
  for (uint64_t _fuseiter_3880 = 0UL; _fuseiter_3880 < 32UL; _fuseiter_3880 += 1UL) {
    float __cached_0;
    __cached_0 = __ins_0[(((fused_0fused_0fused_0_fuseiter_3876___fuseiter_3877_1027___fuseiter_3878_1028___fuseiter_3879_1029 / 16UL) * 512UL) + (_fuseiter_3880 + ((fused_0fused_0fused_0_fuseiter_3876___fuseiter_3877_1027___fuseiter_3878_1028___fuseiter_3879_1029 % 16UL) * 32UL)))];
    float __cached_1;
    __cached_1 = __cached_0;
    __outs_0[(((fused_0fused_0fused_0_fuseiter_3876___fuseiter_3877_1027___fuseiter_3878_1028___fuseiter_3879_1029 / 16UL) * 512UL) + (((fused_0fused_0fused_0_fuseiter_3876___fuseiter_3877_1027___fuseiter_3878_1028___fuseiter_3879_1029 % 16UL) * 32UL) + _fuseiter_3880))] = __cached_1;
  }
}

static void reorder__4550_closure_17_0wrapper(void* __stream, int8_t* __restrict__ __module_data, uint64_t i, generic_val* __restrict__ args) noexcept{
  reorder__4550_closure_17(i, (float*)(args[0UL].v_ptr), (float*)(args[1UL].v_ptr));
}

static void reorder__4580_closure_18(uint64_t fused_0fused_0fused_0_fuseiter_3881___fuseiter_3882_1030___fuseiter_3883_1031___fuseiter_3884_1032, float* __restrict__ __ins_0, float* __restrict__ __outs_0) noexcept{
  for (uint64_t _fuseiter_3885 = 0UL; _fuseiter_3885 < 32UL; _fuseiter_3885 += 1UL) {
    float __cached_0;
    __cached_0 = __ins_0[(((fused_0fused_0fused_0_fuseiter_3881___fuseiter_3882_1030___fuseiter_3883_1031___fuseiter_3884_1032 / 4UL) * 128UL) + (_fuseiter_3885 + ((fused_0fused_0fused_0_fuseiter_3881___fuseiter_3882_1030___fuseiter_3883_1031___fuseiter_3884_1032 % 4UL) * 32UL)))];
    float __cached_1;
    __cached_1 = __cached_0;
    __outs_0[(((fused_0fused_0fused_0_fuseiter_3881___fuseiter_3882_1030___fuseiter_3883_1031___fuseiter_3884_1032 / 4UL) * 128UL) + (((fused_0fused_0fused_0_fuseiter_3881___fuseiter_3882_1030___fuseiter_3883_1031___fuseiter_3884_1032 % 4UL) * 32UL) + _fuseiter_3885))] = __cached_1;
  }
}

static void reorder__4580_closure_18_0wrapper(void* __stream, int8_t* __restrict__ __module_data, uint64_t i, generic_val* __restrict__ args) noexcept{
  reorder__4580_closure_18(i, (float*)(args[0UL].v_ptr), (float*)(args[1UL].v_ptr));
}

static void reorder__4620_closure_19(uint64_t fused_0fused_0fused_0_fuseiter_3886___fuseiter_3887_1033___fuseiter_3888_1034___fuseiter_3889_1035, float* __restrict__ __ins_0, float* __restrict__ __outs_0) noexcept{
  for (uint64_t _fuseiter_3890 = 0UL; _fuseiter_3890 < 128UL; _fuseiter_3890 += 1UL) {
    float __cached_0;
    __cached_0 = __ins_0[(((fused_0fused_0fused_0_fuseiter_3886___fuseiter_3887_1033___fuseiter_3888_1034___fuseiter_3889_1035 / 4UL) * 512UL) + (_fuseiter_3890 + ((fused_0fused_0fused_0_fuseiter_3886___fuseiter_3887_1033___fuseiter_3888_1034___fuseiter_3889_1035 % 4UL) * 128UL)))];
    float __cached_1;
    __cached_1 = __cached_0;
    __outs_0[(((fused_0fused_0fused_0_fuseiter_3886___fuseiter_3887_1033___fuseiter_3888_1034___fuseiter_3889_1035 / 4UL) * 512UL) + (((fused_0fused_0fused_0_fuseiter_3886___fuseiter_3887_1033___fuseiter_3888_1034___fuseiter_3889_1035 % 4UL) * 128UL) + _fuseiter_3890))] = __cached_1;
  }
}

static void reorder__4620_closure_19_0wrapper(void* __stream, int8_t* __restrict__ __module_data, uint64_t i, generic_val* __restrict__ args) noexcept{
  reorder__4620_closure_19(i, (float*)(args[0UL].v_ptr), (float*)(args[1UL].v_ptr));
}

static void reorder__4660_closure_20(uint64_t fused_0fused_0fused_0_fuseiter_3891___fuseiter_3892_1036___fuseiter_3893_1037___fuseiter_3894_1038, float* __restrict__ __ins_0, float* __restrict__ __outs_0) noexcept{
  for (uint64_t _fuseiter_3895 = 0UL; _fuseiter_3895 < 32UL; _fuseiter_3895 += 1UL) {
    float __cached_0;
    __cached_0 = __ins_0[(((fused_0fused_0fused_0_fuseiter_3891___fuseiter_3892_1036___fuseiter_3893_1037___fuseiter_3894_1038 / 4UL) * 128UL) + (_fuseiter_3895 + ((fused_0fused_0fused_0_fuseiter_3891___fuseiter_3892_1036___fuseiter_3893_1037___fuseiter_3894_1038 % 4UL) * 32UL)))];
    float __cached_1;
    __cached_1 = __cached_0;
    __outs_0[(((fused_0fused_0fused_0_fuseiter_3891___fuseiter_3892_1036___fuseiter_3893_1037___fuseiter_3894_1038 / 4UL) * 128UL) + (((fused_0fused_0fused_0_fuseiter_3891___fuseiter_3892_1036___fuseiter_3893_1037___fuseiter_3894_1038 % 4UL) * 32UL) + _fuseiter_3895))] = __cached_1;
  }
}

static void reorder__4660_closure_20_0wrapper(void* __stream, int8_t* __restrict__ __module_data, uint64_t i, generic_val* __restrict__ args) noexcept{
  reorder__4660_closure_20(i, (float*)(args[0UL].v_ptr), (float*)(args[1UL].v_ptr));
}

static void reorder__4690_closure_21(uint64_t fused_0fused_0fused_0_fuseiter_3896___fuseiter_3897_1039___fuseiter_3898_1040___fuseiter_3899_1041, float* __restrict__ __ins_0, float* __restrict__ __outs_0) noexcept{
  for (uint64_t _fuseiter_3900 = 0UL; _fuseiter_3900 < 64UL; _fuseiter_3900 += 1UL) {
    float __cached_0;
    __cached_0 = __ins_0[(((fused_0fused_0fused_0_fuseiter_3896___fuseiter_3897_1039___fuseiter_3898_1040___fuseiter_3899_1041 / 8UL) * 512UL) + (_fuseiter_3900 + ((fused_0fused_0fused_0_fuseiter_3896___fuseiter_3897_1039___fuseiter_3898_1040___fuseiter_3899_1041 % 8UL) * 64UL)))];
    float __cached_1;
    __cached_1 = __cached_0;
    __outs_0[(((fused_0fused_0fused_0_fuseiter_3896___fuseiter_3897_1039___fuseiter_3898_1040___fuseiter_3899_1041 / 8UL) * 512UL) + (((fused_0fused_0fused_0_fuseiter_3896___fuseiter_3897_1039___fuseiter_3898_1040___fuseiter_3899_1041 % 8UL) * 64UL) + _fuseiter_3900))] = __cached_1;
  }
}

static void reorder__4690_closure_21_0wrapper(void* __stream, int8_t* __restrict__ __module_data, uint64_t i, generic_val* __restrict__ args) noexcept{
  reorder__4690_closure_21(i, (float*)(args[0UL].v_ptr), (float*)(args[1UL].v_ptr));
}

static void reorder__4720_closure_22(uint64_t fused_0fused_0fused_0_fuseiter_3901___fuseiter_3902_1042___fuseiter_3903_1043___fuseiter_3904_1044, float* __restrict__ __ins_0, float* __restrict__ __outs_0) noexcept{
  for (uint64_t _fuseiter_3905 = 0UL; _fuseiter_3905 < 64UL; _fuseiter_3905 += 1UL) {
    float __cached_0;
    __cached_0 = __ins_0[(((fused_0fused_0fused_0_fuseiter_3901___fuseiter_3902_1042___fuseiter_3903_1043___fuseiter_3904_1044 / 2UL) * 128UL) + (_fuseiter_3905 + ((fused_0fused_0fused_0_fuseiter_3901___fuseiter_3902_1042___fuseiter_3903_1043___fuseiter_3904_1044 % 2UL) * 64UL)))];
    float __cached_1;
    __cached_1 = __cached_0;
    __outs_0[(((fused_0fused_0fused_0_fuseiter_3901___fuseiter_3902_1042___fuseiter_3903_1043___fuseiter_3904_1044 / 2UL) * 128UL) + (((fused_0fused_0fused_0_fuseiter_3901___fuseiter_3902_1042___fuseiter_3903_1043___fuseiter_3904_1044 % 2UL) * 64UL) + _fuseiter_3905))] = __cached_1;
  }
}

static void reorder__4720_closure_22_0wrapper(void* __stream, int8_t* __restrict__ __module_data, uint64_t i, generic_val* __restrict__ args) noexcept{
  reorder__4720_closure_22(i, (float*)(args[0UL].v_ptr), (float*)(args[1UL].v_ptr));
}

static void reorder__4750_closure_23(uint64_t fused_0fused_0fused_0_fuseiter_3906___fuseiter_3907_1045___fuseiter_3908_1046___fuseiter_3909_1047, float* __restrict__ __ins_0, float* __restrict__ __outs_0) noexcept{
  for (uint64_t _fuseiter_3910 = 0UL; _fuseiter_3910 < 32UL; _fuseiter_3910 += 1UL) {
    float __cached_0;
    __cached_0 = __ins_0[(((fused_0fused_0fused_0_fuseiter_3906___fuseiter_3907_1045___fuseiter_3908_1046___fuseiter_3909_1047 / 4UL) * 128UL) + (_fuseiter_3910 + ((fused_0fused_0fused_0_fuseiter_3906___fuseiter_3907_1045___fuseiter_3908_1046___fuseiter_3909_1047 % 4UL) * 32UL)))];
    float __cached_1;
    __cached_1 = __cached_0;
    __outs_0[(((fused_0fused_0fused_0_fuseiter_3906___fuseiter_3907_1045___fuseiter_3908_1046___fuseiter_3909_1047 / 4UL) * 128UL) + (((fused_0fused_0fused_0_fuseiter_3906___fuseiter_3907_1045___fuseiter_3908_1046___fuseiter_3909_1047 % 4UL) * 32UL) + _fuseiter_3910))] = __cached_1;
  }
}

static void reorder__4750_closure_23_0wrapper(void* __stream, int8_t* __restrict__ __module_data, uint64_t i, generic_val* __restrict__ args) noexcept{
  reorder__4750_closure_23(i, (float*)(args[0UL].v_ptr), (float*)(args[1UL].v_ptr));
}

static void reorder__4780_closure_24(uint64_t fused_0fused_0fused_0_fuseiter_3911___fuseiter_3912_1048___fuseiter_3913_1049___fuseiter_3914_1050, float* __restrict__ __ins_0, float* __restrict__ __outs_0) noexcept{
  for (uint64_t _fuseiter_3915 = 0UL; _fuseiter_3915 < 128UL; _fuseiter_3915 += 1UL) {
    float __cached_0;
    __cached_0 = __ins_0[(((fused_0fused_0fused_0_fuseiter_3911___fuseiter_3912_1048___fuseiter_3913_1049___fuseiter_3914_1050 / 4UL) * 512UL) + (_fuseiter_3915 + ((fused_0fused_0fused_0_fuseiter_3911___fuseiter_3912_1048___fuseiter_3913_1049___fuseiter_3914_1050 % 4UL) * 128UL)))];
    float __cached_1;
    __cached_1 = __cached_0;
    __outs_0[(((fused_0fused_0fused_0_fuseiter_3911___fuseiter_3912_1048___fuseiter_3913_1049___fuseiter_3914_1050 / 4UL) * 512UL) + (((fused_0fused_0fused_0_fuseiter_3911___fuseiter_3912_1048___fuseiter_3913_1049___fuseiter_3914_1050 % 4UL) * 128UL) + _fuseiter_3915))] = __cached_1;
  }
}

static void reorder__4780_closure_24_0wrapper(void* __stream, int8_t* __restrict__ __module_data, uint64_t i, generic_val* __restrict__ args) noexcept{
  reorder__4780_closure_24(i, (float*)(args[0UL].v_ptr), (float*)(args[1UL].v_ptr));
}

static void reorder__4850_closure_25(uint64_t fused_0fused_0fused_0_fuseiter_3916___fuseiter_3917_1051___fuseiter_3918_1052___fuseiter_3919_1053, float* __restrict__ __ins_0, float* __restrict__ __outs_0) noexcept{
  for (uint64_t _fuseiter_3920 = 0UL; _fuseiter_3920 < 128UL; _fuseiter_3920 += 1UL) {
    float __cached_0;
    __cached_0 = __ins_0[(((fused_0fused_0fused_0_fuseiter_3916___fuseiter_3917_1051___fuseiter_3918_1052___fuseiter_3919_1053 / 2UL) * 256UL) + (_fuseiter_3920 + ((fused_0fused_0fused_0_fuseiter_3916___fuseiter_3917_1051___fuseiter_3918_1052___fuseiter_3919_1053 % 2UL) * 128UL)))];
    float __cached_1;
    __cached_1 = __cached_0;
    __outs_0[(((fused_0fused_0fused_0_fuseiter_3916___fuseiter_3917_1051___fuseiter_3918_1052___fuseiter_3919_1053 / 2UL) * 256UL) + (((fused_0fused_0fused_0_fuseiter_3916___fuseiter_3917_1051___fuseiter_3918_1052___fuseiter_3919_1053 % 2UL) * 128UL) + _fuseiter_3920))] = __cached_1;
  }
}

static void reorder__4850_closure_25_0wrapper(void* __stream, int8_t* __restrict__ __module_data, uint64_t i, generic_val* __restrict__ args) noexcept{
  reorder__4850_closure_25(i, (float*)(args[0UL].v_ptr), (float*)(args[1UL].v_ptr));
}

static void reorder__4910_closure_26(uint64_t fused_0fused_0fused_0_fuseiter_3921___fuseiter_3922_1054___fuseiter_3923_1055___fuseiter_3924_1056, float* __restrict__ __ins_0, float* __restrict__ __outs_0) noexcept{
  for (uint64_t _fuseiter_3925 = 0UL; _fuseiter_3925 < 64UL; _fuseiter_3925 += 1UL) {
    float __cached_0;
    __cached_0 = __ins_0[(((fused_0fused_0fused_0_fuseiter_3921___fuseiter_3922_1054___fuseiter_3923_1055___fuseiter_3924_1056 / 4UL) * 256UL) + (_fuseiter_3925 + ((fused_0fused_0fused_0_fuseiter_3921___fuseiter_3922_1054___fuseiter_3923_1055___fuseiter_3924_1056 % 4UL) * 64UL)))];
    float __cached_1;
    __cached_1 = __cached_0;
    __outs_0[(((fused_0fused_0fused_0_fuseiter_3921___fuseiter_3922_1054___fuseiter_3923_1055___fuseiter_3924_1056 / 4UL) * 256UL) + (((fused_0fused_0fused_0_fuseiter_3921___fuseiter_3922_1054___fuseiter_3923_1055___fuseiter_3924_1056 % 4UL) * 64UL) + _fuseiter_3925))] = __cached_1;
  }
}

static void reorder__4910_closure_26_0wrapper(void* __stream, int8_t* __restrict__ __module_data, uint64_t i, generic_val* __restrict__ args) noexcept{
  reorder__4910_closure_26(i, (float*)(args[0UL].v_ptr), (float*)(args[1UL].v_ptr));
}

static void reorder__4940_closure_27(uint64_t fused_0fused_0fused_0_fuseiter_3926___fuseiter_3927_1057___fuseiter_3928_1058___fuseiter_3929_1059, float* __restrict__ __ins_0, float* __restrict__ __outs_0) noexcept{
  for (uint64_t _fuseiter_3930 = 0UL; _fuseiter_3930 < 16UL; _fuseiter_3930 += 1UL) {
    float __cached_0;
    __cached_0 = __ins_0[(((fused_0fused_0fused_0_fuseiter_3926___fuseiter_3927_1057___fuseiter_3928_1058___fuseiter_3929_1059 / 16UL) * 256UL) + (_fuseiter_3930 + ((fused_0fused_0fused_0_fuseiter_3926___fuseiter_3927_1057___fuseiter_3928_1058___fuseiter_3929_1059 % 16UL) * 16UL)))];
    float __cached_1;
    __cached_1 = __cached_0;
    __outs_0[(((fused_0fused_0fused_0_fuseiter_3926___fuseiter_3927_1057___fuseiter_3928_1058___fuseiter_3929_1059 / 16UL) * 256UL) + (((fused_0fused_0fused_0_fuseiter_3926___fuseiter_3927_1057___fuseiter_3928_1058___fuseiter_3929_1059 % 16UL) * 16UL) + _fuseiter_3930))] = __cached_1;
  }
}

static void reorder__4940_closure_27_0wrapper(void* __stream, int8_t* __restrict__ __module_data, uint64_t i, generic_val* __restrict__ args) noexcept{
  reorder__4940_closure_27(i, (float*)(args[0UL].v_ptr), (float*)(args[1UL].v_ptr));
}

static void reorder__4980_closure_28(uint64_t fused_0fused_0fused_0_fuseiter_3931___fuseiter_3932_1060___fuseiter_3933_1061___fuseiter_3934_1062, float* __restrict__ __ins_0, float* __restrict__ __outs_0) noexcept{
  for (uint64_t _fuseiter_3935 = 0UL; _fuseiter_3935 < 32UL; _fuseiter_3935 += 1UL) {
    float __cached_0;
    __cached_0 = __ins_0[(((fused_0fused_0fused_0_fuseiter_3931___fuseiter_3932_1060___fuseiter_3933_1061___fuseiter_3934_1062 / 8UL) * 256UL) + (_fuseiter_3935 + ((fused_0fused_0fused_0_fuseiter_3931___fuseiter_3932_1060___fuseiter_3933_1061___fuseiter_3934_1062 % 8UL) * 32UL)))];
    float __cached_1;
    __cached_1 = __cached_0;
    __outs_0[(((fused_0fused_0fused_0_fuseiter_3931___fuseiter_3932_1060___fuseiter_3933_1061___fuseiter_3934_1062 / 8UL) * 256UL) + (((fused_0fused_0fused_0_fuseiter_3931___fuseiter_3932_1060___fuseiter_3933_1061___fuseiter_3934_1062 % 8UL) * 32UL) + _fuseiter_3935))] = __cached_1;
  }
}

static void reorder__4980_closure_28_0wrapper(void* __stream, int8_t* __restrict__ __module_data, uint64_t i, generic_val* __restrict__ args) noexcept{
  reorder__4980_closure_28(i, (float*)(args[0UL].v_ptr), (float*)(args[1UL].v_ptr));
}

static void reorder__5010_closure_29(uint64_t fused_0fused_0fused_0_fuseiter_3936___fuseiter_3937_1063___fuseiter_3938_1064___fuseiter_3939_1065, float* __restrict__ __ins_0, float* __restrict__ __outs_0) noexcept{
  for (uint64_t _fuseiter_3940 = 0UL; _fuseiter_3940 < 32UL; _fuseiter_3940 += 1UL) {
    float __cached_0;
    __cached_0 = __ins_0[(((fused_0fused_0fused_0_fuseiter_3936___fuseiter_3937_1063___fuseiter_3938_1064___fuseiter_3939_1065 / 8UL) * 256UL) + (_fuseiter_3940 + ((fused_0fused_0fused_0_fuseiter_3936___fuseiter_3937_1063___fuseiter_3938_1064___fuseiter_3939_1065 % 8UL) * 32UL)))];
    float __cached_1;
    __cached_1 = __cached_0;
    __outs_0[(((fused_0fused_0fused_0_fuseiter_3936___fuseiter_3937_1063___fuseiter_3938_1064___fuseiter_3939_1065 / 8UL) * 256UL) + (((fused_0fused_0fused_0_fuseiter_3936___fuseiter_3937_1063___fuseiter_3938_1064___fuseiter_3939_1065 % 8UL) * 32UL) + _fuseiter_3940))] = __cached_1;
  }
}

static void reorder__5010_closure_29_0wrapper(void* __stream, int8_t* __restrict__ __module_data, uint64_t i, generic_val* __restrict__ args) noexcept{
  reorder__5010_closure_29(i, (float*)(args[0UL].v_ptr), (float*)(args[1UL].v_ptr));
}

static void reorder__5070_closure_30(uint64_t fused_0fused_0fused_0_fuseiter_3941___fuseiter_3942_1066___fuseiter_3943_1067___fuseiter_3944_1068, float* __restrict__ __ins_0, float* __restrict__ __outs_0) noexcept{
  for (uint64_t _fuseiter_3945 = 0UL; _fuseiter_3945 < 64UL; _fuseiter_3945 += 1UL) {
    float __cached_0;
    __cached_0 = __ins_0[(((fused_0fused_0fused_0_fuseiter_3941___fuseiter_3942_1066___fuseiter_3943_1067___fuseiter_3944_1068 / 4UL) * 256UL) + (_fuseiter_3945 + ((fused_0fused_0fused_0_fuseiter_3941___fuseiter_3942_1066___fuseiter_3943_1067___fuseiter_3944_1068 % 4UL) * 64UL)))];
    float __cached_1;
    __cached_1 = __cached_0;
    __outs_0[(((fused_0fused_0fused_0_fuseiter_3941___fuseiter_3942_1066___fuseiter_3943_1067___fuseiter_3944_1068 / 4UL) * 256UL) + (((fused_0fused_0fused_0_fuseiter_3941___fuseiter_3942_1066___fuseiter_3943_1067___fuseiter_3944_1068 % 4UL) * 64UL) + _fuseiter_3945))] = __cached_1;
  }
}

static void reorder__5070_closure_30_0wrapper(void* __stream, int8_t* __restrict__ __module_data, uint64_t i, generic_val* __restrict__ args) noexcept{
  reorder__5070_closure_30(i, (float*)(args[0UL].v_ptr), (float*)(args[1UL].v_ptr));
}

static void reorder__5100_closure_31(uint64_t fused_0fused_0fused_0_fuseiter_3946___fuseiter_3947_1069___fuseiter_3948_1070___fuseiter_3949_1071, float* __restrict__ __ins_0, float* __restrict__ __outs_0) noexcept{
  for (uint64_t _fuseiter_3950 = 0UL; _fuseiter_3950 < 64UL; _fuseiter_3950 += 1UL) {
    float __cached_0;
    __cached_0 = __ins_0[(((fused_0fused_0fused_0_fuseiter_3946___fuseiter_3947_1069___fuseiter_3948_1070___fuseiter_3949_1071 / 4UL) * 256UL) + (_fuseiter_3950 + ((fused_0fused_0fused_0_fuseiter_3946___fuseiter_3947_1069___fuseiter_3948_1070___fuseiter_3949_1071 % 4UL) * 64UL)))];
    float __cached_1;
    __cached_1 = __cached_0;
    __outs_0[(((fused_0fused_0fused_0_fuseiter_3946___fuseiter_3947_1069___fuseiter_3948_1070___fuseiter_3949_1071 / 4UL) * 256UL) + (((fused_0fused_0fused_0_fuseiter_3946___fuseiter_3947_1069___fuseiter_3948_1070___fuseiter_3949_1071 % 4UL) * 64UL) + _fuseiter_3950))] = __cached_1;
  }
}

static void reorder__5100_closure_31_0wrapper(void* __stream, int8_t* __restrict__ __module_data, uint64_t i, generic_val* __restrict__ args) noexcept{
  reorder__5100_closure_31(i, (float*)(args[0UL].v_ptr), (float*)(args[1UL].v_ptr));
}

static void reorder__5170_closure_32(uint64_t fused_0fused_0fused_0_fuseiter_3951___fuseiter_3952_1072___fuseiter_3953_1073___fuseiter_3954_1074, float* __restrict__ __ins_0, float* __restrict__ __outs_0) noexcept{
  for (uint64_t _fuseiter_3955 = 0UL; _fuseiter_3955 < 32UL; _fuseiter_3955 += 1UL) {
    float __cached_0;
    __cached_0 = __ins_0[(((fused_0fused_0fused_0_fuseiter_3951___fuseiter_3952_1072___fuseiter_3953_1073___fuseiter_3954_1074 / 8UL) * 256UL) + (_fuseiter_3955 + ((fused_0fused_0fused_0_fuseiter_3951___fuseiter_3952_1072___fuseiter_3953_1073___fuseiter_3954_1074 % 8UL) * 32UL)))];
    float __cached_1;
    __cached_1 = __cached_0;
    __outs_0[(((fused_0fused_0fused_0_fuseiter_3951___fuseiter_3952_1072___fuseiter_3953_1073___fuseiter_3954_1074 / 8UL) * 256UL) + (((fused_0fused_0fused_0_fuseiter_3951___fuseiter_3952_1072___fuseiter_3953_1073___fuseiter_3954_1074 % 8UL) * 32UL) + _fuseiter_3955))] = __cached_1;
  }
}

static void reorder__5170_closure_32_0wrapper(void* __stream, int8_t* __restrict__ __module_data, uint64_t i, generic_val* __restrict__ args) noexcept{
  reorder__5170_closure_32(i, (float*)(args[0UL].v_ptr), (float*)(args[1UL].v_ptr));
}

static void reorder__5230_closure_33(uint64_t fused_0fused_0fused_0_fuseiter_3956___fuseiter_3957_1075___fuseiter_3958_1076___fuseiter_3959_1077, float* __restrict__ __ins_0, float* __restrict__ __outs_0) noexcept{
  for (uint64_t _fuseiter_3960 = 0UL; _fuseiter_3960 < 32UL; _fuseiter_3960 += 1UL) {
    float __cached_0;
    __cached_0 = __ins_0[(((fused_0fused_0fused_0_fuseiter_3956___fuseiter_3957_1075___fuseiter_3958_1076___fuseiter_3959_1077 / 8UL) * 256UL) + (_fuseiter_3960 + ((fused_0fused_0fused_0_fuseiter_3956___fuseiter_3957_1075___fuseiter_3958_1076___fuseiter_3959_1077 % 8UL) * 32UL)))];
    float __cached_1;
    __cached_1 = __cached_0;
    __outs_0[(((fused_0fused_0fused_0_fuseiter_3956___fuseiter_3957_1075___fuseiter_3958_1076___fuseiter_3959_1077 / 8UL) * 256UL) + (((fused_0fused_0fused_0_fuseiter_3956___fuseiter_3957_1075___fuseiter_3958_1076___fuseiter_3959_1077 % 8UL) * 32UL) + _fuseiter_3960))] = __cached_1;
  }
}

static void reorder__5230_closure_33_0wrapper(void* __stream, int8_t* __restrict__ __module_data, uint64_t i, generic_val* __restrict__ args) noexcept{
  reorder__5230_closure_33(i, (float*)(args[0UL].v_ptr), (float*)(args[1UL].v_ptr));
}

static void reorder__5260_closure_34(uint64_t fused_0fused_0fused_0_fuseiter_3961___fuseiter_3962_1078___fuseiter_3963_1079___fuseiter_3964_1080, float* __restrict__ __ins_0, float* __restrict__ __outs_0) noexcept{
  for (uint64_t _fuseiter_3965 = 0UL; _fuseiter_3965 < 64UL; _fuseiter_3965 += 1UL) {
    float __cached_0;
    __cached_0 = __ins_0[(((fused_0fused_0fused_0_fuseiter_3961___fuseiter_3962_1078___fuseiter_3963_1079___fuseiter_3964_1080 / 4UL) * 256UL) + (_fuseiter_3965 + ((fused_0fused_0fused_0_fuseiter_3961___fuseiter_3962_1078___fuseiter_3963_1079___fuseiter_3964_1080 % 4UL) * 64UL)))];
    float __cached_1;
    __cached_1 = __cached_0;
    __outs_0[(((fused_0fused_0fused_0_fuseiter_3961___fuseiter_3962_1078___fuseiter_3963_1079___fuseiter_3964_1080 / 4UL) * 256UL) + (((fused_0fused_0fused_0_fuseiter_3961___fuseiter_3962_1078___fuseiter_3963_1079___fuseiter_3964_1080 % 4UL) * 64UL) + _fuseiter_3965))] = __cached_1;
  }
}

static void reorder__5260_closure_34_0wrapper(void* __stream, int8_t* __restrict__ __module_data, uint64_t i, generic_val* __restrict__ args) noexcept{
  reorder__5260_closure_34(i, (float*)(args[0UL].v_ptr), (float*)(args[1UL].v_ptr));
}

static void reorder__5320_closure_35(uint64_t fused_0_fuseiter_3966___fuseiter_3967_1081, float* __restrict__ __ins_0, float* __restrict__ __outs_0) noexcept{
  for (uint64_t _fuseiter_3970 = 0UL; _fuseiter_3970 < 16UL; _fuseiter_3970 += 1UL) {
    float __cached_0;
    __cached_0 = __ins_0[(((fused_0_fuseiter_3966___fuseiter_3967_1081 / 128UL) * 2048UL) + (_fuseiter_3970 + ((fused_0_fuseiter_3966___fuseiter_3967_1081 % 128UL) * 16UL)))];
    float __cached_1;
    __cached_1 = __cached_0;
    __outs_0[(((fused_0_fuseiter_3966___fuseiter_3967_1081 / 128UL) * 2048UL) + (((fused_0_fuseiter_3966___fuseiter_3967_1081 % 128UL) * 16UL) + _fuseiter_3970))] = __cached_1;
  }
}

static void reorder__5320_closure_35_0wrapper(void* __stream, int8_t* __restrict__ __module_data, uint64_t i, generic_val* __restrict__ args) noexcept{
  reorder__5320_closure_35(i, (float*)(args[0UL].v_ptr), (float*)(args[1UL].v_ptr));
}

static void reorder__5350_closure_36(uint64_t fused_0fused_0fused_0_fuseiter_3971___fuseiter_3972_1082___fuseiter_3973_1083___fuseiter_3974_1084, float* __restrict__ __ins_0, float* __restrict__ __outs_0) noexcept{
  for (uint64_t _fuseiter_3975 = 0UL; _fuseiter_3975 < 64UL; _fuseiter_3975 += 1UL) {
    float __cached_0;
    __cached_0 = __ins_0[(((fused_0fused_0fused_0_fuseiter_3971___fuseiter_3972_1082___fuseiter_3973_1083___fuseiter_3974_1084 / 8UL) * 512UL) + (_fuseiter_3975 + ((fused_0fused_0fused_0_fuseiter_3971___fuseiter_3972_1082___fuseiter_3973_1083___fuseiter_3974_1084 % 8UL) * 64UL)))];
    float __cached_1;
    __cached_1 = __cached_0;
    __outs_0[(((fused_0fused_0fused_0_fuseiter_3971___fuseiter_3972_1082___fuseiter_3973_1083___fuseiter_3974_1084 / 8UL) * 512UL) + (((fused_0fused_0fused_0_fuseiter_3971___fuseiter_3972_1082___fuseiter_3973_1083___fuseiter_3974_1084 % 8UL) * 64UL) + _fuseiter_3975))] = __cached_1;
  }
}

static void reorder__5350_closure_36_0wrapper(void* __stream, int8_t* __restrict__ __module_data, uint64_t i, generic_val* __restrict__ args) noexcept{
  reorder__5350_closure_36(i, (float*)(args[0UL].v_ptr), (float*)(args[1UL].v_ptr));
}

static void reorder__5380_closure_37(uint64_t fused_0fused_0fused_0_fuseiter_3976___fuseiter_3977_1085___fuseiter_3978_1086___fuseiter_3979_1087, float* __restrict__ __ins_0, float* __restrict__ __outs_0) noexcept{
  for (uint64_t _fuseiter_3980 = 0UL; _fuseiter_3980 < 128UL; _fuseiter_3980 += 1UL) {
    float __cached_0;
    __cached_0 = __ins_0[(((fused_0fused_0fused_0_fuseiter_3976___fuseiter_3977_1085___fuseiter_3978_1086___fuseiter_3979_1087 / 4UL) * 512UL) + (_fuseiter_3980 + ((fused_0fused_0fused_0_fuseiter_3976___fuseiter_3977_1085___fuseiter_3978_1086___fuseiter_3979_1087 % 4UL) * 128UL)))];
    float __cached_1;
    __cached_1 = __cached_0;
    __outs_0[(((fused_0fused_0fused_0_fuseiter_3976___fuseiter_3977_1085___fuseiter_3978_1086___fuseiter_3979_1087 / 4UL) * 512UL) + (((fused_0fused_0fused_0_fuseiter_3976___fuseiter_3977_1085___fuseiter_3978_1086___fuseiter_3979_1087 % 4UL) * 128UL) + _fuseiter_3980))] = __cached_1;
  }
}

static void reorder__5380_closure_37_0wrapper(void* __stream, int8_t* __restrict__ __module_data, uint64_t i, generic_val* __restrict__ args) noexcept{
  reorder__5380_closure_37(i, (float*)(args[0UL].v_ptr), (float*)(args[1UL].v_ptr));
}

static void reorder__5410_closure_38(uint64_t fused_0_fuseiter_3981___fuseiter_3982_1088, float* __restrict__ __ins_0, float* __restrict__ __outs_0) noexcept{
  for (uint64_t _fuseiter_3985 = 0UL; _fuseiter_3985 < 64UL; _fuseiter_3985 += 1UL) {
    float __cached_0;
    __cached_0 = __ins_0[(((fused_0_fuseiter_3981___fuseiter_3982_1088 / 32UL) * 2048UL) + (_fuseiter_3985 + ((fused_0_fuseiter_3981___fuseiter_3982_1088 % 32UL) * 64UL)))];
    float __cached_1;
    __cached_1 = __cached_0;
    __outs_0[(((fused_0_fuseiter_3981___fuseiter_3982_1088 / 32UL) * 2048UL) + (((fused_0_fuseiter_3981___fuseiter_3982_1088 % 32UL) * 64UL) + _fuseiter_3985))] = __cached_1;
  }
}

static void reorder__5410_closure_38_0wrapper(void* __stream, int8_t* __restrict__ __module_data, uint64_t i, generic_val* __restrict__ args) noexcept{
  reorder__5410_closure_38(i, (float*)(args[0UL].v_ptr), (float*)(args[1UL].v_ptr));
}

static void reorder__5440_closure_39(uint64_t fused_0fused_0fused_0_fuseiter_3986___fuseiter_3987_1089___fuseiter_3988_1090___fuseiter_3989_1091, float* __restrict__ __ins_0, float* __restrict__ __outs_0) noexcept{
  for (uint64_t _fuseiter_3990 = 0UL; _fuseiter_3990 < 128UL; _fuseiter_3990 += 1UL) {
    float __cached_0;
    __cached_0 = __ins_0[(((fused_0fused_0fused_0_fuseiter_3986___fuseiter_3987_1089___fuseiter_3988_1090___fuseiter_3989_1091 / 4UL) * 512UL) + (_fuseiter_3990 + ((fused_0fused_0fused_0_fuseiter_3986___fuseiter_3987_1089___fuseiter_3988_1090___fuseiter_3989_1091 % 4UL) * 128UL)))];
    float __cached_1;
    __cached_1 = __cached_0;
    __outs_0[(((fused_0fused_0fused_0_fuseiter_3986___fuseiter_3987_1089___fuseiter_3988_1090___fuseiter_3989_1091 / 4UL) * 512UL) + (((fused_0fused_0fused_0_fuseiter_3986___fuseiter_3987_1089___fuseiter_3988_1090___fuseiter_3989_1091 % 4UL) * 128UL) + _fuseiter_3990))] = __cached_1;
  }
}

static void reorder__5440_closure_39_0wrapper(void* __stream, int8_t* __restrict__ __module_data, uint64_t i, generic_val* __restrict__ args) noexcept{
  reorder__5440_closure_39(i, (float*)(args[0UL].v_ptr), (float*)(args[1UL].v_ptr));
}

static void reorder__5470_closure_40(uint64_t fused_0_fuseiter_3991___fuseiter_3992_1092, float* __restrict__ __ins_0, float* __restrict__ __outs_0) noexcept{
  for (uint64_t _fuseiter_3995 = 0UL; _fuseiter_3995 < 16UL; _fuseiter_3995 += 1UL) {
    float __cached_0;
    __cached_0 = __ins_0[(((fused_0_fuseiter_3991___fuseiter_3992_1092 / 32UL) * 512UL) + (_fuseiter_3995 + ((fused_0_fuseiter_3991___fuseiter_3992_1092 % 32UL) * 16UL)))];
    float __cached_1;
    __cached_1 = __cached_0;
    __outs_0[(((fused_0_fuseiter_3991___fuseiter_3992_1092 / 32UL) * 512UL) + (((fused_0_fuseiter_3991___fuseiter_3992_1092 % 32UL) * 16UL) + _fuseiter_3995))] = __cached_1;
  }
}

static void reorder__5470_closure_40_0wrapper(void* __stream, int8_t* __restrict__ __module_data, uint64_t i, generic_val* __restrict__ args) noexcept{
  reorder__5470_closure_40(i, (float*)(args[0UL].v_ptr), (float*)(args[1UL].v_ptr));
}

static void reorder__5500_closure_41(uint64_t fused_0_fuseiter_3996___fuseiter_3997_1093, float* __restrict__ __ins_0, float* __restrict__ __outs_0) noexcept{
  for (uint64_t _fuseiter_4000 = 0UL; _fuseiter_4000 < 64UL; _fuseiter_4000 += 1UL) {
    float __cached_0;
    __cached_0 = __ins_0[(((fused_0_fuseiter_3996___fuseiter_3997_1093 / 32UL) * 2048UL) + (_fuseiter_4000 + ((fused_0_fuseiter_3996___fuseiter_3997_1093 % 32UL) * 64UL)))];
    float __cached_1;
    __cached_1 = __cached_0;
    __outs_0[(((fused_0_fuseiter_3996___fuseiter_3997_1093 / 32UL) * 2048UL) + (((fused_0_fuseiter_3996___fuseiter_3997_1093 % 32UL) * 64UL) + _fuseiter_4000))] = __cached_1;
  }
}

static void reorder__5500_closure_41_0wrapper(void* __stream, int8_t* __restrict__ __module_data, uint64_t i, generic_val* __restrict__ args) noexcept{
  reorder__5500_closure_41(i, (float*)(args[0UL].v_ptr), (float*)(args[1UL].v_ptr));
}

static void reorder__5530_closure_42(uint64_t fused_0fused_0fused_0_fuseiter_4001___fuseiter_4002_1094___fuseiter_4003_1095___fuseiter_4004_1096, float* __restrict__ __ins_0, float* __restrict__ __outs_0) noexcept{
  for (uint64_t _fuseiter_4005 = 0UL; _fuseiter_4005 < 32UL; _fuseiter_4005 += 1UL) {
    float __cached_0;
    __cached_0 = __ins_0[(((fused_0fused_0fused_0_fuseiter_4001___fuseiter_4002_1094___fuseiter_4003_1095___fuseiter_4004_1096 / 16UL) * 512UL) + (_fuseiter_4005 + ((fused_0fused_0fused_0_fuseiter_4001___fuseiter_4002_1094___fuseiter_4003_1095___fuseiter_4004_1096 % 16UL) * 32UL)))];
    float __cached_1;
    __cached_1 = __cached_0;
    __outs_0[(((fused_0fused_0fused_0_fuseiter_4001___fuseiter_4002_1094___fuseiter_4003_1095___fuseiter_4004_1096 / 16UL) * 512UL) + (((fused_0fused_0fused_0_fuseiter_4001___fuseiter_4002_1094___fuseiter_4003_1095___fuseiter_4004_1096 % 16UL) * 32UL) + _fuseiter_4005))] = __cached_1;
  }
}

static void reorder__5530_closure_42_0wrapper(void* __stream, int8_t* __restrict__ __module_data, uint64_t i, generic_val* __restrict__ args) noexcept{
  reorder__5530_closure_42(i, (float*)(args[0UL].v_ptr), (float*)(args[1UL].v_ptr));
}

static void reorder__5560_closure_43(uint64_t fused_0fused_0fused_0_fuseiter_4006___fuseiter_4007_1097___fuseiter_4008_1098___fuseiter_4009_1099, float* __restrict__ __ins_0, float* __restrict__ __outs_0) noexcept{
  for (uint64_t _fuseiter_4010 = 0UL; _fuseiter_4010 < 128UL; _fuseiter_4010 += 1UL) {
    float __cached_0;
    __cached_0 = __ins_0[(((fused_0fused_0fused_0_fuseiter_4006___fuseiter_4007_1097___fuseiter_4008_1098___fuseiter_4009_1099 / 4UL) * 512UL) + (_fuseiter_4010 + ((fused_0fused_0fused_0_fuseiter_4006___fuseiter_4007_1097___fuseiter_4008_1098___fuseiter_4009_1099 % 4UL) * 128UL)))];
    float __cached_1;
    __cached_1 = __cached_0;
    __outs_0[(((fused_0fused_0fused_0_fuseiter_4006___fuseiter_4007_1097___fuseiter_4008_1098___fuseiter_4009_1099 / 4UL) * 512UL) + (((fused_0fused_0fused_0_fuseiter_4006___fuseiter_4007_1097___fuseiter_4008_1098___fuseiter_4009_1099 % 4UL) * 128UL) + _fuseiter_4010))] = __cached_1;
  }
}

static void reorder__5560_closure_43_0wrapper(void* __stream, int8_t* __restrict__ __module_data, uint64_t i, generic_val* __restrict__ args) noexcept{
  reorder__5560_closure_43(i, (float*)(args[0UL].v_ptr), (float*)(args[1UL].v_ptr));
}

static void reorder__5590_closure_44(uint64_t fused_0fused_0fused_0_fuseiter_4011___fuseiter_4012_1100___fuseiter_4013_1101___fuseiter_4014_1102, float* __restrict__ __ins_0, float* __restrict__ __outs_0) noexcept{
  for (uint64_t _fuseiter_4015 = 0UL; _fuseiter_4015 < 512UL; _fuseiter_4015 += 1UL) {
    float __cached_0;
    __cached_0 = __ins_0[(((fused_0fused_0fused_0_fuseiter_4011___fuseiter_4012_1100___fuseiter_4013_1101___fuseiter_4014_1102 / 4UL) * 2048UL) + (_fuseiter_4015 + ((fused_0fused_0fused_0_fuseiter_4011___fuseiter_4012_1100___fuseiter_4013_1101___fuseiter_4014_1102 % 4UL) * 512UL)))];
    float __cached_1;
    __cached_1 = __cached_0;
    __outs_0[(((fused_0fused_0fused_0_fuseiter_4011___fuseiter_4012_1100___fuseiter_4013_1101___fuseiter_4014_1102 / 4UL) * 2048UL) + (((fused_0fused_0fused_0_fuseiter_4011___fuseiter_4012_1100___fuseiter_4013_1101___fuseiter_4014_1102 % 4UL) * 512UL) + _fuseiter_4015))] = __cached_1;
  }
}

static void reorder__5590_closure_44_0wrapper(void* __stream, int8_t* __restrict__ __module_data, uint64_t i, generic_val* __restrict__ args) noexcept{
  reorder__5590_closure_44(i, (float*)(args[0UL].v_ptr), (float*)(args[1UL].v_ptr));
}

static void reorder__4250_closure_45(uint64_t fused_0fused_0fused_0_fuseiter_4016___fuseiter_4017_1103___fuseiter_4018_1104___fuseiter_4019_1105, float* __restrict__ __ins_0, float* __restrict__ __outs_0) noexcept{
  for (uint64_t _fuseiter_4020 = 0UL; _fuseiter_4020 < 32UL; _fuseiter_4020 += 16UL) {
    vec_f32x16 __cached_0;
    __cached_0 = vec_f32x16::load(&__ins_0[(((fused_0fused_0fused_0_fuseiter_4016___fuseiter_4017_1103___fuseiter_4018_1104___fuseiter_4019_1105 / 2UL) * 64UL) + (_fuseiter_4020 + ((fused_0fused_0fused_0_fuseiter_4016___fuseiter_4017_1103___fuseiter_4018_1104___fuseiter_4019_1105 % 2UL) * 32UL)))]);
    vec_f32x16 __cached_1;
    __cached_1 = __cached_0;
    vec_f32x16::store(__cached_1, &__outs_0[(((fused_0fused_0fused_0_fuseiter_4016___fuseiter_4017_1103___fuseiter_4018_1104___fuseiter_4019_1105 / 2UL) * 64UL) + (((fused_0fused_0fused_0_fuseiter_4016___fuseiter_4017_1103___fuseiter_4018_1104___fuseiter_4019_1105 % 2UL) * 32UL) + _fuseiter_4020))]);
  }
}

static void reorder__4250_closure_45_0wrapper(void* __stream, int8_t* __restrict__ __module_data, uint64_t i, generic_val* __restrict__ args) noexcept{
  reorder__4250_closure_45(i, (float*)(args[0UL].v_ptr), (float*)(args[1UL].v_ptr));
}

static void reorder__4320_closure_46(uint64_t fused_0fused_0fused_0_fuseiter_4021___fuseiter_4022_1106___fuseiter_4023_1107___fuseiter_4024_1108, float* __restrict__ __ins_0, float* __restrict__ __outs_0) noexcept{
  vec_f32x16 __cached_0;
  __cached_0 = vec_f32x16::load(&__ins_0[(((fused_0fused_0fused_0_fuseiter_4021___fuseiter_4022_1106___fuseiter_4023_1107___fuseiter_4024_1108 / 4UL) * 64UL) + ((fused_0fused_0fused_0_fuseiter_4021___fuseiter_4022_1106___fuseiter_4023_1107___fuseiter_4024_1108 % 4UL) * 16UL))]);
  vec_f32x16 __cached_1;
  __cached_1 = __cached_0;
  vec_f32x16::store(__cached_1, &__outs_0[(((fused_0fused_0fused_0_fuseiter_4021___fuseiter_4022_1106___fuseiter_4023_1107___fuseiter_4024_1108 / 4UL) * 64UL) + ((fused_0fused_0fused_0_fuseiter_4021___fuseiter_4022_1106___fuseiter_4023_1107___fuseiter_4024_1108 % 4UL) * 16UL))]);
}

static void reorder__4320_closure_46_0wrapper(void* __stream, int8_t* __restrict__ __module_data, uint64_t i, generic_val* __restrict__ args) noexcept{
  reorder__4320_closure_46(i, (float*)(args[0UL].v_ptr), (float*)(args[1UL].v_ptr));
}

static void reorder__4380_closure_47(uint64_t fused_0fused_0fused_0_fuseiter_4026___fuseiter_4027_1109___fuseiter_4028_1110___fuseiter_4029_1111, float* __restrict__ __ins_0, float* __restrict__ __outs_0) noexcept{
  for (uint64_t _fuseiter_4030 = 0UL; _fuseiter_4030 < 32UL; _fuseiter_4030 += 16UL) {
    vec_f32x16 __cached_0;
    __cached_0 = vec_f32x16::load(&__ins_0[(((fused_0fused_0fused_0_fuseiter_4026___fuseiter_4027_1109___fuseiter_4028_1110___fuseiter_4029_1111 / 2UL) * 64UL) + (_fuseiter_4030 + ((fused_0fused_0fused_0_fuseiter_4026___fuseiter_4027_1109___fuseiter_4028_1110___fuseiter_4029_1111 % 2UL) * 32UL)))]);
    vec_f32x16 __cached_1;
    __cached_1 = __cached_0;
    vec_f32x16::store(__cached_1, &__outs_0[(((fused_0fused_0fused_0_fuseiter_4026___fuseiter_4027_1109___fuseiter_4028_1110___fuseiter_4029_1111 / 2UL) * 64UL) + (((fused_0fused_0fused_0_fuseiter_4026___fuseiter_4027_1109___fuseiter_4028_1110___fuseiter_4029_1111 % 2UL) * 32UL) + _fuseiter_4030))]);
  }
}

static void reorder__4380_closure_47_0wrapper(void* __stream, int8_t* __restrict__ __module_data, uint64_t i, generic_val* __restrict__ args) noexcept{
  reorder__4380_closure_47(i, (float*)(args[0UL].v_ptr), (float*)(args[1UL].v_ptr));
}

static void reorder__4410_closure_48(uint64_t fused_0fused_0fused_0_fuseiter_4031___fuseiter_4032_1112___fuseiter_4033_1113___fuseiter_4034_1114, float* __restrict__ __ins_0, float* __restrict__ __outs_0) noexcept{
  for (uint64_t _fuseiter_4035 = 0UL; _fuseiter_4035 < 32UL; _fuseiter_4035 += 16UL) {
    vec_f32x16 __cached_0;
    __cached_0 = vec_f32x16::load(&__ins_0[(((fused_0fused_0fused_0_fuseiter_4031___fuseiter_4032_1112___fuseiter_4033_1113___fuseiter_4034_1114 / 2UL) * 64UL) + (_fuseiter_4035 + ((fused_0fused_0fused_0_fuseiter_4031___fuseiter_4032_1112___fuseiter_4033_1113___fuseiter_4034_1114 % 2UL) * 32UL)))]);
    vec_f32x16 __cached_1;
    __cached_1 = __cached_0;
    vec_f32x16::store(__cached_1, &__outs_0[(((fused_0fused_0fused_0_fuseiter_4031___fuseiter_4032_1112___fuseiter_4033_1113___fuseiter_4034_1114 / 2UL) * 64UL) + (((fused_0fused_0fused_0_fuseiter_4031___fuseiter_4032_1112___fuseiter_4033_1113___fuseiter_4034_1114 % 2UL) * 32UL) + _fuseiter_4035))]);
  }
}

static void reorder__4410_closure_48_0wrapper(void* __stream, int8_t* __restrict__ __module_data, uint64_t i, generic_val* __restrict__ args) noexcept{
  reorder__4410_closure_48(i, (float*)(args[0UL].v_ptr), (float*)(args[1UL].v_ptr));
}

static void reorder__4500_closure_49(uint64_t fused_0fused_0fused_0_fuseiter_4036___fuseiter_4037_1115___fuseiter_4038_1116___fuseiter_4039_1117, float* __restrict__ __ins_0, float* __restrict__ __outs_0) noexcept{
  for (uint64_t _fuseiter_4040 = 0UL; _fuseiter_4040 < 64UL; _fuseiter_4040 += 16UL) {
    vec_f32x16 __cached_0;
    __cached_0 = vec_f32x16::load(&__ins_0[(((fused_0fused_0fused_0_fuseiter_4036___fuseiter_4037_1115___fuseiter_4038_1116___fuseiter_4039_1117 / 2UL) * 128UL) + (_fuseiter_4040 + ((fused_0fused_0fused_0_fuseiter_4036___fuseiter_4037_1115___fuseiter_4038_1116___fuseiter_4039_1117 % 2UL) * 64UL)))]);
    vec_f32x16 __cached_1;
    __cached_1 = __cached_0;
    vec_f32x16::store(__cached_1, &__outs_0[(((fused_0fused_0fused_0_fuseiter_4036___fuseiter_4037_1115___fuseiter_4038_1116___fuseiter_4039_1117 / 2UL) * 128UL) + (((fused_0fused_0fused_0_fuseiter_4036___fuseiter_4037_1115___fuseiter_4038_1116___fuseiter_4039_1117 % 2UL) * 64UL) + _fuseiter_4040))]);
  }
}

static void reorder__4500_closure_49_0wrapper(void* __stream, int8_t* __restrict__ __module_data, uint64_t i, generic_val* __restrict__ args) noexcept{
  reorder__4500_closure_49(i, (float*)(args[0UL].v_ptr), (float*)(args[1UL].v_ptr));
}

static void reorder__4530_closure_50(uint64_t fused_0fused_0fused_0_fuseiter_4041___fuseiter_4042_1118___fuseiter_4043_1119___fuseiter_4044_1120, float* __restrict__ __ins_0, float* __restrict__ __outs_0) noexcept{
  for (uint64_t _fuseiter_4045 = 0UL; _fuseiter_4045 < 32UL; _fuseiter_4045 += 16UL) {
    vec_f32x16 __cached_0;
    __cached_0 = vec_f32x16::load(&__ins_0[(((fused_0fused_0fused_0_fuseiter_4041___fuseiter_4042_1118___fuseiter_4043_1119___fuseiter_4044_1120 / 4UL) * 128UL) + (_fuseiter_4045 + ((fused_0fused_0fused_0_fuseiter_4041___fuseiter_4042_1118___fuseiter_4043_1119___fuseiter_4044_1120 % 4UL) * 32UL)))]);
    vec_f32x16 __cached_1;
    __cached_1 = __cached_0;
    vec_f32x16::store(__cached_1, &__outs_0[(((fused_0fused_0fused_0_fuseiter_4041___fuseiter_4042_1118___fuseiter_4043_1119___fuseiter_4044_1120 / 4UL) * 128UL) + (((fused_0fused_0fused_0_fuseiter_4041___fuseiter_4042_1118___fuseiter_4043_1119___fuseiter_4044_1120 % 4UL) * 32UL) + _fuseiter_4045))]);
  }
}

static void reorder__4530_closure_50_0wrapper(void* __stream, int8_t* __restrict__ __module_data, uint64_t i, generic_val* __restrict__ args) noexcept{
  reorder__4530_closure_50(i, (float*)(args[0UL].v_ptr), (float*)(args[1UL].v_ptr));
}

static void reorder__4590_closure_51(uint64_t fused_0fused_0fused_0_fuseiter_4046___fuseiter_4047_1121___fuseiter_4048_1122___fuseiter_4049_1123, float* __restrict__ __ins_0, float* __restrict__ __outs_0) noexcept{
  for (uint64_t _fuseiter_4050 = 0UL; _fuseiter_4050 < 32UL; _fuseiter_4050 += 16UL) {
    vec_f32x16 __cached_0;
    __cached_0 = vec_f32x16::load(&__ins_0[(((fused_0fused_0fused_0_fuseiter_4046___fuseiter_4047_1121___fuseiter_4048_1122___fuseiter_4049_1123 / 4UL) * 128UL) + (_fuseiter_4050 + ((fused_0fused_0fused_0_fuseiter_4046___fuseiter_4047_1121___fuseiter_4048_1122___fuseiter_4049_1123 % 4UL) * 32UL)))]);
    vec_f32x16 __cached_1;
    __cached_1 = __cached_0;
    vec_f32x16::store(__cached_1, &__outs_0[(((fused_0fused_0fused_0_fuseiter_4046___fuseiter_4047_1121___fuseiter_4048_1122___fuseiter_4049_1123 / 4UL) * 128UL) + (((fused_0fused_0fused_0_fuseiter_4046___fuseiter_4047_1121___fuseiter_4048_1122___fuseiter_4049_1123 % 4UL) * 32UL) + _fuseiter_4050))]);
  }
}

static void reorder__4590_closure_51_0wrapper(void* __stream, int8_t* __restrict__ __module_data, uint64_t i, generic_val* __restrict__ args) noexcept{
  reorder__4590_closure_51(i, (float*)(args[0UL].v_ptr), (float*)(args[1UL].v_ptr));
}

static void reorder__4670_closure_52(uint64_t fused_0fused_0fused_0_fuseiter_4051___fuseiter_4052_1124___fuseiter_4053_1125___fuseiter_4054_1126, float* __restrict__ __ins_0, float* __restrict__ __outs_0) noexcept{
  for (uint64_t _fuseiter_4055 = 0UL; _fuseiter_4055 < 32UL; _fuseiter_4055 += 16UL) {
    vec_f32x16 __cached_0;
    __cached_0 = vec_f32x16::load(&__ins_0[(((fused_0fused_0fused_0_fuseiter_4051___fuseiter_4052_1124___fuseiter_4053_1125___fuseiter_4054_1126 / 4UL) * 128UL) + (_fuseiter_4055 + ((fused_0fused_0fused_0_fuseiter_4051___fuseiter_4052_1124___fuseiter_4053_1125___fuseiter_4054_1126 % 4UL) * 32UL)))]);
    vec_f32x16 __cached_1;
    __cached_1 = __cached_0;
    vec_f32x16::store(__cached_1, &__outs_0[(((fused_0fused_0fused_0_fuseiter_4051___fuseiter_4052_1124___fuseiter_4053_1125___fuseiter_4054_1126 / 4UL) * 128UL) + (((fused_0fused_0fused_0_fuseiter_4051___fuseiter_4052_1124___fuseiter_4053_1125___fuseiter_4054_1126 % 4UL) * 32UL) + _fuseiter_4055))]);
  }
}

static void reorder__4670_closure_52_0wrapper(void* __stream, int8_t* __restrict__ __module_data, uint64_t i, generic_val* __restrict__ args) noexcept{
  reorder__4670_closure_52(i, (float*)(args[0UL].v_ptr), (float*)(args[1UL].v_ptr));
}

static void reorder__4730_closure_53(uint64_t fused_0fused_0fused_0_fuseiter_4056___fuseiter_4057_1127___fuseiter_4058_1128___fuseiter_4059_1129, float* __restrict__ __ins_0, float* __restrict__ __outs_0) noexcept{
  for (uint64_t _fuseiter_4060 = 0UL; _fuseiter_4060 < 64UL; _fuseiter_4060 += 16UL) {
    vec_f32x16 __cached_0;
    __cached_0 = vec_f32x16::load(&__ins_0[(((fused_0fused_0fused_0_fuseiter_4056___fuseiter_4057_1127___fuseiter_4058_1128___fuseiter_4059_1129 / 2UL) * 128UL) + (_fuseiter_4060 + ((fused_0fused_0fused_0_fuseiter_4056___fuseiter_4057_1127___fuseiter_4058_1128___fuseiter_4059_1129 % 2UL) * 64UL)))]);
    vec_f32x16 __cached_1;
    __cached_1 = __cached_0;
    vec_f32x16::store(__cached_1, &__outs_0[(((fused_0fused_0fused_0_fuseiter_4056___fuseiter_4057_1127___fuseiter_4058_1128___fuseiter_4059_1129 / 2UL) * 128UL) + (((fused_0fused_0fused_0_fuseiter_4056___fuseiter_4057_1127___fuseiter_4058_1128___fuseiter_4059_1129 % 2UL) * 64UL) + _fuseiter_4060))]);
  }
}

static void reorder__4730_closure_53_0wrapper(void* __stream, int8_t* __restrict__ __module_data, uint64_t i, generic_val* __restrict__ args) noexcept{
  reorder__4730_closure_53(i, (float*)(args[0UL].v_ptr), (float*)(args[1UL].v_ptr));
}

static void reorder__4760_closure_54(uint64_t fused_0fused_0fused_0_fuseiter_4061___fuseiter_4062_1130___fuseiter_4063_1131___fuseiter_4064_1132, float* __restrict__ __ins_0, float* __restrict__ __outs_0) noexcept{
  for (uint64_t _fuseiter_4065 = 0UL; _fuseiter_4065 < 32UL; _fuseiter_4065 += 16UL) {
    vec_f32x16 __cached_0;
    __cached_0 = vec_f32x16::load(&__ins_0[(((fused_0fused_0fused_0_fuseiter_4061___fuseiter_4062_1130___fuseiter_4063_1131___fuseiter_4064_1132 / 4UL) * 128UL) + (_fuseiter_4065 + ((fused_0fused_0fused_0_fuseiter_4061___fuseiter_4062_1130___fuseiter_4063_1131___fuseiter_4064_1132 % 4UL) * 32UL)))]);
    vec_f32x16 __cached_1;
    __cached_1 = __cached_0;
    vec_f32x16::store(__cached_1, &__outs_0[(((fused_0fused_0fused_0_fuseiter_4061___fuseiter_4062_1130___fuseiter_4063_1131___fuseiter_4064_1132 / 4UL) * 128UL) + (((fused_0fused_0fused_0_fuseiter_4061___fuseiter_4062_1130___fuseiter_4063_1131___fuseiter_4064_1132 % 4UL) * 32UL) + _fuseiter_4065))]);
  }
}

static void reorder__4760_closure_54_0wrapper(void* __stream, int8_t* __restrict__ __module_data, uint64_t i, generic_val* __restrict__ args) noexcept{
  reorder__4760_closure_54(i, (float*)(args[0UL].v_ptr), (float*)(args[1UL].v_ptr));
}

static void reorder__4210_closure_55(uint64_t fused_0fused_0fused_0_fuseiter_4066___fuseiter_4067_1133___fuseiter_4068_1134___fuseiter_4069_1135, float* __restrict__ __ins_0, float* __restrict__ __outs_0) noexcept{
  vec_f32x16 __cached_0;
  __cached_0 = vec_f32x16::load(&__ins_0[(((fused_0fused_0fused_0_fuseiter_4066___fuseiter_4067_1133___fuseiter_4068_1134___fuseiter_4069_1135 / 16UL) * 256UL) + ((fused_0fused_0fused_0_fuseiter_4066___fuseiter_4067_1133___fuseiter_4068_1134___fuseiter_4069_1135 % 16UL) * 16UL))]);
  vec_f32x16 __cached_1;
  __cached_1 = __cached_0;
  vec_f32x16::store(__cached_1, &__outs_0[(((fused_0fused_0fused_0_fuseiter_4066___fuseiter_4067_1133___fuseiter_4068_1134___fuseiter_4069_1135 / 16UL) * 256UL) + ((fused_0fused_0fused_0_fuseiter_4066___fuseiter_4067_1133___fuseiter_4068_1134___fuseiter_4069_1135 % 16UL) * 16UL))]);
}

static void reorder__4210_closure_55_0wrapper(void* __stream, int8_t* __restrict__ __module_data, uint64_t i, generic_val* __restrict__ args) noexcept{
  reorder__4210_closure_55(i, (float*)(args[0UL].v_ptr), (float*)(args[1UL].v_ptr));
}

static void reorder__4280_closure_56(uint64_t fused_0fused_0fused_0_fuseiter_4071___fuseiter_4072_1136___fuseiter_4073_1137___fuseiter_4074_1138, float* __restrict__ __ins_0, float* __restrict__ __outs_0) noexcept{
  for (uint64_t _fuseiter_4075 = 0UL; _fuseiter_4075 < 64UL; _fuseiter_4075 += 16UL) {
    vec_f32x16 __cached_0;
    __cached_0 = vec_f32x16::load(&__ins_0[(((fused_0fused_0fused_0_fuseiter_4071___fuseiter_4072_1136___fuseiter_4073_1137___fuseiter_4074_1138 / 4UL) * 256UL) + (_fuseiter_4075 + ((fused_0fused_0fused_0_fuseiter_4071___fuseiter_4072_1136___fuseiter_4073_1137___fuseiter_4074_1138 % 4UL) * 64UL)))]);
    vec_f32x16 __cached_1;
    __cached_1 = __cached_0;
    vec_f32x16::store(__cached_1, &__outs_0[(((fused_0fused_0fused_0_fuseiter_4071___fuseiter_4072_1136___fuseiter_4073_1137___fuseiter_4074_1138 / 4UL) * 256UL) + (((fused_0fused_0fused_0_fuseiter_4071___fuseiter_4072_1136___fuseiter_4073_1137___fuseiter_4074_1138 % 4UL) * 64UL) + _fuseiter_4075))]);
  }
}

static void reorder__4280_closure_56_0wrapper(void* __stream, int8_t* __restrict__ __module_data, uint64_t i, generic_val* __restrict__ args) noexcept{
  reorder__4280_closure_56(i, (float*)(args[0UL].v_ptr), (float*)(args[1UL].v_ptr));
}

static void reorder__4350_closure_57(uint64_t fused_0fused_0fused_0_fuseiter_4076___fuseiter_4077_1139___fuseiter_4078_1140___fuseiter_4079_1141, float* __restrict__ __ins_0, float* __restrict__ __outs_0) noexcept{
  for (uint64_t _fuseiter_4080 = 0UL; _fuseiter_4080 < 32UL; _fuseiter_4080 += 16UL) {
    vec_f32x16 __cached_0;
    __cached_0 = vec_f32x16::load(&__ins_0[(((fused_0fused_0fused_0_fuseiter_4076___fuseiter_4077_1139___fuseiter_4078_1140___fuseiter_4079_1141 / 8UL) * 256UL) + (_fuseiter_4080 + ((fused_0fused_0fused_0_fuseiter_4076___fuseiter_4077_1139___fuseiter_4078_1140___fuseiter_4079_1141 % 8UL) * 32UL)))]);
    vec_f32x16 __cached_1;
    __cached_1 = __cached_0;
    vec_f32x16::store(__cached_1, &__outs_0[(((fused_0fused_0fused_0_fuseiter_4076___fuseiter_4077_1139___fuseiter_4078_1140___fuseiter_4079_1141 / 8UL) * 256UL) + (((fused_0fused_0fused_0_fuseiter_4076___fuseiter_4077_1139___fuseiter_4078_1140___fuseiter_4079_1141 % 8UL) * 32UL) + _fuseiter_4080))]);
  }
}

static void reorder__4350_closure_57_0wrapper(void* __stream, int8_t* __restrict__ __module_data, uint64_t i, generic_val* __restrict__ args) noexcept{
  reorder__4350_closure_57(i, (float*)(args[0UL].v_ptr), (float*)(args[1UL].v_ptr));
}

static void reorder__4440_closure_58(uint64_t fused_0fused_0fused_0_fuseiter_4081___fuseiter_4082_1142___fuseiter_4083_1143___fuseiter_4084_1144, float* __restrict__ __ins_0, float* __restrict__ __outs_0) noexcept{
  for (uint64_t _fuseiter_4085 = 0UL; _fuseiter_4085 < 64UL; _fuseiter_4085 += 16UL) {
    vec_f32x16 __cached_0;
    __cached_0 = vec_f32x16::load(&__ins_0[(((fused_0fused_0fused_0_fuseiter_4081___fuseiter_4082_1142___fuseiter_4083_1143___fuseiter_4084_1144 / 4UL) * 256UL) + (_fuseiter_4085 + ((fused_0fused_0fused_0_fuseiter_4081___fuseiter_4082_1142___fuseiter_4083_1143___fuseiter_4084_1144 % 4UL) * 64UL)))]);
    vec_f32x16 __cached_1;
    __cached_1 = __cached_0;
    vec_f32x16::store(__cached_1, &__outs_0[(((fused_0fused_0fused_0_fuseiter_4081___fuseiter_4082_1142___fuseiter_4083_1143___fuseiter_4084_1144 / 4UL) * 256UL) + (((fused_0fused_0fused_0_fuseiter_4081___fuseiter_4082_1142___fuseiter_4083_1143___fuseiter_4084_1144 % 4UL) * 64UL) + _fuseiter_4085))]);
  }
}

static void reorder__4440_closure_58_0wrapper(void* __stream, int8_t* __restrict__ __module_data, uint64_t i, generic_val* __restrict__ args) noexcept{
  reorder__4440_closure_58(i, (float*)(args[0UL].v_ptr), (float*)(args[1UL].v_ptr));
}

static void reorder__4860_closure_59(uint64_t fused_0fused_0fused_0_fuseiter_4086___fuseiter_4087_1145___fuseiter_4088_1146___fuseiter_4089_1147, float* __restrict__ __ins_0, float* __restrict__ __outs_0) noexcept{
  for (uint64_t _fuseiter_4090 = 0UL; _fuseiter_4090 < 128UL; _fuseiter_4090 += 16UL) {
    vec_f32x16 __cached_0;
    __cached_0 = vec_f32x16::load(&__ins_0[(((fused_0fused_0fused_0_fuseiter_4086___fuseiter_4087_1145___fuseiter_4088_1146___fuseiter_4089_1147 / 2UL) * 256UL) + (_fuseiter_4090 + ((fused_0fused_0fused_0_fuseiter_4086___fuseiter_4087_1145___fuseiter_4088_1146___fuseiter_4089_1147 % 2UL) * 128UL)))]);
    vec_f32x16 __cached_1;
    __cached_1 = __cached_0;
    vec_f32x16::store(__cached_1, &__outs_0[(((fused_0fused_0fused_0_fuseiter_4086___fuseiter_4087_1145___fuseiter_4088_1146___fuseiter_4089_1147 / 2UL) * 256UL) + (((fused_0fused_0fused_0_fuseiter_4086___fuseiter_4087_1145___fuseiter_4088_1146___fuseiter_4089_1147 % 2UL) * 128UL) + _fuseiter_4090))]);
  }
}

static void reorder__4860_closure_59_0wrapper(void* __stream, int8_t* __restrict__ __module_data, uint64_t i, generic_val* __restrict__ args) noexcept{
  reorder__4860_closure_59(i, (float*)(args[0UL].v_ptr), (float*)(args[1UL].v_ptr));
}

static void reorder__4920_closure_60(uint64_t fused_0fused_0fused_0_fuseiter_4091___fuseiter_4092_1148___fuseiter_4093_1149___fuseiter_4094_1150, float* __restrict__ __ins_0, float* __restrict__ __outs_0) noexcept{
  for (uint64_t _fuseiter_4095 = 0UL; _fuseiter_4095 < 64UL; _fuseiter_4095 += 16UL) {
    vec_f32x16 __cached_0;
    __cached_0 = vec_f32x16::load(&__ins_0[(((fused_0fused_0fused_0_fuseiter_4091___fuseiter_4092_1148___fuseiter_4093_1149___fuseiter_4094_1150 / 4UL) * 256UL) + (_fuseiter_4095 + ((fused_0fused_0fused_0_fuseiter_4091___fuseiter_4092_1148___fuseiter_4093_1149___fuseiter_4094_1150 % 4UL) * 64UL)))]);
    vec_f32x16 __cached_1;
    __cached_1 = __cached_0;
    vec_f32x16::store(__cached_1, &__outs_0[(((fused_0fused_0fused_0_fuseiter_4091___fuseiter_4092_1148___fuseiter_4093_1149___fuseiter_4094_1150 / 4UL) * 256UL) + (((fused_0fused_0fused_0_fuseiter_4091___fuseiter_4092_1148___fuseiter_4093_1149___fuseiter_4094_1150 % 4UL) * 64UL) + _fuseiter_4095))]);
  }
}

static void reorder__4920_closure_60_0wrapper(void* __stream, int8_t* __restrict__ __module_data, uint64_t i, generic_val* __restrict__ args) noexcept{
  reorder__4920_closure_60(i, (float*)(args[0UL].v_ptr), (float*)(args[1UL].v_ptr));
}

static void reorder__4950_closure_61(uint64_t fused_0fused_0fused_0_fuseiter_4096___fuseiter_4097_1151___fuseiter_4098_1152___fuseiter_4099_1153, float* __restrict__ __ins_0, float* __restrict__ __outs_0) noexcept{
  vec_f32x16 __cached_0;
  __cached_0 = vec_f32x16::load(&__ins_0[(((fused_0fused_0fused_0_fuseiter_4096___fuseiter_4097_1151___fuseiter_4098_1152___fuseiter_4099_1153 / 16UL) * 256UL) + ((fused_0fused_0fused_0_fuseiter_4096___fuseiter_4097_1151___fuseiter_4098_1152___fuseiter_4099_1153 % 16UL) * 16UL))]);
  vec_f32x16 __cached_1;
  __cached_1 = __cached_0;
  vec_f32x16::store(__cached_1, &__outs_0[(((fused_0fused_0fused_0_fuseiter_4096___fuseiter_4097_1151___fuseiter_4098_1152___fuseiter_4099_1153 / 16UL) * 256UL) + ((fused_0fused_0fused_0_fuseiter_4096___fuseiter_4097_1151___fuseiter_4098_1152___fuseiter_4099_1153 % 16UL) * 16UL))]);
}

static void reorder__4950_closure_61_0wrapper(void* __stream, int8_t* __restrict__ __module_data, uint64_t i, generic_val* __restrict__ args) noexcept{
  reorder__4950_closure_61(i, (float*)(args[0UL].v_ptr), (float*)(args[1UL].v_ptr));
}

static void reorder__4990_closure_62(uint64_t fused_0fused_0fused_0_fuseiter_4101___fuseiter_4102_1154___fuseiter_4103_1155___fuseiter_4104_1156, float* __restrict__ __ins_0, float* __restrict__ __outs_0) noexcept{
  for (uint64_t _fuseiter_4105 = 0UL; _fuseiter_4105 < 32UL; _fuseiter_4105 += 16UL) {
    vec_f32x16 __cached_0;
    __cached_0 = vec_f32x16::load(&__ins_0[(((fused_0fused_0fused_0_fuseiter_4101___fuseiter_4102_1154___fuseiter_4103_1155___fuseiter_4104_1156 / 8UL) * 256UL) + (_fuseiter_4105 + ((fused_0fused_0fused_0_fuseiter_4101___fuseiter_4102_1154___fuseiter_4103_1155___fuseiter_4104_1156 % 8UL) * 32UL)))]);
    vec_f32x16 __cached_1;
    __cached_1 = __cached_0;
    vec_f32x16::store(__cached_1, &__outs_0[(((fused_0fused_0fused_0_fuseiter_4101___fuseiter_4102_1154___fuseiter_4103_1155___fuseiter_4104_1156 / 8UL) * 256UL) + (((fused_0fused_0fused_0_fuseiter_4101___fuseiter_4102_1154___fuseiter_4103_1155___fuseiter_4104_1156 % 8UL) * 32UL) + _fuseiter_4105))]);
  }
}

static void reorder__4990_closure_62_0wrapper(void* __stream, int8_t* __restrict__ __module_data, uint64_t i, generic_val* __restrict__ args) noexcept{
  reorder__4990_closure_62(i, (float*)(args[0UL].v_ptr), (float*)(args[1UL].v_ptr));
}

static void reorder__5020_closure_63(uint64_t fused_0fused_0fused_0_fuseiter_4106___fuseiter_4107_1157___fuseiter_4108_1158___fuseiter_4109_1159, float* __restrict__ __ins_0, float* __restrict__ __outs_0) noexcept{
  for (uint64_t _fuseiter_4110 = 0UL; _fuseiter_4110 < 32UL; _fuseiter_4110 += 16UL) {
    vec_f32x16 __cached_0;
    __cached_0 = vec_f32x16::load(&__ins_0[(((fused_0fused_0fused_0_fuseiter_4106___fuseiter_4107_1157___fuseiter_4108_1158___fuseiter_4109_1159 / 8UL) * 256UL) + (_fuseiter_4110 + ((fused_0fused_0fused_0_fuseiter_4106___fuseiter_4107_1157___fuseiter_4108_1158___fuseiter_4109_1159 % 8UL) * 32UL)))]);
    vec_f32x16 __cached_1;
    __cached_1 = __cached_0;
    vec_f32x16::store(__cached_1, &__outs_0[(((fused_0fused_0fused_0_fuseiter_4106___fuseiter_4107_1157___fuseiter_4108_1158___fuseiter_4109_1159 / 8UL) * 256UL) + (((fused_0fused_0fused_0_fuseiter_4106___fuseiter_4107_1157___fuseiter_4108_1158___fuseiter_4109_1159 % 8UL) * 32UL) + _fuseiter_4110))]);
  }
}

static void reorder__5020_closure_63_0wrapper(void* __stream, int8_t* __restrict__ __module_data, uint64_t i, generic_val* __restrict__ args) noexcept{
  reorder__5020_closure_63(i, (float*)(args[0UL].v_ptr), (float*)(args[1UL].v_ptr));
}

static void reorder__5080_closure_64(uint64_t fused_0fused_0fused_0_fuseiter_4111___fuseiter_4112_1160___fuseiter_4113_1161___fuseiter_4114_1162, float* __restrict__ __ins_0, float* __restrict__ __outs_0) noexcept{
  for (uint64_t _fuseiter_4115 = 0UL; _fuseiter_4115 < 64UL; _fuseiter_4115 += 16UL) {
    vec_f32x16 __cached_0;
    __cached_0 = vec_f32x16::load(&__ins_0[(((fused_0fused_0fused_0_fuseiter_4111___fuseiter_4112_1160___fuseiter_4113_1161___fuseiter_4114_1162 / 4UL) * 256UL) + (_fuseiter_4115 + ((fused_0fused_0fused_0_fuseiter_4111___fuseiter_4112_1160___fuseiter_4113_1161___fuseiter_4114_1162 % 4UL) * 64UL)))]);
    vec_f32x16 __cached_1;
    __cached_1 = __cached_0;
    vec_f32x16::store(__cached_1, &__outs_0[(((fused_0fused_0fused_0_fuseiter_4111___fuseiter_4112_1160___fuseiter_4113_1161___fuseiter_4114_1162 / 4UL) * 256UL) + (((fused_0fused_0fused_0_fuseiter_4111___fuseiter_4112_1160___fuseiter_4113_1161___fuseiter_4114_1162 % 4UL) * 64UL) + _fuseiter_4115))]);
  }
}

static void reorder__5080_closure_64_0wrapper(void* __stream, int8_t* __restrict__ __module_data, uint64_t i, generic_val* __restrict__ args) noexcept{
  reorder__5080_closure_64(i, (float*)(args[0UL].v_ptr), (float*)(args[1UL].v_ptr));
}

static void reorder__5110_closure_65(uint64_t fused_0fused_0fused_0_fuseiter_4116___fuseiter_4117_1163___fuseiter_4118_1164___fuseiter_4119_1165, float* __restrict__ __ins_0, float* __restrict__ __outs_0) noexcept{
  for (uint64_t _fuseiter_4120 = 0UL; _fuseiter_4120 < 64UL; _fuseiter_4120 += 16UL) {
    vec_f32x16 __cached_0;
    __cached_0 = vec_f32x16::load(&__ins_0[(((fused_0fused_0fused_0_fuseiter_4116___fuseiter_4117_1163___fuseiter_4118_1164___fuseiter_4119_1165 / 4UL) * 256UL) + (_fuseiter_4120 + ((fused_0fused_0fused_0_fuseiter_4116___fuseiter_4117_1163___fuseiter_4118_1164___fuseiter_4119_1165 % 4UL) * 64UL)))]);
    vec_f32x16 __cached_1;
    __cached_1 = __cached_0;
    vec_f32x16::store(__cached_1, &__outs_0[(((fused_0fused_0fused_0_fuseiter_4116___fuseiter_4117_1163___fuseiter_4118_1164___fuseiter_4119_1165 / 4UL) * 256UL) + (((fused_0fused_0fused_0_fuseiter_4116___fuseiter_4117_1163___fuseiter_4118_1164___fuseiter_4119_1165 % 4UL) * 64UL) + _fuseiter_4120))]);
  }
}

static void reorder__5110_closure_65_0wrapper(void* __stream, int8_t* __restrict__ __module_data, uint64_t i, generic_val* __restrict__ args) noexcept{
  reorder__5110_closure_65(i, (float*)(args[0UL].v_ptr), (float*)(args[1UL].v_ptr));
}

static void reorder__5180_closure_66(uint64_t fused_0fused_0fused_0_fuseiter_4121___fuseiter_4122_1166___fuseiter_4123_1167___fuseiter_4124_1168, float* __restrict__ __ins_0, float* __restrict__ __outs_0) noexcept{
  for (uint64_t _fuseiter_4125 = 0UL; _fuseiter_4125 < 32UL; _fuseiter_4125 += 16UL) {
    vec_f32x16 __cached_0;
    __cached_0 = vec_f32x16::load(&__ins_0[(((fused_0fused_0fused_0_fuseiter_4121___fuseiter_4122_1166___fuseiter_4123_1167___fuseiter_4124_1168 / 8UL) * 256UL) + (_fuseiter_4125 + ((fused_0fused_0fused_0_fuseiter_4121___fuseiter_4122_1166___fuseiter_4123_1167___fuseiter_4124_1168 % 8UL) * 32UL)))]);
    vec_f32x16 __cached_1;
    __cached_1 = __cached_0;
    vec_f32x16::store(__cached_1, &__outs_0[(((fused_0fused_0fused_0_fuseiter_4121___fuseiter_4122_1166___fuseiter_4123_1167___fuseiter_4124_1168 / 8UL) * 256UL) + (((fused_0fused_0fused_0_fuseiter_4121___fuseiter_4122_1166___fuseiter_4123_1167___fuseiter_4124_1168 % 8UL) * 32UL) + _fuseiter_4125))]);
  }
}

static void reorder__5180_closure_66_0wrapper(void* __stream, int8_t* __restrict__ __module_data, uint64_t i, generic_val* __restrict__ args) noexcept{
  reorder__5180_closure_66(i, (float*)(args[0UL].v_ptr), (float*)(args[1UL].v_ptr));
}

static void reorder__5240_closure_67(uint64_t fused_0fused_0fused_0_fuseiter_4126___fuseiter_4127_1169___fuseiter_4128_1170___fuseiter_4129_1171, float* __restrict__ __ins_0, float* __restrict__ __outs_0) noexcept{
  for (uint64_t _fuseiter_4130 = 0UL; _fuseiter_4130 < 32UL; _fuseiter_4130 += 16UL) {
    vec_f32x16 __cached_0;
    __cached_0 = vec_f32x16::load(&__ins_0[(((fused_0fused_0fused_0_fuseiter_4126___fuseiter_4127_1169___fuseiter_4128_1170___fuseiter_4129_1171 / 8UL) * 256UL) + (_fuseiter_4130 + ((fused_0fused_0fused_0_fuseiter_4126___fuseiter_4127_1169___fuseiter_4128_1170___fuseiter_4129_1171 % 8UL) * 32UL)))]);
    vec_f32x16 __cached_1;
    __cached_1 = __cached_0;
    vec_f32x16::store(__cached_1, &__outs_0[(((fused_0fused_0fused_0_fuseiter_4126___fuseiter_4127_1169___fuseiter_4128_1170___fuseiter_4129_1171 / 8UL) * 256UL) + (((fused_0fused_0fused_0_fuseiter_4126___fuseiter_4127_1169___fuseiter_4128_1170___fuseiter_4129_1171 % 8UL) * 32UL) + _fuseiter_4130))]);
  }
}

static void reorder__5240_closure_67_0wrapper(void* __stream, int8_t* __restrict__ __module_data, uint64_t i, generic_val* __restrict__ args) noexcept{
  reorder__5240_closure_67(i, (float*)(args[0UL].v_ptr), (float*)(args[1UL].v_ptr));
}

static void reorder__5270_closure_68(uint64_t fused_0fused_0fused_0_fuseiter_4131___fuseiter_4132_1172___fuseiter_4133_1173___fuseiter_4134_1174, float* __restrict__ __ins_0, float* __restrict__ __outs_0) noexcept{
  for (uint64_t _fuseiter_4135 = 0UL; _fuseiter_4135 < 64UL; _fuseiter_4135 += 16UL) {
    vec_f32x16 __cached_0;
    __cached_0 = vec_f32x16::load(&__ins_0[(((fused_0fused_0fused_0_fuseiter_4131___fuseiter_4132_1172___fuseiter_4133_1173___fuseiter_4134_1174 / 4UL) * 256UL) + (_fuseiter_4135 + ((fused_0fused_0fused_0_fuseiter_4131___fuseiter_4132_1172___fuseiter_4133_1173___fuseiter_4134_1174 % 4UL) * 64UL)))]);
    vec_f32x16 __cached_1;
    __cached_1 = __cached_0;
    vec_f32x16::store(__cached_1, &__outs_0[(((fused_0fused_0fused_0_fuseiter_4131___fuseiter_4132_1172___fuseiter_4133_1173___fuseiter_4134_1174 / 4UL) * 256UL) + (((fused_0fused_0fused_0_fuseiter_4131___fuseiter_4132_1172___fuseiter_4133_1173___fuseiter_4134_1174 % 4UL) * 64UL) + _fuseiter_4135))]);
  }
}

static void reorder__5270_closure_68_0wrapper(void* __stream, int8_t* __restrict__ __module_data, uint64_t i, generic_val* __restrict__ args) noexcept{
  reorder__5270_closure_68(i, (float*)(args[0UL].v_ptr), (float*)(args[1UL].v_ptr));
}

static void reorder__4470_closure_69(uint64_t fused_0fused_0fused_0_fuseiter_4136___fuseiter_4137_1175___fuseiter_4138_1176___fuseiter_4139_1177, float* __restrict__ __ins_0, float* __restrict__ __outs_0) noexcept{
  for (uint64_t _fuseiter_4140 = 0UL; _fuseiter_4140 < 32UL; _fuseiter_4140 += 16UL) {
    vec_f32x16 __cached_0;
    __cached_0 = vec_f32x16::load(&__ins_0[(((fused_0fused_0fused_0_fuseiter_4136___fuseiter_4137_1175___fuseiter_4138_1176___fuseiter_4139_1177 / 16UL) * 512UL) + (_fuseiter_4140 + ((fused_0fused_0fused_0_fuseiter_4136___fuseiter_4137_1175___fuseiter_4138_1176___fuseiter_4139_1177 % 16UL) * 32UL)))]);
    vec_f32x16 __cached_1;
    __cached_1 = __cached_0;
    vec_f32x16::store(__cached_1, &__outs_0[(((fused_0fused_0fused_0_fuseiter_4136___fuseiter_4137_1175___fuseiter_4138_1176___fuseiter_4139_1177 / 16UL) * 512UL) + (((fused_0fused_0fused_0_fuseiter_4136___fuseiter_4137_1175___fuseiter_4138_1176___fuseiter_4139_1177 % 16UL) * 32UL) + _fuseiter_4140))]);
  }
}

static void reorder__4470_closure_69_0wrapper(void* __stream, int8_t* __restrict__ __module_data, uint64_t i, generic_val* __restrict__ args) noexcept{
  reorder__4470_closure_69(i, (float*)(args[0UL].v_ptr), (float*)(args[1UL].v_ptr));
}

static void reorder__4560_closure_70(uint64_t fused_0fused_0fused_0_fuseiter_4141___fuseiter_4142_1178___fuseiter_4143_1179___fuseiter_4144_1180, float* __restrict__ __ins_0, float* __restrict__ __outs_0) noexcept{
  for (uint64_t _fuseiter_4145 = 0UL; _fuseiter_4145 < 32UL; _fuseiter_4145 += 16UL) {
    vec_f32x16 __cached_0;
    __cached_0 = vec_f32x16::load(&__ins_0[(((fused_0fused_0fused_0_fuseiter_4141___fuseiter_4142_1178___fuseiter_4143_1179___fuseiter_4144_1180 / 16UL) * 512UL) + (_fuseiter_4145 + ((fused_0fused_0fused_0_fuseiter_4141___fuseiter_4142_1178___fuseiter_4143_1179___fuseiter_4144_1180 % 16UL) * 32UL)))]);
    vec_f32x16 __cached_1;
    __cached_1 = __cached_0;
    vec_f32x16::store(__cached_1, &__outs_0[(((fused_0fused_0fused_0_fuseiter_4141___fuseiter_4142_1178___fuseiter_4143_1179___fuseiter_4144_1180 / 16UL) * 512UL) + (((fused_0fused_0fused_0_fuseiter_4141___fuseiter_4142_1178___fuseiter_4143_1179___fuseiter_4144_1180 % 16UL) * 32UL) + _fuseiter_4145))]);
  }
}

static void reorder__4560_closure_70_0wrapper(void* __stream, int8_t* __restrict__ __module_data, uint64_t i, generic_val* __restrict__ args) noexcept{
  reorder__4560_closure_70(i, (float*)(args[0UL].v_ptr), (float*)(args[1UL].v_ptr));
}

static void reorder__4630_closure_71(uint64_t fused_0fused_0fused_0_fuseiter_4146___fuseiter_4147_1181___fuseiter_4148_1182___fuseiter_4149_1183, float* __restrict__ __ins_0, float* __restrict__ __outs_0) noexcept{
  for (uint64_t _fuseiter_4150 = 0UL; _fuseiter_4150 < 128UL; _fuseiter_4150 += 16UL) {
    vec_f32x16 __cached_0;
    __cached_0 = vec_f32x16::load(&__ins_0[(((fused_0fused_0fused_0_fuseiter_4146___fuseiter_4147_1181___fuseiter_4148_1182___fuseiter_4149_1183 / 4UL) * 512UL) + (_fuseiter_4150 + ((fused_0fused_0fused_0_fuseiter_4146___fuseiter_4147_1181___fuseiter_4148_1182___fuseiter_4149_1183 % 4UL) * 128UL)))]);
    vec_f32x16 __cached_1;
    __cached_1 = __cached_0;
    vec_f32x16::store(__cached_1, &__outs_0[(((fused_0fused_0fused_0_fuseiter_4146___fuseiter_4147_1181___fuseiter_4148_1182___fuseiter_4149_1183 / 4UL) * 512UL) + (((fused_0fused_0fused_0_fuseiter_4146___fuseiter_4147_1181___fuseiter_4148_1182___fuseiter_4149_1183 % 4UL) * 128UL) + _fuseiter_4150))]);
  }
}

static void reorder__4630_closure_71_0wrapper(void* __stream, int8_t* __restrict__ __module_data, uint64_t i, generic_val* __restrict__ args) noexcept{
  reorder__4630_closure_71(i, (float*)(args[0UL].v_ptr), (float*)(args[1UL].v_ptr));
}

static void reorder__4700_closure_72(uint64_t fused_0fused_0fused_0_fuseiter_4151___fuseiter_4152_1184___fuseiter_4153_1185___fuseiter_4154_1186, float* __restrict__ __ins_0, float* __restrict__ __outs_0) noexcept{
  for (uint64_t _fuseiter_4155 = 0UL; _fuseiter_4155 < 64UL; _fuseiter_4155 += 16UL) {
    vec_f32x16 __cached_0;
    __cached_0 = vec_f32x16::load(&__ins_0[(((fused_0fused_0fused_0_fuseiter_4151___fuseiter_4152_1184___fuseiter_4153_1185___fuseiter_4154_1186 / 8UL) * 512UL) + (_fuseiter_4155 + ((fused_0fused_0fused_0_fuseiter_4151___fuseiter_4152_1184___fuseiter_4153_1185___fuseiter_4154_1186 % 8UL) * 64UL)))]);
    vec_f32x16 __cached_1;
    __cached_1 = __cached_0;
    vec_f32x16::store(__cached_1, &__outs_0[(((fused_0fused_0fused_0_fuseiter_4151___fuseiter_4152_1184___fuseiter_4153_1185___fuseiter_4154_1186 / 8UL) * 512UL) + (((fused_0fused_0fused_0_fuseiter_4151___fuseiter_4152_1184___fuseiter_4153_1185___fuseiter_4154_1186 % 8UL) * 64UL) + _fuseiter_4155))]);
  }
}

static void reorder__4700_closure_72_0wrapper(void* __stream, int8_t* __restrict__ __module_data, uint64_t i, generic_val* __restrict__ args) noexcept{
  reorder__4700_closure_72(i, (float*)(args[0UL].v_ptr), (float*)(args[1UL].v_ptr));
}

static void reorder__4790_closure_73(uint64_t fused_0fused_0fused_0_fuseiter_4156___fuseiter_4157_1187___fuseiter_4158_1188___fuseiter_4159_1189, float* __restrict__ __ins_0, float* __restrict__ __outs_0) noexcept{
  for (uint64_t _fuseiter_4160 = 0UL; _fuseiter_4160 < 128UL; _fuseiter_4160 += 16UL) {
    vec_f32x16 __cached_0;
    __cached_0 = vec_f32x16::load(&__ins_0[(((fused_0fused_0fused_0_fuseiter_4156___fuseiter_4157_1187___fuseiter_4158_1188___fuseiter_4159_1189 / 4UL) * 512UL) + (_fuseiter_4160 + ((fused_0fused_0fused_0_fuseiter_4156___fuseiter_4157_1187___fuseiter_4158_1188___fuseiter_4159_1189 % 4UL) * 128UL)))]);
    vec_f32x16 __cached_1;
    __cached_1 = __cached_0;
    vec_f32x16::store(__cached_1, &__outs_0[(((fused_0fused_0fused_0_fuseiter_4156___fuseiter_4157_1187___fuseiter_4158_1188___fuseiter_4159_1189 / 4UL) * 512UL) + (((fused_0fused_0fused_0_fuseiter_4156___fuseiter_4157_1187___fuseiter_4158_1188___fuseiter_4159_1189 % 4UL) * 128UL) + _fuseiter_4160))]);
  }
}

static void reorder__4790_closure_73_0wrapper(void* __stream, int8_t* __restrict__ __module_data, uint64_t i, generic_val* __restrict__ args) noexcept{
  reorder__4790_closure_73(i, (float*)(args[0UL].v_ptr), (float*)(args[1UL].v_ptr));
}

static void reorder__5360_closure_74(uint64_t fused_0fused_0fused_0_fuseiter_4161___fuseiter_4162_1190___fuseiter_4163_1191___fuseiter_4164_1192, float* __restrict__ __ins_0, float* __restrict__ __outs_0) noexcept{
  for (uint64_t _fuseiter_4165 = 0UL; _fuseiter_4165 < 64UL; _fuseiter_4165 += 16UL) {
    vec_f32x16 __cached_0;
    __cached_0 = vec_f32x16::load(&__ins_0[(((fused_0fused_0fused_0_fuseiter_4161___fuseiter_4162_1190___fuseiter_4163_1191___fuseiter_4164_1192 / 8UL) * 512UL) + (_fuseiter_4165 + ((fused_0fused_0fused_0_fuseiter_4161___fuseiter_4162_1190___fuseiter_4163_1191___fuseiter_4164_1192 % 8UL) * 64UL)))]);
    vec_f32x16 __cached_1;
    __cached_1 = __cached_0;
    vec_f32x16::store(__cached_1, &__outs_0[(((fused_0fused_0fused_0_fuseiter_4161___fuseiter_4162_1190___fuseiter_4163_1191___fuseiter_4164_1192 / 8UL) * 512UL) + (((fused_0fused_0fused_0_fuseiter_4161___fuseiter_4162_1190___fuseiter_4163_1191___fuseiter_4164_1192 % 8UL) * 64UL) + _fuseiter_4165))]);
  }
}

static void reorder__5360_closure_74_0wrapper(void* __stream, int8_t* __restrict__ __module_data, uint64_t i, generic_val* __restrict__ args) noexcept{
  reorder__5360_closure_74(i, (float*)(args[0UL].v_ptr), (float*)(args[1UL].v_ptr));
}

static void reorder__5390_closure_75(uint64_t fused_0fused_0fused_0_fuseiter_4166___fuseiter_4167_1193___fuseiter_4168_1194___fuseiter_4169_1195, float* __restrict__ __ins_0, float* __restrict__ __outs_0) noexcept{
  for (uint64_t _fuseiter_4170 = 0UL; _fuseiter_4170 < 128UL; _fuseiter_4170 += 16UL) {
    vec_f32x16 __cached_0;
    __cached_0 = vec_f32x16::load(&__ins_0[(((fused_0fused_0fused_0_fuseiter_4166___fuseiter_4167_1193___fuseiter_4168_1194___fuseiter_4169_1195 / 4UL) * 512UL) + (_fuseiter_4170 + ((fused_0fused_0fused_0_fuseiter_4166___fuseiter_4167_1193___fuseiter_4168_1194___fuseiter_4169_1195 % 4UL) * 128UL)))]);
    vec_f32x16 __cached_1;
    __cached_1 = __cached_0;
    vec_f32x16::store(__cached_1, &__outs_0[(((fused_0fused_0fused_0_fuseiter_4166___fuseiter_4167_1193___fuseiter_4168_1194___fuseiter_4169_1195 / 4UL) * 512UL) + (((fused_0fused_0fused_0_fuseiter_4166___fuseiter_4167_1193___fuseiter_4168_1194___fuseiter_4169_1195 % 4UL) * 128UL) + _fuseiter_4170))]);
  }
}

static void reorder__5390_closure_75_0wrapper(void* __stream, int8_t* __restrict__ __module_data, uint64_t i, generic_val* __restrict__ args) noexcept{
  reorder__5390_closure_75(i, (float*)(args[0UL].v_ptr), (float*)(args[1UL].v_ptr));
}

static void reorder__5450_closure_76(uint64_t fused_0fused_0fused_0_fuseiter_4171___fuseiter_4172_1196___fuseiter_4173_1197___fuseiter_4174_1198, float* __restrict__ __ins_0, float* __restrict__ __outs_0) noexcept{
  for (uint64_t _fuseiter_4175 = 0UL; _fuseiter_4175 < 128UL; _fuseiter_4175 += 16UL) {
    vec_f32x16 __cached_0;
    __cached_0 = vec_f32x16::load(&__ins_0[(((fused_0fused_0fused_0_fuseiter_4171___fuseiter_4172_1196___fuseiter_4173_1197___fuseiter_4174_1198 / 4UL) * 512UL) + (_fuseiter_4175 + ((fused_0fused_0fused_0_fuseiter_4171___fuseiter_4172_1196___fuseiter_4173_1197___fuseiter_4174_1198 % 4UL) * 128UL)))]);
    vec_f32x16 __cached_1;
    __cached_1 = __cached_0;
    vec_f32x16::store(__cached_1, &__outs_0[(((fused_0fused_0fused_0_fuseiter_4171___fuseiter_4172_1196___fuseiter_4173_1197___fuseiter_4174_1198 / 4UL) * 512UL) + (((fused_0fused_0fused_0_fuseiter_4171___fuseiter_4172_1196___fuseiter_4173_1197___fuseiter_4174_1198 % 4UL) * 128UL) + _fuseiter_4175))]);
  }
}

static void reorder__5450_closure_76_0wrapper(void* __stream, int8_t* __restrict__ __module_data, uint64_t i, generic_val* __restrict__ args) noexcept{
  reorder__5450_closure_76(i, (float*)(args[0UL].v_ptr), (float*)(args[1UL].v_ptr));
}

static void reorder__5480_closure_77(uint64_t fused_0_fuseiter_4176___fuseiter_4177_1199, float* __restrict__ __ins_0, float* __restrict__ __outs_0) noexcept{
  vec_f32x16 __cached_0;
  __cached_0 = vec_f32x16::load(&__ins_0[(((fused_0_fuseiter_4176___fuseiter_4177_1199 / 32UL) * 512UL) + ((fused_0_fuseiter_4176___fuseiter_4177_1199 % 32UL) * 16UL))]);
  vec_f32x16 __cached_1;
  __cached_1 = __cached_0;
  vec_f32x16::store(__cached_1, &__outs_0[(((fused_0_fuseiter_4176___fuseiter_4177_1199 / 32UL) * 512UL) + ((fused_0_fuseiter_4176___fuseiter_4177_1199 % 32UL) * 16UL))]);
}

static void reorder__5480_closure_77_0wrapper(void* __stream, int8_t* __restrict__ __module_data, uint64_t i, generic_val* __restrict__ args) noexcept{
  reorder__5480_closure_77(i, (float*)(args[0UL].v_ptr), (float*)(args[1UL].v_ptr));
}

static void reorder__5540_closure_78(uint64_t fused_0fused_0fused_0_fuseiter_4181___fuseiter_4182_1200___fuseiter_4183_1201___fuseiter_4184_1202, float* __restrict__ __ins_0, float* __restrict__ __outs_0) noexcept{
  for (uint64_t _fuseiter_4185 = 0UL; _fuseiter_4185 < 32UL; _fuseiter_4185 += 16UL) {
    vec_f32x16 __cached_0;
    __cached_0 = vec_f32x16::load(&__ins_0[(((fused_0fused_0fused_0_fuseiter_4181___fuseiter_4182_1200___fuseiter_4183_1201___fuseiter_4184_1202 / 16UL) * 512UL) + (_fuseiter_4185 + ((fused_0fused_0fused_0_fuseiter_4181___fuseiter_4182_1200___fuseiter_4183_1201___fuseiter_4184_1202 % 16UL) * 32UL)))]);
    vec_f32x16 __cached_1;
    __cached_1 = __cached_0;
    vec_f32x16::store(__cached_1, &__outs_0[(((fused_0fused_0fused_0_fuseiter_4181___fuseiter_4182_1200___fuseiter_4183_1201___fuseiter_4184_1202 / 16UL) * 512UL) + (((fused_0fused_0fused_0_fuseiter_4181___fuseiter_4182_1200___fuseiter_4183_1201___fuseiter_4184_1202 % 16UL) * 32UL) + _fuseiter_4185))]);
  }
}

static void reorder__5540_closure_78_0wrapper(void* __stream, int8_t* __restrict__ __module_data, uint64_t i, generic_val* __restrict__ args) noexcept{
  reorder__5540_closure_78(i, (float*)(args[0UL].v_ptr), (float*)(args[1UL].v_ptr));
}

static void reorder__5570_closure_79(uint64_t fused_0fused_0fused_0_fuseiter_4186___fuseiter_4187_1203___fuseiter_4188_1204___fuseiter_4189_1205, float* __restrict__ __ins_0, float* __restrict__ __outs_0) noexcept{
  for (uint64_t _fuseiter_4190 = 0UL; _fuseiter_4190 < 128UL; _fuseiter_4190 += 16UL) {
    vec_f32x16 __cached_0;
    __cached_0 = vec_f32x16::load(&__ins_0[(((fused_0fused_0fused_0_fuseiter_4186___fuseiter_4187_1203___fuseiter_4188_1204___fuseiter_4189_1205 / 4UL) * 512UL) + (_fuseiter_4190 + ((fused_0fused_0fused_0_fuseiter_4186___fuseiter_4187_1203___fuseiter_4188_1204___fuseiter_4189_1205 % 4UL) * 128UL)))]);
    vec_f32x16 __cached_1;
    __cached_1 = __cached_0;
    vec_f32x16::store(__cached_1, &__outs_0[(((fused_0fused_0fused_0_fuseiter_4186___fuseiter_4187_1203___fuseiter_4188_1204___fuseiter_4189_1205 / 4UL) * 512UL) + (((fused_0fused_0fused_0_fuseiter_4186___fuseiter_4187_1203___fuseiter_4188_1204___fuseiter_4189_1205 % 4UL) * 128UL) + _fuseiter_4190))]);
  }
}

static void reorder__5570_closure_79_0wrapper(void* __stream, int8_t* __restrict__ __module_data, uint64_t i, generic_val* __restrict__ args) noexcept{
  reorder__5570_closure_79(i, (float*)(args[0UL].v_ptr), (float*)(args[1UL].v_ptr));
}

static void reorder__4820_closure_80(uint64_t fused_0_fuseiter_4191___fuseiter_4192_1206, float* __restrict__ __ins_0, float* __restrict__ __outs_0) noexcept{
  for (uint64_t _fuseiter_4195 = 0UL; _fuseiter_4195 < 32UL; _fuseiter_4195 += 16UL) {
    vec_f32x16 __cached_0;
    __cached_0 = vec_f32x16::load(&__ins_0[(((fused_0_fuseiter_4191___fuseiter_4192_1206 / 32UL) * 1024UL) + (_fuseiter_4195 + ((fused_0_fuseiter_4191___fuseiter_4192_1206 % 32UL) * 32UL)))]);
    vec_f32x16 __cached_1;
    __cached_1 = __cached_0;
    vec_f32x16::store(__cached_1, &__outs_0[(((fused_0_fuseiter_4191___fuseiter_4192_1206 / 32UL) * 1024UL) + (((fused_0_fuseiter_4191___fuseiter_4192_1206 % 32UL) * 32UL) + _fuseiter_4195))]);
  }
}

static void reorder__4820_closure_80_0wrapper(void* __stream, int8_t* __restrict__ __module_data, uint64_t i, generic_val* __restrict__ args) noexcept{
  reorder__4820_closure_80(i, (float*)(args[0UL].v_ptr), (float*)(args[1UL].v_ptr));
}

static void reorder__4890_closure_81(uint64_t fused_0fused_0fused_0_fuseiter_4196___fuseiter_4197_1207___fuseiter_4198_1208___fuseiter_4199_1209, float* __restrict__ __ins_0, float* __restrict__ __outs_0) noexcept{
  for (uint64_t _fuseiter_4200 = 0UL; _fuseiter_4200 < 128UL; _fuseiter_4200 += 16UL) {
    vec_f32x16 __cached_0;
    __cached_0 = vec_f32x16::load(&__ins_0[(((fused_0fused_0fused_0_fuseiter_4196___fuseiter_4197_1207___fuseiter_4198_1208___fuseiter_4199_1209 / 8UL) * 1024UL) + (_fuseiter_4200 + ((fused_0fused_0fused_0_fuseiter_4196___fuseiter_4197_1207___fuseiter_4198_1208___fuseiter_4199_1209 % 8UL) * 128UL)))]);
    vec_f32x16 __cached_1;
    __cached_1 = __cached_0;
    vec_f32x16::store(__cached_1, &__outs_0[(((fused_0fused_0fused_0_fuseiter_4196___fuseiter_4197_1207___fuseiter_4198_1208___fuseiter_4199_1209 / 8UL) * 1024UL) + (((fused_0fused_0fused_0_fuseiter_4196___fuseiter_4197_1207___fuseiter_4198_1208___fuseiter_4199_1209 % 8UL) * 128UL) + _fuseiter_4200))]);
  }
}

static void reorder__4890_closure_81_0wrapper(void* __stream, int8_t* __restrict__ __module_data, uint64_t i, generic_val* __restrict__ args) noexcept{
  reorder__4890_closure_81(i, (float*)(args[0UL].v_ptr), (float*)(args[1UL].v_ptr));
}

static void reorder__5050_closure_82(uint64_t fused_0fused_0fused_0_fuseiter_4201___fuseiter_4202_1210___fuseiter_4203_1211___fuseiter_4204_1212, float* __restrict__ __ins_0, float* __restrict__ __outs_0) noexcept{
  for (uint64_t _fuseiter_4205 = 0UL; _fuseiter_4205 < 128UL; _fuseiter_4205 += 16UL) {
    vec_f32x16 __cached_0;
    __cached_0 = vec_f32x16::load(&__ins_0[(((fused_0fused_0fused_0_fuseiter_4201___fuseiter_4202_1210___fuseiter_4203_1211___fuseiter_4204_1212 / 8UL) * 1024UL) + (_fuseiter_4205 + ((fused_0fused_0fused_0_fuseiter_4201___fuseiter_4202_1210___fuseiter_4203_1211___fuseiter_4204_1212 % 8UL) * 128UL)))]);
    vec_f32x16 __cached_1;
    __cached_1 = __cached_0;
    vec_f32x16::store(__cached_1, &__outs_0[(((fused_0fused_0fused_0_fuseiter_4201___fuseiter_4202_1210___fuseiter_4203_1211___fuseiter_4204_1212 / 8UL) * 1024UL) + (((fused_0fused_0fused_0_fuseiter_4201___fuseiter_4202_1210___fuseiter_4203_1211___fuseiter_4204_1212 % 8UL) * 128UL) + _fuseiter_4205))]);
  }
}

static void reorder__5050_closure_82_0wrapper(void* __stream, int8_t* __restrict__ __module_data, uint64_t i, generic_val* __restrict__ args) noexcept{
  reorder__5050_closure_82(i, (float*)(args[0UL].v_ptr), (float*)(args[1UL].v_ptr));
}

static void reorder__5140_closure_83(uint64_t fused_0fused_0fused_0_fuseiter_4206___fuseiter_4207_1213___fuseiter_4208_1214___fuseiter_4209_1215, float* __restrict__ __ins_0, float* __restrict__ __outs_0) noexcept{
  for (uint64_t _fuseiter_4210 = 0UL; _fuseiter_4210 < 128UL; _fuseiter_4210 += 16UL) {
    vec_f32x16 __cached_0;
    __cached_0 = vec_f32x16::load(&__ins_0[(((fused_0fused_0fused_0_fuseiter_4206___fuseiter_4207_1213___fuseiter_4208_1214___fuseiter_4209_1215 / 8UL) * 1024UL) + (_fuseiter_4210 + ((fused_0fused_0fused_0_fuseiter_4206___fuseiter_4207_1213___fuseiter_4208_1214___fuseiter_4209_1215 % 8UL) * 128UL)))]);
    vec_f32x16 __cached_1;
    __cached_1 = __cached_0;
    vec_f32x16::store(__cached_1, &__outs_0[(((fused_0fused_0fused_0_fuseiter_4206___fuseiter_4207_1213___fuseiter_4208_1214___fuseiter_4209_1215 / 8UL) * 1024UL) + (((fused_0fused_0fused_0_fuseiter_4206___fuseiter_4207_1213___fuseiter_4208_1214___fuseiter_4209_1215 % 8UL) * 128UL) + _fuseiter_4210))]);
  }
}

static void reorder__5140_closure_83_0wrapper(void* __stream, int8_t* __restrict__ __module_data, uint64_t i, generic_val* __restrict__ args) noexcept{
  reorder__5140_closure_83(i, (float*)(args[0UL].v_ptr), (float*)(args[1UL].v_ptr));
}

static void reorder__5210_closure_84(uint64_t fused_0fused_0fused_0_fuseiter_4211___fuseiter_4212_1216___fuseiter_4213_1217___fuseiter_4214_1218, float* __restrict__ __ins_0, float* __restrict__ __outs_0) noexcept{
  for (uint64_t _fuseiter_4215 = 0UL; _fuseiter_4215 < 128UL; _fuseiter_4215 += 16UL) {
    vec_f32x16 __cached_0;
    __cached_0 = vec_f32x16::load(&__ins_0[(((fused_0fused_0fused_0_fuseiter_4211___fuseiter_4212_1216___fuseiter_4213_1217___fuseiter_4214_1218 / 8UL) * 1024UL) + (_fuseiter_4215 + ((fused_0fused_0fused_0_fuseiter_4211___fuseiter_4212_1216___fuseiter_4213_1217___fuseiter_4214_1218 % 8UL) * 128UL)))]);
    vec_f32x16 __cached_1;
    __cached_1 = __cached_0;
    vec_f32x16::store(__cached_1, &__outs_0[(((fused_0fused_0fused_0_fuseiter_4211___fuseiter_4212_1216___fuseiter_4213_1217___fuseiter_4214_1218 / 8UL) * 1024UL) + (((fused_0fused_0fused_0_fuseiter_4211___fuseiter_4212_1216___fuseiter_4213_1217___fuseiter_4214_1218 % 8UL) * 128UL) + _fuseiter_4215))]);
  }
}

static void reorder__5210_closure_84_0wrapper(void* __stream, int8_t* __restrict__ __module_data, uint64_t i, generic_val* __restrict__ args) noexcept{
  reorder__5210_closure_84(i, (float*)(args[0UL].v_ptr), (float*)(args[1UL].v_ptr));
}

static void reorder__5300_closure_85(uint64_t fused_0fused_0fused_0_fuseiter_4216___fuseiter_4217_1219___fuseiter_4218_1220___fuseiter_4219_1221, float* __restrict__ __ins_0, float* __restrict__ __outs_0) noexcept{
  for (uint64_t _fuseiter_4220 = 0UL; _fuseiter_4220 < 256UL; _fuseiter_4220 += 16UL) {
    vec_f32x16 __cached_0;
    __cached_0 = vec_f32x16::load(&__ins_0[(((fused_0fused_0fused_0_fuseiter_4216___fuseiter_4217_1219___fuseiter_4218_1220___fuseiter_4219_1221 / 4UL) * 1024UL) + (_fuseiter_4220 + ((fused_0fused_0fused_0_fuseiter_4216___fuseiter_4217_1219___fuseiter_4218_1220___fuseiter_4219_1221 % 4UL) * 256UL)))]);
    vec_f32x16 __cached_1;
    __cached_1 = __cached_0;
    vec_f32x16::store(__cached_1, &__outs_0[(((fused_0fused_0fused_0_fuseiter_4216___fuseiter_4217_1219___fuseiter_4218_1220___fuseiter_4219_1221 / 4UL) * 1024UL) + (((fused_0fused_0fused_0_fuseiter_4216___fuseiter_4217_1219___fuseiter_4218_1220___fuseiter_4219_1221 % 4UL) * 256UL) + _fuseiter_4220))]);
  }
}

static void reorder__5300_closure_85_0wrapper(void* __stream, int8_t* __restrict__ __module_data, uint64_t i, generic_val* __restrict__ args) noexcept{
  reorder__5300_closure_85(i, (float*)(args[0UL].v_ptr), (float*)(args[1UL].v_ptr));
}

static void reorder__5330_closure_86(uint64_t fused_0_fuseiter_4221___fuseiter_4222_1222, float* __restrict__ __ins_0, float* __restrict__ __outs_0) noexcept{
  vec_f32x16 __cached_0;
  __cached_0 = vec_f32x16::load(&__ins_0[(((fused_0_fuseiter_4221___fuseiter_4222_1222 / 128UL) * 2048UL) + ((fused_0_fuseiter_4221___fuseiter_4222_1222 % 128UL) * 16UL))]);
  vec_f32x16 __cached_1;
  __cached_1 = __cached_0;
  vec_f32x16::store(__cached_1, &__outs_0[(((fused_0_fuseiter_4221___fuseiter_4222_1222 / 128UL) * 2048UL) + ((fused_0_fuseiter_4221___fuseiter_4222_1222 % 128UL) * 16UL))]);
}

static void reorder__5330_closure_86_0wrapper(void* __stream, int8_t* __restrict__ __module_data, uint64_t i, generic_val* __restrict__ args) noexcept{
  reorder__5330_closure_86(i, (float*)(args[0UL].v_ptr), (float*)(args[1UL].v_ptr));
}

static void reorder__5420_closure_87(uint64_t fused_0_fuseiter_4226___fuseiter_4227_1223, float* __restrict__ __ins_0, float* __restrict__ __outs_0) noexcept{
  for (uint64_t _fuseiter_4230 = 0UL; _fuseiter_4230 < 64UL; _fuseiter_4230 += 16UL) {
    vec_f32x16 __cached_0;
    __cached_0 = vec_f32x16::load(&__ins_0[(((fused_0_fuseiter_4226___fuseiter_4227_1223 / 32UL) * 2048UL) + (_fuseiter_4230 + ((fused_0_fuseiter_4226___fuseiter_4227_1223 % 32UL) * 64UL)))]);
    vec_f32x16 __cached_1;
    __cached_1 = __cached_0;
    vec_f32x16::store(__cached_1, &__outs_0[(((fused_0_fuseiter_4226___fuseiter_4227_1223 / 32UL) * 2048UL) + (((fused_0_fuseiter_4226___fuseiter_4227_1223 % 32UL) * 64UL) + _fuseiter_4230))]);
  }
}

static void reorder__5420_closure_87_0wrapper(void* __stream, int8_t* __restrict__ __module_data, uint64_t i, generic_val* __restrict__ args) noexcept{
  reorder__5420_closure_87(i, (float*)(args[0UL].v_ptr), (float*)(args[1UL].v_ptr));
}

static void reorder__5510_closure_88(uint64_t fused_0_fuseiter_4231___fuseiter_4232_1224, float* __restrict__ __ins_0, float* __restrict__ __outs_0) noexcept{
  for (uint64_t _fuseiter_4235 = 0UL; _fuseiter_4235 < 64UL; _fuseiter_4235 += 16UL) {
    vec_f32x16 __cached_0;
    __cached_0 = vec_f32x16::load(&__ins_0[(((fused_0_fuseiter_4231___fuseiter_4232_1224 / 32UL) * 2048UL) + (_fuseiter_4235 + ((fused_0_fuseiter_4231___fuseiter_4232_1224 % 32UL) * 64UL)))]);
    vec_f32x16 __cached_1;
    __cached_1 = __cached_0;
    vec_f32x16::store(__cached_1, &__outs_0[(((fused_0_fuseiter_4231___fuseiter_4232_1224 / 32UL) * 2048UL) + (((fused_0_fuseiter_4231___fuseiter_4232_1224 % 32UL) * 64UL) + _fuseiter_4235))]);
  }
}

static void reorder__5510_closure_88_0wrapper(void* __stream, int8_t* __restrict__ __module_data, uint64_t i, generic_val* __restrict__ args) noexcept{
  reorder__5510_closure_88(i, (float*)(args[0UL].v_ptr), (float*)(args[1UL].v_ptr));
}

static void reorder__5600_closure_89(uint64_t fused_0fused_0fused_0_fuseiter_4236___fuseiter_4237_1225___fuseiter_4238_1226___fuseiter_4239_1227, float* __restrict__ __ins_0, float* __restrict__ __outs_0) noexcept{
  for (uint64_t _fuseiter_4240 = 0UL; _fuseiter_4240 < 512UL; _fuseiter_4240 += 16UL) {
    vec_f32x16 __cached_0;
    __cached_0 = vec_f32x16::load(&__ins_0[(((fused_0fused_0fused_0_fuseiter_4236___fuseiter_4237_1225___fuseiter_4238_1226___fuseiter_4239_1227 / 4UL) * 2048UL) + (_fuseiter_4240 + ((fused_0fused_0fused_0_fuseiter_4236___fuseiter_4237_1225___fuseiter_4238_1226___fuseiter_4239_1227 % 4UL) * 512UL)))]);
    vec_f32x16 __cached_1;
    __cached_1 = __cached_0;
    vec_f32x16::store(__cached_1, &__outs_0[(((fused_0fused_0fused_0_fuseiter_4236___fuseiter_4237_1225___fuseiter_4238_1226___fuseiter_4239_1227 / 4UL) * 2048UL) + (((fused_0fused_0fused_0_fuseiter_4236___fuseiter_4237_1225___fuseiter_4238_1226___fuseiter_4239_1227 % 4UL) * 512UL) + _fuseiter_4240))]);
  }
}

static void reorder__5600_closure_89_0wrapper(void* __stream, int8_t* __restrict__ __module_data, uint64_t i, generic_val* __restrict__ args) noexcept{
  reorder__5600_closure_89(i, (float*)(args[0UL].v_ptr), (float*)(args[1UL].v_ptr));
}

static void mul__1110_closure_90(uint64_t fused_0fused_0__itr_0____itr_1_1228____itr_2_1229, float* __restrict__ __ins_0, float* __restrict__ __ins_1, float* __restrict__ __outs_0) noexcept{
  float __cached_0;
  __cached_0 = __ins_0[(((fused_0fused_0__itr_0____itr_1_1228____itr_2_1229 / 64UL) * 64UL) + (fused_0fused_0__itr_0____itr_1_1228____itr_2_1229 % 64UL))];
  float __cached_1;
  __cached_1 = __ins_1[(fused_0fused_0__itr_0____itr_1_1228____itr_2_1229 / 64UL)];
  float __cached_2;
  __cached_2 = (__cached_0 * __cached_1);
  __outs_0[(((fused_0fused_0__itr_0____itr_1_1228____itr_2_1229 / 64UL) * 64UL) + (fused_0fused_0__itr_0____itr_1_1228____itr_2_1229 % 64UL))] = __cached_2;
}

static void mul__1110_closure_90_0wrapper(void* __stream, int8_t* __restrict__ __module_data, uint64_t i, generic_val* __restrict__ args) noexcept{
  mul__1110_closure_90(i, (float*)(args[0UL].v_ptr), (float*)(args[1UL].v_ptr), (float*)(args[2UL].v_ptr));
}

static void cast__1120_closure_91(uint64_t fused_0fused_0__itr_0____itr_1_1230____itr_2_1231, float* __restrict__ __ins_0, int8_t* __restrict__ __outs_0) noexcept{
  float __cached_0;
  __cached_0 = __ins_0[(((fused_0fused_0__itr_0____itr_1_1230____itr_2_1231 / 64UL) * 64UL) + (fused_0fused_0__itr_0____itr_1_1230____itr_2_1231 % 64UL))];
  int8_t __cached_1;
  __cached_1 = (int8_t)sc_max(sc_min(sc_round_and_cast<int32_t>(__cached_0), 127), -128);
  __outs_0[(((fused_0fused_0__itr_0____itr_1_1230____itr_2_1231 / 64UL) * 64UL) + (fused_0fused_0__itr_0____itr_1_1230____itr_2_1231 % 64UL))] = __cached_1;
}

static void cast__1120_closure_91_0wrapper(void* __stream, int8_t* __restrict__ __module_data, uint64_t i, generic_val* __restrict__ args) noexcept{
  cast__1120_closure_91(i, (float*)(args[0UL].v_ptr), (int8_t*)(args[1UL].v_ptr));
}

static void mul__1080_closure_92(uint64_t fused_0fused_0__itr_0____itr_1_1232____itr_2_1233, float* __restrict__ __ins_0, float* __restrict__ __ins_1, float* __restrict__ __outs_0) noexcept{
  float __cached_0;
  __cached_0 = __ins_0[(((fused_0fused_0__itr_0____itr_1_1232____itr_2_1233 / 64UL) * 64UL) + (fused_0fused_0__itr_0____itr_1_1232____itr_2_1233 % 64UL))];
  float __cached_1;
  __cached_1 = __ins_1[(fused_0fused_0__itr_0____itr_1_1232____itr_2_1233 / 64UL)];
  float __cached_2;
  __cached_2 = (__cached_0 * __cached_1);
  __outs_0[(((fused_0fused_0__itr_0____itr_1_1232____itr_2_1233 / 64UL) * 64UL) + (fused_0fused_0__itr_0____itr_1_1232____itr_2_1233 % 64UL))] = __cached_2;
}

static void mul__1080_closure_92_0wrapper(void* __stream, int8_t* __restrict__ __module_data, uint64_t i, generic_val* __restrict__ args) noexcept{
  mul__1080_closure_92(i, (float*)(args[0UL].v_ptr), (float*)(args[1UL].v_ptr), (float*)(args[2UL].v_ptr));
}

static void cast__1090_closure_93(uint64_t fused_0fused_0__itr_0____itr_1_1234____itr_2_1235, float* __restrict__ __ins_0, int8_t* __restrict__ __outs_0) noexcept{
  float __cached_0;
  __cached_0 = __ins_0[(((fused_0fused_0__itr_0____itr_1_1234____itr_2_1235 / 64UL) * 64UL) + (fused_0fused_0__itr_0____itr_1_1234____itr_2_1235 % 64UL))];
  int8_t __cached_1;
  __cached_1 = (int8_t)sc_max(sc_min(sc_round_and_cast<int32_t>(__cached_0), 127), -128);
  __outs_0[(((fused_0fused_0__itr_0____itr_1_1234____itr_2_1235 / 64UL) * 64UL) + (fused_0fused_0__itr_0____itr_1_1234____itr_2_1235 % 64UL))] = __cached_1;
}

static void cast__1090_closure_93_0wrapper(void* __stream, int8_t* __restrict__ __module_data, uint64_t i, generic_val* __restrict__ args) noexcept{
  cast__1090_closure_93(i, (float*)(args[0UL].v_ptr), (int8_t*)(args[1UL].v_ptr));
}

static void mul__1170_closure_94(uint64_t fused_0fused_0__itr_0____itr_1_1236____itr_2_1237, float* __restrict__ __ins_0, float* __restrict__ __ins_1, float* __restrict__ __outs_0) noexcept{
  float __cached_0;
  __cached_0 = __ins_0[(((fused_0fused_0__itr_0____itr_1_1236____itr_2_1237 / 64UL) * 64UL) + (fused_0fused_0__itr_0____itr_1_1236____itr_2_1237 % 64UL))];
  float __cached_1;
  __cached_1 = __ins_1[(fused_0fused_0__itr_0____itr_1_1236____itr_2_1237 / 64UL)];
  float __cached_2;
  __cached_2 = (__cached_0 * __cached_1);
  __outs_0[(((fused_0fused_0__itr_0____itr_1_1236____itr_2_1237 / 64UL) * 64UL) + (fused_0fused_0__itr_0____itr_1_1236____itr_2_1237 % 64UL))] = __cached_2;
}

static void mul__1170_closure_94_0wrapper(void* __stream, int8_t* __restrict__ __module_data, uint64_t i, generic_val* __restrict__ args) noexcept{
  mul__1170_closure_94(i, (float*)(args[0UL].v_ptr), (float*)(args[1UL].v_ptr), (float*)(args[2UL].v_ptr));
}

static void cast__1180_closure_95(uint64_t fused_0fused_0__itr_0____itr_1_1238____itr_2_1239, float* __restrict__ __ins_0, int8_t* __restrict__ __outs_0) noexcept{
  float __cached_0;
  __cached_0 = __ins_0[(((fused_0fused_0__itr_0____itr_1_1238____itr_2_1239 / 64UL) * 64UL) + (fused_0fused_0__itr_0____itr_1_1238____itr_2_1239 % 64UL))];
  int8_t __cached_1;
  __cached_1 = (int8_t)sc_max(sc_min(sc_round_and_cast<int32_t>(__cached_0), 127), -128);
  __outs_0[(((fused_0fused_0__itr_0____itr_1_1238____itr_2_1239 / 64UL) * 64UL) + (fused_0fused_0__itr_0____itr_1_1238____itr_2_1239 % 64UL))] = __cached_1;
}

static void cast__1180_closure_95_0wrapper(void* __stream, int8_t* __restrict__ __module_data, uint64_t i, generic_val* __restrict__ args) noexcept{
  cast__1180_closure_95(i, (float*)(args[0UL].v_ptr), (int8_t*)(args[1UL].v_ptr));
}

static void mul__1260_closure_96(uint64_t fused_0fused_0__itr_0____itr_1_1240____itr_2_1241, float* __restrict__ __ins_0, float* __restrict__ __ins_1, float* __restrict__ __outs_0) noexcept{
  float __cached_0;
  __cached_0 = __ins_0[(((fused_0fused_0__itr_0____itr_1_1240____itr_2_1241 / 64UL) * 64UL) + (fused_0fused_0__itr_0____itr_1_1240____itr_2_1241 % 64UL))];
  float __cached_1;
  __cached_1 = __ins_1[(fused_0fused_0__itr_0____itr_1_1240____itr_2_1241 / 64UL)];
  float __cached_2;
  __cached_2 = (__cached_0 * __cached_1);
  __outs_0[(((fused_0fused_0__itr_0____itr_1_1240____itr_2_1241 / 64UL) * 64UL) + (fused_0fused_0__itr_0____itr_1_1240____itr_2_1241 % 64UL))] = __cached_2;
}

static void mul__1260_closure_96_0wrapper(void* __stream, int8_t* __restrict__ __module_data, uint64_t i, generic_val* __restrict__ args) noexcept{
  mul__1260_closure_96(i, (float*)(args[0UL].v_ptr), (float*)(args[1UL].v_ptr), (float*)(args[2UL].v_ptr));
}

static void cast__1270_closure_97(uint64_t fused_0fused_0__itr_0____itr_1_1242____itr_2_1243, float* __restrict__ __ins_0, int8_t* __restrict__ __outs_0) noexcept{
  float __cached_0;
  __cached_0 = __ins_0[(((fused_0fused_0__itr_0____itr_1_1242____itr_2_1243 / 64UL) * 64UL) + (fused_0fused_0__itr_0____itr_1_1242____itr_2_1243 % 64UL))];
  int8_t __cached_1;
  __cached_1 = (int8_t)sc_max(sc_min(sc_round_and_cast<int32_t>(__cached_0), 127), -128);
  __outs_0[(((fused_0fused_0__itr_0____itr_1_1242____itr_2_1243 / 64UL) * 64UL) + (fused_0fused_0__itr_0____itr_1_1242____itr_2_1243 % 64UL))] = __cached_1;
}

static void cast__1270_closure_97_0wrapper(void* __stream, int8_t* __restrict__ __module_data, uint64_t i, generic_val* __restrict__ args) noexcept{
  cast__1270_closure_97(i, (float*)(args[0UL].v_ptr), (int8_t*)(args[1UL].v_ptr));
}

static void mul__1350_closure_98(uint64_t fused_0fused_0__itr_0____itr_1_1244____itr_2_1245, float* __restrict__ __ins_0, float* __restrict__ __ins_1, float* __restrict__ __outs_0) noexcept{
  float __cached_0;
  __cached_0 = __ins_0[(((fused_0fused_0__itr_0____itr_1_1244____itr_2_1245 / 64UL) * 64UL) + (fused_0fused_0__itr_0____itr_1_1244____itr_2_1245 % 64UL))];
  float __cached_1;
  __cached_1 = __ins_1[(fused_0fused_0__itr_0____itr_1_1244____itr_2_1245 / 64UL)];
  float __cached_2;
  __cached_2 = (__cached_0 * __cached_1);
  __outs_0[(((fused_0fused_0__itr_0____itr_1_1244____itr_2_1245 / 64UL) * 64UL) + (fused_0fused_0__itr_0____itr_1_1244____itr_2_1245 % 64UL))] = __cached_2;
}

static void mul__1350_closure_98_0wrapper(void* __stream, int8_t* __restrict__ __module_data, uint64_t i, generic_val* __restrict__ args) noexcept{
  mul__1350_closure_98(i, (float*)(args[0UL].v_ptr), (float*)(args[1UL].v_ptr), (float*)(args[2UL].v_ptr));
}

static void cast__1360_closure_99(uint64_t fused_0fused_0__itr_0____itr_1_1246____itr_2_1247, float* __restrict__ __ins_0, int8_t* __restrict__ __outs_0) noexcept{
  float __cached_0;
  __cached_0 = __ins_0[(((fused_0fused_0__itr_0____itr_1_1246____itr_2_1247 / 64UL) * 64UL) + (fused_0fused_0__itr_0____itr_1_1246____itr_2_1247 % 64UL))];
  int8_t __cached_1;
  __cached_1 = (int8_t)sc_max(sc_min(sc_round_and_cast<int32_t>(__cached_0), 127), -128);
  __outs_0[(((fused_0fused_0__itr_0____itr_1_1246____itr_2_1247 / 64UL) * 64UL) + (fused_0fused_0__itr_0____itr_1_1246____itr_2_1247 % 64UL))] = __cached_1;
}

static void cast__1360_closure_99_0wrapper(void* __stream, int8_t* __restrict__ __module_data, uint64_t i, generic_val* __restrict__ args) noexcept{
  cast__1360_closure_99(i, (float*)(args[0UL].v_ptr), (int8_t*)(args[1UL].v_ptr));
}

static void mul__1200_closure_100(uint64_t fused_0fused_0__itr_0____itr_1_1248____itr_2_1249, float* __restrict__ __ins_0, float* __restrict__ __ins_1, float* __restrict__ __outs_0) noexcept{
  float __cached_0;
  __cached_0 = __ins_0[(((fused_0fused_0__itr_0____itr_1_1248____itr_2_1249 / 256UL) * 256UL) + (fused_0fused_0__itr_0____itr_1_1248____itr_2_1249 % 256UL))];
  float __cached_1;
  __cached_1 = __ins_1[(fused_0fused_0__itr_0____itr_1_1248____itr_2_1249 / 256UL)];
  float __cached_2;
  __cached_2 = (__cached_0 * __cached_1);
  __outs_0[(((fused_0fused_0__itr_0____itr_1_1248____itr_2_1249 / 256UL) * 256UL) + (fused_0fused_0__itr_0____itr_1_1248____itr_2_1249 % 256UL))] = __cached_2;
}

static void mul__1200_closure_100_0wrapper(void* __stream, int8_t* __restrict__ __module_data, uint64_t i, generic_val* __restrict__ args) noexcept{
  mul__1200_closure_100(i, (float*)(args[0UL].v_ptr), (float*)(args[1UL].v_ptr), (float*)(args[2UL].v_ptr));
}

static void cast__1210_closure_101(uint64_t fused_0fused_0__itr_0____itr_1_1250____itr_2_1251, float* __restrict__ __ins_0, int8_t* __restrict__ __outs_0) noexcept{
  float __cached_0;
  __cached_0 = __ins_0[(((fused_0fused_0__itr_0____itr_1_1250____itr_2_1251 / 256UL) * 256UL) + (fused_0fused_0__itr_0____itr_1_1250____itr_2_1251 % 256UL))];
  int8_t __cached_1;
  __cached_1 = (int8_t)sc_max(sc_min(sc_round_and_cast<int32_t>(__cached_0), 127), -128);
  __outs_0[(((fused_0fused_0__itr_0____itr_1_1250____itr_2_1251 / 256UL) * 256UL) + (fused_0fused_0__itr_0____itr_1_1250____itr_2_1251 % 256UL))] = __cached_1;
}

static void cast__1210_closure_101_0wrapper(void* __stream, int8_t* __restrict__ __module_data, uint64_t i, generic_val* __restrict__ args) noexcept{
  cast__1210_closure_101(i, (float*)(args[0UL].v_ptr), (int8_t*)(args[1UL].v_ptr));
}

static void mul__1290_closure_102(uint64_t fused_0fused_0__itr_0____itr_1_1252____itr_2_1253, float* __restrict__ __ins_0, float* __restrict__ __ins_1, float* __restrict__ __outs_0) noexcept{
  float __cached_0;
  __cached_0 = __ins_0[(((fused_0fused_0__itr_0____itr_1_1252____itr_2_1253 / 256UL) * 256UL) + (fused_0fused_0__itr_0____itr_1_1252____itr_2_1253 % 256UL))];
  float __cached_1;
  __cached_1 = __ins_1[(fused_0fused_0__itr_0____itr_1_1252____itr_2_1253 / 256UL)];
  float __cached_2;
  __cached_2 = (__cached_0 * __cached_1);
  __outs_0[(((fused_0fused_0__itr_0____itr_1_1252____itr_2_1253 / 256UL) * 256UL) + (fused_0fused_0__itr_0____itr_1_1252____itr_2_1253 % 256UL))] = __cached_2;
}

static void mul__1290_closure_102_0wrapper(void* __stream, int8_t* __restrict__ __module_data, uint64_t i, generic_val* __restrict__ args) noexcept{
  mul__1290_closure_102(i, (float*)(args[0UL].v_ptr), (float*)(args[1UL].v_ptr), (float*)(args[2UL].v_ptr));
}

static void cast__1300_closure_103(uint64_t fused_0fused_0__itr_0____itr_1_1254____itr_2_1255, float* __restrict__ __ins_0, int8_t* __restrict__ __outs_0) noexcept{
  float __cached_0;
  __cached_0 = __ins_0[(((fused_0fused_0__itr_0____itr_1_1254____itr_2_1255 / 256UL) * 256UL) + (fused_0fused_0__itr_0____itr_1_1254____itr_2_1255 % 256UL))];
  int8_t __cached_1;
  __cached_1 = (int8_t)sc_max(sc_min(sc_round_and_cast<int32_t>(__cached_0), 127), -128);
  __outs_0[(((fused_0fused_0__itr_0____itr_1_1254____itr_2_1255 / 256UL) * 256UL) + (fused_0fused_0__itr_0____itr_1_1254____itr_2_1255 % 256UL))] = __cached_1;
}

static void cast__1300_closure_103_0wrapper(void* __stream, int8_t* __restrict__ __module_data, uint64_t i, generic_val* __restrict__ args) noexcept{
  cast__1300_closure_103(i, (float*)(args[0UL].v_ptr), (int8_t*)(args[1UL].v_ptr));
}

static void mul__1410_closure_104(uint64_t fused_0fused_0__itr_0____itr_1_1256____itr_2_1257, float* __restrict__ __ins_0, float* __restrict__ __ins_1, float* __restrict__ __outs_0) noexcept{
  float __cached_0;
  __cached_0 = __ins_0[(((fused_0fused_0__itr_0____itr_1_1256____itr_2_1257 / 256UL) * 256UL) + (fused_0fused_0__itr_0____itr_1_1256____itr_2_1257 % 256UL))];
  float __cached_1;
  __cached_1 = __ins_1[(fused_0fused_0__itr_0____itr_1_1256____itr_2_1257 / 256UL)];
  float __cached_2;
  __cached_2 = (__cached_0 * __cached_1);
  __outs_0[(((fused_0fused_0__itr_0____itr_1_1256____itr_2_1257 / 256UL) * 256UL) + (fused_0fused_0__itr_0____itr_1_1256____itr_2_1257 % 256UL))] = __cached_2;
}

static void mul__1410_closure_104_0wrapper(void* __stream, int8_t* __restrict__ __module_data, uint64_t i, generic_val* __restrict__ args) noexcept{
  mul__1410_closure_104(i, (float*)(args[0UL].v_ptr), (float*)(args[1UL].v_ptr), (float*)(args[2UL].v_ptr));
}

static void cast__1420_closure_105(uint64_t fused_0fused_0__itr_0____itr_1_1258____itr_2_1259, float* __restrict__ __ins_0, int8_t* __restrict__ __outs_0) noexcept{
  float __cached_0;
  __cached_0 = __ins_0[(((fused_0fused_0__itr_0____itr_1_1258____itr_2_1259 / 256UL) * 256UL) + (fused_0fused_0__itr_0____itr_1_1258____itr_2_1259 % 256UL))];
  int8_t __cached_1;
  __cached_1 = (int8_t)sc_max(sc_min(sc_round_and_cast<int32_t>(__cached_0), 127), -128);
  __outs_0[(((fused_0fused_0__itr_0____itr_1_1258____itr_2_1259 / 256UL) * 256UL) + (fused_0fused_0__itr_0____itr_1_1258____itr_2_1259 % 256UL))] = __cached_1;
}

static void cast__1420_closure_105_0wrapper(void* __stream, int8_t* __restrict__ __module_data, uint64_t i, generic_val* __restrict__ args) noexcept{
  cast__1420_closure_105(i, (float*)(args[0UL].v_ptr), (int8_t*)(args[1UL].v_ptr));
}

static void mul__1140_closure_106(uint64_t fused_0fused_0__itr_0____itr_1_1260____itr_2_1261, float* __restrict__ __ins_0, float* __restrict__ __ins_1, float* __restrict__ __outs_0) noexcept{
  for (uint64_t _fuseiter_4325 = 0UL; _fuseiter_4325 < 3UL; _fuseiter_4325 += 1UL) {
    float __cached_0;
    __cached_0 = __ins_0[((((fused_0fused_0__itr_0____itr_1_1260____itr_2_1261 / 192UL) * 576UL) + ((((fused_0fused_0__itr_0____itr_1_1260____itr_2_1261 / 3UL) % 64UL) * 9UL) + ((fused_0fused_0__itr_0____itr_1_1260____itr_2_1261 % 3UL) * 3UL))) + _fuseiter_4325)];
    float __cached_1;
    __cached_1 = __ins_1[(fused_0fused_0__itr_0____itr_1_1260____itr_2_1261 / 192UL)];
    float __cached_2;
    __cached_2 = (__cached_0 * __cached_1);
    __outs_0[((((fused_0fused_0__itr_0____itr_1_1260____itr_2_1261 / 192UL) * 576UL) + ((((fused_0fused_0__itr_0____itr_1_1260____itr_2_1261 / 3UL) % 64UL) * 9UL) + ((fused_0fused_0__itr_0____itr_1_1260____itr_2_1261 % 3UL) * 3UL))) + _fuseiter_4325)] = __cached_2;
  }
}

static void mul__1140_closure_106_0wrapper(void* __stream, int8_t* __restrict__ __module_data, uint64_t i, generic_val* __restrict__ args) noexcept{
  mul__1140_closure_106(i, (float*)(args[0UL].v_ptr), (float*)(args[1UL].v_ptr), (float*)(args[2UL].v_ptr));
}

static void cast__1150_closure_107(uint64_t fused_0fused_0__itr_0____itr_1_1262____itr_2_1263, float* __restrict__ __ins_0, int8_t* __restrict__ __outs_0) noexcept{
  for (uint64_t _fuseiter4330 = 0UL; _fuseiter4330 < 3UL; _fuseiter4330 += 1UL) {
    float __cached_0;
    __cached_0 = __ins_0[((((fused_0fused_0__itr_0____itr_1_1262____itr_2_1263 / 192UL) * 576UL) + ((((fused_0fused_0__itr_0____itr_1_1262____itr_2_1263 / 3UL) % 64UL) * 9UL) + ((fused_0fused_0__itr_0____itr_1_1262____itr_2_1263 % 3UL) * 3UL))) + _fuseiter4330)];
    int8_t __cached_1;
    __cached_1 = (int8_t)sc_max(sc_min(sc_round_and_cast<int32_t>(__cached_0), 127), -128);
    __outs_0[((((fused_0fused_0__itr_0____itr_1_1262____itr_2_1263 / 192UL) * 576UL) + ((((fused_0fused_0__itr_0____itr_1_1262____itr_2_1263 / 3UL) % 64UL) * 9UL) + ((fused_0fused_0__itr_0____itr_1_1262____itr_2_1263 % 3UL) * 3UL))) + _fuseiter4330)] = __cached_1;
  }
}

static void cast__1150_closure_107_0wrapper(void* __stream, int8_t* __restrict__ __module_data, uint64_t i, generic_val* __restrict__ args) noexcept{
  cast__1150_closure_107(i, (float*)(args[0UL].v_ptr), (int8_t*)(args[1UL].v_ptr));
}

static void mul__1230_closure_108(uint64_t fused_0fused_0__itr_0____itr_1_1264____itr_2_1265, float* __restrict__ __ins_0, float* __restrict__ __ins_1, float* __restrict__ __outs_0) noexcept{
  for (uint64_t _fuseiter_4335 = 0UL; _fuseiter_4335 < 3UL; _fuseiter_4335 += 1UL) {
    float __cached_0;
    __cached_0 = __ins_0[((((fused_0fused_0__itr_0____itr_1_1264____itr_2_1265 / 192UL) * 576UL) + ((((fused_0fused_0__itr_0____itr_1_1264____itr_2_1265 / 3UL) % 64UL) * 9UL) + ((fused_0fused_0__itr_0____itr_1_1264____itr_2_1265 % 3UL) * 3UL))) + _fuseiter_4335)];
    float __cached_1;
    __cached_1 = __ins_1[(fused_0fused_0__itr_0____itr_1_1264____itr_2_1265 / 192UL)];
    float __cached_2;
    __cached_2 = (__cached_0 * __cached_1);
    __outs_0[((((fused_0fused_0__itr_0____itr_1_1264____itr_2_1265 / 192UL) * 576UL) + ((((fused_0fused_0__itr_0____itr_1_1264____itr_2_1265 / 3UL) % 64UL) * 9UL) + ((fused_0fused_0__itr_0____itr_1_1264____itr_2_1265 % 3UL) * 3UL))) + _fuseiter_4335)] = __cached_2;
  }
}

static void mul__1230_closure_108_0wrapper(void* __stream, int8_t* __restrict__ __module_data, uint64_t i, generic_val* __restrict__ args) noexcept{
  mul__1230_closure_108(i, (float*)(args[0UL].v_ptr), (float*)(args[1UL].v_ptr), (float*)(args[2UL].v_ptr));
}

static void cast__1240_closure_109(uint64_t fused_0fused_0__itr_0____itr_1_1266____itr_2_1267, float* __restrict__ __ins_0, int8_t* __restrict__ __outs_0) noexcept{
  for (uint64_t _fuseiter4340 = 0UL; _fuseiter4340 < 3UL; _fuseiter4340 += 1UL) {
    float __cached_0;
    __cached_0 = __ins_0[((((fused_0fused_0__itr_0____itr_1_1266____itr_2_1267 / 192UL) * 576UL) + ((((fused_0fused_0__itr_0____itr_1_1266____itr_2_1267 / 3UL) % 64UL) * 9UL) + ((fused_0fused_0__itr_0____itr_1_1266____itr_2_1267 % 3UL) * 3UL))) + _fuseiter4340)];
    int8_t __cached_1;
    __cached_1 = (int8_t)sc_max(sc_min(sc_round_and_cast<int32_t>(__cached_0), 127), -128);
    __outs_0[((((fused_0fused_0__itr_0____itr_1_1266____itr_2_1267 / 192UL) * 576UL) + ((((fused_0fused_0__itr_0____itr_1_1266____itr_2_1267 / 3UL) % 64UL) * 9UL) + ((fused_0fused_0__itr_0____itr_1_1266____itr_2_1267 % 3UL) * 3UL))) + _fuseiter4340)] = __cached_1;
  }
}

static void cast__1240_closure_109_0wrapper(void* __stream, int8_t* __restrict__ __module_data, uint64_t i, generic_val* __restrict__ args) noexcept{
  cast__1240_closure_109(i, (float*)(args[0UL].v_ptr), (int8_t*)(args[1UL].v_ptr));
}

static void mul__1320_closure_110(uint64_t fused_0fused_0__itr_0____itr_1_1268____itr_2_1269, float* __restrict__ __ins_0, float* __restrict__ __ins_1, float* __restrict__ __outs_0) noexcept{
  for (uint64_t _fuseiter_4345 = 0UL; _fuseiter_4345 < 3UL; _fuseiter_4345 += 1UL) {
    float __cached_0;
    __cached_0 = __ins_0[((((fused_0fused_0__itr_0____itr_1_1268____itr_2_1269 / 192UL) * 576UL) + ((((fused_0fused_0__itr_0____itr_1_1268____itr_2_1269 / 3UL) % 64UL) * 9UL) + ((fused_0fused_0__itr_0____itr_1_1268____itr_2_1269 % 3UL) * 3UL))) + _fuseiter_4345)];
    float __cached_1;
    __cached_1 = __ins_1[(fused_0fused_0__itr_0____itr_1_1268____itr_2_1269 / 192UL)];
    float __cached_2;
    __cached_2 = (__cached_0 * __cached_1);
    __outs_0[((((fused_0fused_0__itr_0____itr_1_1268____itr_2_1269 / 192UL) * 576UL) + ((((fused_0fused_0__itr_0____itr_1_1268____itr_2_1269 / 3UL) % 64UL) * 9UL) + ((fused_0fused_0__itr_0____itr_1_1268____itr_2_1269 % 3UL) * 3UL))) + _fuseiter_4345)] = __cached_2;
  }
}

static void mul__1320_closure_110_0wrapper(void* __stream, int8_t* __restrict__ __module_data, uint64_t i, generic_val* __restrict__ args) noexcept{
  mul__1320_closure_110(i, (float*)(args[0UL].v_ptr), (float*)(args[1UL].v_ptr), (float*)(args[2UL].v_ptr));
}

static void cast__1330_closure_111(uint64_t fused_0fused_0__itr_0____itr_1_1270____itr_2_1271, float* __restrict__ __ins_0, int8_t* __restrict__ __outs_0) noexcept{
  for (uint64_t _fuseiter4350 = 0UL; _fuseiter4350 < 3UL; _fuseiter4350 += 1UL) {
    float __cached_0;
    __cached_0 = __ins_0[((((fused_0fused_0__itr_0____itr_1_1270____itr_2_1271 / 192UL) * 576UL) + ((((fused_0fused_0__itr_0____itr_1_1270____itr_2_1271 / 3UL) % 64UL) * 9UL) + ((fused_0fused_0__itr_0____itr_1_1270____itr_2_1271 % 3UL) * 3UL))) + _fuseiter4350)];
    int8_t __cached_1;
    __cached_1 = (int8_t)sc_max(sc_min(sc_round_and_cast<int32_t>(__cached_0), 127), -128);
    __outs_0[((((fused_0fused_0__itr_0____itr_1_1270____itr_2_1271 / 192UL) * 576UL) + ((((fused_0fused_0__itr_0____itr_1_1270____itr_2_1271 / 3UL) % 64UL) * 9UL) + ((fused_0fused_0__itr_0____itr_1_1270____itr_2_1271 % 3UL) * 3UL))) + _fuseiter4350)] = __cached_1;
  }
}

static void cast__1330_closure_111_0wrapper(void* __stream, int8_t* __restrict__ __module_data, uint64_t i, generic_val* __restrict__ args) noexcept{
  cast__1330_closure_111(i, (float*)(args[0UL].v_ptr), (int8_t*)(args[1UL].v_ptr));
}

static void mul__1470_closure_112(uint64_t fused_0fused_0__itr_0____itr_1_1272____itr_2_1273, float* __restrict__ __ins_0, float* __restrict__ __ins_1, float* __restrict__ __outs_0) noexcept{
  float __cached_0;
  __cached_0 = __ins_0[(((fused_0fused_0__itr_0____itr_1_1272____itr_2_1273 / 128UL) * 128UL) + (fused_0fused_0__itr_0____itr_1_1272____itr_2_1273 % 128UL))];
  float __cached_1;
  __cached_1 = __ins_1[(fused_0fused_0__itr_0____itr_1_1272____itr_2_1273 / 128UL)];
  float __cached_2;
  __cached_2 = (__cached_0 * __cached_1);
  __outs_0[(((fused_0fused_0__itr_0____itr_1_1272____itr_2_1273 / 128UL) * 128UL) + (fused_0fused_0__itr_0____itr_1_1272____itr_2_1273 % 128UL))] = __cached_2;
}

static void mul__1470_closure_112_0wrapper(void* __stream, int8_t* __restrict__ __module_data, uint64_t i, generic_val* __restrict__ args) noexcept{
  mul__1470_closure_112(i, (float*)(args[0UL].v_ptr), (float*)(args[1UL].v_ptr), (float*)(args[2UL].v_ptr));
}

static void cast__1480_closure_113(uint64_t fused_0fused_0__itr_0____itr_1_1274____itr_2_1275, float* __restrict__ __ins_0, int8_t* __restrict__ __outs_0) noexcept{
  float __cached_0;
  __cached_0 = __ins_0[(((fused_0fused_0__itr_0____itr_1_1274____itr_2_1275 / 128UL) * 128UL) + (fused_0fused_0__itr_0____itr_1_1274____itr_2_1275 % 128UL))];
  int8_t __cached_1;
  __cached_1 = (int8_t)sc_max(sc_min(sc_round_and_cast<int32_t>(__cached_0), 127), -128);
  __outs_0[(((fused_0fused_0__itr_0____itr_1_1274____itr_2_1275 / 128UL) * 128UL) + (fused_0fused_0__itr_0____itr_1_1274____itr_2_1275 % 128UL))] = __cached_1;
}

static void cast__1480_closure_113_0wrapper(void* __stream, int8_t* __restrict__ __module_data, uint64_t i, generic_val* __restrict__ args) noexcept{
  cast__1480_closure_113(i, (float*)(args[0UL].v_ptr), (int8_t*)(args[1UL].v_ptr));
}

static void mul__1560_closure_114(uint64_t fused_0fused_0__itr_0____itr_1_1276____itr_2_1277, float* __restrict__ __ins_0, float* __restrict__ __ins_1, float* __restrict__ __outs_0) noexcept{
  float __cached_0;
  __cached_0 = __ins_0[(((fused_0fused_0__itr_0____itr_1_1276____itr_2_1277 / 128UL) * 128UL) + (fused_0fused_0__itr_0____itr_1_1276____itr_2_1277 % 128UL))];
  float __cached_1;
  __cached_1 = __ins_1[(fused_0fused_0__itr_0____itr_1_1276____itr_2_1277 / 128UL)];
  float __cached_2;
  __cached_2 = (__cached_0 * __cached_1);
  __outs_0[(((fused_0fused_0__itr_0____itr_1_1276____itr_2_1277 / 128UL) * 128UL) + (fused_0fused_0__itr_0____itr_1_1276____itr_2_1277 % 128UL))] = __cached_2;
}

static void mul__1560_closure_114_0wrapper(void* __stream, int8_t* __restrict__ __module_data, uint64_t i, generic_val* __restrict__ args) noexcept{
  mul__1560_closure_114(i, (float*)(args[0UL].v_ptr), (float*)(args[1UL].v_ptr), (float*)(args[2UL].v_ptr));
}

static void cast__1570_closure_115(uint64_t fused_0fused_0__itr_0____itr_1_1278____itr_2_1279, float* __restrict__ __ins_0, int8_t* __restrict__ __outs_0) noexcept{
  float __cached_0;
  __cached_0 = __ins_0[(((fused_0fused_0__itr_0____itr_1_1278____itr_2_1279 / 128UL) * 128UL) + (fused_0fused_0__itr_0____itr_1_1278____itr_2_1279 % 128UL))];
  int8_t __cached_1;
  __cached_1 = (int8_t)sc_max(sc_min(sc_round_and_cast<int32_t>(__cached_0), 127), -128);
  __outs_0[(((fused_0fused_0__itr_0____itr_1_1278____itr_2_1279 / 128UL) * 128UL) + (fused_0fused_0__itr_0____itr_1_1278____itr_2_1279 % 128UL))] = __cached_1;
}

static void cast__1570_closure_115_0wrapper(void* __stream, int8_t* __restrict__ __module_data, uint64_t i, generic_val* __restrict__ args) noexcept{
  cast__1570_closure_115(i, (float*)(args[0UL].v_ptr), (int8_t*)(args[1UL].v_ptr));
}

static void mul__1650_closure_116(uint64_t fused_0fused_0__itr_0____itr_1_1280____itr_2_1281, float* __restrict__ __ins_0, float* __restrict__ __ins_1, float* __restrict__ __outs_0) noexcept{
  float __cached_0;
  __cached_0 = __ins_0[(((fused_0fused_0__itr_0____itr_1_1280____itr_2_1281 / 128UL) * 128UL) + (fused_0fused_0__itr_0____itr_1_1280____itr_2_1281 % 128UL))];
  float __cached_1;
  __cached_1 = __ins_1[(fused_0fused_0__itr_0____itr_1_1280____itr_2_1281 / 128UL)];
  float __cached_2;
  __cached_2 = (__cached_0 * __cached_1);
  __outs_0[(((fused_0fused_0__itr_0____itr_1_1280____itr_2_1281 / 128UL) * 128UL) + (fused_0fused_0__itr_0____itr_1_1280____itr_2_1281 % 128UL))] = __cached_2;
}

static void mul__1650_closure_116_0wrapper(void* __stream, int8_t* __restrict__ __module_data, uint64_t i, generic_val* __restrict__ args) noexcept{
  mul__1650_closure_116(i, (float*)(args[0UL].v_ptr), (float*)(args[1UL].v_ptr), (float*)(args[2UL].v_ptr));
}

static void cast__1660_closure_117(uint64_t fused_0fused_0__itr_0____itr_1_1282____itr_2_1283, float* __restrict__ __ins_0, int8_t* __restrict__ __outs_0) noexcept{
  float __cached_0;
  __cached_0 = __ins_0[(((fused_0fused_0__itr_0____itr_1_1282____itr_2_1283 / 128UL) * 128UL) + (fused_0fused_0__itr_0____itr_1_1282____itr_2_1283 % 128UL))];
  int8_t __cached_1;
  __cached_1 = (int8_t)sc_max(sc_min(sc_round_and_cast<int32_t>(__cached_0), 127), -128);
  __outs_0[(((fused_0fused_0__itr_0____itr_1_1282____itr_2_1283 / 128UL) * 128UL) + (fused_0fused_0__itr_0____itr_1_1282____itr_2_1283 % 128UL))] = __cached_1;
}

static void cast__1660_closure_117_0wrapper(void* __stream, int8_t* __restrict__ __module_data, uint64_t i, generic_val* __restrict__ args) noexcept{
  cast__1660_closure_117(i, (float*)(args[0UL].v_ptr), (int8_t*)(args[1UL].v_ptr));
}

static void mul__1740_closure_118(uint64_t fused_0fused_0__itr_0____itr_1_1284____itr_2_1285, float* __restrict__ __ins_0, float* __restrict__ __ins_1, float* __restrict__ __outs_0) noexcept{
  float __cached_0;
  __cached_0 = __ins_0[(((fused_0fused_0__itr_0____itr_1_1284____itr_2_1285 / 128UL) * 128UL) + (fused_0fused_0__itr_0____itr_1_1284____itr_2_1285 % 128UL))];
  float __cached_1;
  __cached_1 = __ins_1[(fused_0fused_0__itr_0____itr_1_1284____itr_2_1285 / 128UL)];
  float __cached_2;
  __cached_2 = (__cached_0 * __cached_1);
  __outs_0[(((fused_0fused_0__itr_0____itr_1_1284____itr_2_1285 / 128UL) * 128UL) + (fused_0fused_0__itr_0____itr_1_1284____itr_2_1285 % 128UL))] = __cached_2;
}

static void mul__1740_closure_118_0wrapper(void* __stream, int8_t* __restrict__ __module_data, uint64_t i, generic_val* __restrict__ args) noexcept{
  mul__1740_closure_118(i, (float*)(args[0UL].v_ptr), (float*)(args[1UL].v_ptr), (float*)(args[2UL].v_ptr));
}

static void cast__1750_closure_119(uint64_t fused_0fused_0__itr_0____itr_1_1286____itr_2_1287, float* __restrict__ __ins_0, int8_t* __restrict__ __outs_0) noexcept{
  float __cached_0;
  __cached_0 = __ins_0[(((fused_0fused_0__itr_0____itr_1_1286____itr_2_1287 / 128UL) * 128UL) + (fused_0fused_0__itr_0____itr_1_1286____itr_2_1287 % 128UL))];
  int8_t __cached_1;
  __cached_1 = (int8_t)sc_max(sc_min(sc_round_and_cast<int32_t>(__cached_0), 127), -128);
  __outs_0[(((fused_0fused_0__itr_0____itr_1_1286____itr_2_1287 / 128UL) * 128UL) + (fused_0fused_0__itr_0____itr_1_1286____itr_2_1287 % 128UL))] = __cached_1;
}

static void cast__1750_closure_119_0wrapper(void* __stream, int8_t* __restrict__ __module_data, uint64_t i, generic_val* __restrict__ args) noexcept{
  cast__1750_closure_119(i, (float*)(args[0UL].v_ptr), (int8_t*)(args[1UL].v_ptr));
}

static void mul__1500_closure_120(uint64_t fused_0fused_0__itr_0____itr_1_1288____itr_2_1289, float* __restrict__ __ins_0, float* __restrict__ __ins_1, float* __restrict__ __outs_0) noexcept{
  float __cached_0;
  __cached_0 = __ins_0[(((fused_0fused_0__itr_0____itr_1_1288____itr_2_1289 / 512UL) * 512UL) + (fused_0fused_0__itr_0____itr_1_1288____itr_2_1289 % 512UL))];
  float __cached_1;
  __cached_1 = __ins_1[(fused_0fused_0__itr_0____itr_1_1288____itr_2_1289 / 512UL)];
  float __cached_2;
  __cached_2 = (__cached_0 * __cached_1);
  __outs_0[(((fused_0fused_0__itr_0____itr_1_1288____itr_2_1289 / 512UL) * 512UL) + (fused_0fused_0__itr_0____itr_1_1288____itr_2_1289 % 512UL))] = __cached_2;
}

static void mul__1500_closure_120_0wrapper(void* __stream, int8_t* __restrict__ __module_data, uint64_t i, generic_val* __restrict__ args) noexcept{
  mul__1500_closure_120(i, (float*)(args[0UL].v_ptr), (float*)(args[1UL].v_ptr), (float*)(args[2UL].v_ptr));
}

static void cast__1510_closure_121(uint64_t fused_0fused_0__itr_0____itr_1_1290____itr_2_1291, float* __restrict__ __ins_0, int8_t* __restrict__ __outs_0) noexcept{
  float __cached_0;
  __cached_0 = __ins_0[(((fused_0fused_0__itr_0____itr_1_1290____itr_2_1291 / 512UL) * 512UL) + (fused_0fused_0__itr_0____itr_1_1290____itr_2_1291 % 512UL))];
  int8_t __cached_1;
  __cached_1 = (int8_t)sc_max(sc_min(sc_round_and_cast<int32_t>(__cached_0), 127), -128);
  __outs_0[(((fused_0fused_0__itr_0____itr_1_1290____itr_2_1291 / 512UL) * 512UL) + (fused_0fused_0__itr_0____itr_1_1290____itr_2_1291 % 512UL))] = __cached_1;
}

static void cast__1510_closure_121_0wrapper(void* __stream, int8_t* __restrict__ __module_data, uint64_t i, generic_val* __restrict__ args) noexcept{
  cast__1510_closure_121(i, (float*)(args[0UL].v_ptr), (int8_t*)(args[1UL].v_ptr));
}

static void mul__1590_closure_122(uint64_t fused_0fused_0__itr_0____itr_1_1292____itr_2_1293, float* __restrict__ __ins_0, float* __restrict__ __ins_1, float* __restrict__ __outs_0) noexcept{
  float __cached_0;
  __cached_0 = __ins_0[(((fused_0fused_0__itr_0____itr_1_1292____itr_2_1293 / 512UL) * 512UL) + (fused_0fused_0__itr_0____itr_1_1292____itr_2_1293 % 512UL))];
  float __cached_1;
  __cached_1 = __ins_1[(fused_0fused_0__itr_0____itr_1_1292____itr_2_1293 / 512UL)];
  float __cached_2;
  __cached_2 = (__cached_0 * __cached_1);
  __outs_0[(((fused_0fused_0__itr_0____itr_1_1292____itr_2_1293 / 512UL) * 512UL) + (fused_0fused_0__itr_0____itr_1_1292____itr_2_1293 % 512UL))] = __cached_2;
}

static void mul__1590_closure_122_0wrapper(void* __stream, int8_t* __restrict__ __module_data, uint64_t i, generic_val* __restrict__ args) noexcept{
  mul__1590_closure_122(i, (float*)(args[0UL].v_ptr), (float*)(args[1UL].v_ptr), (float*)(args[2UL].v_ptr));
}

static void cast__1600_closure_123(uint64_t fused_0fused_0__itr_0____itr_1_1294____itr_2_1295, float* __restrict__ __ins_0, int8_t* __restrict__ __outs_0) noexcept{
  float __cached_0;
  __cached_0 = __ins_0[(((fused_0fused_0__itr_0____itr_1_1294____itr_2_1295 / 512UL) * 512UL) + (fused_0fused_0__itr_0____itr_1_1294____itr_2_1295 % 512UL))];
  int8_t __cached_1;
  __cached_1 = (int8_t)sc_max(sc_min(sc_round_and_cast<int32_t>(__cached_0), 127), -128);
  __outs_0[(((fused_0fused_0__itr_0____itr_1_1294____itr_2_1295 / 512UL) * 512UL) + (fused_0fused_0__itr_0____itr_1_1294____itr_2_1295 % 512UL))] = __cached_1;
}

static void cast__1600_closure_123_0wrapper(void* __stream, int8_t* __restrict__ __module_data, uint64_t i, generic_val* __restrict__ args) noexcept{
  cast__1600_closure_123(i, (float*)(args[0UL].v_ptr), (int8_t*)(args[1UL].v_ptr));
}

static void mul__1680_closure_124(uint64_t fused_0fused_0__itr_0____itr_1_1296____itr_2_1297, float* __restrict__ __ins_0, float* __restrict__ __ins_1, float* __restrict__ __outs_0) noexcept{
  float __cached_0;
  __cached_0 = __ins_0[(((fused_0fused_0__itr_0____itr_1_1296____itr_2_1297 / 512UL) * 512UL) + (fused_0fused_0__itr_0____itr_1_1296____itr_2_1297 % 512UL))];
  float __cached_1;
  __cached_1 = __ins_1[(fused_0fused_0__itr_0____itr_1_1296____itr_2_1297 / 512UL)];
  float __cached_2;
  __cached_2 = (__cached_0 * __cached_1);
  __outs_0[(((fused_0fused_0__itr_0____itr_1_1296____itr_2_1297 / 512UL) * 512UL) + (fused_0fused_0__itr_0____itr_1_1296____itr_2_1297 % 512UL))] = __cached_2;
}

static void mul__1680_closure_124_0wrapper(void* __stream, int8_t* __restrict__ __module_data, uint64_t i, generic_val* __restrict__ args) noexcept{
  mul__1680_closure_124(i, (float*)(args[0UL].v_ptr), (float*)(args[1UL].v_ptr), (float*)(args[2UL].v_ptr));
}

static void cast__1690_closure_125(uint64_t fused_0fused_0__itr_0____itr_1_1298____itr_2_1299, float* __restrict__ __ins_0, int8_t* __restrict__ __outs_0) noexcept{
  float __cached_0;
  __cached_0 = __ins_0[(((fused_0fused_0__itr_0____itr_1_1298____itr_2_1299 / 512UL) * 512UL) + (fused_0fused_0__itr_0____itr_1_1298____itr_2_1299 % 512UL))];
  int8_t __cached_1;
  __cached_1 = (int8_t)sc_max(sc_min(sc_round_and_cast<int32_t>(__cached_0), 127), -128);
  __outs_0[(((fused_0fused_0__itr_0____itr_1_1298____itr_2_1299 / 512UL) * 512UL) + (fused_0fused_0__itr_0____itr_1_1298____itr_2_1299 % 512UL))] = __cached_1;
}

static void cast__1690_closure_125_0wrapper(void* __stream, int8_t* __restrict__ __module_data, uint64_t i, generic_val* __restrict__ args) noexcept{
  cast__1690_closure_125(i, (float*)(args[0UL].v_ptr), (int8_t*)(args[1UL].v_ptr));
}

static void mul__1380_closure_126(uint64_t fused_0fused_0__itr_0____itr_1_1300____itr_2_1301, float* __restrict__ __ins_0, float* __restrict__ __ins_1, float* __restrict__ __outs_0) noexcept{
  float __cached_0;
  __cached_0 = __ins_0[(((fused_0fused_0__itr_0____itr_1_1300____itr_2_1301 / 256UL) * 256UL) + (fused_0fused_0__itr_0____itr_1_1300____itr_2_1301 % 256UL))];
  float __cached_1;
  __cached_1 = __ins_1[(fused_0fused_0__itr_0____itr_1_1300____itr_2_1301 / 256UL)];
  float __cached_2;
  __cached_2 = (__cached_0 * __cached_1);
  __outs_0[(((fused_0fused_0__itr_0____itr_1_1300____itr_2_1301 / 256UL) * 256UL) + (fused_0fused_0__itr_0____itr_1_1300____itr_2_1301 % 256UL))] = __cached_2;
}

static void mul__1380_closure_126_0wrapper(void* __stream, int8_t* __restrict__ __module_data, uint64_t i, generic_val* __restrict__ args) noexcept{
  mul__1380_closure_126(i, (float*)(args[0UL].v_ptr), (float*)(args[1UL].v_ptr), (float*)(args[2UL].v_ptr));
}

static void cast__1390_closure_127(uint64_t fused_0fused_0__itr_0____itr_1_1302____itr_2_1303, float* __restrict__ __ins_0, int8_t* __restrict__ __outs_0) noexcept{
  float __cached_0;
  __cached_0 = __ins_0[(((fused_0fused_0__itr_0____itr_1_1302____itr_2_1303 / 256UL) * 256UL) + (fused_0fused_0__itr_0____itr_1_1302____itr_2_1303 % 256UL))];
  int8_t __cached_1;
  __cached_1 = (int8_t)sc_max(sc_min(sc_round_and_cast<int32_t>(__cached_0), 127), -128);
  __outs_0[(((fused_0fused_0__itr_0____itr_1_1302____itr_2_1303 / 256UL) * 256UL) + (fused_0fused_0__itr_0____itr_1_1302____itr_2_1303 % 256UL))] = __cached_1;
}

static void cast__1390_closure_127_0wrapper(void* __stream, int8_t* __restrict__ __module_data, uint64_t i, generic_val* __restrict__ args) noexcept{
  cast__1390_closure_127(i, (float*)(args[0UL].v_ptr), (int8_t*)(args[1UL].v_ptr));
}

static void mul__1800_closure_128(uint64_t fused_0fused_0__itr_0____itr_1_1304____itr_2_1305, float* __restrict__ __ins_0, float* __restrict__ __ins_1, float* __restrict__ __outs_0) noexcept{
  float __cached_0;
  __cached_0 = __ins_0[(((fused_0fused_0__itr_0____itr_1_1304____itr_2_1305 / 512UL) * 512UL) + (fused_0fused_0__itr_0____itr_1_1304____itr_2_1305 % 512UL))];
  float __cached_1;
  __cached_1 = __ins_1[(fused_0fused_0__itr_0____itr_1_1304____itr_2_1305 / 512UL)];
  float __cached_2;
  __cached_2 = (__cached_0 * __cached_1);
  __outs_0[(((fused_0fused_0__itr_0____itr_1_1304____itr_2_1305 / 512UL) * 512UL) + (fused_0fused_0__itr_0____itr_1_1304____itr_2_1305 % 512UL))] = __cached_2;
}

static void mul__1800_closure_128_0wrapper(void* __stream, int8_t* __restrict__ __module_data, uint64_t i, generic_val* __restrict__ args) noexcept{
  mul__1800_closure_128(i, (float*)(args[0UL].v_ptr), (float*)(args[1UL].v_ptr), (float*)(args[2UL].v_ptr));
}

static void cast__1810_closure_129(uint64_t fused_0fused_0__itr_0____itr_1_1306____itr_2_1307, float* __restrict__ __ins_0, int8_t* __restrict__ __outs_0) noexcept{
  float __cached_0;
  __cached_0 = __ins_0[(((fused_0fused_0__itr_0____itr_1_1306____itr_2_1307 / 512UL) * 512UL) + (fused_0fused_0__itr_0____itr_1_1306____itr_2_1307 % 512UL))];
  int8_t __cached_1;
  __cached_1 = (int8_t)sc_max(sc_min(sc_round_and_cast<int32_t>(__cached_0), 127), -128);
  __outs_0[(((fused_0fused_0__itr_0____itr_1_1306____itr_2_1307 / 512UL) * 512UL) + (fused_0fused_0__itr_0____itr_1_1306____itr_2_1307 % 512UL))] = __cached_1;
}

static void cast__1810_closure_129_0wrapper(void* __stream, int8_t* __restrict__ __module_data, uint64_t i, generic_val* __restrict__ args) noexcept{
  cast__1810_closure_129(i, (float*)(args[0UL].v_ptr), (int8_t*)(args[1UL].v_ptr));
}

static void mul__1440_closure_130(uint64_t fused_0fused_0__itr_0____itr_1_1308____itr_2_1309, float* __restrict__ __ins_0, float* __restrict__ __ins_1, float* __restrict__ __outs_0) noexcept{
  for (uint64_t _fuseiter_4445 = 0UL; _fuseiter_4445 < 3UL; _fuseiter_4445 += 1UL) {
    float __cached_0;
    __cached_0 = __ins_0[((((fused_0fused_0__itr_0____itr_1_1308____itr_2_1309 / 384UL) * 1152UL) + ((((fused_0fused_0__itr_0____itr_1_1308____itr_2_1309 / 3UL) % 128UL) * 9UL) + ((fused_0fused_0__itr_0____itr_1_1308____itr_2_1309 % 3UL) * 3UL))) + _fuseiter_4445)];
    float __cached_1;
    __cached_1 = __ins_1[(fused_0fused_0__itr_0____itr_1_1308____itr_2_1309 / 384UL)];
    float __cached_2;
    __cached_2 = (__cached_0 * __cached_1);
    __outs_0[((((fused_0fused_0__itr_0____itr_1_1308____itr_2_1309 / 384UL) * 1152UL) + ((((fused_0fused_0__itr_0____itr_1_1308____itr_2_1309 / 3UL) % 128UL) * 9UL) + ((fused_0fused_0__itr_0____itr_1_1308____itr_2_1309 % 3UL) * 3UL))) + _fuseiter_4445)] = __cached_2;
  }
}

static void mul__1440_closure_130_0wrapper(void* __stream, int8_t* __restrict__ __module_data, uint64_t i, generic_val* __restrict__ args) noexcept{
  mul__1440_closure_130(i, (float*)(args[0UL].v_ptr), (float*)(args[1UL].v_ptr), (float*)(args[2UL].v_ptr));
}

static void cast__1450_closure_131(uint64_t fused_0fused_0__itr_0____itr_1_1310____itr_2_1311, float* __restrict__ __ins_0, int8_t* __restrict__ __outs_0) noexcept{
  for (uint64_t _fuseiter4450 = 0UL; _fuseiter4450 < 3UL; _fuseiter4450 += 1UL) {
    float __cached_0;
    __cached_0 = __ins_0[((((fused_0fused_0__itr_0____itr_1_1310____itr_2_1311 / 384UL) * 1152UL) + ((((fused_0fused_0__itr_0____itr_1_1310____itr_2_1311 / 3UL) % 128UL) * 9UL) + ((fused_0fused_0__itr_0____itr_1_1310____itr_2_1311 % 3UL) * 3UL))) + _fuseiter4450)];
    int8_t __cached_1;
    __cached_1 = (int8_t)sc_max(sc_min(sc_round_and_cast<int32_t>(__cached_0), 127), -128);
    __outs_0[((((fused_0fused_0__itr_0____itr_1_1310____itr_2_1311 / 384UL) * 1152UL) + ((((fused_0fused_0__itr_0____itr_1_1310____itr_2_1311 / 3UL) % 128UL) * 9UL) + ((fused_0fused_0__itr_0____itr_1_1310____itr_2_1311 % 3UL) * 3UL))) + _fuseiter4450)] = __cached_1;
  }
}

static void cast__1450_closure_131_0wrapper(void* __stream, int8_t* __restrict__ __module_data, uint64_t i, generic_val* __restrict__ args) noexcept{
  cast__1450_closure_131(i, (float*)(args[0UL].v_ptr), (int8_t*)(args[1UL].v_ptr));
}

static void mul__1530_closure_132(uint64_t fused_0fused_0__itr_0____itr_1_1312____itr_2_1313, float* __restrict__ __ins_0, float* __restrict__ __ins_1, float* __restrict__ __outs_0) noexcept{
  for (uint64_t _fuseiter_4455 = 0UL; _fuseiter_4455 < 3UL; _fuseiter_4455 += 1UL) {
    float __cached_0;
    __cached_0 = __ins_0[((((fused_0fused_0__itr_0____itr_1_1312____itr_2_1313 / 384UL) * 1152UL) + ((((fused_0fused_0__itr_0____itr_1_1312____itr_2_1313 / 3UL) % 128UL) * 9UL) + ((fused_0fused_0__itr_0____itr_1_1312____itr_2_1313 % 3UL) * 3UL))) + _fuseiter_4455)];
    float __cached_1;
    __cached_1 = __ins_1[(fused_0fused_0__itr_0____itr_1_1312____itr_2_1313 / 384UL)];
    float __cached_2;
    __cached_2 = (__cached_0 * __cached_1);
    __outs_0[((((fused_0fused_0__itr_0____itr_1_1312____itr_2_1313 / 384UL) * 1152UL) + ((((fused_0fused_0__itr_0____itr_1_1312____itr_2_1313 / 3UL) % 128UL) * 9UL) + ((fused_0fused_0__itr_0____itr_1_1312____itr_2_1313 % 3UL) * 3UL))) + _fuseiter_4455)] = __cached_2;
  }
}

static void mul__1530_closure_132_0wrapper(void* __stream, int8_t* __restrict__ __module_data, uint64_t i, generic_val* __restrict__ args) noexcept{
  mul__1530_closure_132(i, (float*)(args[0UL].v_ptr), (float*)(args[1UL].v_ptr), (float*)(args[2UL].v_ptr));
}

static void cast__1540_closure_133(uint64_t fused_0fused_0__itr_0____itr_1_1314____itr_2_1315, float* __restrict__ __ins_0, int8_t* __restrict__ __outs_0) noexcept{
  for (uint64_t _fuseiter4460 = 0UL; _fuseiter4460 < 3UL; _fuseiter4460 += 1UL) {
    float __cached_0;
    __cached_0 = __ins_0[((((fused_0fused_0__itr_0____itr_1_1314____itr_2_1315 / 384UL) * 1152UL) + ((((fused_0fused_0__itr_0____itr_1_1314____itr_2_1315 / 3UL) % 128UL) * 9UL) + ((fused_0fused_0__itr_0____itr_1_1314____itr_2_1315 % 3UL) * 3UL))) + _fuseiter4460)];
    int8_t __cached_1;
    __cached_1 = (int8_t)sc_max(sc_min(sc_round_and_cast<int32_t>(__cached_0), 127), -128);
    __outs_0[((((fused_0fused_0__itr_0____itr_1_1314____itr_2_1315 / 384UL) * 1152UL) + ((((fused_0fused_0__itr_0____itr_1_1314____itr_2_1315 / 3UL) % 128UL) * 9UL) + ((fused_0fused_0__itr_0____itr_1_1314____itr_2_1315 % 3UL) * 3UL))) + _fuseiter4460)] = __cached_1;
  }
}

static void cast__1540_closure_133_0wrapper(void* __stream, int8_t* __restrict__ __module_data, uint64_t i, generic_val* __restrict__ args) noexcept{
  cast__1540_closure_133(i, (float*)(args[0UL].v_ptr), (int8_t*)(args[1UL].v_ptr));
}

static void mul__1620_closure_134(uint64_t fused_0fused_0__itr_0____itr_1_1316____itr_2_1317, float* __restrict__ __ins_0, float* __restrict__ __ins_1, float* __restrict__ __outs_0) noexcept{
  for (uint64_t _fuseiter_4465 = 0UL; _fuseiter_4465 < 3UL; _fuseiter_4465 += 1UL) {
    float __cached_0;
    __cached_0 = __ins_0[((((fused_0fused_0__itr_0____itr_1_1316____itr_2_1317 / 384UL) * 1152UL) + ((((fused_0fused_0__itr_0____itr_1_1316____itr_2_1317 / 3UL) % 128UL) * 9UL) + ((fused_0fused_0__itr_0____itr_1_1316____itr_2_1317 % 3UL) * 3UL))) + _fuseiter_4465)];
    float __cached_1;
    __cached_1 = __ins_1[(fused_0fused_0__itr_0____itr_1_1316____itr_2_1317 / 384UL)];
    float __cached_2;
    __cached_2 = (__cached_0 * __cached_1);
    __outs_0[((((fused_0fused_0__itr_0____itr_1_1316____itr_2_1317 / 384UL) * 1152UL) + ((((fused_0fused_0__itr_0____itr_1_1316____itr_2_1317 / 3UL) % 128UL) * 9UL) + ((fused_0fused_0__itr_0____itr_1_1316____itr_2_1317 % 3UL) * 3UL))) + _fuseiter_4465)] = __cached_2;
  }
}

static void mul__1620_closure_134_0wrapper(void* __stream, int8_t* __restrict__ __module_data, uint64_t i, generic_val* __restrict__ args) noexcept{
  mul__1620_closure_134(i, (float*)(args[0UL].v_ptr), (float*)(args[1UL].v_ptr), (float*)(args[2UL].v_ptr));
}

static void cast__1630_closure_135(uint64_t fused_0fused_0__itr_0____itr_1_1318____itr_2_1319, float* __restrict__ __ins_0, int8_t* __restrict__ __outs_0) noexcept{
  for (uint64_t _fuseiter4470 = 0UL; _fuseiter4470 < 3UL; _fuseiter4470 += 1UL) {
    float __cached_0;
    __cached_0 = __ins_0[((((fused_0fused_0__itr_0____itr_1_1318____itr_2_1319 / 384UL) * 1152UL) + ((((fused_0fused_0__itr_0____itr_1_1318____itr_2_1319 / 3UL) % 128UL) * 9UL) + ((fused_0fused_0__itr_0____itr_1_1318____itr_2_1319 % 3UL) * 3UL))) + _fuseiter4470)];
    int8_t __cached_1;
    __cached_1 = (int8_t)sc_max(sc_min(sc_round_and_cast<int32_t>(__cached_0), 127), -128);
    __outs_0[((((fused_0fused_0__itr_0____itr_1_1318____itr_2_1319 / 384UL) * 1152UL) + ((((fused_0fused_0__itr_0____itr_1_1318____itr_2_1319 / 3UL) % 128UL) * 9UL) + ((fused_0fused_0__itr_0____itr_1_1318____itr_2_1319 % 3UL) * 3UL))) + _fuseiter4470)] = __cached_1;
  }
}

static void cast__1630_closure_135_0wrapper(void* __stream, int8_t* __restrict__ __module_data, uint64_t i, generic_val* __restrict__ args) noexcept{
  cast__1630_closure_135(i, (float*)(args[0UL].v_ptr), (int8_t*)(args[1UL].v_ptr));
}

static void mul__1710_closure_136(uint64_t fused_0fused_0__itr_0____itr_1_1320____itr_2_1321, float* __restrict__ __ins_0, float* __restrict__ __ins_1, float* __restrict__ __outs_0) noexcept{
  for (uint64_t _fuseiter_4475 = 0UL; _fuseiter_4475 < 3UL; _fuseiter_4475 += 1UL) {
    float __cached_0;
    __cached_0 = __ins_0[((((fused_0fused_0__itr_0____itr_1_1320____itr_2_1321 / 384UL) * 1152UL) + ((((fused_0fused_0__itr_0____itr_1_1320____itr_2_1321 / 3UL) % 128UL) * 9UL) + ((fused_0fused_0__itr_0____itr_1_1320____itr_2_1321 % 3UL) * 3UL))) + _fuseiter_4475)];
    float __cached_1;
    __cached_1 = __ins_1[(fused_0fused_0__itr_0____itr_1_1320____itr_2_1321 / 384UL)];
    float __cached_2;
    __cached_2 = (__cached_0 * __cached_1);
    __outs_0[((((fused_0fused_0__itr_0____itr_1_1320____itr_2_1321 / 384UL) * 1152UL) + ((((fused_0fused_0__itr_0____itr_1_1320____itr_2_1321 / 3UL) % 128UL) * 9UL) + ((fused_0fused_0__itr_0____itr_1_1320____itr_2_1321 % 3UL) * 3UL))) + _fuseiter_4475)] = __cached_2;
  }
}

static void mul__1710_closure_136_0wrapper(void* __stream, int8_t* __restrict__ __module_data, uint64_t i, generic_val* __restrict__ args) noexcept{
  mul__1710_closure_136(i, (float*)(args[0UL].v_ptr), (float*)(args[1UL].v_ptr), (float*)(args[2UL].v_ptr));
}

static void cast__1720_closure_137(uint64_t fused_0fused_0__itr_0____itr_1_1322____itr_2_1323, float* __restrict__ __ins_0, int8_t* __restrict__ __outs_0) noexcept{
  for (uint64_t _fuseiter4480 = 0UL; _fuseiter4480 < 3UL; _fuseiter4480 += 1UL) {
    float __cached_0;
    __cached_0 = __ins_0[((((fused_0fused_0__itr_0____itr_1_1322____itr_2_1323 / 384UL) * 1152UL) + ((((fused_0fused_0__itr_0____itr_1_1322____itr_2_1323 / 3UL) % 128UL) * 9UL) + ((fused_0fused_0__itr_0____itr_1_1322____itr_2_1323 % 3UL) * 3UL))) + _fuseiter4480)];
    int8_t __cached_1;
    __cached_1 = (int8_t)sc_max(sc_min(sc_round_and_cast<int32_t>(__cached_0), 127), -128);
    __outs_0[((((fused_0fused_0__itr_0____itr_1_1322____itr_2_1323 / 384UL) * 1152UL) + ((((fused_0fused_0__itr_0____itr_1_1322____itr_2_1323 / 3UL) % 128UL) * 9UL) + ((fused_0fused_0__itr_0____itr_1_1322____itr_2_1323 % 3UL) * 3UL))) + _fuseiter4480)] = __cached_1;
  }
}

static void cast__1720_closure_137_0wrapper(void* __stream, int8_t* __restrict__ __module_data, uint64_t i, generic_val* __restrict__ args) noexcept{
  cast__1720_closure_137(i, (float*)(args[0UL].v_ptr), (int8_t*)(args[1UL].v_ptr));
}

static void mul__1860_closure_138(uint64_t fused_0fused_0__itr_0____itr_1_1324____itr_2_1325, float* __restrict__ __ins_0, float* __restrict__ __ins_1, float* __restrict__ __outs_0) noexcept{
  float __cached_0;
  __cached_0 = __ins_0[(((fused_0fused_0__itr_0____itr_1_1324____itr_2_1325 / 256UL) * 256UL) + (fused_0fused_0__itr_0____itr_1_1324____itr_2_1325 % 256UL))];
  float __cached_1;
  __cached_1 = __ins_1[(fused_0fused_0__itr_0____itr_1_1324____itr_2_1325 / 256UL)];
  float __cached_2;
  __cached_2 = (__cached_0 * __cached_1);
  __outs_0[(((fused_0fused_0__itr_0____itr_1_1324____itr_2_1325 / 256UL) * 256UL) + (fused_0fused_0__itr_0____itr_1_1324____itr_2_1325 % 256UL))] = __cached_2;
}

static void mul__1860_closure_138_0wrapper(void* __stream, int8_t* __restrict__ __module_data, uint64_t i, generic_val* __restrict__ args) noexcept{
  mul__1860_closure_138(i, (float*)(args[0UL].v_ptr), (float*)(args[1UL].v_ptr), (float*)(args[2UL].v_ptr));
}

static void cast__1870_closure_139(uint64_t fused_0fused_0__itr_0____itr_1_1326____itr_2_1327, float* __restrict__ __ins_0, int8_t* __restrict__ __outs_0) noexcept{
  float __cached_0;
  __cached_0 = __ins_0[(((fused_0fused_0__itr_0____itr_1_1326____itr_2_1327 / 256UL) * 256UL) + (fused_0fused_0__itr_0____itr_1_1326____itr_2_1327 % 256UL))];
  int8_t __cached_1;
  __cached_1 = (int8_t)sc_max(sc_min(sc_round_and_cast<int32_t>(__cached_0), 127), -128);
  __outs_0[(((fused_0fused_0__itr_0____itr_1_1326____itr_2_1327 / 256UL) * 256UL) + (fused_0fused_0__itr_0____itr_1_1326____itr_2_1327 % 256UL))] = __cached_1;
}

static void cast__1870_closure_139_0wrapper(void* __stream, int8_t* __restrict__ __module_data, uint64_t i, generic_val* __restrict__ args) noexcept{
  cast__1870_closure_139(i, (float*)(args[0UL].v_ptr), (int8_t*)(args[1UL].v_ptr));
}

static void mul__1950_closure_140(uint64_t fused_0fused_0__itr_0____itr_1_1328____itr_2_1329, float* __restrict__ __ins_0, float* __restrict__ __ins_1, float* __restrict__ __outs_0) noexcept{
  float __cached_0;
  __cached_0 = __ins_0[(((fused_0fused_0__itr_0____itr_1_1328____itr_2_1329 / 256UL) * 256UL) + (fused_0fused_0__itr_0____itr_1_1328____itr_2_1329 % 256UL))];
  float __cached_1;
  __cached_1 = __ins_1[(fused_0fused_0__itr_0____itr_1_1328____itr_2_1329 / 256UL)];
  float __cached_2;
  __cached_2 = (__cached_0 * __cached_1);
  __outs_0[(((fused_0fused_0__itr_0____itr_1_1328____itr_2_1329 / 256UL) * 256UL) + (fused_0fused_0__itr_0____itr_1_1328____itr_2_1329 % 256UL))] = __cached_2;
}

static void mul__1950_closure_140_0wrapper(void* __stream, int8_t* __restrict__ __module_data, uint64_t i, generic_val* __restrict__ args) noexcept{
  mul__1950_closure_140(i, (float*)(args[0UL].v_ptr), (float*)(args[1UL].v_ptr), (float*)(args[2UL].v_ptr));
}

static void cast__1960_closure_141(uint64_t fused_0fused_0__itr_0____itr_1_1330____itr_2_1331, float* __restrict__ __ins_0, int8_t* __restrict__ __outs_0) noexcept{
  float __cached_0;
  __cached_0 = __ins_0[(((fused_0fused_0__itr_0____itr_1_1330____itr_2_1331 / 256UL) * 256UL) + (fused_0fused_0__itr_0____itr_1_1330____itr_2_1331 % 256UL))];
  int8_t __cached_1;
  __cached_1 = (int8_t)sc_max(sc_min(sc_round_and_cast<int32_t>(__cached_0), 127), -128);
  __outs_0[(((fused_0fused_0__itr_0____itr_1_1330____itr_2_1331 / 256UL) * 256UL) + (fused_0fused_0__itr_0____itr_1_1330____itr_2_1331 % 256UL))] = __cached_1;
}

static void cast__1960_closure_141_0wrapper(void* __stream, int8_t* __restrict__ __module_data, uint64_t i, generic_val* __restrict__ args) noexcept{
  cast__1960_closure_141(i, (float*)(args[0UL].v_ptr), (int8_t*)(args[1UL].v_ptr));
}

static void mul__2040_closure_142(uint64_t fused_0fused_0__itr_0____itr_1_1332____itr_2_1333, float* __restrict__ __ins_0, float* __restrict__ __ins_1, float* __restrict__ __outs_0) noexcept{
  float __cached_0;
  __cached_0 = __ins_0[(((fused_0fused_0__itr_0____itr_1_1332____itr_2_1333 / 256UL) * 256UL) + (fused_0fused_0__itr_0____itr_1_1332____itr_2_1333 % 256UL))];
  float __cached_1;
  __cached_1 = __ins_1[(fused_0fused_0__itr_0____itr_1_1332____itr_2_1333 / 256UL)];
  float __cached_2;
  __cached_2 = (__cached_0 * __cached_1);
  __outs_0[(((fused_0fused_0__itr_0____itr_1_1332____itr_2_1333 / 256UL) * 256UL) + (fused_0fused_0__itr_0____itr_1_1332____itr_2_1333 % 256UL))] = __cached_2;
}

static void mul__2040_closure_142_0wrapper(void* __stream, int8_t* __restrict__ __module_data, uint64_t i, generic_val* __restrict__ args) noexcept{
  mul__2040_closure_142(i, (float*)(args[0UL].v_ptr), (float*)(args[1UL].v_ptr), (float*)(args[2UL].v_ptr));
}

static void cast__2050_closure_143(uint64_t fused_0fused_0__itr_0____itr_1_1334____itr_2_1335, float* __restrict__ __ins_0, int8_t* __restrict__ __outs_0) noexcept{
  float __cached_0;
  __cached_0 = __ins_0[(((fused_0fused_0__itr_0____itr_1_1334____itr_2_1335 / 256UL) * 256UL) + (fused_0fused_0__itr_0____itr_1_1334____itr_2_1335 % 256UL))];
  int8_t __cached_1;
  __cached_1 = (int8_t)sc_max(sc_min(sc_round_and_cast<int32_t>(__cached_0), 127), -128);
  __outs_0[(((fused_0fused_0__itr_0____itr_1_1334____itr_2_1335 / 256UL) * 256UL) + (fused_0fused_0__itr_0____itr_1_1334____itr_2_1335 % 256UL))] = __cached_1;
}

static void cast__2050_closure_143_0wrapper(void* __stream, int8_t* __restrict__ __module_data, uint64_t i, generic_val* __restrict__ args) noexcept{
  cast__2050_closure_143(i, (float*)(args[0UL].v_ptr), (int8_t*)(args[1UL].v_ptr));
}

static void mul__2130_closure_144(uint64_t fused_0fused_0__itr_0____itr_1_1336____itr_2_1337, float* __restrict__ __ins_0, float* __restrict__ __ins_1, float* __restrict__ __outs_0) noexcept{
  float __cached_0;
  __cached_0 = __ins_0[(((fused_0fused_0__itr_0____itr_1_1336____itr_2_1337 / 256UL) * 256UL) + (fused_0fused_0__itr_0____itr_1_1336____itr_2_1337 % 256UL))];
  float __cached_1;
  __cached_1 = __ins_1[(fused_0fused_0__itr_0____itr_1_1336____itr_2_1337 / 256UL)];
  float __cached_2;
  __cached_2 = (__cached_0 * __cached_1);
  __outs_0[(((fused_0fused_0__itr_0____itr_1_1336____itr_2_1337 / 256UL) * 256UL) + (fused_0fused_0__itr_0____itr_1_1336____itr_2_1337 % 256UL))] = __cached_2;
}

static void mul__2130_closure_144_0wrapper(void* __stream, int8_t* __restrict__ __module_data, uint64_t i, generic_val* __restrict__ args) noexcept{
  mul__2130_closure_144(i, (float*)(args[0UL].v_ptr), (float*)(args[1UL].v_ptr), (float*)(args[2UL].v_ptr));
}

static void cast__2140_closure_145(uint64_t fused_0fused_0__itr_0____itr_1_1338____itr_2_1339, float* __restrict__ __ins_0, int8_t* __restrict__ __outs_0) noexcept{
  float __cached_0;
  __cached_0 = __ins_0[(((fused_0fused_0__itr_0____itr_1_1338____itr_2_1339 / 256UL) * 256UL) + (fused_0fused_0__itr_0____itr_1_1338____itr_2_1339 % 256UL))];
  int8_t __cached_1;
  __cached_1 = (int8_t)sc_max(sc_min(sc_round_and_cast<int32_t>(__cached_0), 127), -128);
  __outs_0[(((fused_0fused_0__itr_0____itr_1_1338____itr_2_1339 / 256UL) * 256UL) + (fused_0fused_0__itr_0____itr_1_1338____itr_2_1339 % 256UL))] = __cached_1;
}

static void cast__2140_closure_145_0wrapper(void* __stream, int8_t* __restrict__ __module_data, uint64_t i, generic_val* __restrict__ args) noexcept{
  cast__2140_closure_145(i, (float*)(args[0UL].v_ptr), (int8_t*)(args[1UL].v_ptr));
}

static void mul__2220_closure_146(uint64_t fused_0fused_0__itr_0____itr_1_1340____itr_2_1341, float* __restrict__ __ins_0, float* __restrict__ __ins_1, float* __restrict__ __outs_0) noexcept{
  float __cached_0;
  __cached_0 = __ins_0[(((fused_0fused_0__itr_0____itr_1_1340____itr_2_1341 / 256UL) * 256UL) + (fused_0fused_0__itr_0____itr_1_1340____itr_2_1341 % 256UL))];
  float __cached_1;
  __cached_1 = __ins_1[(fused_0fused_0__itr_0____itr_1_1340____itr_2_1341 / 256UL)];
  float __cached_2;
  __cached_2 = (__cached_0 * __cached_1);
  __outs_0[(((fused_0fused_0__itr_0____itr_1_1340____itr_2_1341 / 256UL) * 256UL) + (fused_0fused_0__itr_0____itr_1_1340____itr_2_1341 % 256UL))] = __cached_2;
}

static void mul__2220_closure_146_0wrapper(void* __stream, int8_t* __restrict__ __module_data, uint64_t i, generic_val* __restrict__ args) noexcept{
  mul__2220_closure_146(i, (float*)(args[0UL].v_ptr), (float*)(args[1UL].v_ptr), (float*)(args[2UL].v_ptr));
}

static void cast__2230_closure_147(uint64_t fused_0fused_0__itr_0____itr_1_1342____itr_2_1343, float* __restrict__ __ins_0, int8_t* __restrict__ __outs_0) noexcept{
  float __cached_0;
  __cached_0 = __ins_0[(((fused_0fused_0__itr_0____itr_1_1342____itr_2_1343 / 256UL) * 256UL) + (fused_0fused_0__itr_0____itr_1_1342____itr_2_1343 % 256UL))];
  int8_t __cached_1;
  __cached_1 = (int8_t)sc_max(sc_min(sc_round_and_cast<int32_t>(__cached_0), 127), -128);
  __outs_0[(((fused_0fused_0__itr_0____itr_1_1342____itr_2_1343 / 256UL) * 256UL) + (fused_0fused_0__itr_0____itr_1_1342____itr_2_1343 % 256UL))] = __cached_1;
}

static void cast__2230_closure_147_0wrapper(void* __stream, int8_t* __restrict__ __module_data, uint64_t i, generic_val* __restrict__ args) noexcept{
  cast__2230_closure_147(i, (float*)(args[0UL].v_ptr), (int8_t*)(args[1UL].v_ptr));
}

static void mul__2310_closure_148(uint64_t fused_0fused_0__itr_0____itr_1_1344____itr_2_1345, float* __restrict__ __ins_0, float* __restrict__ __ins_1, float* __restrict__ __outs_0) noexcept{
  float __cached_0;
  __cached_0 = __ins_0[(((fused_0fused_0__itr_0____itr_1_1344____itr_2_1345 / 256UL) * 256UL) + (fused_0fused_0__itr_0____itr_1_1344____itr_2_1345 % 256UL))];
  float __cached_1;
  __cached_1 = __ins_1[(fused_0fused_0__itr_0____itr_1_1344____itr_2_1345 / 256UL)];
  float __cached_2;
  __cached_2 = (__cached_0 * __cached_1);
  __outs_0[(((fused_0fused_0__itr_0____itr_1_1344____itr_2_1345 / 256UL) * 256UL) + (fused_0fused_0__itr_0____itr_1_1344____itr_2_1345 % 256UL))] = __cached_2;
}

static void mul__2310_closure_148_0wrapper(void* __stream, int8_t* __restrict__ __module_data, uint64_t i, generic_val* __restrict__ args) noexcept{
  mul__2310_closure_148(i, (float*)(args[0UL].v_ptr), (float*)(args[1UL].v_ptr), (float*)(args[2UL].v_ptr));
}

static void cast__2320_closure_149(uint64_t fused_0fused_0__itr_0____itr_1_1346____itr_2_1347, float* __restrict__ __ins_0, int8_t* __restrict__ __outs_0) noexcept{
  float __cached_0;
  __cached_0 = __ins_0[(((fused_0fused_0__itr_0____itr_1_1346____itr_2_1347 / 256UL) * 256UL) + (fused_0fused_0__itr_0____itr_1_1346____itr_2_1347 % 256UL))];
  int8_t __cached_1;
  __cached_1 = (int8_t)sc_max(sc_min(sc_round_and_cast<int32_t>(__cached_0), 127), -128);
  __outs_0[(((fused_0fused_0__itr_0____itr_1_1346____itr_2_1347 / 256UL) * 256UL) + (fused_0fused_0__itr_0____itr_1_1346____itr_2_1347 % 256UL))] = __cached_1;
}

static void cast__2320_closure_149_0wrapper(void* __stream, int8_t* __restrict__ __module_data, uint64_t i, generic_val* __restrict__ args) noexcept{
  cast__2320_closure_149(i, (float*)(args[0UL].v_ptr), (int8_t*)(args[1UL].v_ptr));
}

static void mul__1890_closure_150(uint64_t fused_0fused_0__itr_0____itr_1_1348____itr_2_1349, float* __restrict__ __ins_0, float* __restrict__ __ins_1, float* __restrict__ __outs_0) noexcept{
  float __cached_0;
  __cached_0 = __ins_0[(((fused_0fused_0__itr_0____itr_1_1348____itr_2_1349 / 1024UL) * 1024UL) + (fused_0fused_0__itr_0____itr_1_1348____itr_2_1349 % 1024UL))];
  float __cached_1;
  __cached_1 = __ins_1[(fused_0fused_0__itr_0____itr_1_1348____itr_2_1349 / 1024UL)];
  float __cached_2;
  __cached_2 = (__cached_0 * __cached_1);
  __outs_0[(((fused_0fused_0__itr_0____itr_1_1348____itr_2_1349 / 1024UL) * 1024UL) + (fused_0fused_0__itr_0____itr_1_1348____itr_2_1349 % 1024UL))] = __cached_2;
}

static void mul__1890_closure_150_0wrapper(void* __stream, int8_t* __restrict__ __module_data, uint64_t i, generic_val* __restrict__ args) noexcept{
  mul__1890_closure_150(i, (float*)(args[0UL].v_ptr), (float*)(args[1UL].v_ptr), (float*)(args[2UL].v_ptr));
}

static void cast__1900_closure_151(uint64_t fused_0fused_0__itr_0____itr_1_1350____itr_2_1351, float* __restrict__ __ins_0, int8_t* __restrict__ __outs_0) noexcept{
  float __cached_0;
  __cached_0 = __ins_0[(((fused_0fused_0__itr_0____itr_1_1350____itr_2_1351 / 1024UL) * 1024UL) + (fused_0fused_0__itr_0____itr_1_1350____itr_2_1351 % 1024UL))];
  int8_t __cached_1;
  __cached_1 = (int8_t)sc_max(sc_min(sc_round_and_cast<int32_t>(__cached_0), 127), -128);
  __outs_0[(((fused_0fused_0__itr_0____itr_1_1350____itr_2_1351 / 1024UL) * 1024UL) + (fused_0fused_0__itr_0____itr_1_1350____itr_2_1351 % 1024UL))] = __cached_1;
}

static void cast__1900_closure_151_0wrapper(void* __stream, int8_t* __restrict__ __module_data, uint64_t i, generic_val* __restrict__ args) noexcept{
  cast__1900_closure_151(i, (float*)(args[0UL].v_ptr), (int8_t*)(args[1UL].v_ptr));
}

static void mul__1980_closure_152(uint64_t fused_0fused_0__itr_0____itr_1_1352____itr_2_1353, float* __restrict__ __ins_0, float* __restrict__ __ins_1, float* __restrict__ __outs_0) noexcept{
  float __cached_0;
  __cached_0 = __ins_0[(((fused_0fused_0__itr_0____itr_1_1352____itr_2_1353 / 1024UL) * 1024UL) + (fused_0fused_0__itr_0____itr_1_1352____itr_2_1353 % 1024UL))];
  float __cached_1;
  __cached_1 = __ins_1[(fused_0fused_0__itr_0____itr_1_1352____itr_2_1353 / 1024UL)];
  float __cached_2;
  __cached_2 = (__cached_0 * __cached_1);
  __outs_0[(((fused_0fused_0__itr_0____itr_1_1352____itr_2_1353 / 1024UL) * 1024UL) + (fused_0fused_0__itr_0____itr_1_1352____itr_2_1353 % 1024UL))] = __cached_2;
}

static void mul__1980_closure_152_0wrapper(void* __stream, int8_t* __restrict__ __module_data, uint64_t i, generic_val* __restrict__ args) noexcept{
  mul__1980_closure_152(i, (float*)(args[0UL].v_ptr), (float*)(args[1UL].v_ptr), (float*)(args[2UL].v_ptr));
}

static void cast__1990_closure_153(uint64_t fused_0fused_0__itr_0____itr_1_1354____itr_2_1355, float* __restrict__ __ins_0, int8_t* __restrict__ __outs_0) noexcept{
  float __cached_0;
  __cached_0 = __ins_0[(((fused_0fused_0__itr_0____itr_1_1354____itr_2_1355 / 1024UL) * 1024UL) + (fused_0fused_0__itr_0____itr_1_1354____itr_2_1355 % 1024UL))];
  int8_t __cached_1;
  __cached_1 = (int8_t)sc_max(sc_min(sc_round_and_cast<int32_t>(__cached_0), 127), -128);
  __outs_0[(((fused_0fused_0__itr_0____itr_1_1354____itr_2_1355 / 1024UL) * 1024UL) + (fused_0fused_0__itr_0____itr_1_1354____itr_2_1355 % 1024UL))] = __cached_1;
}

static void cast__1990_closure_153_0wrapper(void* __stream, int8_t* __restrict__ __module_data, uint64_t i, generic_val* __restrict__ args) noexcept{
  cast__1990_closure_153(i, (float*)(args[0UL].v_ptr), (int8_t*)(args[1UL].v_ptr));
}

static void mul__2070_closure_154(uint64_t fused_0fused_0__itr_0____itr_1_1356____itr_2_1357, float* __restrict__ __ins_0, float* __restrict__ __ins_1, float* __restrict__ __outs_0) noexcept{
  float __cached_0;
  __cached_0 = __ins_0[(((fused_0fused_0__itr_0____itr_1_1356____itr_2_1357 / 1024UL) * 1024UL) + (fused_0fused_0__itr_0____itr_1_1356____itr_2_1357 % 1024UL))];
  float __cached_1;
  __cached_1 = __ins_1[(fused_0fused_0__itr_0____itr_1_1356____itr_2_1357 / 1024UL)];
  float __cached_2;
  __cached_2 = (__cached_0 * __cached_1);
  __outs_0[(((fused_0fused_0__itr_0____itr_1_1356____itr_2_1357 / 1024UL) * 1024UL) + (fused_0fused_0__itr_0____itr_1_1356____itr_2_1357 % 1024UL))] = __cached_2;
}

static void mul__2070_closure_154_0wrapper(void* __stream, int8_t* __restrict__ __module_data, uint64_t i, generic_val* __restrict__ args) noexcept{
  mul__2070_closure_154(i, (float*)(args[0UL].v_ptr), (float*)(args[1UL].v_ptr), (float*)(args[2UL].v_ptr));
}

static void cast__2080_closure_155(uint64_t fused_0fused_0__itr_0____itr_1_1358____itr_2_1359, float* __restrict__ __ins_0, int8_t* __restrict__ __outs_0) noexcept{
  float __cached_0;
  __cached_0 = __ins_0[(((fused_0fused_0__itr_0____itr_1_1358____itr_2_1359 / 1024UL) * 1024UL) + (fused_0fused_0__itr_0____itr_1_1358____itr_2_1359 % 1024UL))];
  int8_t __cached_1;
  __cached_1 = (int8_t)sc_max(sc_min(sc_round_and_cast<int32_t>(__cached_0), 127), -128);
  __outs_0[(((fused_0fused_0__itr_0____itr_1_1358____itr_2_1359 / 1024UL) * 1024UL) + (fused_0fused_0__itr_0____itr_1_1358____itr_2_1359 % 1024UL))] = __cached_1;
}

static void cast__2080_closure_155_0wrapper(void* __stream, int8_t* __restrict__ __module_data, uint64_t i, generic_val* __restrict__ args) noexcept{
  cast__2080_closure_155(i, (float*)(args[0UL].v_ptr), (int8_t*)(args[1UL].v_ptr));
}

static void mul__2160_closure_156(uint64_t fused_0fused_0__itr_0____itr_1_1360____itr_2_1361, float* __restrict__ __ins_0, float* __restrict__ __ins_1, float* __restrict__ __outs_0) noexcept{
  float __cached_0;
  __cached_0 = __ins_0[(((fused_0fused_0__itr_0____itr_1_1360____itr_2_1361 / 1024UL) * 1024UL) + (fused_0fused_0__itr_0____itr_1_1360____itr_2_1361 % 1024UL))];
  float __cached_1;
  __cached_1 = __ins_1[(fused_0fused_0__itr_0____itr_1_1360____itr_2_1361 / 1024UL)];
  float __cached_2;
  __cached_2 = (__cached_0 * __cached_1);
  __outs_0[(((fused_0fused_0__itr_0____itr_1_1360____itr_2_1361 / 1024UL) * 1024UL) + (fused_0fused_0__itr_0____itr_1_1360____itr_2_1361 % 1024UL))] = __cached_2;
}

static void mul__2160_closure_156_0wrapper(void* __stream, int8_t* __restrict__ __module_data, uint64_t i, generic_val* __restrict__ args) noexcept{
  mul__2160_closure_156(i, (float*)(args[0UL].v_ptr), (float*)(args[1UL].v_ptr), (float*)(args[2UL].v_ptr));
}

static void cast__2170_closure_157(uint64_t fused_0fused_0__itr_0____itr_1_1362____itr_2_1363, float* __restrict__ __ins_0, int8_t* __restrict__ __outs_0) noexcept{
  float __cached_0;
  __cached_0 = __ins_0[(((fused_0fused_0__itr_0____itr_1_1362____itr_2_1363 / 1024UL) * 1024UL) + (fused_0fused_0__itr_0____itr_1_1362____itr_2_1363 % 1024UL))];
  int8_t __cached_1;
  __cached_1 = (int8_t)sc_max(sc_min(sc_round_and_cast<int32_t>(__cached_0), 127), -128);
  __outs_0[(((fused_0fused_0__itr_0____itr_1_1362____itr_2_1363 / 1024UL) * 1024UL) + (fused_0fused_0__itr_0____itr_1_1362____itr_2_1363 % 1024UL))] = __cached_1;
}

static void cast__2170_closure_157_0wrapper(void* __stream, int8_t* __restrict__ __module_data, uint64_t i, generic_val* __restrict__ args) noexcept{
  cast__2170_closure_157(i, (float*)(args[0UL].v_ptr), (int8_t*)(args[1UL].v_ptr));
}

static void mul__2250_closure_158(uint64_t fused_0fused_0__itr_0____itr_1_1364____itr_2_1365, float* __restrict__ __ins_0, float* __restrict__ __ins_1, float* __restrict__ __outs_0) noexcept{
  float __cached_0;
  __cached_0 = __ins_0[(((fused_0fused_0__itr_0____itr_1_1364____itr_2_1365 / 1024UL) * 1024UL) + (fused_0fused_0__itr_0____itr_1_1364____itr_2_1365 % 1024UL))];
  float __cached_1;
  __cached_1 = __ins_1[(fused_0fused_0__itr_0____itr_1_1364____itr_2_1365 / 1024UL)];
  float __cached_2;
  __cached_2 = (__cached_0 * __cached_1);
  __outs_0[(((fused_0fused_0__itr_0____itr_1_1364____itr_2_1365 / 1024UL) * 1024UL) + (fused_0fused_0__itr_0____itr_1_1364____itr_2_1365 % 1024UL))] = __cached_2;
}

static void mul__2250_closure_158_0wrapper(void* __stream, int8_t* __restrict__ __module_data, uint64_t i, generic_val* __restrict__ args) noexcept{
  mul__2250_closure_158(i, (float*)(args[0UL].v_ptr), (float*)(args[1UL].v_ptr), (float*)(args[2UL].v_ptr));
}

static void cast__2260_closure_159(uint64_t fused_0fused_0__itr_0____itr_1_1366____itr_2_1367, float* __restrict__ __ins_0, int8_t* __restrict__ __outs_0) noexcept{
  float __cached_0;
  __cached_0 = __ins_0[(((fused_0fused_0__itr_0____itr_1_1366____itr_2_1367 / 1024UL) * 1024UL) + (fused_0fused_0__itr_0____itr_1_1366____itr_2_1367 % 1024UL))];
  int8_t __cached_1;
  __cached_1 = (int8_t)sc_max(sc_min(sc_round_and_cast<int32_t>(__cached_0), 127), -128);
  __outs_0[(((fused_0fused_0__itr_0____itr_1_1366____itr_2_1367 / 1024UL) * 1024UL) + (fused_0fused_0__itr_0____itr_1_1366____itr_2_1367 % 1024UL))] = __cached_1;
}

static void cast__2260_closure_159_0wrapper(void* __stream, int8_t* __restrict__ __module_data, uint64_t i, generic_val* __restrict__ args) noexcept{
  cast__2260_closure_159(i, (float*)(args[0UL].v_ptr), (int8_t*)(args[1UL].v_ptr));
}

static void mul__1770_closure_160(uint64_t fused_0fused_0__itr_0____itr_1_1368____itr_2_1369, float* __restrict__ __ins_0, float* __restrict__ __ins_1, float* __restrict__ __outs_0) noexcept{
  float __cached_0;
  __cached_0 = __ins_0[(((fused_0fused_0__itr_0____itr_1_1368____itr_2_1369 / 512UL) * 512UL) + (fused_0fused_0__itr_0____itr_1_1368____itr_2_1369 % 512UL))];
  float __cached_1;
  __cached_1 = __ins_1[(fused_0fused_0__itr_0____itr_1_1368____itr_2_1369 / 512UL)];
  float __cached_2;
  __cached_2 = (__cached_0 * __cached_1);
  __outs_0[(((fused_0fused_0__itr_0____itr_1_1368____itr_2_1369 / 512UL) * 512UL) + (fused_0fused_0__itr_0____itr_1_1368____itr_2_1369 % 512UL))] = __cached_2;
}

static void mul__1770_closure_160_0wrapper(void* __stream, int8_t* __restrict__ __module_data, uint64_t i, generic_val* __restrict__ args) noexcept{
  mul__1770_closure_160(i, (float*)(args[0UL].v_ptr), (float*)(args[1UL].v_ptr), (float*)(args[2UL].v_ptr));
}

static void cast__1780_closure_161(uint64_t fused_0fused_0__itr_0____itr_1_1370____itr_2_1371, float* __restrict__ __ins_0, int8_t* __restrict__ __outs_0) noexcept{
  float __cached_0;
  __cached_0 = __ins_0[(((fused_0fused_0__itr_0____itr_1_1370____itr_2_1371 / 512UL) * 512UL) + (fused_0fused_0__itr_0____itr_1_1370____itr_2_1371 % 512UL))];
  int8_t __cached_1;
  __cached_1 = (int8_t)sc_max(sc_min(sc_round_and_cast<int32_t>(__cached_0), 127), -128);
  __outs_0[(((fused_0fused_0__itr_0____itr_1_1370____itr_2_1371 / 512UL) * 512UL) + (fused_0fused_0__itr_0____itr_1_1370____itr_2_1371 % 512UL))] = __cached_1;
}

static void cast__1780_closure_161_0wrapper(void* __stream, int8_t* __restrict__ __module_data, uint64_t i, generic_val* __restrict__ args) noexcept{
  cast__1780_closure_161(i, (float*)(args[0UL].v_ptr), (int8_t*)(args[1UL].v_ptr));
}

static void mul__2370_closure_162(uint64_t fused_0fused_0__itr_0____itr_1_1372____itr_2_1373, float* __restrict__ __ins_0, float* __restrict__ __ins_1, float* __restrict__ __outs_0) noexcept{
  float __cached_0;
  __cached_0 = __ins_0[(((fused_0fused_0__itr_0____itr_1_1372____itr_2_1373 / 1024UL) * 1024UL) + (fused_0fused_0__itr_0____itr_1_1372____itr_2_1373 % 1024UL))];
  float __cached_1;
  __cached_1 = __ins_1[(fused_0fused_0__itr_0____itr_1_1372____itr_2_1373 / 1024UL)];
  float __cached_2;
  __cached_2 = (__cached_0 * __cached_1);
  __outs_0[(((fused_0fused_0__itr_0____itr_1_1372____itr_2_1373 / 1024UL) * 1024UL) + (fused_0fused_0__itr_0____itr_1_1372____itr_2_1373 % 1024UL))] = __cached_2;
}

static void mul__2370_closure_162_0wrapper(void* __stream, int8_t* __restrict__ __module_data, uint64_t i, generic_val* __restrict__ args) noexcept{
  mul__2370_closure_162(i, (float*)(args[0UL].v_ptr), (float*)(args[1UL].v_ptr), (float*)(args[2UL].v_ptr));
}

static void cast__2380_closure_163(uint64_t fused_0fused_0__itr_0____itr_1_1374____itr_2_1375, float* __restrict__ __ins_0, int8_t* __restrict__ __outs_0) noexcept{
  float __cached_0;
  __cached_0 = __ins_0[(((fused_0fused_0__itr_0____itr_1_1374____itr_2_1375 / 1024UL) * 1024UL) + (fused_0fused_0__itr_0____itr_1_1374____itr_2_1375 % 1024UL))];
  int8_t __cached_1;
  __cached_1 = (int8_t)sc_max(sc_min(sc_round_and_cast<int32_t>(__cached_0), 127), -128);
  __outs_0[(((fused_0fused_0__itr_0____itr_1_1374____itr_2_1375 / 1024UL) * 1024UL) + (fused_0fused_0__itr_0____itr_1_1374____itr_2_1375 % 1024UL))] = __cached_1;
}

static void cast__2380_closure_163_0wrapper(void* __stream, int8_t* __restrict__ __module_data, uint64_t i, generic_val* __restrict__ args) noexcept{
  cast__2380_closure_163(i, (float*)(args[0UL].v_ptr), (int8_t*)(args[1UL].v_ptr));
}

static void mul__1830_closure_164(uint64_t fused_0fused_0__itr_0____itr_1_1376____itr_2_1377, float* __restrict__ __ins_0, float* __restrict__ __ins_1, float* __restrict__ __outs_0) noexcept{
  for (uint64_t _fuseiter_4615 = 0UL; _fuseiter_4615 < 3UL; _fuseiter_4615 += 1UL) {
    float __cached_0;
    __cached_0 = __ins_0[((((fused_0fused_0__itr_0____itr_1_1376____itr_2_1377 / 768UL) * 2304UL) + ((((fused_0fused_0__itr_0____itr_1_1376____itr_2_1377 / 3UL) % 256UL) * 9UL) + ((fused_0fused_0__itr_0____itr_1_1376____itr_2_1377 % 3UL) * 3UL))) + _fuseiter_4615)];
    float __cached_1;
    __cached_1 = __ins_1[(fused_0fused_0__itr_0____itr_1_1376____itr_2_1377 / 768UL)];
    float __cached_2;
    __cached_2 = (__cached_0 * __cached_1);
    __outs_0[((((fused_0fused_0__itr_0____itr_1_1376____itr_2_1377 / 768UL) * 2304UL) + ((((fused_0fused_0__itr_0____itr_1_1376____itr_2_1377 / 3UL) % 256UL) * 9UL) + ((fused_0fused_0__itr_0____itr_1_1376____itr_2_1377 % 3UL) * 3UL))) + _fuseiter_4615)] = __cached_2;
  }
}

static void mul__1830_closure_164_0wrapper(void* __stream, int8_t* __restrict__ __module_data, uint64_t i, generic_val* __restrict__ args) noexcept{
  mul__1830_closure_164(i, (float*)(args[0UL].v_ptr), (float*)(args[1UL].v_ptr), (float*)(args[2UL].v_ptr));
}

static void cast__1840_closure_165(uint64_t fused_0fused_0__itr_0____itr_1_1378____itr_2_1379, float* __restrict__ __ins_0, int8_t* __restrict__ __outs_0) noexcept{
  for (uint64_t _fuseiter4620 = 0UL; _fuseiter4620 < 3UL; _fuseiter4620 += 1UL) {
    float __cached_0;
    __cached_0 = __ins_0[((((fused_0fused_0__itr_0____itr_1_1378____itr_2_1379 / 768UL) * 2304UL) + ((((fused_0fused_0__itr_0____itr_1_1378____itr_2_1379 / 3UL) % 256UL) * 9UL) + ((fused_0fused_0__itr_0____itr_1_1378____itr_2_1379 % 3UL) * 3UL))) + _fuseiter4620)];
    int8_t __cached_1;
    __cached_1 = (int8_t)sc_max(sc_min(sc_round_and_cast<int32_t>(__cached_0), 127), -128);
    __outs_0[((((fused_0fused_0__itr_0____itr_1_1378____itr_2_1379 / 768UL) * 2304UL) + ((((fused_0fused_0__itr_0____itr_1_1378____itr_2_1379 / 3UL) % 256UL) * 9UL) + ((fused_0fused_0__itr_0____itr_1_1378____itr_2_1379 % 3UL) * 3UL))) + _fuseiter4620)] = __cached_1;
  }
}

static void cast__1840_closure_165_0wrapper(void* __stream, int8_t* __restrict__ __module_data, uint64_t i, generic_val* __restrict__ args) noexcept{
  cast__1840_closure_165(i, (float*)(args[0UL].v_ptr), (int8_t*)(args[1UL].v_ptr));
}

static void mul__1920_closure_166(uint64_t fused_0fused_0__itr_0____itr_1_1380____itr_2_1381, float* __restrict__ __ins_0, float* __restrict__ __ins_1, float* __restrict__ __outs_0) noexcept{
  for (uint64_t _fuseiter_4625 = 0UL; _fuseiter_4625 < 3UL; _fuseiter_4625 += 1UL) {
    float __cached_0;
    __cached_0 = __ins_0[((((fused_0fused_0__itr_0____itr_1_1380____itr_2_1381 / 768UL) * 2304UL) + ((((fused_0fused_0__itr_0____itr_1_1380____itr_2_1381 / 3UL) % 256UL) * 9UL) + ((fused_0fused_0__itr_0____itr_1_1380____itr_2_1381 % 3UL) * 3UL))) + _fuseiter_4625)];
    float __cached_1;
    __cached_1 = __ins_1[(fused_0fused_0__itr_0____itr_1_1380____itr_2_1381 / 768UL)];
    float __cached_2;
    __cached_2 = (__cached_0 * __cached_1);
    __outs_0[((((fused_0fused_0__itr_0____itr_1_1380____itr_2_1381 / 768UL) * 2304UL) + ((((fused_0fused_0__itr_0____itr_1_1380____itr_2_1381 / 3UL) % 256UL) * 9UL) + ((fused_0fused_0__itr_0____itr_1_1380____itr_2_1381 % 3UL) * 3UL))) + _fuseiter_4625)] = __cached_2;
  }
}

static void mul__1920_closure_166_0wrapper(void* __stream, int8_t* __restrict__ __module_data, uint64_t i, generic_val* __restrict__ args) noexcept{
  mul__1920_closure_166(i, (float*)(args[0UL].v_ptr), (float*)(args[1UL].v_ptr), (float*)(args[2UL].v_ptr));
}

static void cast__1930_closure_167(uint64_t fused_0fused_0__itr_0____itr_1_1382____itr_2_1383, float* __restrict__ __ins_0, int8_t* __restrict__ __outs_0) noexcept{
  for (uint64_t _fuseiter4630 = 0UL; _fuseiter4630 < 3UL; _fuseiter4630 += 1UL) {
    float __cached_0;
    __cached_0 = __ins_0[((((fused_0fused_0__itr_0____itr_1_1382____itr_2_1383 / 768UL) * 2304UL) + ((((fused_0fused_0__itr_0____itr_1_1382____itr_2_1383 / 3UL) % 256UL) * 9UL) + ((fused_0fused_0__itr_0____itr_1_1382____itr_2_1383 % 3UL) * 3UL))) + _fuseiter4630)];
    int8_t __cached_1;
    __cached_1 = (int8_t)sc_max(sc_min(sc_round_and_cast<int32_t>(__cached_0), 127), -128);
    __outs_0[((((fused_0fused_0__itr_0____itr_1_1382____itr_2_1383 / 768UL) * 2304UL) + ((((fused_0fused_0__itr_0____itr_1_1382____itr_2_1383 / 3UL) % 256UL) * 9UL) + ((fused_0fused_0__itr_0____itr_1_1382____itr_2_1383 % 3UL) * 3UL))) + _fuseiter4630)] = __cached_1;
  }
}

static void cast__1930_closure_167_0wrapper(void* __stream, int8_t* __restrict__ __module_data, uint64_t i, generic_val* __restrict__ args) noexcept{
  cast__1930_closure_167(i, (float*)(args[0UL].v_ptr), (int8_t*)(args[1UL].v_ptr));
}

static void mul__2010_closure_168(uint64_t fused_0fused_0__itr_0____itr_1_1384____itr_2_1385, float* __restrict__ __ins_0, float* __restrict__ __ins_1, float* __restrict__ __outs_0) noexcept{
  for (uint64_t _fuseiter_4635 = 0UL; _fuseiter_4635 < 3UL; _fuseiter_4635 += 1UL) {
    float __cached_0;
    __cached_0 = __ins_0[((((fused_0fused_0__itr_0____itr_1_1384____itr_2_1385 / 768UL) * 2304UL) + ((((fused_0fused_0__itr_0____itr_1_1384____itr_2_1385 / 3UL) % 256UL) * 9UL) + ((fused_0fused_0__itr_0____itr_1_1384____itr_2_1385 % 3UL) * 3UL))) + _fuseiter_4635)];
    float __cached_1;
    __cached_1 = __ins_1[(fused_0fused_0__itr_0____itr_1_1384____itr_2_1385 / 768UL)];
    float __cached_2;
    __cached_2 = (__cached_0 * __cached_1);
    __outs_0[((((fused_0fused_0__itr_0____itr_1_1384____itr_2_1385 / 768UL) * 2304UL) + ((((fused_0fused_0__itr_0____itr_1_1384____itr_2_1385 / 3UL) % 256UL) * 9UL) + ((fused_0fused_0__itr_0____itr_1_1384____itr_2_1385 % 3UL) * 3UL))) + _fuseiter_4635)] = __cached_2;
  }
}

static void mul__2010_closure_168_0wrapper(void* __stream, int8_t* __restrict__ __module_data, uint64_t i, generic_val* __restrict__ args) noexcept{
  mul__2010_closure_168(i, (float*)(args[0UL].v_ptr), (float*)(args[1UL].v_ptr), (float*)(args[2UL].v_ptr));
}

static void cast__2020_closure_169(uint64_t fused_0fused_0__itr_0____itr_1_1386____itr_2_1387, float* __restrict__ __ins_0, int8_t* __restrict__ __outs_0) noexcept{
  for (uint64_t _fuseiter4640 = 0UL; _fuseiter4640 < 3UL; _fuseiter4640 += 1UL) {
    float __cached_0;
    __cached_0 = __ins_0[((((fused_0fused_0__itr_0____itr_1_1386____itr_2_1387 / 768UL) * 2304UL) + ((((fused_0fused_0__itr_0____itr_1_1386____itr_2_1387 / 3UL) % 256UL) * 9UL) + ((fused_0fused_0__itr_0____itr_1_1386____itr_2_1387 % 3UL) * 3UL))) + _fuseiter4640)];
    int8_t __cached_1;
    __cached_1 = (int8_t)sc_max(sc_min(sc_round_and_cast<int32_t>(__cached_0), 127), -128);
    __outs_0[((((fused_0fused_0__itr_0____itr_1_1386____itr_2_1387 / 768UL) * 2304UL) + ((((fused_0fused_0__itr_0____itr_1_1386____itr_2_1387 / 3UL) % 256UL) * 9UL) + ((fused_0fused_0__itr_0____itr_1_1386____itr_2_1387 % 3UL) * 3UL))) + _fuseiter4640)] = __cached_1;
  }
}

static void cast__2020_closure_169_0wrapper(void* __stream, int8_t* __restrict__ __module_data, uint64_t i, generic_val* __restrict__ args) noexcept{
  cast__2020_closure_169(i, (float*)(args[0UL].v_ptr), (int8_t*)(args[1UL].v_ptr));
}

static void mul__2100_closure_170(uint64_t fused_0fused_0__itr_0____itr_1_1388____itr_2_1389, float* __restrict__ __ins_0, float* __restrict__ __ins_1, float* __restrict__ __outs_0) noexcept{
  for (uint64_t _fuseiter_4645 = 0UL; _fuseiter_4645 < 3UL; _fuseiter_4645 += 1UL) {
    float __cached_0;
    __cached_0 = __ins_0[((((fused_0fused_0__itr_0____itr_1_1388____itr_2_1389 / 768UL) * 2304UL) + ((((fused_0fused_0__itr_0____itr_1_1388____itr_2_1389 / 3UL) % 256UL) * 9UL) + ((fused_0fused_0__itr_0____itr_1_1388____itr_2_1389 % 3UL) * 3UL))) + _fuseiter_4645)];
    float __cached_1;
    __cached_1 = __ins_1[(fused_0fused_0__itr_0____itr_1_1388____itr_2_1389 / 768UL)];
    float __cached_2;
    __cached_2 = (__cached_0 * __cached_1);
    __outs_0[((((fused_0fused_0__itr_0____itr_1_1388____itr_2_1389 / 768UL) * 2304UL) + ((((fused_0fused_0__itr_0____itr_1_1388____itr_2_1389 / 3UL) % 256UL) * 9UL) + ((fused_0fused_0__itr_0____itr_1_1388____itr_2_1389 % 3UL) * 3UL))) + _fuseiter_4645)] = __cached_2;
  }
}

static void mul__2100_closure_170_0wrapper(void* __stream, int8_t* __restrict__ __module_data, uint64_t i, generic_val* __restrict__ args) noexcept{
  mul__2100_closure_170(i, (float*)(args[0UL].v_ptr), (float*)(args[1UL].v_ptr), (float*)(args[2UL].v_ptr));
}

static void cast__2110_closure_171(uint64_t fused_0fused_0__itr_0____itr_1_1390____itr_2_1391, float* __restrict__ __ins_0, int8_t* __restrict__ __outs_0) noexcept{
  for (uint64_t _fuseiter4650 = 0UL; _fuseiter4650 < 3UL; _fuseiter4650 += 1UL) {
    float __cached_0;
    __cached_0 = __ins_0[((((fused_0fused_0__itr_0____itr_1_1390____itr_2_1391 / 768UL) * 2304UL) + ((((fused_0fused_0__itr_0____itr_1_1390____itr_2_1391 / 3UL) % 256UL) * 9UL) + ((fused_0fused_0__itr_0____itr_1_1390____itr_2_1391 % 3UL) * 3UL))) + _fuseiter4650)];
    int8_t __cached_1;
    __cached_1 = (int8_t)sc_max(sc_min(sc_round_and_cast<int32_t>(__cached_0), 127), -128);
    __outs_0[((((fused_0fused_0__itr_0____itr_1_1390____itr_2_1391 / 768UL) * 2304UL) + ((((fused_0fused_0__itr_0____itr_1_1390____itr_2_1391 / 3UL) % 256UL) * 9UL) + ((fused_0fused_0__itr_0____itr_1_1390____itr_2_1391 % 3UL) * 3UL))) + _fuseiter4650)] = __cached_1;
  }
}

static void cast__2110_closure_171_0wrapper(void* __stream, int8_t* __restrict__ __module_data, uint64_t i, generic_val* __restrict__ args) noexcept{
  cast__2110_closure_171(i, (float*)(args[0UL].v_ptr), (int8_t*)(args[1UL].v_ptr));
}

static void mul__2190_closure_172(uint64_t fused_0fused_0__itr_0____itr_1_1392____itr_2_1393, float* __restrict__ __ins_0, float* __restrict__ __ins_1, float* __restrict__ __outs_0) noexcept{
  for (uint64_t _fuseiter_4655 = 0UL; _fuseiter_4655 < 3UL; _fuseiter_4655 += 1UL) {
    float __cached_0;
    __cached_0 = __ins_0[((((fused_0fused_0__itr_0____itr_1_1392____itr_2_1393 / 768UL) * 2304UL) + ((((fused_0fused_0__itr_0____itr_1_1392____itr_2_1393 / 3UL) % 256UL) * 9UL) + ((fused_0fused_0__itr_0____itr_1_1392____itr_2_1393 % 3UL) * 3UL))) + _fuseiter_4655)];
    float __cached_1;
    __cached_1 = __ins_1[(fused_0fused_0__itr_0____itr_1_1392____itr_2_1393 / 768UL)];
    float __cached_2;
    __cached_2 = (__cached_0 * __cached_1);
    __outs_0[((((fused_0fused_0__itr_0____itr_1_1392____itr_2_1393 / 768UL) * 2304UL) + ((((fused_0fused_0__itr_0____itr_1_1392____itr_2_1393 / 3UL) % 256UL) * 9UL) + ((fused_0fused_0__itr_0____itr_1_1392____itr_2_1393 % 3UL) * 3UL))) + _fuseiter_4655)] = __cached_2;
  }
}

static void mul__2190_closure_172_0wrapper(void* __stream, int8_t* __restrict__ __module_data, uint64_t i, generic_val* __restrict__ args) noexcept{
  mul__2190_closure_172(i, (float*)(args[0UL].v_ptr), (float*)(args[1UL].v_ptr), (float*)(args[2UL].v_ptr));
}

static void cast__2200_closure_173(uint64_t fused_0fused_0__itr_0____itr_1_1394____itr_2_1395, float* __restrict__ __ins_0, int8_t* __restrict__ __outs_0) noexcept{
  for (uint64_t _fuseiter4660 = 0UL; _fuseiter4660 < 3UL; _fuseiter4660 += 1UL) {
    float __cached_0;
    __cached_0 = __ins_0[((((fused_0fused_0__itr_0____itr_1_1394____itr_2_1395 / 768UL) * 2304UL) + ((((fused_0fused_0__itr_0____itr_1_1394____itr_2_1395 / 3UL) % 256UL) * 9UL) + ((fused_0fused_0__itr_0____itr_1_1394____itr_2_1395 % 3UL) * 3UL))) + _fuseiter4660)];
    int8_t __cached_1;
    __cached_1 = (int8_t)sc_max(sc_min(sc_round_and_cast<int32_t>(__cached_0), 127), -128);
    __outs_0[((((fused_0fused_0__itr_0____itr_1_1394____itr_2_1395 / 768UL) * 2304UL) + ((((fused_0fused_0__itr_0____itr_1_1394____itr_2_1395 / 3UL) % 256UL) * 9UL) + ((fused_0fused_0__itr_0____itr_1_1394____itr_2_1395 % 3UL) * 3UL))) + _fuseiter4660)] = __cached_1;
  }
}

static void cast__2200_closure_173_0wrapper(void* __stream, int8_t* __restrict__ __module_data, uint64_t i, generic_val* __restrict__ args) noexcept{
  cast__2200_closure_173(i, (float*)(args[0UL].v_ptr), (int8_t*)(args[1UL].v_ptr));
}

static void mul__2280_closure_174(uint64_t fused_0fused_0__itr_0____itr_1_1396____itr_2_1397, float* __restrict__ __ins_0, float* __restrict__ __ins_1, float* __restrict__ __outs_0) noexcept{
  for (uint64_t _fuseiter_4665 = 0UL; _fuseiter_4665 < 3UL; _fuseiter_4665 += 1UL) {
    float __cached_0;
    __cached_0 = __ins_0[((((fused_0fused_0__itr_0____itr_1_1396____itr_2_1397 / 768UL) * 2304UL) + ((((fused_0fused_0__itr_0____itr_1_1396____itr_2_1397 / 3UL) % 256UL) * 9UL) + ((fused_0fused_0__itr_0____itr_1_1396____itr_2_1397 % 3UL) * 3UL))) + _fuseiter_4665)];
    float __cached_1;
    __cached_1 = __ins_1[(fused_0fused_0__itr_0____itr_1_1396____itr_2_1397 / 768UL)];
    float __cached_2;
    __cached_2 = (__cached_0 * __cached_1);
    __outs_0[((((fused_0fused_0__itr_0____itr_1_1396____itr_2_1397 / 768UL) * 2304UL) + ((((fused_0fused_0__itr_0____itr_1_1396____itr_2_1397 / 3UL) % 256UL) * 9UL) + ((fused_0fused_0__itr_0____itr_1_1396____itr_2_1397 % 3UL) * 3UL))) + _fuseiter_4665)] = __cached_2;
  }
}

static void mul__2280_closure_174_0wrapper(void* __stream, int8_t* __restrict__ __module_data, uint64_t i, generic_val* __restrict__ args) noexcept{
  mul__2280_closure_174(i, (float*)(args[0UL].v_ptr), (float*)(args[1UL].v_ptr), (float*)(args[2UL].v_ptr));
}

static void cast__2290_closure_175(uint64_t fused_0fused_0__itr_0____itr_1_1398____itr_2_1399, float* __restrict__ __ins_0, int8_t* __restrict__ __outs_0) noexcept{
  for (uint64_t _fuseiter4670 = 0UL; _fuseiter4670 < 3UL; _fuseiter4670 += 1UL) {
    float __cached_0;
    __cached_0 = __ins_0[((((fused_0fused_0__itr_0____itr_1_1398____itr_2_1399 / 768UL) * 2304UL) + ((((fused_0fused_0__itr_0____itr_1_1398____itr_2_1399 / 3UL) % 256UL) * 9UL) + ((fused_0fused_0__itr_0____itr_1_1398____itr_2_1399 % 3UL) * 3UL))) + _fuseiter4670)];
    int8_t __cached_1;
    __cached_1 = (int8_t)sc_max(sc_min(sc_round_and_cast<int32_t>(__cached_0), 127), -128);
    __outs_0[((((fused_0fused_0__itr_0____itr_1_1398____itr_2_1399 / 768UL) * 2304UL) + ((((fused_0fused_0__itr_0____itr_1_1398____itr_2_1399 / 3UL) % 256UL) * 9UL) + ((fused_0fused_0__itr_0____itr_1_1398____itr_2_1399 % 3UL) * 3UL))) + _fuseiter4670)] = __cached_1;
  }
}

static void cast__2290_closure_175_0wrapper(void* __stream, int8_t* __restrict__ __module_data, uint64_t i, generic_val* __restrict__ args) noexcept{
  cast__2290_closure_175(i, (float*)(args[0UL].v_ptr), (int8_t*)(args[1UL].v_ptr));
}

static void reorder__5250_closure_176(uint64_t fused_0fused_0_fuseiter_4671___fuseiter_4672_1400___fuseiter_4673_1401, int8_t* __restrict__ __ins_0, int8_t* __restrict__ __outs_0) noexcept{
  for (uint64_t _fuseiter_4674 = 0UL; _fuseiter_4674 < 3UL; _fuseiter_4674 += 1UL) {
    for (uint64_t _fuseiter_4675 = 0UL; _fuseiter_4675 < 16UL; _fuseiter_4675 += 1UL) {
      for (uint64_t _fuseiter_4676 = 0UL; _fuseiter_4676 < 64UL; _fuseiter_4676 += 1UL) {
        for (uint64_t _fuseiter_4677 = 0UL; _fuseiter_4677 < 4UL; _fuseiter_4677 += 1UL) {
          int8_t __cached_0;
          __cached_0 = __ins_0[(((_fuseiter_4676 + ((fused_0fused_0_fuseiter_4671___fuseiter_4672_1400___fuseiter_4673_1401 / 12UL) * 64UL)) * 2304UL) + ((((_fuseiter_4677 + (_fuseiter_4675 * 4UL)) + (((fused_0fused_0_fuseiter_4671___fuseiter_4672_1400___fuseiter_4673_1401 / 3UL) % 4UL) * 64UL)) * 9UL) + (((fused_0fused_0_fuseiter_4671___fuseiter_4672_1400___fuseiter_4673_1401 % 3UL) * 3UL) + _fuseiter_4674)))];
          int8_t __cached_1;
          __cached_1 = __cached_0;
          __outs_0[(((fused_0fused_0_fuseiter_4671___fuseiter_4672_1400___fuseiter_4673_1401 / 12UL) * 147456UL) + ((((fused_0fused_0_fuseiter_4671___fuseiter_4672_1400___fuseiter_4673_1401 / 3UL) % 4UL) * 36864UL) + (((fused_0fused_0_fuseiter_4671___fuseiter_4672_1400___fuseiter_4673_1401 % 3UL) * 12288UL) + ((_fuseiter_4674 * 4096UL) + ((_fuseiter_4675 * 256UL) + ((_fuseiter_4676 * 4UL) + _fuseiter_4677))))))] = __cached_1;
        }
      }
    }
  }
}

static void reorder__5250_closure_176_0wrapper(void* __stream, int8_t* __restrict__ __module_data, uint64_t i, generic_val* __restrict__ args) noexcept{
  reorder__5250_closure_176(i, (int8_t*)(args[0UL].v_ptr), (int8_t*)(args[1UL].v_ptr));
}

static void mul__6520_closure_177(uint64_t fused_0fused_0fused_0__itr_0____itr_1_1402____itr_2_1403____itr_3_1404, float* __restrict__ __ins_0, float* __restrict__ __ins_1, float* __restrict__ __outs_0) noexcept{
  for (uint64_t _fuseiter_4682 = 0UL; _fuseiter_4682 < 256UL; _fuseiter_4682 += 16UL) {
    vec_f32x16 __cached_0;
    __cached_0 = vec_f32x16::load(&__ins_0[((((fused_0fused_0fused_0__itr_0____itr_1_1402____itr_2_1403____itr_3_1404 / 4UL) * 1024UL) + ((fused_0fused_0fused_0__itr_0____itr_1_1402____itr_2_1403____itr_3_1404 % 4UL) * 256UL)) + _fuseiter_4682)]);
    float __cached_1;
    __cached_1 = __ins_1[0];
    vec_f32x16 __cached_2;
    __cached_2 = (__cached_0 * vec_f32x16(__cached_1));
    vec_f32x16::store(__cached_2, &__outs_0[((((fused_0fused_0fused_0__itr_0____itr_1_1402____itr_2_1403____itr_3_1404 / 4UL) * 1024UL) + ((fused_0fused_0fused_0__itr_0____itr_1_1402____itr_2_1403____itr_3_1404 % 4UL) * 256UL)) + _fuseiter_4682)]);
  }
}

static void mul__6520_closure_177_0wrapper(void* __stream, int8_t* __restrict__ __module_data, uint64_t i, generic_val* __restrict__ args) noexcept{
  mul__6520_closure_177(i, (float*)(args[0UL].v_ptr), (float*)(args[1UL].v_ptr), (float*)(args[2UL].v_ptr));
}

static void mul__6510_closure_178(uint64_t fused_0fused_0fused_0__itr_0____itr_1_1405____itr_2_1406____itr_3_1407, float* __restrict__ __ins_0, float* __restrict__ __ins_1, float* __restrict__ __outs_0) noexcept{
  for (uint64_t _fuseiter_4688 = 0UL; _fuseiter_4688 < 256UL; _fuseiter_4688 += 16UL) {
    vec_f32x16 __cached_0;
    __cached_0 = vec_f32x16::load(&__ins_0[((((fused_0fused_0fused_0__itr_0____itr_1_1405____itr_2_1406____itr_3_1407 / 4UL) * 1024UL) + ((fused_0fused_0fused_0__itr_0____itr_1_1405____itr_2_1406____itr_3_1407 % 4UL) * 256UL)) + _fuseiter_4688)]);
    float __cached_1;
    __cached_1 = __ins_1[0];
    vec_f32x16 __cached_2;
    __cached_2 = (__cached_0 * vec_f32x16(__cached_1));
    vec_f32x16::store(__cached_2, &__outs_0[((((fused_0fused_0fused_0__itr_0____itr_1_1405____itr_2_1406____itr_3_1407 / 4UL) * 1024UL) + ((fused_0fused_0fused_0__itr_0____itr_1_1405____itr_2_1406____itr_3_1407 % 4UL) * 256UL)) + _fuseiter_4688)]);
  }
}

static void mul__6510_closure_178_0wrapper(void* __stream, int8_t* __restrict__ __module_data, uint64_t i, generic_val* __restrict__ args) noexcept{
  mul__6510_closure_178(i, (float*)(args[0UL].v_ptr), (float*)(args[1UL].v_ptr), (float*)(args[2UL].v_ptr));
}

static void mul__6460_closure_179(uint64_t fused_0fused_0fused_0__itr_0____itr_1_1408____itr_2_1409____itr_3_1410, float* __restrict__ __ins_0, float* __restrict__ __ins_1, float* __restrict__ __outs_0) noexcept{
  for (uint64_t _fuseiter_4694 = 0UL; _fuseiter_4694 < 128UL; _fuseiter_4694 += 16UL) {
    vec_f32x16 __cached_0;
    __cached_0 = vec_f32x16::load(&__ins_0[((((fused_0fused_0fused_0__itr_0____itr_1_1408____itr_2_1409____itr_3_1410 / 8UL) * 1024UL) + ((fused_0fused_0fused_0__itr_0____itr_1_1408____itr_2_1409____itr_3_1410 % 8UL) * 128UL)) + _fuseiter_4694)]);
    float __cached_1;
    __cached_1 = __ins_1[0];
    vec_f32x16 __cached_2;
    __cached_2 = (__cached_0 * vec_f32x16(__cached_1));
    vec_f32x16::store(__cached_2, &__outs_0[((((fused_0fused_0fused_0__itr_0____itr_1_1408____itr_2_1409____itr_3_1410 / 8UL) * 1024UL) + ((fused_0fused_0fused_0__itr_0____itr_1_1408____itr_2_1409____itr_3_1410 % 8UL) * 128UL)) + _fuseiter_4694)]);
  }
}

static void mul__6460_closure_179_0wrapper(void* __stream, int8_t* __restrict__ __module_data, uint64_t i, generic_val* __restrict__ args) noexcept{
  mul__6460_closure_179(i, (float*)(args[0UL].v_ptr), (float*)(args[1UL].v_ptr), (float*)(args[2UL].v_ptr));
}

static void mul__6450_closure_180(uint64_t fused_0fused_0fused_0__itr_0____itr_1_1411____itr_2_1412____itr_3_1413, float* __restrict__ __ins_0, float* __restrict__ __ins_1, float* __restrict__ __outs_0) noexcept{
  for (uint64_t _fuseiter_4700 = 0UL; _fuseiter_4700 < 128UL; _fuseiter_4700 += 16UL) {
    vec_f32x16 __cached_0;
    __cached_0 = vec_f32x16::load(&__ins_0[((((fused_0fused_0fused_0__itr_0____itr_1_1411____itr_2_1412____itr_3_1413 / 8UL) * 1024UL) + ((fused_0fused_0fused_0__itr_0____itr_1_1411____itr_2_1412____itr_3_1413 % 8UL) * 128UL)) + _fuseiter_4700)]);
    float __cached_1;
    __cached_1 = __ins_1[0];
    vec_f32x16 __cached_2;
    __cached_2 = (__cached_0 * vec_f32x16(__cached_1));
    vec_f32x16::store(__cached_2, &__outs_0[((((fused_0fused_0fused_0__itr_0____itr_1_1411____itr_2_1412____itr_3_1413 / 8UL) * 1024UL) + ((fused_0fused_0fused_0__itr_0____itr_1_1411____itr_2_1412____itr_3_1413 % 8UL) * 128UL)) + _fuseiter_4700)]);
  }
}

static void mul__6450_closure_180_0wrapper(void* __stream, int8_t* __restrict__ __module_data, uint64_t i, generic_val* __restrict__ args) noexcept{
  mul__6450_closure_180(i, (float*)(args[0UL].v_ptr), (float*)(args[1UL].v_ptr), (float*)(args[2UL].v_ptr));
}

static void mul__6400_closure_181(uint64_t fused_0fused_0fused_0__itr_0____itr_1_1414____itr_2_1415____itr_3_1416, float* __restrict__ __ins_0, float* __restrict__ __ins_1, float* __restrict__ __outs_0) noexcept{
  for (uint64_t _fuseiter_4706 = 0UL; _fuseiter_4706 < 128UL; _fuseiter_4706 += 16UL) {
    vec_f32x16 __cached_0;
    __cached_0 = vec_f32x16::load(&__ins_0[((((fused_0fused_0fused_0__itr_0____itr_1_1414____itr_2_1415____itr_3_1416 / 8UL) * 1024UL) + ((fused_0fused_0fused_0__itr_0____itr_1_1414____itr_2_1415____itr_3_1416 % 8UL) * 128UL)) + _fuseiter_4706)]);
    float __cached_1;
    __cached_1 = __ins_1[0];
    vec_f32x16 __cached_2;
    __cached_2 = (__cached_0 * vec_f32x16(__cached_1));
    vec_f32x16::store(__cached_2, &__outs_0[((((fused_0fused_0fused_0__itr_0____itr_1_1414____itr_2_1415____itr_3_1416 / 8UL) * 1024UL) + ((fused_0fused_0fused_0__itr_0____itr_1_1414____itr_2_1415____itr_3_1416 % 8UL) * 128UL)) + _fuseiter_4706)]);
  }
}

static void mul__6400_closure_181_0wrapper(void* __stream, int8_t* __restrict__ __module_data, uint64_t i, generic_val* __restrict__ args) noexcept{
  mul__6400_closure_181(i, (float*)(args[0UL].v_ptr), (float*)(args[1UL].v_ptr), (float*)(args[2UL].v_ptr));
}

static void mul__6390_closure_182(uint64_t fused_0fused_0fused_0__itr_0____itr_1_1417____itr_2_1418____itr_3_1419, float* __restrict__ __ins_0, float* __restrict__ __ins_1, float* __restrict__ __outs_0) noexcept{
  for (uint64_t _fuseiter_4712 = 0UL; _fuseiter_4712 < 128UL; _fuseiter_4712 += 16UL) {
    vec_f32x16 __cached_0;
    __cached_0 = vec_f32x16::load(&__ins_0[((((fused_0fused_0fused_0__itr_0____itr_1_1417____itr_2_1418____itr_3_1419 / 8UL) * 1024UL) + ((fused_0fused_0fused_0__itr_0____itr_1_1417____itr_2_1418____itr_3_1419 % 8UL) * 128UL)) + _fuseiter_4712)]);
    float __cached_1;
    __cached_1 = __ins_1[0];
    vec_f32x16 __cached_2;
    __cached_2 = (__cached_0 * vec_f32x16(__cached_1));
    vec_f32x16::store(__cached_2, &__outs_0[((((fused_0fused_0fused_0__itr_0____itr_1_1417____itr_2_1418____itr_3_1419 / 8UL) * 1024UL) + ((fused_0fused_0fused_0__itr_0____itr_1_1417____itr_2_1418____itr_3_1419 % 8UL) * 128UL)) + _fuseiter_4712)]);
  }
}

static void mul__6390_closure_182_0wrapper(void* __stream, int8_t* __restrict__ __module_data, uint64_t i, generic_val* __restrict__ args) noexcept{
  mul__6390_closure_182(i, (float*)(args[0UL].v_ptr), (float*)(args[1UL].v_ptr), (float*)(args[2UL].v_ptr));
}

static void mul__6340_closure_183(uint64_t fused_0fused_0fused_0__itr_0____itr_1_1420____itr_2_1421____itr_3_1422, float* __restrict__ __ins_0, float* __restrict__ __ins_1, float* __restrict__ __outs_0) noexcept{
  for (uint64_t _fuseiter_4718 = 0UL; _fuseiter_4718 < 128UL; _fuseiter_4718 += 16UL) {
    vec_f32x16 __cached_0;
    __cached_0 = vec_f32x16::load(&__ins_0[((((fused_0fused_0fused_0__itr_0____itr_1_1420____itr_2_1421____itr_3_1422 / 8UL) * 1024UL) + ((fused_0fused_0fused_0__itr_0____itr_1_1420____itr_2_1421____itr_3_1422 % 8UL) * 128UL)) + _fuseiter_4718)]);
    float __cached_1;
    __cached_1 = __ins_1[0];
    vec_f32x16 __cached_2;
    __cached_2 = (__cached_0 * vec_f32x16(__cached_1));
    vec_f32x16::store(__cached_2, &__outs_0[((((fused_0fused_0fused_0__itr_0____itr_1_1420____itr_2_1421____itr_3_1422 / 8UL) * 1024UL) + ((fused_0fused_0fused_0__itr_0____itr_1_1420____itr_2_1421____itr_3_1422 % 8UL) * 128UL)) + _fuseiter_4718)]);
  }
}

static void mul__6340_closure_183_0wrapper(void* __stream, int8_t* __restrict__ __module_data, uint64_t i, generic_val* __restrict__ args) noexcept{
  mul__6340_closure_183(i, (float*)(args[0UL].v_ptr), (float*)(args[1UL].v_ptr), (float*)(args[2UL].v_ptr));
}

static void mul__6330_closure_184(uint64_t fused_0fused_0fused_0__itr_0____itr_1_1423____itr_2_1424____itr_3_1425, float* __restrict__ __ins_0, float* __restrict__ __ins_1, float* __restrict__ __outs_0) noexcept{
  for (uint64_t _fuseiter_4724 = 0UL; _fuseiter_4724 < 128UL; _fuseiter_4724 += 16UL) {
    vec_f32x16 __cached_0;
    __cached_0 = vec_f32x16::load(&__ins_0[((((fused_0fused_0fused_0__itr_0____itr_1_1423____itr_2_1424____itr_3_1425 / 8UL) * 1024UL) + ((fused_0fused_0fused_0__itr_0____itr_1_1423____itr_2_1424____itr_3_1425 % 8UL) * 128UL)) + _fuseiter_4724)]);
    float __cached_1;
    __cached_1 = __ins_1[0];
    vec_f32x16 __cached_2;
    __cached_2 = (__cached_0 * vec_f32x16(__cached_1));
    vec_f32x16::store(__cached_2, &__outs_0[((((fused_0fused_0fused_0__itr_0____itr_1_1423____itr_2_1424____itr_3_1425 / 8UL) * 1024UL) + ((fused_0fused_0fused_0__itr_0____itr_1_1423____itr_2_1424____itr_3_1425 % 8UL) * 128UL)) + _fuseiter_4724)]);
  }
}

static void mul__6330_closure_184_0wrapper(void* __stream, int8_t* __restrict__ __module_data, uint64_t i, generic_val* __restrict__ args) noexcept{
  mul__6330_closure_184(i, (float*)(args[0UL].v_ptr), (float*)(args[1UL].v_ptr), (float*)(args[2UL].v_ptr));
}

static void mul__6220_closure_185(uint64_t fused_0fused_0fused_0__itr_0____itr_1_1432____itr_2_1433____itr_3_1434, float* __restrict__ __ins_0, float* __restrict__ __ins_1, float* __restrict__ __outs_0) noexcept{
  for (uint64_t _fuseiter_4742 = 0UL; _fuseiter_4742 < 128UL; _fuseiter_4742 += 16UL) {
    vec_f32x16 __cached_0;
    __cached_0 = vec_f32x16::load(&__ins_0[((((fused_0fused_0fused_0__itr_0____itr_1_1432____itr_2_1433____itr_3_1434 / 8UL) * 1024UL) + ((fused_0fused_0fused_0__itr_0____itr_1_1432____itr_2_1433____itr_3_1434 % 8UL) * 128UL)) + _fuseiter_4742)]);
    float __cached_1;
    __cached_1 = __ins_1[0];
    vec_f32x16 __cached_2;
    __cached_2 = (__cached_0 * vec_f32x16(__cached_1));
    vec_f32x16::store(__cached_2, &__outs_0[((((fused_0fused_0fused_0__itr_0____itr_1_1432____itr_2_1433____itr_3_1434 / 8UL) * 1024UL) + ((fused_0fused_0fused_0__itr_0____itr_1_1432____itr_2_1433____itr_3_1434 % 8UL) * 128UL)) + _fuseiter_4742)]);
  }
}

static void mul__6220_closure_185_0wrapper(void* __stream, int8_t* __restrict__ __module_data, uint64_t i, generic_val* __restrict__ args) noexcept{
  mul__6220_closure_185(i, (float*)(args[0UL].v_ptr), (float*)(args[1UL].v_ptr), (float*)(args[2UL].v_ptr));
}

static void mul__6210_closure_186(uint64_t fused_0fused_0fused_0__itr_0____itr_1_1435____itr_2_1436____itr_3_1437, float* __restrict__ __ins_0, float* __restrict__ __ins_1, float* __restrict__ __outs_0) noexcept{
  for (uint64_t _fuseiter_4748 = 0UL; _fuseiter_4748 < 128UL; _fuseiter_4748 += 16UL) {
    vec_f32x16 __cached_0;
    __cached_0 = vec_f32x16::load(&__ins_0[((((fused_0fused_0fused_0__itr_0____itr_1_1435____itr_2_1436____itr_3_1437 / 8UL) * 1024UL) + ((fused_0fused_0fused_0__itr_0____itr_1_1435____itr_2_1436____itr_3_1437 % 8UL) * 128UL)) + _fuseiter_4748)]);
    float __cached_1;
    __cached_1 = __ins_1[0];
    vec_f32x16 __cached_2;
    __cached_2 = (__cached_0 * vec_f32x16(__cached_1));
    vec_f32x16::store(__cached_2, &__outs_0[((((fused_0fused_0fused_0__itr_0____itr_1_1435____itr_2_1436____itr_3_1437 / 8UL) * 1024UL) + ((fused_0fused_0fused_0__itr_0____itr_1_1435____itr_2_1436____itr_3_1437 % 8UL) * 128UL)) + _fuseiter_4748)]);
  }
}

static void mul__6210_closure_186_0wrapper(void* __stream, int8_t* __restrict__ __module_data, uint64_t i, generic_val* __restrict__ args) noexcept{
  mul__6210_closure_186(i, (float*)(args[0UL].v_ptr), (float*)(args[1UL].v_ptr), (float*)(args[2UL].v_ptr));
}

static void mul__6160_closure_187(uint64_t fused_0fused_0fused_0__itr_0____itr_1_1438____itr_2_1439____itr_3_1440, float* __restrict__ __ins_0, float* __restrict__ __ins_1, float* __restrict__ __outs_0) noexcept{
  for (uint64_t _fuseiter_4754 = 0UL; _fuseiter_4754 < 32UL; _fuseiter_4754 += 16UL) {
    vec_f32x16 __cached_0;
    __cached_0 = vec_f32x16::load(&__ins_0[((((fused_0fused_0fused_0__itr_0____itr_1_1438____itr_2_1439____itr_3_1440 / 32UL) * 1024UL) + ((fused_0fused_0fused_0__itr_0____itr_1_1438____itr_2_1439____itr_3_1440 % 32UL) * 32UL)) + _fuseiter_4754)]);
    float __cached_1;
    __cached_1 = __ins_1[0];
    vec_f32x16 __cached_2;
    __cached_2 = (__cached_0 * vec_f32x16(__cached_1));
    vec_f32x16::store(__cached_2, &__outs_0[((((fused_0fused_0fused_0__itr_0____itr_1_1438____itr_2_1439____itr_3_1440 / 32UL) * 1024UL) + ((fused_0fused_0fused_0__itr_0____itr_1_1438____itr_2_1439____itr_3_1440 % 32UL) * 32UL)) + _fuseiter_4754)]);
  }
}

static void mul__6160_closure_187_0wrapper(void* __stream, int8_t* __restrict__ __module_data, uint64_t i, generic_val* __restrict__ args) noexcept{
  mul__6160_closure_187(i, (float*)(args[0UL].v_ptr), (float*)(args[1UL].v_ptr), (float*)(args[2UL].v_ptr));
}

static void mul__6150_closure_188(uint64_t fused_0fused_0fused_0__itr_0____itr_1_1441____itr_2_1442____itr_3_1443, float* __restrict__ __ins_0, float* __restrict__ __ins_1, float* __restrict__ __outs_0) noexcept{
  for (uint64_t _fuseiter_4760 = 0UL; _fuseiter_4760 < 32UL; _fuseiter_4760 += 16UL) {
    vec_f32x16 __cached_0;
    __cached_0 = vec_f32x16::load(&__ins_0[((((fused_0fused_0fused_0__itr_0____itr_1_1441____itr_2_1442____itr_3_1443 / 32UL) * 1024UL) + ((fused_0fused_0fused_0__itr_0____itr_1_1441____itr_2_1442____itr_3_1443 % 32UL) * 32UL)) + _fuseiter_4760)]);
    float __cached_1;
    __cached_1 = __ins_1[0];
    vec_f32x16 __cached_2;
    __cached_2 = (__cached_0 * vec_f32x16(__cached_1));
    vec_f32x16::store(__cached_2, &__outs_0[((((fused_0fused_0fused_0__itr_0____itr_1_1441____itr_2_1442____itr_3_1443 / 32UL) * 1024UL) + ((fused_0fused_0fused_0__itr_0____itr_1_1441____itr_2_1442____itr_3_1443 % 32UL) * 32UL)) + _fuseiter_4760)]);
  }
}

static void mul__6150_closure_188_0wrapper(void* __stream, int8_t* __restrict__ __module_data, uint64_t i, generic_val* __restrict__ args) noexcept{
  mul__6150_closure_188(i, (float*)(args[0UL].v_ptr), (float*)(args[1UL].v_ptr), (float*)(args[2UL].v_ptr));
}

static void mul__6140_closure_189(uint64_t fused_0fused_0fused_0__itr_0____itr_1_1444____itr_2_1445____itr_3_1446, float* __restrict__ __ins_0, float* __restrict__ __ins_1, float* __restrict__ __outs_0) noexcept{
  for (uint64_t _fuseiter_4766 = 0UL; _fuseiter_4766 < 128UL; _fuseiter_4766 += 16UL) {
    vec_f32x16 __cached_0;
    __cached_0 = vec_f32x16::load(&__ins_0[((((fused_0fused_0fused_0__itr_0____itr_1_1444____itr_2_1445____itr_3_1446 / 4UL) * 512UL) + ((fused_0fused_0fused_0__itr_0____itr_1_1444____itr_2_1445____itr_3_1446 % 4UL) * 128UL)) + _fuseiter_4766)]);
    float __cached_1;
    __cached_1 = __ins_1[0];
    vec_f32x16 __cached_2;
    __cached_2 = (__cached_0 * vec_f32x16(__cached_1));
    vec_f32x16::store(__cached_2, &__outs_0[((((fused_0fused_0fused_0__itr_0____itr_1_1444____itr_2_1445____itr_3_1446 / 4UL) * 512UL) + ((fused_0fused_0fused_0__itr_0____itr_1_1444____itr_2_1445____itr_3_1446 % 4UL) * 128UL)) + _fuseiter_4766)]);
  }
}

static void mul__6140_closure_189_0wrapper(void* __stream, int8_t* __restrict__ __module_data, uint64_t i, generic_val* __restrict__ args) noexcept{
  mul__6140_closure_189(i, (float*)(args[0UL].v_ptr), (float*)(args[1UL].v_ptr), (float*)(args[2UL].v_ptr));
}

static void mul__6130_closure_190(uint64_t fused_0fused_0fused_0__itr_0____itr_1_1447____itr_2_1448____itr_3_1449, float* __restrict__ __ins_0, float* __restrict__ __ins_1, float* __restrict__ __outs_0) noexcept{
  for (uint64_t _fuseiter_4772 = 0UL; _fuseiter_4772 < 128UL; _fuseiter_4772 += 16UL) {
    vec_f32x16 __cached_0;
    __cached_0 = vec_f32x16::load(&__ins_0[((((fused_0fused_0fused_0__itr_0____itr_1_1447____itr_2_1448____itr_3_1449 / 4UL) * 512UL) + ((fused_0fused_0fused_0__itr_0____itr_1_1447____itr_2_1448____itr_3_1449 % 4UL) * 128UL)) + _fuseiter_4772)]);
    float __cached_1;
    __cached_1 = __ins_1[0];
    vec_f32x16 __cached_2;
    __cached_2 = (__cached_0 * vec_f32x16(__cached_1));
    vec_f32x16::store(__cached_2, &__outs_0[((((fused_0fused_0fused_0__itr_0____itr_1_1447____itr_2_1448____itr_3_1449 / 4UL) * 512UL) + ((fused_0fused_0fused_0__itr_0____itr_1_1447____itr_2_1448____itr_3_1449 % 4UL) * 128UL)) + _fuseiter_4772)]);
  }
}

static void mul__6130_closure_190_0wrapper(void* __stream, int8_t* __restrict__ __module_data, uint64_t i, generic_val* __restrict__ args) noexcept{
  mul__6130_closure_190(i, (float*)(args[0UL].v_ptr), (float*)(args[1UL].v_ptr), (float*)(args[2UL].v_ptr));
}

static void mul__6080_closure_191(uint64_t fused_0fused_0fused_0__itr_0____itr_1_1450____itr_2_1451____itr_3_1452, float* __restrict__ __ins_0, float* __restrict__ __ins_1, float* __restrict__ __outs_0) noexcept{
  for (uint64_t _fuseiter_4778 = 0UL; _fuseiter_4778 < 64UL; _fuseiter_4778 += 16UL) {
    vec_f32x16 __cached_0;
    __cached_0 = vec_f32x16::load(&__ins_0[((((fused_0fused_0fused_0__itr_0____itr_1_1450____itr_2_1451____itr_3_1452 / 8UL) * 512UL) + ((fused_0fused_0fused_0__itr_0____itr_1_1450____itr_2_1451____itr_3_1452 % 8UL) * 64UL)) + _fuseiter_4778)]);
    float __cached_1;
    __cached_1 = __ins_1[0];
    vec_f32x16 __cached_2;
    __cached_2 = (__cached_0 * vec_f32x16(__cached_1));
    vec_f32x16::store(__cached_2, &__outs_0[((((fused_0fused_0fused_0__itr_0____itr_1_1450____itr_2_1451____itr_3_1452 / 8UL) * 512UL) + ((fused_0fused_0fused_0__itr_0____itr_1_1450____itr_2_1451____itr_3_1452 % 8UL) * 64UL)) + _fuseiter_4778)]);
  }
}

static void mul__6080_closure_191_0wrapper(void* __stream, int8_t* __restrict__ __module_data, uint64_t i, generic_val* __restrict__ args) noexcept{
  mul__6080_closure_191(i, (float*)(args[0UL].v_ptr), (float*)(args[1UL].v_ptr), (float*)(args[2UL].v_ptr));
}

static void mul__6070_closure_192(uint64_t fused_0fused_0fused_0__itr_0____itr_1_1453____itr_2_1454____itr_3_1455, float* __restrict__ __ins_0, float* __restrict__ __ins_1, float* __restrict__ __outs_0) noexcept{
  for (uint64_t _fuseiter_4784 = 0UL; _fuseiter_4784 < 64UL; _fuseiter_4784 += 16UL) {
    vec_f32x16 __cached_0;
    __cached_0 = vec_f32x16::load(&__ins_0[((((fused_0fused_0fused_0__itr_0____itr_1_1453____itr_2_1454____itr_3_1455 / 8UL) * 512UL) + ((fused_0fused_0fused_0__itr_0____itr_1_1453____itr_2_1454____itr_3_1455 % 8UL) * 64UL)) + _fuseiter_4784)]);
    float __cached_1;
    __cached_1 = __ins_1[0];
    vec_f32x16 __cached_2;
    __cached_2 = (__cached_0 * vec_f32x16(__cached_1));
    vec_f32x16::store(__cached_2, &__outs_0[((((fused_0fused_0fused_0__itr_0____itr_1_1453____itr_2_1454____itr_3_1455 / 8UL) * 512UL) + ((fused_0fused_0fused_0__itr_0____itr_1_1453____itr_2_1454____itr_3_1455 % 8UL) * 64UL)) + _fuseiter_4784)]);
  }
}

static void mul__6070_closure_192_0wrapper(void* __stream, int8_t* __restrict__ __module_data, uint64_t i, generic_val* __restrict__ args) noexcept{
  mul__6070_closure_192(i, (float*)(args[0UL].v_ptr), (float*)(args[1UL].v_ptr), (float*)(args[2UL].v_ptr));
}

static void mul__6020_closure_193(uint64_t fused_0fused_0fused_0__itr_0____itr_1_1456____itr_2_1457____itr_3_1458, float* __restrict__ __ins_0, float* __restrict__ __ins_1, float* __restrict__ __outs_0) noexcept{
  for (uint64_t _fuseiter_4790 = 0UL; _fuseiter_4790 < 128UL; _fuseiter_4790 += 16UL) {
    vec_f32x16 __cached_0;
    __cached_0 = vec_f32x16::load(&__ins_0[((((fused_0fused_0fused_0__itr_0____itr_1_1456____itr_2_1457____itr_3_1458 / 4UL) * 512UL) + ((fused_0fused_0fused_0__itr_0____itr_1_1456____itr_2_1457____itr_3_1458 % 4UL) * 128UL)) + _fuseiter_4790)]);
    float __cached_1;
    __cached_1 = __ins_1[0];
    vec_f32x16 __cached_2;
    __cached_2 = (__cached_0 * vec_f32x16(__cached_1));
    vec_f32x16::store(__cached_2, &__outs_0[((((fused_0fused_0fused_0__itr_0____itr_1_1456____itr_2_1457____itr_3_1458 / 4UL) * 512UL) + ((fused_0fused_0fused_0__itr_0____itr_1_1456____itr_2_1457____itr_3_1458 % 4UL) * 128UL)) + _fuseiter_4790)]);
  }
}

static void mul__6020_closure_193_0wrapper(void* __stream, int8_t* __restrict__ __module_data, uint64_t i, generic_val* __restrict__ args) noexcept{
  mul__6020_closure_193(i, (float*)(args[0UL].v_ptr), (float*)(args[1UL].v_ptr), (float*)(args[2UL].v_ptr));
}

static void mul__6010_closure_194(uint64_t fused_0fused_0fused_0__itr_0____itr_1_1459____itr_2_1460____itr_3_1461, float* __restrict__ __ins_0, float* __restrict__ __ins_1, float* __restrict__ __outs_0) noexcept{
  for (uint64_t _fuseiter_4796 = 0UL; _fuseiter_4796 < 128UL; _fuseiter_4796 += 16UL) {
    vec_f32x16 __cached_0;
    __cached_0 = vec_f32x16::load(&__ins_0[((((fused_0fused_0fused_0__itr_0____itr_1_1459____itr_2_1460____itr_3_1461 / 4UL) * 512UL) + ((fused_0fused_0fused_0__itr_0____itr_1_1459____itr_2_1460____itr_3_1461 % 4UL) * 128UL)) + _fuseiter_4796)]);
    float __cached_1;
    __cached_1 = __ins_1[0];
    vec_f32x16 __cached_2;
    __cached_2 = (__cached_0 * vec_f32x16(__cached_1));
    vec_f32x16::store(__cached_2, &__outs_0[((((fused_0fused_0fused_0__itr_0____itr_1_1459____itr_2_1460____itr_3_1461 / 4UL) * 512UL) + ((fused_0fused_0fused_0__itr_0____itr_1_1459____itr_2_1460____itr_3_1461 % 4UL) * 128UL)) + _fuseiter_4796)]);
  }
}

static void mul__6010_closure_194_0wrapper(void* __stream, int8_t* __restrict__ __module_data, uint64_t i, generic_val* __restrict__ args) noexcept{
  mul__6010_closure_194(i, (float*)(args[0UL].v_ptr), (float*)(args[1UL].v_ptr), (float*)(args[2UL].v_ptr));
}

static void mul__5960_closure_195(uint64_t fused_0fused_0fused_0__itr_0____itr_1_1462____itr_2_1463____itr_3_1464, float* __restrict__ __ins_0, float* __restrict__ __ins_1, float* __restrict__ __outs_0) noexcept{
  for (uint64_t _fuseiter_4802 = 0UL; _fuseiter_4802 < 32UL; _fuseiter_4802 += 16UL) {
    vec_f32x16 __cached_0;
    __cached_0 = vec_f32x16::load(&__ins_0[((((fused_0fused_0fused_0__itr_0____itr_1_1462____itr_2_1463____itr_3_1464 / 16UL) * 512UL) + ((fused_0fused_0fused_0__itr_0____itr_1_1462____itr_2_1463____itr_3_1464 % 16UL) * 32UL)) + _fuseiter_4802)]);
    float __cached_1;
    __cached_1 = __ins_1[0];
    vec_f32x16 __cached_2;
    __cached_2 = (__cached_0 * vec_f32x16(__cached_1));
    vec_f32x16::store(__cached_2, &__outs_0[((((fused_0fused_0fused_0__itr_0____itr_1_1462____itr_2_1463____itr_3_1464 / 16UL) * 512UL) + ((fused_0fused_0fused_0__itr_0____itr_1_1462____itr_2_1463____itr_3_1464 % 16UL) * 32UL)) + _fuseiter_4802)]);
  }
}

static void mul__5960_closure_195_0wrapper(void* __stream, int8_t* __restrict__ __module_data, uint64_t i, generic_val* __restrict__ args) noexcept{
  mul__5960_closure_195(i, (float*)(args[0UL].v_ptr), (float*)(args[1UL].v_ptr), (float*)(args[2UL].v_ptr));
}

static void mul__5950_closure_196(uint64_t fused_0fused_0fused_0__itr_0____itr_1_1465____itr_2_1466____itr_3_1467, float* __restrict__ __ins_0, float* __restrict__ __ins_1, float* __restrict__ __outs_0) noexcept{
  for (uint64_t _fuseiter_4808 = 0UL; _fuseiter_4808 < 32UL; _fuseiter_4808 += 16UL) {
    vec_f32x16 __cached_0;
    __cached_0 = vec_f32x16::load(&__ins_0[((((fused_0fused_0fused_0__itr_0____itr_1_1465____itr_2_1466____itr_3_1467 / 16UL) * 512UL) + ((fused_0fused_0fused_0__itr_0____itr_1_1465____itr_2_1466____itr_3_1467 % 16UL) * 32UL)) + _fuseiter_4808)]);
    float __cached_1;
    __cached_1 = __ins_1[0];
    vec_f32x16 __cached_2;
    __cached_2 = (__cached_0 * vec_f32x16(__cached_1));
    vec_f32x16::store(__cached_2, &__outs_0[((((fused_0fused_0fused_0__itr_0____itr_1_1465____itr_2_1466____itr_3_1467 / 16UL) * 512UL) + ((fused_0fused_0fused_0__itr_0____itr_1_1465____itr_2_1466____itr_3_1467 % 16UL) * 32UL)) + _fuseiter_4808)]);
  }
}

static void mul__5950_closure_196_0wrapper(void* __stream, int8_t* __restrict__ __module_data, uint64_t i, generic_val* __restrict__ args) noexcept{
  mul__5950_closure_196(i, (float*)(args[0UL].v_ptr), (float*)(args[1UL].v_ptr), (float*)(args[2UL].v_ptr));
}

static void mul__5900_closure_197(uint64_t fused_0fused_0fused_0__itr_0____itr_1_1468____itr_2_1469____itr_3_1470, float* __restrict__ __ins_0, float* __restrict__ __ins_1, float* __restrict__ __outs_0) noexcept{
  for (uint64_t _fuseiter_4814 = 0UL; _fuseiter_4814 < 32UL; _fuseiter_4814 += 16UL) {
    vec_f32x16 __cached_0;
    __cached_0 = vec_f32x16::load(&__ins_0[((((fused_0fused_0fused_0__itr_0____itr_1_1468____itr_2_1469____itr_3_1470 / 16UL) * 512UL) + ((fused_0fused_0fused_0__itr_0____itr_1_1468____itr_2_1469____itr_3_1470 % 16UL) * 32UL)) + _fuseiter_4814)]);
    float __cached_1;
    __cached_1 = __ins_1[0];
    vec_f32x16 __cached_2;
    __cached_2 = (__cached_0 * vec_f32x16(__cached_1));
    vec_f32x16::store(__cached_2, &__outs_0[((((fused_0fused_0fused_0__itr_0____itr_1_1468____itr_2_1469____itr_3_1470 / 16UL) * 512UL) + ((fused_0fused_0fused_0__itr_0____itr_1_1468____itr_2_1469____itr_3_1470 % 16UL) * 32UL)) + _fuseiter_4814)]);
  }
}

static void mul__5900_closure_197_0wrapper(void* __stream, int8_t* __restrict__ __module_data, uint64_t i, generic_val* __restrict__ args) noexcept{
  mul__5900_closure_197(i, (float*)(args[0UL].v_ptr), (float*)(args[1UL].v_ptr), (float*)(args[2UL].v_ptr));
}

static void mul__5890_closure_198(uint64_t fused_0fused_0fused_0__itr_0____itr_1_1471____itr_2_1472____itr_3_1473, float* __restrict__ __ins_0, float* __restrict__ __ins_1, float* __restrict__ __outs_0) noexcept{
  for (uint64_t _fuseiter_4820 = 0UL; _fuseiter_4820 < 32UL; _fuseiter_4820 += 16UL) {
    vec_f32x16 __cached_0;
    __cached_0 = vec_f32x16::load(&__ins_0[((((fused_0fused_0fused_0__itr_0____itr_1_1471____itr_2_1472____itr_3_1473 / 16UL) * 512UL) + ((fused_0fused_0fused_0__itr_0____itr_1_1471____itr_2_1472____itr_3_1473 % 16UL) * 32UL)) + _fuseiter_4820)]);
    float __cached_1;
    __cached_1 = __ins_1[0];
    vec_f32x16 __cached_2;
    __cached_2 = (__cached_0 * vec_f32x16(__cached_1));
    vec_f32x16::store(__cached_2, &__outs_0[((((fused_0fused_0fused_0__itr_0____itr_1_1471____itr_2_1472____itr_3_1473 / 16UL) * 512UL) + ((fused_0fused_0fused_0__itr_0____itr_1_1471____itr_2_1472____itr_3_1473 % 16UL) * 32UL)) + _fuseiter_4820)]);
  }
}

static void mul__5890_closure_198_0wrapper(void* __stream, int8_t* __restrict__ __module_data, uint64_t i, generic_val* __restrict__ args) noexcept{
  mul__5890_closure_198(i, (float*)(args[0UL].v_ptr), (float*)(args[1UL].v_ptr), (float*)(args[2UL].v_ptr));
}

static void mul__6500_closure_199(uint64_t fused_0fused_0fused_0__itr_0____itr_1_1474____itr_2_1475____itr_3_1476, float* __restrict__ __ins_0, float* __restrict__ __ins_1, float* __restrict__ __outs_0) noexcept{
  for (uint64_t _fuseiter_4826 = 0UL; _fuseiter_4826 < 64UL; _fuseiter_4826 += 16UL) {
    vec_f32x16 __cached_0;
    __cached_0 = vec_f32x16::load(&__ins_0[((((fused_0fused_0fused_0__itr_0____itr_1_1474____itr_2_1475____itr_3_1476 / 4UL) * 256UL) + ((fused_0fused_0fused_0__itr_0____itr_1_1474____itr_2_1475____itr_3_1476 % 4UL) * 64UL)) + _fuseiter_4826)]);
    float __cached_1;
    __cached_1 = __ins_1[0];
    vec_f32x16 __cached_2;
    __cached_2 = (__cached_0 * vec_f32x16(__cached_1));
    vec_f32x16::store(__cached_2, &__outs_0[((((fused_0fused_0fused_0__itr_0____itr_1_1474____itr_2_1475____itr_3_1476 / 4UL) * 256UL) + ((fused_0fused_0fused_0__itr_0____itr_1_1474____itr_2_1475____itr_3_1476 % 4UL) * 64UL)) + _fuseiter_4826)]);
  }
}

static void mul__6500_closure_199_0wrapper(void* __stream, int8_t* __restrict__ __module_data, uint64_t i, generic_val* __restrict__ args) noexcept{
  mul__6500_closure_199(i, (float*)(args[0UL].v_ptr), (float*)(args[1UL].v_ptr), (float*)(args[2UL].v_ptr));
}

static void mul__6490_closure_200(uint64_t fused_0fused_0fused_0__itr_0____itr_1_1477____itr_2_1478____itr_3_1479, float* __restrict__ __ins_0, float* __restrict__ __ins_1, float* __restrict__ __outs_0) noexcept{
  for (uint64_t _fuseiter_4832 = 0UL; _fuseiter_4832 < 64UL; _fuseiter_4832 += 16UL) {
    vec_f32x16 __cached_0;
    __cached_0 = vec_f32x16::load(&__ins_0[((((fused_0fused_0fused_0__itr_0____itr_1_1477____itr_2_1478____itr_3_1479 / 4UL) * 256UL) + ((fused_0fused_0fused_0__itr_0____itr_1_1477____itr_2_1478____itr_3_1479 % 4UL) * 64UL)) + _fuseiter_4832)]);
    float __cached_1;
    __cached_1 = __ins_1[0];
    vec_f32x16 __cached_2;
    __cached_2 = (__cached_0 * vec_f32x16(__cached_1));
    vec_f32x16::store(__cached_2, &__outs_0[((((fused_0fused_0fused_0__itr_0____itr_1_1477____itr_2_1478____itr_3_1479 / 4UL) * 256UL) + ((fused_0fused_0fused_0__itr_0____itr_1_1477____itr_2_1478____itr_3_1479 % 4UL) * 64UL)) + _fuseiter_4832)]);
  }
}

static void mul__6490_closure_200_0wrapper(void* __stream, int8_t* __restrict__ __module_data, uint64_t i, generic_val* __restrict__ args) noexcept{
  mul__6490_closure_200(i, (float*)(args[0UL].v_ptr), (float*)(args[1UL].v_ptr), (float*)(args[2UL].v_ptr));
}

static void mul__6480_closure_201(uint64_t fused_0fused_0fused_0__itr_0____itr_1_1480____itr_2_1481____itr_3_1482, float* __restrict__ __ins_0, float* __restrict__ __ins_1, float* __restrict__ __outs_0) noexcept{
  for (uint64_t _fuseiter_4838 = 0UL; _fuseiter_4838 < 32UL; _fuseiter_4838 += 16UL) {
    vec_f32x16 __cached_0;
    __cached_0 = vec_f32x16::load(&__ins_0[((((fused_0fused_0fused_0__itr_0____itr_1_1480____itr_2_1481____itr_3_1482 / 8UL) * 256UL) + ((fused_0fused_0fused_0__itr_0____itr_1_1480____itr_2_1481____itr_3_1482 % 8UL) * 32UL)) + _fuseiter_4838)]);
    float __cached_1;
    __cached_1 = __ins_1[0];
    vec_f32x16 __cached_2;
    __cached_2 = (__cached_0 * vec_f32x16(__cached_1));
    vec_f32x16::store(__cached_2, &__outs_0[((((fused_0fused_0fused_0__itr_0____itr_1_1480____itr_2_1481____itr_3_1482 / 8UL) * 256UL) + ((fused_0fused_0fused_0__itr_0____itr_1_1480____itr_2_1481____itr_3_1482 % 8UL) * 32UL)) + _fuseiter_4838)]);
  }
}

static void mul__6480_closure_201_0wrapper(void* __stream, int8_t* __restrict__ __module_data, uint64_t i, generic_val* __restrict__ args) noexcept{
  mul__6480_closure_201(i, (float*)(args[0UL].v_ptr), (float*)(args[1UL].v_ptr), (float*)(args[2UL].v_ptr));
}

static void mul__6470_closure_202(uint64_t fused_0fused_0fused_0__itr_0____itr_1_1483____itr_2_1484____itr_3_1485, float* __restrict__ __ins_0, float* __restrict__ __ins_1, float* __restrict__ __outs_0) noexcept{
  for (uint64_t _fuseiter_4844 = 0UL; _fuseiter_4844 < 32UL; _fuseiter_4844 += 16UL) {
    vec_f32x16 __cached_0;
    __cached_0 = vec_f32x16::load(&__ins_0[((((fused_0fused_0fused_0__itr_0____itr_1_1483____itr_2_1484____itr_3_1485 / 8UL) * 256UL) + ((fused_0fused_0fused_0__itr_0____itr_1_1483____itr_2_1484____itr_3_1485 % 8UL) * 32UL)) + _fuseiter_4844)]);
    float __cached_1;
    __cached_1 = __ins_1[0];
    vec_f32x16 __cached_2;
    __cached_2 = (__cached_0 * vec_f32x16(__cached_1));
    vec_f32x16::store(__cached_2, &__outs_0[((((fused_0fused_0fused_0__itr_0____itr_1_1483____itr_2_1484____itr_3_1485 / 8UL) * 256UL) + ((fused_0fused_0fused_0__itr_0____itr_1_1483____itr_2_1484____itr_3_1485 % 8UL) * 32UL)) + _fuseiter_4844)]);
  }
}

static void mul__6470_closure_202_0wrapper(void* __stream, int8_t* __restrict__ __module_data, uint64_t i, generic_val* __restrict__ args) noexcept{
  mul__6470_closure_202(i, (float*)(args[0UL].v_ptr), (float*)(args[1UL].v_ptr), (float*)(args[2UL].v_ptr));
}

static void mul__6440_closure_203(uint64_t fused_0fused_0fused_0__itr_0____itr_1_1486____itr_2_1487____itr_3_1488, float* __restrict__ __ins_0, float* __restrict__ __ins_1, float* __restrict__ __outs_0) noexcept{
  for (uint64_t _fuseiter_4850 = 0UL; _fuseiter_4850 < 32UL; _fuseiter_4850 += 16UL) {
    vec_f32x16 __cached_0;
    __cached_0 = vec_f32x16::load(&__ins_0[((((fused_0fused_0fused_0__itr_0____itr_1_1486____itr_2_1487____itr_3_1488 / 8UL) * 256UL) + ((fused_0fused_0fused_0__itr_0____itr_1_1486____itr_2_1487____itr_3_1488 % 8UL) * 32UL)) + _fuseiter_4850)]);
    float __cached_1;
    __cached_1 = __ins_1[0];
    vec_f32x16 __cached_2;
    __cached_2 = (__cached_0 * vec_f32x16(__cached_1));
    vec_f32x16::store(__cached_2, &__outs_0[((((fused_0fused_0fused_0__itr_0____itr_1_1486____itr_2_1487____itr_3_1488 / 8UL) * 256UL) + ((fused_0fused_0fused_0__itr_0____itr_1_1486____itr_2_1487____itr_3_1488 % 8UL) * 32UL)) + _fuseiter_4850)]);
  }
}

static void mul__6440_closure_203_0wrapper(void* __stream, int8_t* __restrict__ __module_data, uint64_t i, generic_val* __restrict__ args) noexcept{
  mul__6440_closure_203(i, (float*)(args[0UL].v_ptr), (float*)(args[1UL].v_ptr), (float*)(args[2UL].v_ptr));
}

static void mul__6430_closure_204(uint64_t fused_0fused_0fused_0__itr_0____itr_1_1489____itr_2_1490____itr_3_1491, float* __restrict__ __ins_0, float* __restrict__ __ins_1, float* __restrict__ __outs_0) noexcept{
  for (uint64_t _fuseiter_4856 = 0UL; _fuseiter_4856 < 32UL; _fuseiter_4856 += 16UL) {
    vec_f32x16 __cached_0;
    __cached_0 = vec_f32x16::load(&__ins_0[((((fused_0fused_0fused_0__itr_0____itr_1_1489____itr_2_1490____itr_3_1491 / 8UL) * 256UL) + ((fused_0fused_0fused_0__itr_0____itr_1_1489____itr_2_1490____itr_3_1491 % 8UL) * 32UL)) + _fuseiter_4856)]);
    float __cached_1;
    __cached_1 = __ins_1[0];
    vec_f32x16 __cached_2;
    __cached_2 = (__cached_0 * vec_f32x16(__cached_1));
    vec_f32x16::store(__cached_2, &__outs_0[((((fused_0fused_0fused_0__itr_0____itr_1_1489____itr_2_1490____itr_3_1491 / 8UL) * 256UL) + ((fused_0fused_0fused_0__itr_0____itr_1_1489____itr_2_1490____itr_3_1491 % 8UL) * 32UL)) + _fuseiter_4856)]);
  }
}

static void mul__6430_closure_204_0wrapper(void* __stream, int8_t* __restrict__ __module_data, uint64_t i, generic_val* __restrict__ args) noexcept{
  mul__6430_closure_204(i, (float*)(args[0UL].v_ptr), (float*)(args[1UL].v_ptr), (float*)(args[2UL].v_ptr));
}

static void mul__6380_closure_205(uint64_t fused_0fused_0fused_0__itr_0____itr_1_1498____itr_2_1499____itr_3_1500, float* __restrict__ __ins_0, float* __restrict__ __ins_1, float* __restrict__ __outs_0) noexcept{
  for (uint64_t _fuseiter_4874 = 0UL; _fuseiter_4874 < 64UL; _fuseiter_4874 += 16UL) {
    vec_f32x16 __cached_0;
    __cached_0 = vec_f32x16::load(&__ins_0[((((fused_0fused_0fused_0__itr_0____itr_1_1498____itr_2_1499____itr_3_1500 / 4UL) * 256UL) + ((fused_0fused_0fused_0__itr_0____itr_1_1498____itr_2_1499____itr_3_1500 % 4UL) * 64UL)) + _fuseiter_4874)]);
    float __cached_1;
    __cached_1 = __ins_1[0];
    vec_f32x16 __cached_2;
    __cached_2 = (__cached_0 * vec_f32x16(__cached_1));
    vec_f32x16::store(__cached_2, &__outs_0[((((fused_0fused_0fused_0__itr_0____itr_1_1498____itr_2_1499____itr_3_1500 / 4UL) * 256UL) + ((fused_0fused_0fused_0__itr_0____itr_1_1498____itr_2_1499____itr_3_1500 % 4UL) * 64UL)) + _fuseiter_4874)]);
  }
}

static void mul__6380_closure_205_0wrapper(void* __stream, int8_t* __restrict__ __module_data, uint64_t i, generic_val* __restrict__ args) noexcept{
  mul__6380_closure_205(i, (float*)(args[0UL].v_ptr), (float*)(args[1UL].v_ptr), (float*)(args[2UL].v_ptr));
}

static void mul__6370_closure_206(uint64_t fused_0fused_0fused_0__itr_0____itr_1_1501____itr_2_1502____itr_3_1503, float* __restrict__ __ins_0, float* __restrict__ __ins_1, float* __restrict__ __outs_0) noexcept{
  for (uint64_t _fuseiter_4880 = 0UL; _fuseiter_4880 < 64UL; _fuseiter_4880 += 16UL) {
    vec_f32x16 __cached_0;
    __cached_0 = vec_f32x16::load(&__ins_0[((((fused_0fused_0fused_0__itr_0____itr_1_1501____itr_2_1502____itr_3_1503 / 4UL) * 256UL) + ((fused_0fused_0fused_0__itr_0____itr_1_1501____itr_2_1502____itr_3_1503 % 4UL) * 64UL)) + _fuseiter_4880)]);
    float __cached_1;
    __cached_1 = __ins_1[0];
    vec_f32x16 __cached_2;
    __cached_2 = (__cached_0 * vec_f32x16(__cached_1));
    vec_f32x16::store(__cached_2, &__outs_0[((((fused_0fused_0fused_0__itr_0____itr_1_1501____itr_2_1502____itr_3_1503 / 4UL) * 256UL) + ((fused_0fused_0fused_0__itr_0____itr_1_1501____itr_2_1502____itr_3_1503 % 4UL) * 64UL)) + _fuseiter_4880)]);
  }
}

static void mul__6370_closure_206_0wrapper(void* __stream, int8_t* __restrict__ __module_data, uint64_t i, generic_val* __restrict__ args) noexcept{
  mul__6370_closure_206(i, (float*)(args[0UL].v_ptr), (float*)(args[1UL].v_ptr), (float*)(args[2UL].v_ptr));
}

static void mul__6360_closure_207(uint64_t fused_0fused_0fused_0__itr_0____itr_1_1504____itr_2_1505____itr_3_1506, float* __restrict__ __ins_0, float* __restrict__ __ins_1, float* __restrict__ __outs_0) noexcept{
  for (uint64_t _fuseiter_4886 = 0UL; _fuseiter_4886 < 64UL; _fuseiter_4886 += 16UL) {
    vec_f32x16 __cached_0;
    __cached_0 = vec_f32x16::load(&__ins_0[((((fused_0fused_0fused_0__itr_0____itr_1_1504____itr_2_1505____itr_3_1506 / 4UL) * 256UL) + ((fused_0fused_0fused_0__itr_0____itr_1_1504____itr_2_1505____itr_3_1506 % 4UL) * 64UL)) + _fuseiter_4886)]);
    float __cached_1;
    __cached_1 = __ins_1[0];
    vec_f32x16 __cached_2;
    __cached_2 = (__cached_0 * vec_f32x16(__cached_1));
    vec_f32x16::store(__cached_2, &__outs_0[((((fused_0fused_0fused_0__itr_0____itr_1_1504____itr_2_1505____itr_3_1506 / 4UL) * 256UL) + ((fused_0fused_0fused_0__itr_0____itr_1_1504____itr_2_1505____itr_3_1506 % 4UL) * 64UL)) + _fuseiter_4886)]);
  }
}

static void mul__6360_closure_207_0wrapper(void* __stream, int8_t* __restrict__ __module_data, uint64_t i, generic_val* __restrict__ args) noexcept{
  mul__6360_closure_207(i, (float*)(args[0UL].v_ptr), (float*)(args[1UL].v_ptr), (float*)(args[2UL].v_ptr));
}

static void mul__6350_closure_208(uint64_t fused_0fused_0fused_0__itr_0____itr_1_1507____itr_2_1508____itr_3_1509, float* __restrict__ __ins_0, float* __restrict__ __ins_1, float* __restrict__ __outs_0) noexcept{
  for (uint64_t _fuseiter_4892 = 0UL; _fuseiter_4892 < 64UL; _fuseiter_4892 += 16UL) {
    vec_f32x16 __cached_0;
    __cached_0 = vec_f32x16::load(&__ins_0[((((fused_0fused_0fused_0__itr_0____itr_1_1507____itr_2_1508____itr_3_1509 / 4UL) * 256UL) + ((fused_0fused_0fused_0__itr_0____itr_1_1507____itr_2_1508____itr_3_1509 % 4UL) * 64UL)) + _fuseiter_4892)]);
    float __cached_1;
    __cached_1 = __ins_1[0];
    vec_f32x16 __cached_2;
    __cached_2 = (__cached_0 * vec_f32x16(__cached_1));
    vec_f32x16::store(__cached_2, &__outs_0[((((fused_0fused_0fused_0__itr_0____itr_1_1507____itr_2_1508____itr_3_1509 / 4UL) * 256UL) + ((fused_0fused_0fused_0__itr_0____itr_1_1507____itr_2_1508____itr_3_1509 % 4UL) * 64UL)) + _fuseiter_4892)]);
  }
}

static void mul__6350_closure_208_0wrapper(void* __stream, int8_t* __restrict__ __module_data, uint64_t i, generic_val* __restrict__ args) noexcept{
  mul__6350_closure_208(i, (float*)(args[0UL].v_ptr), (float*)(args[1UL].v_ptr), (float*)(args[2UL].v_ptr));
}

static void mul__6320_closure_209(uint64_t fused_0fused_0fused_0__itr_0____itr_1_1510____itr_2_1511____itr_3_1512, float* __restrict__ __ins_0, float* __restrict__ __ins_1, float* __restrict__ __outs_0) noexcept{
  for (uint64_t _fuseiter_4898 = 0UL; _fuseiter_4898 < 32UL; _fuseiter_4898 += 16UL) {
    vec_f32x16 __cached_0;
    __cached_0 = vec_f32x16::load(&__ins_0[((((fused_0fused_0fused_0__itr_0____itr_1_1510____itr_2_1511____itr_3_1512 / 8UL) * 256UL) + ((fused_0fused_0fused_0__itr_0____itr_1_1510____itr_2_1511____itr_3_1512 % 8UL) * 32UL)) + _fuseiter_4898)]);
    float __cached_1;
    __cached_1 = __ins_1[0];
    vec_f32x16 __cached_2;
    __cached_2 = (__cached_0 * vec_f32x16(__cached_1));
    vec_f32x16::store(__cached_2, &__outs_0[((((fused_0fused_0fused_0__itr_0____itr_1_1510____itr_2_1511____itr_3_1512 / 8UL) * 256UL) + ((fused_0fused_0fused_0__itr_0____itr_1_1510____itr_2_1511____itr_3_1512 % 8UL) * 32UL)) + _fuseiter_4898)]);
  }
}

static void mul__6320_closure_209_0wrapper(void* __stream, int8_t* __restrict__ __module_data, uint64_t i, generic_val* __restrict__ args) noexcept{
  mul__6320_closure_209(i, (float*)(args[0UL].v_ptr), (float*)(args[1UL].v_ptr), (float*)(args[2UL].v_ptr));
}

static void mul__6310_closure_210(uint64_t fused_0fused_0fused_0__itr_0____itr_1_1513____itr_2_1514____itr_3_1515, float* __restrict__ __ins_0, float* __restrict__ __ins_1, float* __restrict__ __outs_0) noexcept{
  for (uint64_t _fuseiter_4904 = 0UL; _fuseiter_4904 < 32UL; _fuseiter_4904 += 16UL) {
    vec_f32x16 __cached_0;
    __cached_0 = vec_f32x16::load(&__ins_0[((((fused_0fused_0fused_0__itr_0____itr_1_1513____itr_2_1514____itr_3_1515 / 8UL) * 256UL) + ((fused_0fused_0fused_0__itr_0____itr_1_1513____itr_2_1514____itr_3_1515 % 8UL) * 32UL)) + _fuseiter_4904)]);
    float __cached_1;
    __cached_1 = __ins_1[0];
    vec_f32x16 __cached_2;
    __cached_2 = (__cached_0 * vec_f32x16(__cached_1));
    vec_f32x16::store(__cached_2, &__outs_0[((((fused_0fused_0fused_0__itr_0____itr_1_1513____itr_2_1514____itr_3_1515 / 8UL) * 256UL) + ((fused_0fused_0fused_0__itr_0____itr_1_1513____itr_2_1514____itr_3_1515 % 8UL) * 32UL)) + _fuseiter_4904)]);
  }
}

static void mul__6310_closure_210_0wrapper(void* __stream, int8_t* __restrict__ __module_data, uint64_t i, generic_val* __restrict__ args) noexcept{
  mul__6310_closure_210(i, (float*)(args[0UL].v_ptr), (float*)(args[1UL].v_ptr), (float*)(args[2UL].v_ptr));
}

static void mul__6300_closure_211(uint64_t fused_0fused_0fused_0__itr_0____itr_1_1516____itr_2_1517____itr_3_1518, float* __restrict__ __ins_0, float* __restrict__ __ins_1, float* __restrict__ __outs_0) noexcept{
  for (uint64_t _fuseiter_4910 = 0UL; _fuseiter_4910 < 32UL; _fuseiter_4910 += 16UL) {
    vec_f32x16 __cached_0;
    __cached_0 = vec_f32x16::load(&__ins_0[((((fused_0fused_0fused_0__itr_0____itr_1_1516____itr_2_1517____itr_3_1518 / 8UL) * 256UL) + ((fused_0fused_0fused_0__itr_0____itr_1_1516____itr_2_1517____itr_3_1518 % 8UL) * 32UL)) + _fuseiter_4910)]);
    float __cached_1;
    __cached_1 = __ins_1[0];
    vec_f32x16 __cached_2;
    __cached_2 = (__cached_0 * vec_f32x16(__cached_1));
    vec_f32x16::store(__cached_2, &__outs_0[((((fused_0fused_0fused_0__itr_0____itr_1_1516____itr_2_1517____itr_3_1518 / 8UL) * 256UL) + ((fused_0fused_0fused_0__itr_0____itr_1_1516____itr_2_1517____itr_3_1518 % 8UL) * 32UL)) + _fuseiter_4910)]);
  }
}

static void mul__6300_closure_211_0wrapper(void* __stream, int8_t* __restrict__ __module_data, uint64_t i, generic_val* __restrict__ args) noexcept{
  mul__6300_closure_211(i, (float*)(args[0UL].v_ptr), (float*)(args[1UL].v_ptr), (float*)(args[2UL].v_ptr));
}

static void mul__6290_closure_212(uint64_t fused_0fused_0fused_0__itr_0____itr_1_1519____itr_2_1520____itr_3_1521, float* __restrict__ __ins_0, float* __restrict__ __ins_1, float* __restrict__ __outs_0) noexcept{
  for (uint64_t _fuseiter_4916 = 0UL; _fuseiter_4916 < 32UL; _fuseiter_4916 += 16UL) {
    vec_f32x16 __cached_0;
    __cached_0 = vec_f32x16::load(&__ins_0[((((fused_0fused_0fused_0__itr_0____itr_1_1519____itr_2_1520____itr_3_1521 / 8UL) * 256UL) + ((fused_0fused_0fused_0__itr_0____itr_1_1519____itr_2_1520____itr_3_1521 % 8UL) * 32UL)) + _fuseiter_4916)]);
    float __cached_1;
    __cached_1 = __ins_1[0];
    vec_f32x16 __cached_2;
    __cached_2 = (__cached_0 * vec_f32x16(__cached_1));
    vec_f32x16::store(__cached_2, &__outs_0[((((fused_0fused_0fused_0__itr_0____itr_1_1519____itr_2_1520____itr_3_1521 / 8UL) * 256UL) + ((fused_0fused_0fused_0__itr_0____itr_1_1519____itr_2_1520____itr_3_1521 % 8UL) * 32UL)) + _fuseiter_4916)]);
  }
}

static void mul__6290_closure_212_0wrapper(void* __stream, int8_t* __restrict__ __module_data, uint64_t i, generic_val* __restrict__ args) noexcept{
  mul__6290_closure_212(i, (float*)(args[0UL].v_ptr), (float*)(args[1UL].v_ptr), (float*)(args[2UL].v_ptr));
}

static void mul__6260_closure_213(uint64_t fused_0fused_0fused_0__itr_0____itr_1_1522____itr_2_1523____itr_3_1524, float* __restrict__ __ins_0, float* __restrict__ __ins_1, float* __restrict__ __outs_0) noexcept{
  vec_f32x16 __cached_0;
  __cached_0 = vec_f32x16::load(&__ins_0[(((fused_0fused_0fused_0__itr_0____itr_1_1522____itr_2_1523____itr_3_1524 / 16UL) * 256UL) + ((fused_0fused_0fused_0__itr_0____itr_1_1522____itr_2_1523____itr_3_1524 % 16UL) * 16UL))]);
  float __cached_1;
  __cached_1 = __ins_1[0];
  vec_f32x16 __cached_2;
  __cached_2 = (__cached_0 * vec_f32x16(__cached_1));
  vec_f32x16::store(__cached_2, &__outs_0[(((fused_0fused_0fused_0__itr_0____itr_1_1522____itr_2_1523____itr_3_1524 / 16UL) * 256UL) + ((fused_0fused_0fused_0__itr_0____itr_1_1522____itr_2_1523____itr_3_1524 % 16UL) * 16UL))]);
}

static void mul__6260_closure_213_0wrapper(void* __stream, int8_t* __restrict__ __module_data, uint64_t i, generic_val* __restrict__ args) noexcept{
  mul__6260_closure_213(i, (float*)(args[0UL].v_ptr), (float*)(args[1UL].v_ptr), (float*)(args[2UL].v_ptr));
}

static void mul__6250_closure_214(uint64_t fused_0fused_0fused_0__itr_0____itr_1_1525____itr_2_1526____itr_3_1527, float* __restrict__ __ins_0, float* __restrict__ __ins_1, float* __restrict__ __outs_0) noexcept{
  vec_f32x16 __cached_0;
  __cached_0 = vec_f32x16::load(&__ins_0[(((fused_0fused_0fused_0__itr_0____itr_1_1525____itr_2_1526____itr_3_1527 / 16UL) * 256UL) + ((fused_0fused_0fused_0__itr_0____itr_1_1525____itr_2_1526____itr_3_1527 % 16UL) * 16UL))]);
  float __cached_1;
  __cached_1 = __ins_1[0];
  vec_f32x16 __cached_2;
  __cached_2 = (__cached_0 * vec_f32x16(__cached_1));
  vec_f32x16::store(__cached_2, &__outs_0[(((fused_0fused_0fused_0__itr_0____itr_1_1525____itr_2_1526____itr_3_1527 / 16UL) * 256UL) + ((fused_0fused_0fused_0__itr_0____itr_1_1525____itr_2_1526____itr_3_1527 % 16UL) * 16UL))]);
}

static void mul__6250_closure_214_0wrapper(void* __stream, int8_t* __restrict__ __module_data, uint64_t i, generic_val* __restrict__ args) noexcept{
  mul__6250_closure_214(i, (float*)(args[0UL].v_ptr), (float*)(args[1UL].v_ptr), (float*)(args[2UL].v_ptr));
}

static void mul__6240_closure_215(uint64_t fused_0fused_0fused_0__itr_0____itr_1_1528____itr_2_1529____itr_3_1530, float* __restrict__ __ins_0, float* __restrict__ __ins_1, float* __restrict__ __outs_0) noexcept{
  for (uint64_t _fuseiter_4934 = 0UL; _fuseiter_4934 < 64UL; _fuseiter_4934 += 16UL) {
    vec_f32x16 __cached_0;
    __cached_0 = vec_f32x16::load(&__ins_0[((((fused_0fused_0fused_0__itr_0____itr_1_1528____itr_2_1529____itr_3_1530 / 4UL) * 256UL) + ((fused_0fused_0fused_0__itr_0____itr_1_1528____itr_2_1529____itr_3_1530 % 4UL) * 64UL)) + _fuseiter_4934)]);
    float __cached_1;
    __cached_1 = __ins_1[0];
    vec_f32x16 __cached_2;
    __cached_2 = (__cached_0 * vec_f32x16(__cached_1));
    vec_f32x16::store(__cached_2, &__outs_0[((((fused_0fused_0fused_0__itr_0____itr_1_1528____itr_2_1529____itr_3_1530 / 4UL) * 256UL) + ((fused_0fused_0fused_0__itr_0____itr_1_1528____itr_2_1529____itr_3_1530 % 4UL) * 64UL)) + _fuseiter_4934)]);
  }
}

static void mul__6240_closure_215_0wrapper(void* __stream, int8_t* __restrict__ __module_data, uint64_t i, generic_val* __restrict__ args) noexcept{
  mul__6240_closure_215(i, (float*)(args[0UL].v_ptr), (float*)(args[1UL].v_ptr), (float*)(args[2UL].v_ptr));
}

static void mul__6230_closure_216(uint64_t fused_0fused_0fused_0__itr_0____itr_1_1531____itr_2_1532____itr_3_1533, float* __restrict__ __ins_0, float* __restrict__ __ins_1, float* __restrict__ __outs_0) noexcept{
  for (uint64_t _fuseiter_4940 = 0UL; _fuseiter_4940 < 64UL; _fuseiter_4940 += 16UL) {
    vec_f32x16 __cached_0;
    __cached_0 = vec_f32x16::load(&__ins_0[((((fused_0fused_0fused_0__itr_0____itr_1_1531____itr_2_1532____itr_3_1533 / 4UL) * 256UL) + ((fused_0fused_0fused_0__itr_0____itr_1_1531____itr_2_1532____itr_3_1533 % 4UL) * 64UL)) + _fuseiter_4940)]);
    float __cached_1;
    __cached_1 = __ins_1[0];
    vec_f32x16 __cached_2;
    __cached_2 = (__cached_0 * vec_f32x16(__cached_1));
    vec_f32x16::store(__cached_2, &__outs_0[((((fused_0fused_0fused_0__itr_0____itr_1_1531____itr_2_1532____itr_3_1533 / 4UL) * 256UL) + ((fused_0fused_0fused_0__itr_0____itr_1_1531____itr_2_1532____itr_3_1533 % 4UL) * 64UL)) + _fuseiter_4940)]);
  }
}

static void mul__6230_closure_216_0wrapper(void* __stream, int8_t* __restrict__ __module_data, uint64_t i, generic_val* __restrict__ args) noexcept{
  mul__6230_closure_216(i, (float*)(args[0UL].v_ptr), (float*)(args[1UL].v_ptr), (float*)(args[2UL].v_ptr));
}

static void mul__6200_closure_217(uint64_t fused_0fused_0fused_0__itr_0____itr_1_1534____itr_2_1535____itr_3_1536, float* __restrict__ __ins_0, float* __restrict__ __ins_1, float* __restrict__ __outs_0) noexcept{
  for (uint64_t _fuseiter_4946 = 0UL; _fuseiter_4946 < 128UL; _fuseiter_4946 += 16UL) {
    vec_f32x16 __cached_0;
    __cached_0 = vec_f32x16::load(&__ins_0[((((fused_0fused_0fused_0__itr_0____itr_1_1534____itr_2_1535____itr_3_1536 / 2UL) * 256UL) + ((fused_0fused_0fused_0__itr_0____itr_1_1534____itr_2_1535____itr_3_1536 % 2UL) * 128UL)) + _fuseiter_4946)]);
    float __cached_1;
    __cached_1 = __ins_1[0];
    vec_f32x16 __cached_2;
    __cached_2 = (__cached_0 * vec_f32x16(__cached_1));
    vec_f32x16::store(__cached_2, &__outs_0[((((fused_0fused_0fused_0__itr_0____itr_1_1534____itr_2_1535____itr_3_1536 / 2UL) * 256UL) + ((fused_0fused_0fused_0__itr_0____itr_1_1534____itr_2_1535____itr_3_1536 % 2UL) * 128UL)) + _fuseiter_4946)]);
  }
}

static void mul__6200_closure_217_0wrapper(void* __stream, int8_t* __restrict__ __module_data, uint64_t i, generic_val* __restrict__ args) noexcept{
  mul__6200_closure_217(i, (float*)(args[0UL].v_ptr), (float*)(args[1UL].v_ptr), (float*)(args[2UL].v_ptr));
}

static void mul__6190_closure_218(uint64_t fused_0fused_0fused_0__itr_0____itr_1_1537____itr_2_1538____itr_3_1539, float* __restrict__ __ins_0, float* __restrict__ __ins_1, float* __restrict__ __outs_0) noexcept{
  for (uint64_t _fuseiter_4952 = 0UL; _fuseiter_4952 < 128UL; _fuseiter_4952 += 16UL) {
    vec_f32x16 __cached_0;
    __cached_0 = vec_f32x16::load(&__ins_0[((((fused_0fused_0fused_0__itr_0____itr_1_1537____itr_2_1538____itr_3_1539 / 2UL) * 256UL) + ((fused_0fused_0fused_0__itr_0____itr_1_1537____itr_2_1538____itr_3_1539 % 2UL) * 128UL)) + _fuseiter_4952)]);
    float __cached_1;
    __cached_1 = __ins_1[0];
    vec_f32x16 __cached_2;
    __cached_2 = (__cached_0 * vec_f32x16(__cached_1));
    vec_f32x16::store(__cached_2, &__outs_0[((((fused_0fused_0fused_0__itr_0____itr_1_1537____itr_2_1538____itr_3_1539 / 2UL) * 256UL) + ((fused_0fused_0fused_0__itr_0____itr_1_1537____itr_2_1538____itr_3_1539 % 2UL) * 128UL)) + _fuseiter_4952)]);
  }
}

static void mul__6190_closure_218_0wrapper(void* __stream, int8_t* __restrict__ __module_data, uint64_t i, generic_val* __restrict__ args) noexcept{
  mul__6190_closure_218(i, (float*)(args[0UL].v_ptr), (float*)(args[1UL].v_ptr), (float*)(args[2UL].v_ptr));
}

static void mul__5880_closure_219(uint64_t fused_0fused_0fused_0__itr_0____itr_1_1546____itr_2_1547____itr_3_1548, float* __restrict__ __ins_0, float* __restrict__ __ins_1, float* __restrict__ __outs_0) noexcept{
  for (uint64_t _fuseiter_4970 = 0UL; _fuseiter_4970 < 64UL; _fuseiter_4970 += 16UL) {
    vec_f32x16 __cached_0;
    __cached_0 = vec_f32x16::load(&__ins_0[((((fused_0fused_0fused_0__itr_0____itr_1_1546____itr_2_1547____itr_3_1548 / 4UL) * 256UL) + ((fused_0fused_0fused_0__itr_0____itr_1_1546____itr_2_1547____itr_3_1548 % 4UL) * 64UL)) + _fuseiter_4970)]);
    float __cached_1;
    __cached_1 = __ins_1[0];
    vec_f32x16 __cached_2;
    __cached_2 = (__cached_0 * vec_f32x16(__cached_1));
    vec_f32x16::store(__cached_2, &__outs_0[((((fused_0fused_0fused_0__itr_0____itr_1_1546____itr_2_1547____itr_3_1548 / 4UL) * 256UL) + ((fused_0fused_0fused_0__itr_0____itr_1_1546____itr_2_1547____itr_3_1548 % 4UL) * 64UL)) + _fuseiter_4970)]);
  }
}

static void mul__5880_closure_219_0wrapper(void* __stream, int8_t* __restrict__ __module_data, uint64_t i, generic_val* __restrict__ args) noexcept{
  mul__5880_closure_219(i, (float*)(args[0UL].v_ptr), (float*)(args[1UL].v_ptr), (float*)(args[2UL].v_ptr));
}

static void mul__5870_closure_220(uint64_t fused_0fused_0fused_0__itr_0____itr_1_1549____itr_2_1550____itr_3_1551, float* __restrict__ __ins_0, float* __restrict__ __ins_1, float* __restrict__ __outs_0) noexcept{
  for (uint64_t _fuseiter_4976 = 0UL; _fuseiter_4976 < 64UL; _fuseiter_4976 += 16UL) {
    vec_f32x16 __cached_0;
    __cached_0 = vec_f32x16::load(&__ins_0[((((fused_0fused_0fused_0__itr_0____itr_1_1549____itr_2_1550____itr_3_1551 / 4UL) * 256UL) + ((fused_0fused_0fused_0__itr_0____itr_1_1549____itr_2_1550____itr_3_1551 % 4UL) * 64UL)) + _fuseiter_4976)]);
    float __cached_1;
    __cached_1 = __ins_1[0];
    vec_f32x16 __cached_2;
    __cached_2 = (__cached_0 * vec_f32x16(__cached_1));
    vec_f32x16::store(__cached_2, &__outs_0[((((fused_0fused_0fused_0__itr_0____itr_1_1549____itr_2_1550____itr_3_1551 / 4UL) * 256UL) + ((fused_0fused_0fused_0__itr_0____itr_1_1549____itr_2_1550____itr_3_1551 % 4UL) * 64UL)) + _fuseiter_4976)]);
  }
}

static void mul__5870_closure_220_0wrapper(void* __stream, int8_t* __restrict__ __module_data, uint64_t i, generic_val* __restrict__ args) noexcept{
  mul__5870_closure_220(i, (float*)(args[0UL].v_ptr), (float*)(args[1UL].v_ptr), (float*)(args[2UL].v_ptr));
}

static void mul__5820_closure_221(uint64_t fused_0fused_0fused_0__itr_0____itr_1_1552____itr_2_1553____itr_3_1554, float* __restrict__ __ins_0, float* __restrict__ __ins_1, float* __restrict__ __outs_0) noexcept{
  for (uint64_t _fuseiter_4982 = 0UL; _fuseiter_4982 < 32UL; _fuseiter_4982 += 16UL) {
    vec_f32x16 __cached_0;
    __cached_0 = vec_f32x16::load(&__ins_0[((((fused_0fused_0fused_0__itr_0____itr_1_1552____itr_2_1553____itr_3_1554 / 8UL) * 256UL) + ((fused_0fused_0fused_0__itr_0____itr_1_1552____itr_2_1553____itr_3_1554 % 8UL) * 32UL)) + _fuseiter_4982)]);
    float __cached_1;
    __cached_1 = __ins_1[0];
    vec_f32x16 __cached_2;
    __cached_2 = (__cached_0 * vec_f32x16(__cached_1));
    vec_f32x16::store(__cached_2, &__outs_0[((((fused_0fused_0fused_0__itr_0____itr_1_1552____itr_2_1553____itr_3_1554 / 8UL) * 256UL) + ((fused_0fused_0fused_0__itr_0____itr_1_1552____itr_2_1553____itr_3_1554 % 8UL) * 32UL)) + _fuseiter_4982)]);
  }
}

static void mul__5820_closure_221_0wrapper(void* __stream, int8_t* __restrict__ __module_data, uint64_t i, generic_val* __restrict__ args) noexcept{
  mul__5820_closure_221(i, (float*)(args[0UL].v_ptr), (float*)(args[1UL].v_ptr), (float*)(args[2UL].v_ptr));
}

static void mul__5810_closure_222(uint64_t fused_0fused_0fused_0__itr_0____itr_1_1555____itr_2_1556____itr_3_1557, float* __restrict__ __ins_0, float* __restrict__ __ins_1, float* __restrict__ __outs_0) noexcept{
  for (uint64_t _fuseiter_4988 = 0UL; _fuseiter_4988 < 32UL; _fuseiter_4988 += 16UL) {
    vec_f32x16 __cached_0;
    __cached_0 = vec_f32x16::load(&__ins_0[((((fused_0fused_0fused_0__itr_0____itr_1_1555____itr_2_1556____itr_3_1557 / 8UL) * 256UL) + ((fused_0fused_0fused_0__itr_0____itr_1_1555____itr_2_1556____itr_3_1557 % 8UL) * 32UL)) + _fuseiter_4988)]);
    float __cached_1;
    __cached_1 = __ins_1[0];
    vec_f32x16 __cached_2;
    __cached_2 = (__cached_0 * vec_f32x16(__cached_1));
    vec_f32x16::store(__cached_2, &__outs_0[((((fused_0fused_0fused_0__itr_0____itr_1_1555____itr_2_1556____itr_3_1557 / 8UL) * 256UL) + ((fused_0fused_0fused_0__itr_0____itr_1_1555____itr_2_1556____itr_3_1557 % 8UL) * 32UL)) + _fuseiter_4988)]);
  }
}

static void mul__5810_closure_222_0wrapper(void* __stream, int8_t* __restrict__ __module_data, uint64_t i, generic_val* __restrict__ args) noexcept{
  mul__5810_closure_222(i, (float*)(args[0UL].v_ptr), (float*)(args[1UL].v_ptr), (float*)(args[2UL].v_ptr));
}

static void mul__5760_closure_223(uint64_t fused_0fused_0fused_0__itr_0____itr_1_1558____itr_2_1559____itr_3_1560, float* __restrict__ __ins_0, float* __restrict__ __ins_1, float* __restrict__ __outs_0) noexcept{
  for (uint64_t _fuseiter_4994 = 0UL; _fuseiter_4994 < 64UL; _fuseiter_4994 += 16UL) {
    vec_f32x16 __cached_0;
    __cached_0 = vec_f32x16::load(&__ins_0[((((fused_0fused_0fused_0__itr_0____itr_1_1558____itr_2_1559____itr_3_1560 / 4UL) * 256UL) + ((fused_0fused_0fused_0__itr_0____itr_1_1558____itr_2_1559____itr_3_1560 % 4UL) * 64UL)) + _fuseiter_4994)]);
    float __cached_1;
    __cached_1 = __ins_1[0];
    vec_f32x16 __cached_2;
    __cached_2 = (__cached_0 * vec_f32x16(__cached_1));
    vec_f32x16::store(__cached_2, &__outs_0[((((fused_0fused_0fused_0__itr_0____itr_1_1558____itr_2_1559____itr_3_1560 / 4UL) * 256UL) + ((fused_0fused_0fused_0__itr_0____itr_1_1558____itr_2_1559____itr_3_1560 % 4UL) * 64UL)) + _fuseiter_4994)]);
  }
}

static void mul__5760_closure_223_0wrapper(void* __stream, int8_t* __restrict__ __module_data, uint64_t i, generic_val* __restrict__ args) noexcept{
  mul__5760_closure_223(i, (float*)(args[0UL].v_ptr), (float*)(args[1UL].v_ptr), (float*)(args[2UL].v_ptr));
}

static void mul__5750_closure_224(uint64_t fused_0fused_0fused_0__itr_0____itr_1_1561____itr_2_1562____itr_3_1563, float* __restrict__ __ins_0, float* __restrict__ __ins_1, float* __restrict__ __outs_0) noexcept{
  for (uint64_t _fuseiter_5000 = 0UL; _fuseiter_5000 < 64UL; _fuseiter_5000 += 16UL) {
    vec_f32x16 __cached_0;
    __cached_0 = vec_f32x16::load(&__ins_0[((((fused_0fused_0fused_0__itr_0____itr_1_1561____itr_2_1562____itr_3_1563 / 4UL) * 256UL) + ((fused_0fused_0fused_0__itr_0____itr_1_1561____itr_2_1562____itr_3_1563 % 4UL) * 64UL)) + _fuseiter_5000)]);
    float __cached_1;
    __cached_1 = __ins_1[0];
    vec_f32x16 __cached_2;
    __cached_2 = (__cached_0 * vec_f32x16(__cached_1));
    vec_f32x16::store(__cached_2, &__outs_0[((((fused_0fused_0fused_0__itr_0____itr_1_1561____itr_2_1562____itr_3_1563 / 4UL) * 256UL) + ((fused_0fused_0fused_0__itr_0____itr_1_1561____itr_2_1562____itr_3_1563 % 4UL) * 64UL)) + _fuseiter_5000)]);
  }
}

static void mul__5750_closure_224_0wrapper(void* __stream, int8_t* __restrict__ __module_data, uint64_t i, generic_val* __restrict__ args) noexcept{
  mul__5750_closure_224(i, (float*)(args[0UL].v_ptr), (float*)(args[1UL].v_ptr), (float*)(args[2UL].v_ptr));
}

static void mul__5700_closure_225(uint64_t fused_0fused_0fused_0__itr_0____itr_1_1564____itr_2_1565____itr_3_1566, float* __restrict__ __ins_0, float* __restrict__ __ins_1, float* __restrict__ __outs_0) noexcept{
  vec_f32x16 __cached_0;
  __cached_0 = vec_f32x16::load(&__ins_0[(((fused_0fused_0fused_0__itr_0____itr_1_1564____itr_2_1565____itr_3_1566 / 16UL) * 256UL) + ((fused_0fused_0fused_0__itr_0____itr_1_1564____itr_2_1565____itr_3_1566 % 16UL) * 16UL))]);
  float __cached_1;
  __cached_1 = __ins_1[0];
  vec_f32x16 __cached_2;
  __cached_2 = (__cached_0 * vec_f32x16(__cached_1));
  vec_f32x16::store(__cached_2, &__outs_0[(((fused_0fused_0fused_0__itr_0____itr_1_1564____itr_2_1565____itr_3_1566 / 16UL) * 256UL) + ((fused_0fused_0fused_0__itr_0____itr_1_1564____itr_2_1565____itr_3_1566 % 16UL) * 16UL))]);
}

static void mul__5700_closure_225_0wrapper(void* __stream, int8_t* __restrict__ __module_data, uint64_t i, generic_val* __restrict__ args) noexcept{
  mul__5700_closure_225(i, (float*)(args[0UL].v_ptr), (float*)(args[1UL].v_ptr), (float*)(args[2UL].v_ptr));
}

static void mul__5690_closure_226(uint64_t fused_0fused_0fused_0__itr_0____itr_1_1567____itr_2_1568____itr_3_1569, float* __restrict__ __ins_0, float* __restrict__ __ins_1, float* __restrict__ __outs_0) noexcept{
  vec_f32x16 __cached_0;
  __cached_0 = vec_f32x16::load(&__ins_0[(((fused_0fused_0fused_0__itr_0____itr_1_1567____itr_2_1568____itr_3_1569 / 16UL) * 256UL) + ((fused_0fused_0fused_0__itr_0____itr_1_1567____itr_2_1568____itr_3_1569 % 16UL) * 16UL))]);
  float __cached_1;
  __cached_1 = __ins_1[0];
  vec_f32x16 __cached_2;
  __cached_2 = (__cached_0 * vec_f32x16(__cached_1));
  vec_f32x16::store(__cached_2, &__outs_0[(((fused_0fused_0fused_0__itr_0____itr_1_1567____itr_2_1568____itr_3_1569 / 16UL) * 256UL) + ((fused_0fused_0fused_0__itr_0____itr_1_1567____itr_2_1568____itr_3_1569 % 16UL) * 16UL))]);
}

static void mul__5690_closure_226_0wrapper(void* __stream, int8_t* __restrict__ __module_data, uint64_t i, generic_val* __restrict__ args) noexcept{
  mul__5690_closure_226(i, (float*)(args[0UL].v_ptr), (float*)(args[1UL].v_ptr), (float*)(args[2UL].v_ptr));
}

static void reorder__5220_closure_227(uint64_t fused_0fused_0fused_0fused_0_fuseiter_5014___fuseiter_5015_1570___fuseiter_5016_1571___fuseiter_5017_1572___fuseiter_5018_1573, int8_t* __restrict__ __ins_0, int8_t* __restrict__ __outs_0) noexcept{
  for (uint64_t _fuseiter_5019 = 0UL; _fuseiter_5019 < 32UL; _fuseiter_5019 += 1UL) {
    for (uint64_t _fuseiter_5020 = 0UL; _fuseiter_5020 < 4UL; _fuseiter_5020 += 1UL) {
      int8_t __cached_0;
      __cached_0 = __ins_0[(((_fuseiter_5019 + ((fused_0fused_0fused_0fused_0_fuseiter_5014___fuseiter_5015_1570___fuseiter_5016_1571___fuseiter_5017_1572___fuseiter_5018_1573 / 256UL) * 32UL)) * 1024UL) + ((_fuseiter_5020 + ((fused_0fused_0fused_0fused_0_fuseiter_5014___fuseiter_5015_1570___fuseiter_5016_1571___fuseiter_5017_1572___fuseiter_5018_1573 % 128UL) * 4UL)) + (((fused_0fused_0fused_0fused_0_fuseiter_5014___fuseiter_5015_1570___fuseiter_5016_1571___fuseiter_5017_1572___fuseiter_5018_1573 / 128UL) % 2UL) * 512UL)))];
      int8_t __cached_1;
      __cached_1 = __cached_0;
      __outs_0[(((fused_0fused_0fused_0fused_0_fuseiter_5014___fuseiter_5015_1570___fuseiter_5016_1571___fuseiter_5017_1572___fuseiter_5018_1573 / 256UL) * 32768UL) + ((((fused_0fused_0fused_0fused_0_fuseiter_5014___fuseiter_5015_1570___fuseiter_5016_1571___fuseiter_5017_1572___fuseiter_5018_1573 / 128UL) % 2UL) * 16384UL) + (((fused_0fused_0fused_0fused_0_fuseiter_5014___fuseiter_5015_1570___fuseiter_5016_1571___fuseiter_5017_1572___fuseiter_5018_1573 % 128UL) * 128UL) + ((_fuseiter_5019 * 4UL) + _fuseiter_5020))))] = __cached_1;
    }
  }
}

static void reorder__5220_closure_227_0wrapper(void* __stream, int8_t* __restrict__ __module_data, uint64_t i, generic_val* __restrict__ args) noexcept{
  reorder__5220_closure_227(i, (int8_t*)(args[0UL].v_ptr), (int8_t*)(args[1UL].v_ptr));
}

static void reorder__5150_closure_228(uint64_t fused_0fused_0fused_0fused_0_fuseiter_5021___fuseiter_5022_1574___fuseiter_5023_1575___fuseiter_5024_1576___fuseiter_5025_1577, int8_t* __restrict__ __ins_0, int8_t* __restrict__ __outs_0) noexcept{
  for (uint64_t _fuseiter_5026 = 0UL; _fuseiter_5026 < 256UL; _fuseiter_5026 += 1UL) {
    for (uint64_t _fuseiter_5027 = 0UL; _fuseiter_5027 < 4UL; _fuseiter_5027 += 1UL) {
      int8_t __cached_0;
      __cached_0 = __ins_0[(((_fuseiter_5026 + ((fused_0fused_0fused_0fused_0_fuseiter_5021___fuseiter_5022_1574___fuseiter_5023_1575___fuseiter_5024_1576___fuseiter_5025_1577 / 256UL) * 256UL)) * 1024UL) + ((_fuseiter_5027 + ((fused_0fused_0fused_0fused_0_fuseiter_5021___fuseiter_5022_1574___fuseiter_5023_1575___fuseiter_5024_1576___fuseiter_5025_1577 % 32UL) * 4UL)) + (((fused_0fused_0fused_0fused_0_fuseiter_5021___fuseiter_5022_1574___fuseiter_5023_1575___fuseiter_5024_1576___fuseiter_5025_1577 / 32UL) % 8UL) * 128UL)))];
      int8_t __cached_1;
      __cached_1 = __cached_0;
      __outs_0[(((fused_0fused_0fused_0fused_0_fuseiter_5021___fuseiter_5022_1574___fuseiter_5023_1575___fuseiter_5024_1576___fuseiter_5025_1577 / 256UL) * 262144UL) + ((((fused_0fused_0fused_0fused_0_fuseiter_5021___fuseiter_5022_1574___fuseiter_5023_1575___fuseiter_5024_1576___fuseiter_5025_1577 / 32UL) % 8UL) * 32768UL) + (((fused_0fused_0fused_0fused_0_fuseiter_5021___fuseiter_5022_1574___fuseiter_5023_1575___fuseiter_5024_1576___fuseiter_5025_1577 % 32UL) * 1024UL) + ((_fuseiter_5026 * 4UL) + _fuseiter_5027))))] = __cached_1;
    }
  }
}

static void reorder__5150_closure_228_0wrapper(void* __stream, int8_t* __restrict__ __module_data, uint64_t i, generic_val* __restrict__ args) noexcept{
  reorder__5150_closure_228(i, (int8_t*)(args[0UL].v_ptr), (int8_t*)(args[1UL].v_ptr));
}

static void reorder__5060_closure_229(uint64_t fused_0_fuseiter_5028___fuseiter_5029_1578, int8_t* __restrict__ __ins_0, int8_t* __restrict__ __outs_0) noexcept{
  for (uint64_t _fuseiter_5032 = 0UL; _fuseiter_5032 < 32UL; _fuseiter_5032 += 1UL) {
    for (uint64_t _fuseiter_5033 = 0UL; _fuseiter_5033 < 64UL; _fuseiter_5033 += 1UL) {
      for (uint64_t _fuseiter_5034 = 0UL; _fuseiter_5034 < 4UL; _fuseiter_5034 += 1UL) {
        int8_t __cached_0;
        __cached_0 = __ins_0[(((_fuseiter_5033 + ((fused_0_fuseiter_5028___fuseiter_5029_1578 / 8UL) * 64UL)) * 1024UL) + ((_fuseiter_5034 + (_fuseiter_5032 * 4UL)) + ((fused_0_fuseiter_5028___fuseiter_5029_1578 % 8UL) * 128UL)))];
        int8_t __cached_1;
        __cached_1 = __cached_0;
        __outs_0[(((fused_0_fuseiter_5028___fuseiter_5029_1578 / 8UL) * 65536UL) + (((fused_0_fuseiter_5028___fuseiter_5029_1578 % 8UL) * 8192UL) + ((_fuseiter_5032 * 256UL) + ((_fuseiter_5033 * 4UL) + _fuseiter_5034))))] = __cached_1;
      }
    }
  }
}

static void reorder__5060_closure_229_0wrapper(void* __stream, int8_t* __restrict__ __module_data, uint64_t i, generic_val* __restrict__ args) noexcept{
  reorder__5060_closure_229(i, (int8_t*)(args[0UL].v_ptr), (int8_t*)(args[1UL].v_ptr));
}

static void reorder__4970_closure_230(uint64_t fused_0_fuseiter_5035___fuseiter_5036_1579, int8_t* __restrict__ __ins_0, int8_t* __restrict__ __outs_0) noexcept{
  for (uint64_t _fuseiter_5039 = 0UL; _fuseiter_5039 < 32UL; _fuseiter_5039 += 1UL) {
    for (uint64_t _fuseiter_5040 = 0UL; _fuseiter_5040 < 32UL; _fuseiter_5040 += 1UL) {
      for (uint64_t _fuseiter_5041 = 0UL; _fuseiter_5041 < 4UL; _fuseiter_5041 += 1UL) {
        int8_t __cached_0;
        __cached_0 = __ins_0[(((_fuseiter_5040 + ((fused_0_fuseiter_5035___fuseiter_5036_1579 / 8UL) * 32UL)) * 1024UL) + ((_fuseiter_5041 + (_fuseiter_5039 * 4UL)) + ((fused_0_fuseiter_5035___fuseiter_5036_1579 % 8UL) * 128UL)))];
        int8_t __cached_1;
        __cached_1 = __cached_0;
        __outs_0[(((fused_0_fuseiter_5035___fuseiter_5036_1579 / 8UL) * 32768UL) + (((fused_0_fuseiter_5035___fuseiter_5036_1579 % 8UL) * 4096UL) + ((_fuseiter_5039 * 128UL) + ((_fuseiter_5040 * 4UL) + _fuseiter_5041))))] = __cached_1;
      }
    }
  }
}

static void reorder__4970_closure_230_0wrapper(void* __stream, int8_t* __restrict__ __module_data, uint64_t i, generic_val* __restrict__ args) noexcept{
  reorder__4970_closure_230(i, (int8_t*)(args[0UL].v_ptr), (int8_t*)(args[1UL].v_ptr));
}

static void reorder__4900_closure_231(uint64_t fused_0fused_0fused_0fused_0_fuseiter_5042___fuseiter_5043_1580___fuseiter_5044_1581___fuseiter_5045_1582___fuseiter_5046_1583, int8_t* __restrict__ __ins_0, int8_t* __restrict__ __outs_0) noexcept{
  for (uint64_t _fuseiter_5047 = 0UL; _fuseiter_5047 < 64UL; _fuseiter_5047 += 1UL) {
    for (uint64_t _fuseiter_5048 = 0UL; _fuseiter_5048 < 4UL; _fuseiter_5048 += 1UL) {
      int8_t __cached_0;
      __cached_0 = __ins_0[(((_fuseiter_5047 + ((fused_0fused_0fused_0fused_0_fuseiter_5042___fuseiter_5043_1580___fuseiter_5044_1581___fuseiter_5045_1582___fuseiter_5046_1583 / 256UL) * 64UL)) * 1024UL) + (_fuseiter_5048 + ((fused_0fused_0fused_0fused_0_fuseiter_5042___fuseiter_5043_1580___fuseiter_5044_1581___fuseiter_5045_1582___fuseiter_5046_1583 % 256UL) * 4UL)))];
      int8_t __cached_1;
      __cached_1 = __cached_0;
      __outs_0[(((fused_0fused_0fused_0fused_0_fuseiter_5042___fuseiter_5043_1580___fuseiter_5044_1581___fuseiter_5045_1582___fuseiter_5046_1583 / 256UL) * 65536UL) + (((fused_0fused_0fused_0fused_0_fuseiter_5042___fuseiter_5043_1580___fuseiter_5044_1581___fuseiter_5045_1582___fuseiter_5046_1583 % 256UL) * 256UL) + ((_fuseiter_5047 * 4UL) + _fuseiter_5048)))] = __cached_1;
    }
  }
}

static void reorder__4900_closure_231_0wrapper(void* __stream, int8_t* __restrict__ __module_data, uint64_t i, generic_val* __restrict__ args) noexcept{
  reorder__4900_closure_231(i, (int8_t*)(args[0UL].v_ptr), (int8_t*)(args[1UL].v_ptr));
}

static void reorder__5280_closure_232(uint64_t fused_0fused_0fused_0fused_0_fuseiter_5049___fuseiter_5050_1584___fuseiter_5051_1585___fuseiter_5052_1586___fuseiter_5053_1587, int8_t* __restrict__ __ins_0, int8_t* __restrict__ __outs_0) noexcept{
  for (uint64_t _fuseiter_5054 = 0UL; _fuseiter_5054 < 256UL; _fuseiter_5054 += 1UL) {
    for (uint64_t _fuseiter_5055 = 0UL; _fuseiter_5055 < 4UL; _fuseiter_5055 += 1UL) {
      int8_t __cached_0;
      __cached_0 = __ins_0[(((_fuseiter_5054 + ((fused_0fused_0fused_0fused_0_fuseiter_5049___fuseiter_5050_1584___fuseiter_5051_1585___fuseiter_5052_1586___fuseiter_5053_1587 / 64UL) * 256UL)) * 256UL) + ((_fuseiter_5055 + ((fused_0fused_0fused_0fused_0_fuseiter_5049___fuseiter_5050_1584___fuseiter_5051_1585___fuseiter_5052_1586___fuseiter_5053_1587 % 32UL) * 4UL)) + (((fused_0fused_0fused_0fused_0_fuseiter_5049___fuseiter_5050_1584___fuseiter_5051_1585___fuseiter_5052_1586___fuseiter_5053_1587 / 32UL) % 2UL) * 128UL)))];
      int8_t __cached_1;
      __cached_1 = __cached_0;
      __outs_0[(((fused_0fused_0fused_0fused_0_fuseiter_5049___fuseiter_5050_1584___fuseiter_5051_1585___fuseiter_5052_1586___fuseiter_5053_1587 / 64UL) * 65536UL) + ((((fused_0fused_0fused_0fused_0_fuseiter_5049___fuseiter_5050_1584___fuseiter_5051_1585___fuseiter_5052_1586___fuseiter_5053_1587 / 32UL) % 2UL) * 32768UL) + (((fused_0fused_0fused_0fused_0_fuseiter_5049___fuseiter_5050_1584___fuseiter_5051_1585___fuseiter_5052_1586___fuseiter_5053_1587 % 32UL) * 1024UL) + ((_fuseiter_5054 * 4UL) + _fuseiter_5055))))] = __cached_1;
    }
  }
}

static void reorder__5280_closure_232_0wrapper(void* __stream, int8_t* __restrict__ __module_data, uint64_t i, generic_val* __restrict__ args) noexcept{
  reorder__5280_closure_232(i, (int8_t*)(args[0UL].v_ptr), (int8_t*)(args[1UL].v_ptr));
}

static void reorder__5190_closure_233(uint64_t fused_0fused_0fused_0fused_0_fuseiter_5056___fuseiter_5057_1588___fuseiter_5058_1589___fuseiter_5059_1590___fuseiter_5060_1591, int8_t* __restrict__ __ins_0, int8_t* __restrict__ __outs_0) noexcept{
  for (uint64_t _fuseiter_5061 = 0UL; _fuseiter_5061 < 128UL; _fuseiter_5061 += 1UL) {
    for (uint64_t _fuseiter_5062 = 0UL; _fuseiter_5062 < 4UL; _fuseiter_5062 += 1UL) {
      int8_t __cached_0;
      __cached_0 = __ins_0[(((_fuseiter_5061 + ((fused_0fused_0fused_0fused_0_fuseiter_5056___fuseiter_5057_1588___fuseiter_5058_1589___fuseiter_5059_1590___fuseiter_5060_1591 / 64UL) * 128UL)) * 256UL) + (_fuseiter_5062 + ((fused_0fused_0fused_0fused_0_fuseiter_5056___fuseiter_5057_1588___fuseiter_5058_1589___fuseiter_5059_1590___fuseiter_5060_1591 % 64UL) * 4UL)))];
      int8_t __cached_1;
      __cached_1 = __cached_0;
      __outs_0[(((fused_0fused_0fused_0fused_0_fuseiter_5056___fuseiter_5057_1588___fuseiter_5058_1589___fuseiter_5059_1590___fuseiter_5060_1591 / 64UL) * 32768UL) + (((fused_0fused_0fused_0fused_0_fuseiter_5056___fuseiter_5057_1588___fuseiter_5058_1589___fuseiter_5059_1590___fuseiter_5060_1591 % 64UL) * 512UL) + ((_fuseiter_5061 * 4UL) + _fuseiter_5062)))] = __cached_1;
    }
  }
}

static void reorder__5190_closure_233_0wrapper(void* __stream, int8_t* __restrict__ __module_data, uint64_t i, generic_val* __restrict__ args) noexcept{
  reorder__5190_closure_233(i, (int8_t*)(args[0UL].v_ptr), (int8_t*)(args[1UL].v_ptr));
}

static void reorder__5120_closure_234(uint64_t fused_0fused_0fused_0fused_0_fuseiter_5063___fuseiter_5064_1592___fuseiter_5065_1593___fuseiter_5066_1594___fuseiter_5067_1595, int8_t* __restrict__ __ins_0, int8_t* __restrict__ __outs_0) noexcept{
  for (uint64_t _fuseiter_5068 = 0UL; _fuseiter_5068 < 128UL; _fuseiter_5068 += 1UL) {
    for (uint64_t _fuseiter_5069 = 0UL; _fuseiter_5069 < 4UL; _fuseiter_5069 += 1UL) {
      int8_t __cached_0;
      __cached_0 = __ins_0[(((_fuseiter_5068 + ((fused_0fused_0fused_0fused_0_fuseiter_5063___fuseiter_5064_1592___fuseiter_5065_1593___fuseiter_5066_1594___fuseiter_5067_1595 / 64UL) * 128UL)) * 256UL) + ((_fuseiter_5069 + ((fused_0fused_0fused_0fused_0_fuseiter_5063___fuseiter_5064_1592___fuseiter_5065_1593___fuseiter_5066_1594___fuseiter_5067_1595 % 32UL) * 4UL)) + (((fused_0fused_0fused_0fused_0_fuseiter_5063___fuseiter_5064_1592___fuseiter_5065_1593___fuseiter_5066_1594___fuseiter_5067_1595 / 32UL) % 2UL) * 128UL)))];
      int8_t __cached_1;
      __cached_1 = __cached_0;
      __outs_0[(((fused_0fused_0fused_0fused_0_fuseiter_5063___fuseiter_5064_1592___fuseiter_5065_1593___fuseiter_5066_1594___fuseiter_5067_1595 / 64UL) * 32768UL) + ((((fused_0fused_0fused_0fused_0_fuseiter_5063___fuseiter_5064_1592___fuseiter_5065_1593___fuseiter_5066_1594___fuseiter_5067_1595 / 32UL) % 2UL) * 16384UL) + (((fused_0fused_0fused_0fused_0_fuseiter_5063___fuseiter_5064_1592___fuseiter_5065_1593___fuseiter_5066_1594___fuseiter_5067_1595 % 32UL) * 512UL) + ((_fuseiter_5068 * 4UL) + _fuseiter_5069))))] = __cached_1;
    }
  }
}

static void reorder__5120_closure_234_0wrapper(void* __stream, int8_t* __restrict__ __module_data, uint64_t i, generic_val* __restrict__ args) noexcept{
  reorder__5120_closure_234(i, (int8_t*)(args[0UL].v_ptr), (int8_t*)(args[1UL].v_ptr));
}

static void reorder__5030_closure_235(uint64_t fused_0fused_0fused_0fused_0_fuseiter_5070___fuseiter_5071_1596___fuseiter_5072_1597___fuseiter_5073_1598___fuseiter_5074_1599, int8_t* __restrict__ __ins_0, int8_t* __restrict__ __outs_0) noexcept{
  for (uint64_t _fuseiter_5075 = 0UL; _fuseiter_5075 < 128UL; _fuseiter_5075 += 1UL) {
    for (uint64_t _fuseiter_5076 = 0UL; _fuseiter_5076 < 4UL; _fuseiter_5076 += 1UL) {
      int8_t __cached_0;
      __cached_0 = __ins_0[(((_fuseiter_5075 + ((fused_0fused_0fused_0fused_0_fuseiter_5070___fuseiter_5071_1596___fuseiter_5072_1597___fuseiter_5073_1598___fuseiter_5074_1599 / 64UL) * 128UL)) * 256UL) + (_fuseiter_5076 + ((fused_0fused_0fused_0fused_0_fuseiter_5070___fuseiter_5071_1596___fuseiter_5072_1597___fuseiter_5073_1598___fuseiter_5074_1599 % 64UL) * 4UL)))];
      int8_t __cached_1;
      __cached_1 = __cached_0;
      __outs_0[(((fused_0fused_0fused_0fused_0_fuseiter_5070___fuseiter_5071_1596___fuseiter_5072_1597___fuseiter_5073_1598___fuseiter_5074_1599 / 64UL) * 32768UL) + (((fused_0fused_0fused_0fused_0_fuseiter_5070___fuseiter_5071_1596___fuseiter_5072_1597___fuseiter_5073_1598___fuseiter_5074_1599 % 64UL) * 512UL) + ((_fuseiter_5075 * 4UL) + _fuseiter_5076)))] = __cached_1;
    }
  }
}

static void reorder__5030_closure_235_0wrapper(void* __stream, int8_t* __restrict__ __module_data, uint64_t i, generic_val* __restrict__ args) noexcept{
  reorder__5030_closure_235(i, (int8_t*)(args[0UL].v_ptr), (int8_t*)(args[1UL].v_ptr));
}

static void reorder__4960_closure_236(uint64_t fused_0fused_0fused_0fused_0_fuseiter_5077___fuseiter_5078_1600___fuseiter_5079_1601___fuseiter_5080_1602___fuseiter_5081_1603, int8_t* __restrict__ __ins_0, int8_t* __restrict__ __outs_0) noexcept{
  for (uint64_t _fuseiter_5082 = 0UL; _fuseiter_5082 < 1024UL; _fuseiter_5082 += 1UL) {
    for (uint64_t _fuseiter_5083 = 0UL; _fuseiter_5083 < 4UL; _fuseiter_5083 += 1UL) {
      int8_t __cached_0;
      __cached_0 = __ins_0[(((_fuseiter_5082 + ((fused_0fused_0fused_0fused_0_fuseiter_5077___fuseiter_5078_1600___fuseiter_5079_1601___fuseiter_5080_1602___fuseiter_5081_1603 / 64UL) * 1024UL)) * 256UL) + ((_fuseiter_5083 + ((fused_0fused_0fused_0fused_0_fuseiter_5077___fuseiter_5078_1600___fuseiter_5079_1601___fuseiter_5080_1602___fuseiter_5081_1603 % 16UL) * 4UL)) + (((fused_0fused_0fused_0fused_0_fuseiter_5077___fuseiter_5078_1600___fuseiter_5079_1601___fuseiter_5080_1602___fuseiter_5081_1603 / 16UL) % 4UL) * 64UL)))];
      int8_t __cached_1;
      __cached_1 = __cached_0;
      __outs_0[(((fused_0fused_0fused_0fused_0_fuseiter_5077___fuseiter_5078_1600___fuseiter_5079_1601___fuseiter_5080_1602___fuseiter_5081_1603 / 64UL) * 262144UL) + ((((fused_0fused_0fused_0fused_0_fuseiter_5077___fuseiter_5078_1600___fuseiter_5079_1601___fuseiter_5080_1602___fuseiter_5081_1603 / 16UL) % 4UL) * 65536UL) + (((fused_0fused_0fused_0fused_0_fuseiter_5077___fuseiter_5078_1600___fuseiter_5079_1601___fuseiter_5080_1602___fuseiter_5081_1603 % 16UL) * 4096UL) + ((_fuseiter_5082 * 4UL) + _fuseiter_5083))))] = __cached_1;
    }
  }
}

static void reorder__4960_closure_236_0wrapper(void* __stream, int8_t* __restrict__ __module_data, uint64_t i, generic_val* __restrict__ args) noexcept{
  reorder__4960_closure_236(i, (int8_t*)(args[0UL].v_ptr), (int8_t*)(args[1UL].v_ptr));
}

static void reorder__4870_closure_237(uint64_t fused_0fused_0fused_0fused_0_fuseiter_5084___fuseiter_5085_1604___fuseiter_5086_1605___fuseiter_5087_1606___fuseiter_5088_1607, int8_t* __restrict__ __ins_0, int8_t* __restrict__ __outs_0) noexcept{
  for (uint64_t _fuseiter_5089 = 0UL; _fuseiter_5089 < 128UL; _fuseiter_5089 += 1UL) {
    for (uint64_t _fuseiter_5090 = 0UL; _fuseiter_5090 < 4UL; _fuseiter_5090 += 1UL) {
      int8_t __cached_0;
      __cached_0 = __ins_0[(((_fuseiter_5089 + ((fused_0fused_0fused_0fused_0_fuseiter_5084___fuseiter_5085_1604___fuseiter_5086_1605___fuseiter_5087_1606___fuseiter_5088_1607 / 64UL) * 128UL)) * 256UL) + (_fuseiter_5090 + ((fused_0fused_0fused_0fused_0_fuseiter_5084___fuseiter_5085_1604___fuseiter_5086_1605___fuseiter_5087_1606___fuseiter_5088_1607 % 64UL) * 4UL)))];
      int8_t __cached_1;
      __cached_1 = __cached_0;
      __outs_0[(((fused_0fused_0fused_0fused_0_fuseiter_5084___fuseiter_5085_1604___fuseiter_5086_1605___fuseiter_5087_1606___fuseiter_5088_1607 / 64UL) * 32768UL) + (((fused_0fused_0fused_0fused_0_fuseiter_5084___fuseiter_5085_1604___fuseiter_5086_1605___fuseiter_5087_1606___fuseiter_5088_1607 % 64UL) * 512UL) + ((_fuseiter_5089 * 4UL) + _fuseiter_5090)))] = __cached_1;
    }
  }
}

static void reorder__4870_closure_237_0wrapper(void* __stream, int8_t* __restrict__ __module_data, uint64_t i, generic_val* __restrict__ args) noexcept{
  reorder__4870_closure_237(i, (int8_t*)(args[0UL].v_ptr), (int8_t*)(args[1UL].v_ptr));
}

static void reorder__4220_closure_238(uint64_t fused_0fused_0fused_0fused_0fused_0_fuseiter_5091___fuseiter_5092_1608___fuseiter_5093_1609___fuseiter_5094_1610___fuseiter_5095_1611___fuseiter_5096_1612, int8_t* __restrict__ __ins_0, int8_t* __restrict__ __outs_0) noexcept{
  for (uint64_t _fuseiter_5097 = 0UL; _fuseiter_5097 < 4UL; _fuseiter_5097 += 1UL) {
    int8_t __cached_0;
    __cached_0 = __ins_0[((((fused_0fused_0fused_0fused_0fused_0_fuseiter_5091___fuseiter_5092_1608___fuseiter_5093_1609___fuseiter_5094_1610___fuseiter_5095_1611___fuseiter_5096_1612 % 64UL) + ((fused_0fused_0fused_0fused_0fused_0_fuseiter_5091___fuseiter_5092_1608___fuseiter_5093_1609___fuseiter_5094_1610___fuseiter_5095_1611___fuseiter_5096_1612 / 1024UL) * 64UL)) * 64UL) + (_fuseiter_5097 + (((fused_0fused_0fused_0fused_0fused_0_fuseiter_5091___fuseiter_5092_1608___fuseiter_5093_1609___fuseiter_5094_1610___fuseiter_5095_1611___fuseiter_5096_1612 / 64UL) % 16UL) * 4UL)))];
    int8_t __cached_1;
    __cached_1 = __cached_0;
    __outs_0[(((fused_0fused_0fused_0fused_0fused_0_fuseiter_5091___fuseiter_5092_1608___fuseiter_5093_1609___fuseiter_5094_1610___fuseiter_5095_1611___fuseiter_5096_1612 / 1024UL) * 4096UL) + ((((fused_0fused_0fused_0fused_0fused_0_fuseiter_5091___fuseiter_5092_1608___fuseiter_5093_1609___fuseiter_5094_1610___fuseiter_5095_1611___fuseiter_5096_1612 / 64UL) % 16UL) * 256UL) + (((fused_0fused_0fused_0fused_0fused_0_fuseiter_5091___fuseiter_5092_1608___fuseiter_5093_1609___fuseiter_5094_1610___fuseiter_5095_1611___fuseiter_5096_1612 % 64UL) * 4UL) + _fuseiter_5097)))] = __cached_1;
  }
}

static void reorder__4220_closure_238_0wrapper(void* __stream, int8_t* __restrict__ __module_data, uint64_t i, generic_val* __restrict__ args) noexcept{
  reorder__4220_closure_238(i, (int8_t*)(args[0UL].v_ptr), (int8_t*)(args[1UL].v_ptr));
}

static void mul__6120_closure_239(uint64_t fused_0fused_0fused_0__itr_0____itr_1_1613____itr_2_1614____itr_3_1615, float* __restrict__ __ins_0, float* __restrict__ __ins_1, float* __restrict__ __outs_0) noexcept{
  for (uint64_t _fuseiter_5102 = 0UL; _fuseiter_5102 < 32UL; _fuseiter_5102 += 16UL) {
    vec_f32x16 __cached_0;
    __cached_0 = vec_f32x16::load(&__ins_0[((((fused_0fused_0fused_0__itr_0____itr_1_1613____itr_2_1614____itr_3_1615 / 4UL) * 128UL) + ((fused_0fused_0fused_0__itr_0____itr_1_1613____itr_2_1614____itr_3_1615 % 4UL) * 32UL)) + _fuseiter_5102)]);
    float __cached_1;
    __cached_1 = __ins_1[0];
    vec_f32x16 __cached_2;
    __cached_2 = (__cached_0 * vec_f32x16(__cached_1));
    vec_f32x16::store(__cached_2, &__outs_0[((((fused_0fused_0fused_0__itr_0____itr_1_1613____itr_2_1614____itr_3_1615 / 4UL) * 128UL) + ((fused_0fused_0fused_0__itr_0____itr_1_1613____itr_2_1614____itr_3_1615 % 4UL) * 32UL)) + _fuseiter_5102)]);
  }
}

static void mul__6120_closure_239_0wrapper(void* __stream, int8_t* __restrict__ __module_data, uint64_t i, generic_val* __restrict__ args) noexcept{
  mul__6120_closure_239(i, (float*)(args[0UL].v_ptr), (float*)(args[1UL].v_ptr), (float*)(args[2UL].v_ptr));
}

static void mul__6110_closure_240(uint64_t fused_0fused_0fused_0__itr_0____itr_1_1616____itr_2_1617____itr_3_1618, float* __restrict__ __ins_0, float* __restrict__ __ins_1, float* __restrict__ __outs_0) noexcept{
  for (uint64_t _fuseiter_5108 = 0UL; _fuseiter_5108 < 32UL; _fuseiter_5108 += 16UL) {
    vec_f32x16 __cached_0;
    __cached_0 = vec_f32x16::load(&__ins_0[((((fused_0fused_0fused_0__itr_0____itr_1_1616____itr_2_1617____itr_3_1618 / 4UL) * 128UL) + ((fused_0fused_0fused_0__itr_0____itr_1_1616____itr_2_1617____itr_3_1618 % 4UL) * 32UL)) + _fuseiter_5108)]);
    float __cached_1;
    __cached_1 = __ins_1[0];
    vec_f32x16 __cached_2;
    __cached_2 = (__cached_0 * vec_f32x16(__cached_1));
    vec_f32x16::store(__cached_2, &__outs_0[((((fused_0fused_0fused_0__itr_0____itr_1_1616____itr_2_1617____itr_3_1618 / 4UL) * 128UL) + ((fused_0fused_0fused_0__itr_0____itr_1_1616____itr_2_1617____itr_3_1618 % 4UL) * 32UL)) + _fuseiter_5108)]);
  }
}

static void mul__6110_closure_240_0wrapper(void* __stream, int8_t* __restrict__ __module_data, uint64_t i, generic_val* __restrict__ args) noexcept{
  mul__6110_closure_240(i, (float*)(args[0UL].v_ptr), (float*)(args[1UL].v_ptr), (float*)(args[2UL].v_ptr));
}

static void mul__6100_closure_241(uint64_t fused_0fused_0fused_0__itr_0____itr_1_1619____itr_2_1620____itr_3_1621, float* __restrict__ __ins_0, float* __restrict__ __ins_1, float* __restrict__ __outs_0) noexcept{
  for (uint64_t _fuseiter_5114 = 0UL; _fuseiter_5114 < 64UL; _fuseiter_5114 += 16UL) {
    vec_f32x16 __cached_0;
    __cached_0 = vec_f32x16::load(&__ins_0[((((fused_0fused_0fused_0__itr_0____itr_1_1619____itr_2_1620____itr_3_1621 / 2UL) * 128UL) + ((fused_0fused_0fused_0__itr_0____itr_1_1619____itr_2_1620____itr_3_1621 % 2UL) * 64UL)) + _fuseiter_5114)]);
    float __cached_1;
    __cached_1 = __ins_1[0];
    vec_f32x16 __cached_2;
    __cached_2 = (__cached_0 * vec_f32x16(__cached_1));
    vec_f32x16::store(__cached_2, &__outs_0[((((fused_0fused_0fused_0__itr_0____itr_1_1619____itr_2_1620____itr_3_1621 / 2UL) * 128UL) + ((fused_0fused_0fused_0__itr_0____itr_1_1619____itr_2_1620____itr_3_1621 % 2UL) * 64UL)) + _fuseiter_5114)]);
  }
}

static void mul__6100_closure_241_0wrapper(void* __stream, int8_t* __restrict__ __module_data, uint64_t i, generic_val* __restrict__ args) noexcept{
  mul__6100_closure_241(i, (float*)(args[0UL].v_ptr), (float*)(args[1UL].v_ptr), (float*)(args[2UL].v_ptr));
}

static void mul__6090_closure_242(uint64_t fused_0fused_0fused_0__itr_0____itr_1_1622____itr_2_1623____itr_3_1624, float* __restrict__ __ins_0, float* __restrict__ __ins_1, float* __restrict__ __outs_0) noexcept{
  for (uint64_t _fuseiter_5120 = 0UL; _fuseiter_5120 < 64UL; _fuseiter_5120 += 16UL) {
    vec_f32x16 __cached_0;
    __cached_0 = vec_f32x16::load(&__ins_0[((((fused_0fused_0fused_0__itr_0____itr_1_1622____itr_2_1623____itr_3_1624 / 2UL) * 128UL) + ((fused_0fused_0fused_0__itr_0____itr_1_1622____itr_2_1623____itr_3_1624 % 2UL) * 64UL)) + _fuseiter_5120)]);
    float __cached_1;
    __cached_1 = __ins_1[0];
    vec_f32x16 __cached_2;
    __cached_2 = (__cached_0 * vec_f32x16(__cached_1));
    vec_f32x16::store(__cached_2, &__outs_0[((((fused_0fused_0fused_0__itr_0____itr_1_1622____itr_2_1623____itr_3_1624 / 2UL) * 128UL) + ((fused_0fused_0fused_0__itr_0____itr_1_1622____itr_2_1623____itr_3_1624 % 2UL) * 64UL)) + _fuseiter_5120)]);
  }
}

static void mul__6090_closure_242_0wrapper(void* __stream, int8_t* __restrict__ __module_data, uint64_t i, generic_val* __restrict__ args) noexcept{
  mul__6090_closure_242(i, (float*)(args[0UL].v_ptr), (float*)(args[1UL].v_ptr), (float*)(args[2UL].v_ptr));
}

static void mul__6060_closure_243(uint64_t fused_0fused_0fused_0__itr_0____itr_1_1625____itr_2_1626____itr_3_1627, float* __restrict__ __ins_0, float* __restrict__ __ins_1, float* __restrict__ __outs_0) noexcept{
  for (uint64_t _fuseiter_5126 = 0UL; _fuseiter_5126 < 32UL; _fuseiter_5126 += 16UL) {
    vec_f32x16 __cached_0;
    __cached_0 = vec_f32x16::load(&__ins_0[((((fused_0fused_0fused_0__itr_0____itr_1_1625____itr_2_1626____itr_3_1627 / 4UL) * 128UL) + ((fused_0fused_0fused_0__itr_0____itr_1_1625____itr_2_1626____itr_3_1627 % 4UL) * 32UL)) + _fuseiter_5126)]);
    float __cached_1;
    __cached_1 = __ins_1[0];
    vec_f32x16 __cached_2;
    __cached_2 = (__cached_0 * vec_f32x16(__cached_1));
    vec_f32x16::store(__cached_2, &__outs_0[((((fused_0fused_0fused_0__itr_0____itr_1_1625____itr_2_1626____itr_3_1627 / 4UL) * 128UL) + ((fused_0fused_0fused_0__itr_0____itr_1_1625____itr_2_1626____itr_3_1627 % 4UL) * 32UL)) + _fuseiter_5126)]);
  }
}

static void mul__6060_closure_243_0wrapper(void* __stream, int8_t* __restrict__ __module_data, uint64_t i, generic_val* __restrict__ args) noexcept{
  mul__6060_closure_243(i, (float*)(args[0UL].v_ptr), (float*)(args[1UL].v_ptr), (float*)(args[2UL].v_ptr));
}

static void mul__6050_closure_244(uint64_t fused_0fused_0fused_0__itr_0____itr_1_1628____itr_2_1629____itr_3_1630, float* __restrict__ __ins_0, float* __restrict__ __ins_1, float* __restrict__ __outs_0) noexcept{
  for (uint64_t _fuseiter_5132 = 0UL; _fuseiter_5132 < 32UL; _fuseiter_5132 += 16UL) {
    vec_f32x16 __cached_0;
    __cached_0 = vec_f32x16::load(&__ins_0[((((fused_0fused_0fused_0__itr_0____itr_1_1628____itr_2_1629____itr_3_1630 / 4UL) * 128UL) + ((fused_0fused_0fused_0__itr_0____itr_1_1628____itr_2_1629____itr_3_1630 % 4UL) * 32UL)) + _fuseiter_5132)]);
    float __cached_1;
    __cached_1 = __ins_1[0];
    vec_f32x16 __cached_2;
    __cached_2 = (__cached_0 * vec_f32x16(__cached_1));
    vec_f32x16::store(__cached_2, &__outs_0[((((fused_0fused_0fused_0__itr_0____itr_1_1628____itr_2_1629____itr_3_1630 / 4UL) * 128UL) + ((fused_0fused_0fused_0__itr_0____itr_1_1628____itr_2_1629____itr_3_1630 % 4UL) * 32UL)) + _fuseiter_5132)]);
  }
}

static void mul__6050_closure_244_0wrapper(void* __stream, int8_t* __restrict__ __module_data, uint64_t i, generic_val* __restrict__ args) noexcept{
  mul__6050_closure_244(i, (float*)(args[0UL].v_ptr), (float*)(args[1UL].v_ptr), (float*)(args[2UL].v_ptr));
}

static void mul__5980_closure_245(uint64_t fused_0fused_0fused_0__itr_0____itr_1_1643____itr_2_1644____itr_3_1645, float* __restrict__ __ins_0, float* __restrict__ __ins_1, float* __restrict__ __outs_0) noexcept{
  for (uint64_t _fuseiter_5162 = 0UL; _fuseiter_5162 < 32UL; _fuseiter_5162 += 16UL) {
    vec_f32x16 __cached_0;
    __cached_0 = vec_f32x16::load(&__ins_0[((((fused_0fused_0fused_0__itr_0____itr_1_1643____itr_2_1644____itr_3_1645 / 4UL) * 128UL) + ((fused_0fused_0fused_0__itr_0____itr_1_1643____itr_2_1644____itr_3_1645 % 4UL) * 32UL)) + _fuseiter_5162)]);
    float __cached_1;
    __cached_1 = __ins_1[0];
    vec_f32x16 __cached_2;
    __cached_2 = (__cached_0 * vec_f32x16(__cached_1));
    vec_f32x16::store(__cached_2, &__outs_0[((((fused_0fused_0fused_0__itr_0____itr_1_1643____itr_2_1644____itr_3_1645 / 4UL) * 128UL) + ((fused_0fused_0fused_0__itr_0____itr_1_1643____itr_2_1644____itr_3_1645 % 4UL) * 32UL)) + _fuseiter_5162)]);
  }
}

static void mul__5980_closure_245_0wrapper(void* __stream, int8_t* __restrict__ __module_data, uint64_t i, generic_val* __restrict__ args) noexcept{
  mul__5980_closure_245(i, (float*)(args[0UL].v_ptr), (float*)(args[1UL].v_ptr), (float*)(args[2UL].v_ptr));
}

static void mul__5970_closure_246(uint64_t fused_0fused_0fused_0__itr_0____itr_1_1646____itr_2_1647____itr_3_1648, float* __restrict__ __ins_0, float* __restrict__ __ins_1, float* __restrict__ __outs_0) noexcept{
  for (uint64_t _fuseiter_5168 = 0UL; _fuseiter_5168 < 32UL; _fuseiter_5168 += 16UL) {
    vec_f32x16 __cached_0;
    __cached_0 = vec_f32x16::load(&__ins_0[((((fused_0fused_0fused_0__itr_0____itr_1_1646____itr_2_1647____itr_3_1648 / 4UL) * 128UL) + ((fused_0fused_0fused_0__itr_0____itr_1_1646____itr_2_1647____itr_3_1648 % 4UL) * 32UL)) + _fuseiter_5168)]);
    float __cached_1;
    __cached_1 = __ins_1[0];
    vec_f32x16 __cached_2;
    __cached_2 = (__cached_0 * vec_f32x16(__cached_1));
    vec_f32x16::store(__cached_2, &__outs_0[((((fused_0fused_0fused_0__itr_0____itr_1_1646____itr_2_1647____itr_3_1648 / 4UL) * 128UL) + ((fused_0fused_0fused_0__itr_0____itr_1_1646____itr_2_1647____itr_3_1648 % 4UL) * 32UL)) + _fuseiter_5168)]);
  }
}

static void mul__5970_closure_246_0wrapper(void* __stream, int8_t* __restrict__ __module_data, uint64_t i, generic_val* __restrict__ args) noexcept{
  mul__5970_closure_246(i, (float*)(args[0UL].v_ptr), (float*)(args[1UL].v_ptr), (float*)(args[2UL].v_ptr));
}

static void mul__5940_closure_247(uint64_t fused_0fused_0fused_0__itr_0____itr_1_1649____itr_2_1650____itr_3_1651, float* __restrict__ __ins_0, float* __restrict__ __ins_1, float* __restrict__ __outs_0) noexcept{
  for (uint64_t _fuseiter_5174 = 0UL; _fuseiter_5174 < 32UL; _fuseiter_5174 += 16UL) {
    vec_f32x16 __cached_0;
    __cached_0 = vec_f32x16::load(&__ins_0[((((fused_0fused_0fused_0__itr_0____itr_1_1649____itr_2_1650____itr_3_1651 / 4UL) * 128UL) + ((fused_0fused_0fused_0__itr_0____itr_1_1649____itr_2_1650____itr_3_1651 % 4UL) * 32UL)) + _fuseiter_5174)]);
    float __cached_1;
    __cached_1 = __ins_1[0];
    vec_f32x16 __cached_2;
    __cached_2 = (__cached_0 * vec_f32x16(__cached_1));
    vec_f32x16::store(__cached_2, &__outs_0[((((fused_0fused_0fused_0__itr_0____itr_1_1649____itr_2_1650____itr_3_1651 / 4UL) * 128UL) + ((fused_0fused_0fused_0__itr_0____itr_1_1649____itr_2_1650____itr_3_1651 % 4UL) * 32UL)) + _fuseiter_5174)]);
  }
}

static void mul__5940_closure_247_0wrapper(void* __stream, int8_t* __restrict__ __module_data, uint64_t i, generic_val* __restrict__ args) noexcept{
  mul__5940_closure_247(i, (float*)(args[0UL].v_ptr), (float*)(args[1UL].v_ptr), (float*)(args[2UL].v_ptr));
}

static void mul__5930_closure_248(uint64_t fused_0fused_0fused_0__itr_0____itr_1_1652____itr_2_1653____itr_3_1654, float* __restrict__ __ins_0, float* __restrict__ __ins_1, float* __restrict__ __outs_0) noexcept{
  for (uint64_t _fuseiter_5180 = 0UL; _fuseiter_5180 < 32UL; _fuseiter_5180 += 16UL) {
    vec_f32x16 __cached_0;
    __cached_0 = vec_f32x16::load(&__ins_0[((((fused_0fused_0fused_0__itr_0____itr_1_1652____itr_2_1653____itr_3_1654 / 4UL) * 128UL) + ((fused_0fused_0fused_0__itr_0____itr_1_1652____itr_2_1653____itr_3_1654 % 4UL) * 32UL)) + _fuseiter_5180)]);
    float __cached_1;
    __cached_1 = __ins_1[0];
    vec_f32x16 __cached_2;
    __cached_2 = (__cached_0 * vec_f32x16(__cached_1));
    vec_f32x16::store(__cached_2, &__outs_0[((((fused_0fused_0fused_0__itr_0____itr_1_1652____itr_2_1653____itr_3_1654 / 4UL) * 128UL) + ((fused_0fused_0fused_0__itr_0____itr_1_1652____itr_2_1653____itr_3_1654 % 4UL) * 32UL)) + _fuseiter_5180)]);
  }
}

static void mul__5930_closure_248_0wrapper(void* __stream, int8_t* __restrict__ __module_data, uint64_t i, generic_val* __restrict__ args) noexcept{
  mul__5930_closure_248(i, (float*)(args[0UL].v_ptr), (float*)(args[1UL].v_ptr), (float*)(args[2UL].v_ptr));
}

static void mul__5920_closure_249(uint64_t fused_0fused_0fused_0__itr_0____itr_1_1655____itr_2_1656____itr_3_1657, float* __restrict__ __ins_0, float* __restrict__ __ins_1, float* __restrict__ __outs_0) noexcept{
  for (uint64_t _fuseiter_5186 = 0UL; _fuseiter_5186 < 64UL; _fuseiter_5186 += 16UL) {
    vec_f32x16 __cached_0;
    __cached_0 = vec_f32x16::load(&__ins_0[((((fused_0fused_0fused_0__itr_0____itr_1_1655____itr_2_1656____itr_3_1657 / 2UL) * 128UL) + ((fused_0fused_0fused_0__itr_0____itr_1_1655____itr_2_1656____itr_3_1657 % 2UL) * 64UL)) + _fuseiter_5186)]);
    float __cached_1;
    __cached_1 = __ins_1[0];
    vec_f32x16 __cached_2;
    __cached_2 = (__cached_0 * vec_f32x16(__cached_1));
    vec_f32x16::store(__cached_2, &__outs_0[((((fused_0fused_0fused_0__itr_0____itr_1_1655____itr_2_1656____itr_3_1657 / 2UL) * 128UL) + ((fused_0fused_0fused_0__itr_0____itr_1_1655____itr_2_1656____itr_3_1657 % 2UL) * 64UL)) + _fuseiter_5186)]);
  }
}

static void mul__5920_closure_249_0wrapper(void* __stream, int8_t* __restrict__ __module_data, uint64_t i, generic_val* __restrict__ args) noexcept{
  mul__5920_closure_249(i, (float*)(args[0UL].v_ptr), (float*)(args[1UL].v_ptr), (float*)(args[2UL].v_ptr));
}

static void mul__5910_closure_250(uint64_t fused_0fused_0fused_0__itr_0____itr_1_1658____itr_2_1659____itr_3_1660, float* __restrict__ __ins_0, float* __restrict__ __ins_1, float* __restrict__ __outs_0) noexcept{
  for (uint64_t _fuseiter_5192 = 0UL; _fuseiter_5192 < 64UL; _fuseiter_5192 += 16UL) {
    vec_f32x16 __cached_0;
    __cached_0 = vec_f32x16::load(&__ins_0[((((fused_0fused_0fused_0__itr_0____itr_1_1658____itr_2_1659____itr_3_1660 / 2UL) * 128UL) + ((fused_0fused_0fused_0__itr_0____itr_1_1658____itr_2_1659____itr_3_1660 % 2UL) * 64UL)) + _fuseiter_5192)]);
    float __cached_1;
    __cached_1 = __ins_1[0];
    vec_f32x16 __cached_2;
    __cached_2 = (__cached_0 * vec_f32x16(__cached_1));
    vec_f32x16::store(__cached_2, &__outs_0[((((fused_0fused_0fused_0__itr_0____itr_1_1658____itr_2_1659____itr_3_1660 / 2UL) * 128UL) + ((fused_0fused_0fused_0__itr_0____itr_1_1658____itr_2_1659____itr_3_1660 % 2UL) * 64UL)) + _fuseiter_5192)]);
  }
}

static void mul__5910_closure_250_0wrapper(void* __stream, int8_t* __restrict__ __module_data, uint64_t i, generic_val* __restrict__ args) noexcept{
  mul__5910_closure_250(i, (float*)(args[0UL].v_ptr), (float*)(args[1UL].v_ptr), (float*)(args[2UL].v_ptr));
}

static void mul__5860_closure_251(uint64_t fused_0fused_0fused_0__itr_0____itr_1_1661____itr_2_1662____itr_3_1663, float* __restrict__ __ins_0, float* __restrict__ __ins_1, float* __restrict__ __outs_0) noexcept{
  for (uint64_t _fuseiter_5198 = 0UL; _fuseiter_5198 < 32UL; _fuseiter_5198 += 16UL) {
    vec_f32x16 __cached_0;
    __cached_0 = vec_f32x16::load(&__ins_0[((((fused_0fused_0fused_0__itr_0____itr_1_1661____itr_2_1662____itr_3_1663 / 2UL) * 64UL) + ((fused_0fused_0fused_0__itr_0____itr_1_1661____itr_2_1662____itr_3_1663 % 2UL) * 32UL)) + _fuseiter_5198)]);
    float __cached_1;
    __cached_1 = __ins_1[0];
    vec_f32x16 __cached_2;
    __cached_2 = (__cached_0 * vec_f32x16(__cached_1));
    vec_f32x16::store(__cached_2, &__outs_0[((((fused_0fused_0fused_0__itr_0____itr_1_1661____itr_2_1662____itr_3_1663 / 2UL) * 64UL) + ((fused_0fused_0fused_0__itr_0____itr_1_1661____itr_2_1662____itr_3_1663 % 2UL) * 32UL)) + _fuseiter_5198)]);
  }
}

static void mul__5860_closure_251_0wrapper(void* __stream, int8_t* __restrict__ __module_data, uint64_t i, generic_val* __restrict__ args) noexcept{
  mul__5860_closure_251(i, (float*)(args[0UL].v_ptr), (float*)(args[1UL].v_ptr), (float*)(args[2UL].v_ptr));
}

static void mul__5850_closure_252(uint64_t fused_0fused_0fused_0__itr_0____itr_1_1664____itr_2_1665____itr_3_1666, float* __restrict__ __ins_0, float* __restrict__ __ins_1, float* __restrict__ __outs_0) noexcept{
  for (uint64_t _fuseiter_5204 = 0UL; _fuseiter_5204 < 32UL; _fuseiter_5204 += 16UL) {
    vec_f32x16 __cached_0;
    __cached_0 = vec_f32x16::load(&__ins_0[((((fused_0fused_0fused_0__itr_0____itr_1_1664____itr_2_1665____itr_3_1666 / 2UL) * 64UL) + ((fused_0fused_0fused_0__itr_0____itr_1_1664____itr_2_1665____itr_3_1666 % 2UL) * 32UL)) + _fuseiter_5204)]);
    float __cached_1;
    __cached_1 = __ins_1[0];
    vec_f32x16 __cached_2;
    __cached_2 = (__cached_0 * vec_f32x16(__cached_1));
    vec_f32x16::store(__cached_2, &__outs_0[((((fused_0fused_0fused_0__itr_0____itr_1_1664____itr_2_1665____itr_3_1666 / 2UL) * 64UL) + ((fused_0fused_0fused_0__itr_0____itr_1_1664____itr_2_1665____itr_3_1666 % 2UL) * 32UL)) + _fuseiter_5204)]);
  }
}

static void mul__5850_closure_252_0wrapper(void* __stream, int8_t* __restrict__ __module_data, uint64_t i, generic_val* __restrict__ args) noexcept{
  mul__5850_closure_252(i, (float*)(args[0UL].v_ptr), (float*)(args[1UL].v_ptr), (float*)(args[2UL].v_ptr));
}

static void mul__5840_closure_253(uint64_t fused_0fused_0fused_0__itr_0____itr_1_1667____itr_2_1668____itr_3_1669, float* __restrict__ __ins_0, float* __restrict__ __ins_1, float* __restrict__ __outs_0) noexcept{
  for (uint64_t _fuseiter_5210 = 0UL; _fuseiter_5210 < 32UL; _fuseiter_5210 += 16UL) {
    vec_f32x16 __cached_0;
    __cached_0 = vec_f32x16::load(&__ins_0[((((fused_0fused_0fused_0__itr_0____itr_1_1667____itr_2_1668____itr_3_1669 / 2UL) * 64UL) + ((fused_0fused_0fused_0__itr_0____itr_1_1667____itr_2_1668____itr_3_1669 % 2UL) * 32UL)) + _fuseiter_5210)]);
    float __cached_1;
    __cached_1 = __ins_1[0];
    vec_f32x16 __cached_2;
    __cached_2 = (__cached_0 * vec_f32x16(__cached_1));
    vec_f32x16::store(__cached_2, &__outs_0[((((fused_0fused_0fused_0__itr_0____itr_1_1667____itr_2_1668____itr_3_1669 / 2UL) * 64UL) + ((fused_0fused_0fused_0__itr_0____itr_1_1667____itr_2_1668____itr_3_1669 % 2UL) * 32UL)) + _fuseiter_5210)]);
  }
}

static void mul__5840_closure_253_0wrapper(void* __stream, int8_t* __restrict__ __module_data, uint64_t i, generic_val* __restrict__ args) noexcept{
  mul__5840_closure_253(i, (float*)(args[0UL].v_ptr), (float*)(args[1UL].v_ptr), (float*)(args[2UL].v_ptr));
}

static void mul__5830_closure_254(uint64_t fused_0fused_0fused_0__itr_0____itr_1_1670____itr_2_1671____itr_3_1672, float* __restrict__ __ins_0, float* __restrict__ __ins_1, float* __restrict__ __outs_0) noexcept{
  for (uint64_t _fuseiter_5216 = 0UL; _fuseiter_5216 < 32UL; _fuseiter_5216 += 16UL) {
    vec_f32x16 __cached_0;
    __cached_0 = vec_f32x16::load(&__ins_0[((((fused_0fused_0fused_0__itr_0____itr_1_1670____itr_2_1671____itr_3_1672 / 2UL) * 64UL) + ((fused_0fused_0fused_0__itr_0____itr_1_1670____itr_2_1671____itr_3_1672 % 2UL) * 32UL)) + _fuseiter_5216)]);
    float __cached_1;
    __cached_1 = __ins_1[0];
    vec_f32x16 __cached_2;
    __cached_2 = (__cached_0 * vec_f32x16(__cached_1));
    vec_f32x16::store(__cached_2, &__outs_0[((((fused_0fused_0fused_0__itr_0____itr_1_1670____itr_2_1671____itr_3_1672 / 2UL) * 64UL) + ((fused_0fused_0fused_0__itr_0____itr_1_1670____itr_2_1671____itr_3_1672 % 2UL) * 32UL)) + _fuseiter_5216)]);
  }
}

static void mul__5830_closure_254_0wrapper(void* __stream, int8_t* __restrict__ __module_data, uint64_t i, generic_val* __restrict__ args) noexcept{
  mul__5830_closure_254(i, (float*)(args[0UL].v_ptr), (float*)(args[1UL].v_ptr), (float*)(args[2UL].v_ptr));
}

static void mul__5800_closure_255(uint64_t fused_0fused_0fused_0__itr_0____itr_1_1673____itr_2_1674____itr_3_1675, float* __restrict__ __ins_0, float* __restrict__ __ins_1, float* __restrict__ __outs_0) noexcept{
  vec_f32x16 __cached_0;
  __cached_0 = vec_f32x16::load(&__ins_0[(((fused_0fused_0fused_0__itr_0____itr_1_1673____itr_2_1674____itr_3_1675 / 4UL) * 64UL) + ((fused_0fused_0fused_0__itr_0____itr_1_1673____itr_2_1674____itr_3_1675 % 4UL) * 16UL))]);
  float __cached_1;
  __cached_1 = __ins_1[0];
  vec_f32x16 __cached_2;
  __cached_2 = (__cached_0 * vec_f32x16(__cached_1));
  vec_f32x16::store(__cached_2, &__outs_0[(((fused_0fused_0fused_0__itr_0____itr_1_1673____itr_2_1674____itr_3_1675 / 4UL) * 64UL) + ((fused_0fused_0fused_0__itr_0____itr_1_1673____itr_2_1674____itr_3_1675 % 4UL) * 16UL))]);
}

static void mul__5800_closure_255_0wrapper(void* __stream, int8_t* __restrict__ __module_data, uint64_t i, generic_val* __restrict__ args) noexcept{
  mul__5800_closure_255(i, (float*)(args[0UL].v_ptr), (float*)(args[1UL].v_ptr), (float*)(args[2UL].v_ptr));
}

static void mul__5790_closure_256(uint64_t fused_0fused_0fused_0__itr_0____itr_1_1676____itr_2_1677____itr_3_1678, float* __restrict__ __ins_0, float* __restrict__ __ins_1, float* __restrict__ __outs_0) noexcept{
  vec_f32x16 __cached_0;
  __cached_0 = vec_f32x16::load(&__ins_0[(((fused_0fused_0fused_0__itr_0____itr_1_1676____itr_2_1677____itr_3_1678 / 4UL) * 64UL) + ((fused_0fused_0fused_0__itr_0____itr_1_1676____itr_2_1677____itr_3_1678 % 4UL) * 16UL))]);
  float __cached_1;
  __cached_1 = __ins_1[0];
  vec_f32x16 __cached_2;
  __cached_2 = (__cached_0 * vec_f32x16(__cached_1));
  vec_f32x16::store(__cached_2, &__outs_0[(((fused_0fused_0fused_0__itr_0____itr_1_1676____itr_2_1677____itr_3_1678 / 4UL) * 64UL) + ((fused_0fused_0fused_0__itr_0____itr_1_1676____itr_2_1677____itr_3_1678 % 4UL) * 16UL))]);
}

static void mul__5790_closure_256_0wrapper(void* __stream, int8_t* __restrict__ __module_data, uint64_t i, generic_val* __restrict__ args) noexcept{
  mul__5790_closure_256(i, (float*)(args[0UL].v_ptr), (float*)(args[1UL].v_ptr), (float*)(args[2UL].v_ptr));
}

static void mul__5740_closure_257(uint64_t fused_0fused_0fused_0__itr_0____itr_1_1685____itr_2_1686____itr_3_1687, float* __restrict__ __ins_0, float* __restrict__ __ins_1, float* __restrict__ __outs_0) noexcept{
  for (uint64_t _fuseiter_5246 = 0UL; _fuseiter_5246 < 32UL; _fuseiter_5246 += 16UL) {
    vec_f32x16 __cached_0;
    __cached_0 = vec_f32x16::load(&__ins_0[((((fused_0fused_0fused_0__itr_0____itr_1_1685____itr_2_1686____itr_3_1687 / 2UL) * 64UL) + ((fused_0fused_0fused_0__itr_0____itr_1_1685____itr_2_1686____itr_3_1687 % 2UL) * 32UL)) + _fuseiter_5246)]);
    float __cached_1;
    __cached_1 = __ins_1[0];
    vec_f32x16 __cached_2;
    __cached_2 = (__cached_0 * vec_f32x16(__cached_1));
    vec_f32x16::store(__cached_2, &__outs_0[((((fused_0fused_0fused_0__itr_0____itr_1_1685____itr_2_1686____itr_3_1687 / 2UL) * 64UL) + ((fused_0fused_0fused_0__itr_0____itr_1_1685____itr_2_1686____itr_3_1687 % 2UL) * 32UL)) + _fuseiter_5246)]);
  }
}

static void mul__5740_closure_257_0wrapper(void* __stream, int8_t* __restrict__ __module_data, uint64_t i, generic_val* __restrict__ args) noexcept{
  mul__5740_closure_257(i, (float*)(args[0UL].v_ptr), (float*)(args[1UL].v_ptr), (float*)(args[2UL].v_ptr));
}

static void mul__5730_closure_258(uint64_t fused_0fused_0fused_0__itr_0____itr_1_1688____itr_2_1689____itr_3_1690, float* __restrict__ __ins_0, float* __restrict__ __ins_1, float* __restrict__ __outs_0) noexcept{
  for (uint64_t _fuseiter_5252 = 0UL; _fuseiter_5252 < 32UL; _fuseiter_5252 += 16UL) {
    vec_f32x16 __cached_0;
    __cached_0 = vec_f32x16::load(&__ins_0[((((fused_0fused_0fused_0__itr_0____itr_1_1688____itr_2_1689____itr_3_1690 / 2UL) * 64UL) + ((fused_0fused_0fused_0__itr_0____itr_1_1688____itr_2_1689____itr_3_1690 % 2UL) * 32UL)) + _fuseiter_5252)]);
    float __cached_1;
    __cached_1 = __ins_1[0];
    vec_f32x16 __cached_2;
    __cached_2 = (__cached_0 * vec_f32x16(__cached_1));
    vec_f32x16::store(__cached_2, &__outs_0[((((fused_0fused_0fused_0__itr_0____itr_1_1688____itr_2_1689____itr_3_1690 / 2UL) * 64UL) + ((fused_0fused_0fused_0__itr_0____itr_1_1688____itr_2_1689____itr_3_1690 % 2UL) * 32UL)) + _fuseiter_5252)]);
  }
}

static void mul__5730_closure_258_0wrapper(void* __stream, int8_t* __restrict__ __module_data, uint64_t i, generic_val* __restrict__ args) noexcept{
  mul__5730_closure_258(i, (float*)(args[0UL].v_ptr), (float*)(args[1UL].v_ptr), (float*)(args[2UL].v_ptr));
}

static void reorder__5160_closure_259(uint64_t fused_0fused_0_fuseiter_5266___fuseiter_5267_1697___fuseiter_5268_1698, int8_t* __restrict__ __ins_0, int8_t* __restrict__ __outs_0) noexcept{
  for (uint64_t _fuseiter_5269 = 0UL; _fuseiter_5269 < 3UL; _fuseiter_5269 += 1UL) {
    for (uint64_t _fuseiter_5270 = 0UL; _fuseiter_5270 < 32UL; _fuseiter_5270 += 1UL) {
      for (uint64_t _fuseiter_5271 = 0UL; _fuseiter_5271 < 32UL; _fuseiter_5271 += 1UL) {
        for (uint64_t _fuseiter_5272 = 0UL; _fuseiter_5272 < 4UL; _fuseiter_5272 += 1UL) {
          int8_t __cached_0;
          __cached_0 = __ins_0[(((_fuseiter_5271 + ((fused_0fused_0_fuseiter_5266___fuseiter_5267_1697___fuseiter_5268_1698 / 6UL) * 32UL)) * 2304UL) + ((((_fuseiter_5272 + (_fuseiter_5270 * 4UL)) + (((fused_0fused_0_fuseiter_5266___fuseiter_5267_1697___fuseiter_5268_1698 / 3UL) % 2UL) * 128UL)) * 9UL) + (((fused_0fused_0_fuseiter_5266___fuseiter_5267_1697___fuseiter_5268_1698 % 3UL) * 3UL) + _fuseiter_5269)))];
          int8_t __cached_1;
          __cached_1 = __cached_0;
          __outs_0[(((fused_0fused_0_fuseiter_5266___fuseiter_5267_1697___fuseiter_5268_1698 / 6UL) * 73728UL) + ((((fused_0fused_0_fuseiter_5266___fuseiter_5267_1697___fuseiter_5268_1698 / 3UL) % 2UL) * 36864UL) + (((fused_0fused_0_fuseiter_5266___fuseiter_5267_1697___fuseiter_5268_1698 % 3UL) * 12288UL) + ((_fuseiter_5269 * 4096UL) + ((_fuseiter_5270 * 128UL) + ((_fuseiter_5271 * 4UL) + _fuseiter_5272))))))] = __cached_1;
        }
      }
    }
  }
}

static void reorder__5160_closure_259_0wrapper(void* __stream, int8_t* __restrict__ __module_data, uint64_t i, generic_val* __restrict__ args) noexcept{
  reorder__5160_closure_259(i, (int8_t*)(args[0UL].v_ptr), (int8_t*)(args[1UL].v_ptr));
}

static void reorder__5090_closure_260(uint64_t fused_0fused_0_fuseiter_5273___fuseiter_5274_1699___fuseiter_5275_1700, int8_t* __restrict__ __ins_0, int8_t* __restrict__ __outs_0) noexcept{
  for (uint64_t _fuseiter_5276 = 0UL; _fuseiter_5276 < 3UL; _fuseiter_5276 += 1UL) {
    for (uint64_t _fuseiter_5277 = 0UL; _fuseiter_5277 < 16UL; _fuseiter_5277 += 1UL) {
      for (uint64_t _fuseiter_5278 = 0UL; _fuseiter_5278 < 64UL; _fuseiter_5278 += 1UL) {
        for (uint64_t _fuseiter_5279 = 0UL; _fuseiter_5279 < 4UL; _fuseiter_5279 += 1UL) {
          int8_t __cached_0;
          __cached_0 = __ins_0[(((_fuseiter_5278 + ((fused_0fused_0_fuseiter_5273___fuseiter_5274_1699___fuseiter_5275_1700 / 12UL) * 64UL)) * 2304UL) + ((((_fuseiter_5279 + (_fuseiter_5277 * 4UL)) + (((fused_0fused_0_fuseiter_5273___fuseiter_5274_1699___fuseiter_5275_1700 / 3UL) % 4UL) * 64UL)) * 9UL) + (((fused_0fused_0_fuseiter_5273___fuseiter_5274_1699___fuseiter_5275_1700 % 3UL) * 3UL) + _fuseiter_5276)))];
          int8_t __cached_1;
          __cached_1 = __cached_0;
          __outs_0[(((fused_0fused_0_fuseiter_5273___fuseiter_5274_1699___fuseiter_5275_1700 / 12UL) * 147456UL) + ((((fused_0fused_0_fuseiter_5273___fuseiter_5274_1699___fuseiter_5275_1700 / 3UL) % 4UL) * 36864UL) + (((fused_0fused_0_fuseiter_5273___fuseiter_5274_1699___fuseiter_5275_1700 % 3UL) * 12288UL) + ((_fuseiter_5276 * 4096UL) + ((_fuseiter_5277 * 256UL) + ((_fuseiter_5278 * 4UL) + _fuseiter_5279))))))] = __cached_1;
        }
      }
    }
  }
}

static void reorder__5090_closure_260_0wrapper(void* __stream, int8_t* __restrict__ __module_data, uint64_t i, generic_val* __restrict__ args) noexcept{
  reorder__5090_closure_260(i, (int8_t*)(args[0UL].v_ptr), (int8_t*)(args[1UL].v_ptr));
}

static void reorder__5000_closure_261(uint64_t fused_0fused_0_fuseiter_5280___fuseiter_5281_1701___fuseiter_5282_1702, int8_t* __restrict__ __ins_0, int8_t* __restrict__ __outs_0) noexcept{
  for (uint64_t _fuseiter_5283 = 0UL; _fuseiter_5283 < 3UL; _fuseiter_5283 += 1UL) {
    for (uint64_t _fuseiter_5284 = 0UL; _fuseiter_5284 < 32UL; _fuseiter_5284 += 1UL) {
      for (uint64_t _fuseiter_5285 = 0UL; _fuseiter_5285 < 32UL; _fuseiter_5285 += 1UL) {
        for (uint64_t _fuseiter_5286 = 0UL; _fuseiter_5286 < 4UL; _fuseiter_5286 += 1UL) {
          int8_t __cached_0;
          __cached_0 = __ins_0[(((_fuseiter_5285 + ((fused_0fused_0_fuseiter_5280___fuseiter_5281_1701___fuseiter_5282_1702 / 6UL) * 32UL)) * 2304UL) + ((((_fuseiter_5286 + (_fuseiter_5284 * 4UL)) + (((fused_0fused_0_fuseiter_5280___fuseiter_5281_1701___fuseiter_5282_1702 / 3UL) % 2UL) * 128UL)) * 9UL) + (((fused_0fused_0_fuseiter_5280___fuseiter_5281_1701___fuseiter_5282_1702 % 3UL) * 3UL) + _fuseiter_5283)))];
          int8_t __cached_1;
          __cached_1 = __cached_0;
          __outs_0[(((fused_0fused_0_fuseiter_5280___fuseiter_5281_1701___fuseiter_5282_1702 / 6UL) * 73728UL) + ((((fused_0fused_0_fuseiter_5280___fuseiter_5281_1701___fuseiter_5282_1702 / 3UL) % 2UL) * 36864UL) + (((fused_0fused_0_fuseiter_5280___fuseiter_5281_1701___fuseiter_5282_1702 % 3UL) * 12288UL) + ((_fuseiter_5283 * 4096UL) + ((_fuseiter_5284 * 128UL) + ((_fuseiter_5285 * 4UL) + _fuseiter_5286))))))] = __cached_1;
        }
      }
    }
  }
}

static void reorder__5000_closure_261_0wrapper(void* __stream, int8_t* __restrict__ __module_data, uint64_t i, generic_val* __restrict__ args) noexcept{
  reorder__5000_closure_261(i, (int8_t*)(args[0UL].v_ptr), (int8_t*)(args[1UL].v_ptr));
}

static void reorder__4930_closure_262(uint64_t fused_0fused_0_fuseiter_5287___fuseiter_5288_1703___fuseiter_5289_1704, int8_t* __restrict__ __ins_0, int8_t* __restrict__ __outs_0) noexcept{
  for (uint64_t _fuseiter_5290 = 0UL; _fuseiter_5290 < 3UL; _fuseiter_5290 += 1UL) {
    for (uint64_t _fuseiter_5291 = 0UL; _fuseiter_5291 < 64UL; _fuseiter_5291 += 1UL) {
      for (uint64_t _fuseiter_5292 = 0UL; _fuseiter_5292 < 16UL; _fuseiter_5292 += 1UL) {
        for (uint64_t _fuseiter_5293 = 0UL; _fuseiter_5293 < 4UL; _fuseiter_5293 += 1UL) {
          int8_t __cached_0;
          __cached_0 = __ins_0[(((_fuseiter_5292 + ((fused_0fused_0_fuseiter_5287___fuseiter_5288_1703___fuseiter_5289_1704 / 3UL) * 16UL)) * 2304UL) + (((_fuseiter_5293 + (_fuseiter_5291 * 4UL)) * 9UL) + (((fused_0fused_0_fuseiter_5287___fuseiter_5288_1703___fuseiter_5289_1704 % 3UL) * 3UL) + _fuseiter_5290)))];
          int8_t __cached_1;
          __cached_1 = __cached_0;
          __outs_0[(((fused_0fused_0_fuseiter_5287___fuseiter_5288_1703___fuseiter_5289_1704 / 3UL) * 36864UL) + (((fused_0fused_0_fuseiter_5287___fuseiter_5288_1703___fuseiter_5289_1704 % 3UL) * 12288UL) + ((_fuseiter_5290 * 4096UL) + ((_fuseiter_5291 * 64UL) + ((_fuseiter_5292 * 4UL) + _fuseiter_5293)))))] = __cached_1;
        }
      }
    }
  }
}

static void reorder__4930_closure_262_0wrapper(void* __stream, int8_t* __restrict__ __module_data, uint64_t i, generic_val* __restrict__ args) noexcept{
  reorder__4930_closure_262(i, (int8_t*)(args[0UL].v_ptr), (int8_t*)(args[1UL].v_ptr));
}

static void reorder__4840_closure_263(uint64_t fused_0fused_0fused_0fused_0_fuseiter_5294___fuseiter_5295_1705___fuseiter_5296_1706___fuseiter_5297_1707___fuseiter_5298_1708, int8_t* __restrict__ __ins_0, int8_t* __restrict__ __outs_0) noexcept{
  for (uint64_t _fuseiter_5299 = 0UL; _fuseiter_5299 < 128UL; _fuseiter_5299 += 1UL) {
    for (uint64_t _fuseiter_5300 = 0UL; _fuseiter_5300 < 4UL; _fuseiter_5300 += 1UL) {
      int8_t __cached_0;
      __cached_0 = __ins_0[(((_fuseiter_5299 + ((fused_0fused_0fused_0fused_0_fuseiter_5294___fuseiter_5295_1705___fuseiter_5296_1706___fuseiter_5297_1707___fuseiter_5298_1708 / 576UL) * 128UL)) * 2304UL) + (((_fuseiter_5300 + ((fused_0fused_0fused_0fused_0_fuseiter_5294___fuseiter_5295_1705___fuseiter_5296_1706___fuseiter_5297_1707___fuseiter_5298_1708 % 64UL) * 4UL)) * 9UL) + ((((fused_0fused_0fused_0fused_0_fuseiter_5294___fuseiter_5295_1705___fuseiter_5296_1706___fuseiter_5297_1707___fuseiter_5298_1708 / 192UL) % 3UL) * 3UL) + ((fused_0fused_0fused_0fused_0_fuseiter_5294___fuseiter_5295_1705___fuseiter_5296_1706___fuseiter_5297_1707___fuseiter_5298_1708 / 64UL) % 3UL))))];
      int8_t __cached_1;
      __cached_1 = __cached_0;
      __outs_0[(((fused_0fused_0fused_0fused_0_fuseiter_5294___fuseiter_5295_1705___fuseiter_5296_1706___fuseiter_5297_1707___fuseiter_5298_1708 / 576UL) * 294912UL) + ((((fused_0fused_0fused_0fused_0_fuseiter_5294___fuseiter_5295_1705___fuseiter_5296_1706___fuseiter_5297_1707___fuseiter_5298_1708 / 192UL) % 3UL) * 98304UL) + ((((fused_0fused_0fused_0fused_0_fuseiter_5294___fuseiter_5295_1705___fuseiter_5296_1706___fuseiter_5297_1707___fuseiter_5298_1708 / 64UL) % 3UL) * 32768UL) + (((fused_0fused_0fused_0fused_0_fuseiter_5294___fuseiter_5295_1705___fuseiter_5296_1706___fuseiter_5297_1707___fuseiter_5298_1708 % 64UL) * 512UL) + ((_fuseiter_5299 * 4UL) + _fuseiter_5300)))))] = __cached_1;
    }
  }
}

static void reorder__4840_closure_263_0wrapper(void* __stream, int8_t* __restrict__ __module_data, uint64_t i, generic_val* __restrict__ args) noexcept{
  reorder__4840_closure_263(i, (int8_t*)(args[0UL].v_ptr), (int8_t*)(args[1UL].v_ptr));
}

static void reorder__4800_closure_264(uint64_t _fuseiter_5301, int8_t* __restrict__ __ins_0, int8_t* __restrict__ __outs_0) noexcept{
  for (uint64_t _fuseiter_5302 = 0UL; _fuseiter_5302 < 4UL; _fuseiter_5302 += 1UL) {
    for (uint64_t _fuseiter_5305 = 0UL; _fuseiter_5305 < 32UL; _fuseiter_5305 += 1UL) {
      for (uint64_t _fuseiter_5306 = 0UL; _fuseiter_5306 < 32UL; _fuseiter_5306 += 1UL) {
        for (uint64_t _fuseiter_5307 = 0UL; _fuseiter_5307 < 4UL; _fuseiter_5307 += 1UL) {
          int8_t __cached_0;
          __cached_0 = __ins_0[(((_fuseiter_5306 + (_fuseiter_5301 * 32UL)) * 512UL) + ((_fuseiter_5307 + (_fuseiter_5305 * 4UL)) + (_fuseiter_5302 * 128UL)))];
          int8_t __cached_1;
          __cached_1 = __cached_0;
          __outs_0[((_fuseiter_5301 * 16384UL) + ((_fuseiter_5302 * 4096UL) + ((_fuseiter_5305 * 128UL) + ((_fuseiter_5306 * 4UL) + _fuseiter_5307))))] = __cached_1;
        }
      }
    }
  }
}

static void reorder__4800_closure_264_0wrapper(void* __stream, int8_t* __restrict__ __module_data, uint64_t i, generic_val* __restrict__ args) noexcept{
  reorder__4800_closure_264(i, (int8_t*)(args[0UL].v_ptr), (int8_t*)(args[1UL].v_ptr));
}

static void reorder__4740_closure_265(uint64_t fused_0fused_0fused_0_fuseiter_5308___fuseiter_5309_1709___fuseiter_5310_1710___fuseiter_5311_1711, int8_t* __restrict__ __ins_0, int8_t* __restrict__ __outs_0) noexcept{
  for (uint64_t _fuseiter_5312 = 0UL; _fuseiter_5312 < 32UL; _fuseiter_5312 += 1UL) {
    for (uint64_t _fuseiter_5313 = 0UL; _fuseiter_5313 < 32UL; _fuseiter_5313 += 1UL) {
      for (uint64_t _fuseiter_5314 = 0UL; _fuseiter_5314 < 4UL; _fuseiter_5314 += 1UL) {
        int8_t __cached_0;
        __cached_0 = __ins_0[(((_fuseiter_5313 + ((fused_0fused_0fused_0_fuseiter_5308___fuseiter_5309_1709___fuseiter_5310_1710___fuseiter_5311_1711 / 9UL) * 32UL)) * 1152UL) + (((_fuseiter_5314 + (_fuseiter_5312 * 4UL)) * 9UL) + ((((fused_0fused_0fused_0_fuseiter_5308___fuseiter_5309_1709___fuseiter_5310_1710___fuseiter_5311_1711 / 3UL) % 3UL) * 3UL) + (fused_0fused_0fused_0_fuseiter_5308___fuseiter_5309_1709___fuseiter_5310_1710___fuseiter_5311_1711 % 3UL))))];
        int8_t __cached_1;
        __cached_1 = __cached_0;
        __outs_0[(((fused_0fused_0fused_0_fuseiter_5308___fuseiter_5309_1709___fuseiter_5310_1710___fuseiter_5311_1711 / 9UL) * 36864UL) + ((((fused_0fused_0fused_0_fuseiter_5308___fuseiter_5309_1709___fuseiter_5310_1710___fuseiter_5311_1711 / 3UL) % 3UL) * 12288UL) + (((fused_0fused_0fused_0_fuseiter_5308___fuseiter_5309_1709___fuseiter_5310_1710___fuseiter_5311_1711 % 3UL) * 4096UL) + ((_fuseiter_5312 * 128UL) + ((_fuseiter_5313 * 4UL) + _fuseiter_5314)))))] = __cached_1;
      }
    }
  }
}

static void reorder__4740_closure_265_0wrapper(void* __stream, int8_t* __restrict__ __module_data, uint64_t i, generic_val* __restrict__ args) noexcept{
  reorder__4740_closure_265(i, (int8_t*)(args[0UL].v_ptr), (int8_t*)(args[1UL].v_ptr));
}

static void reorder__4650_closure_266(uint64_t fused_0fused_0_fuseiter_5315___fuseiter_5316_1712___fuseiter_5317_1713, int8_t* __restrict__ __ins_0, int8_t* __restrict__ __outs_0) noexcept{
  for (uint64_t _fuseiter_5318 = 0UL; _fuseiter_5318 < 3UL; _fuseiter_5318 += 1UL) {
    for (uint64_t _fuseiter_5319 = 0UL; _fuseiter_5319 < 16UL; _fuseiter_5319 += 1UL) {
      for (uint64_t _fuseiter_5320 = 0UL; _fuseiter_5320 < 32UL; _fuseiter_5320 += 1UL) {
        for (uint64_t _fuseiter_5321 = 0UL; _fuseiter_5321 < 4UL; _fuseiter_5321 += 1UL) {
          int8_t __cached_0;
          __cached_0 = __ins_0[(((_fuseiter_5320 + ((fused_0fused_0_fuseiter_5315___fuseiter_5316_1712___fuseiter_5317_1713 / 6UL) * 32UL)) * 1152UL) + ((((_fuseiter_5321 + (_fuseiter_5319 * 4UL)) + (((fused_0fused_0_fuseiter_5315___fuseiter_5316_1712___fuseiter_5317_1713 / 3UL) % 2UL) * 64UL)) * 9UL) + (((fused_0fused_0_fuseiter_5315___fuseiter_5316_1712___fuseiter_5317_1713 % 3UL) * 3UL) + _fuseiter_5318)))];
          int8_t __cached_1;
          __cached_1 = __cached_0;
          __outs_0[(((fused_0fused_0_fuseiter_5315___fuseiter_5316_1712___fuseiter_5317_1713 / 6UL) * 36864UL) + ((((fused_0fused_0_fuseiter_5315___fuseiter_5316_1712___fuseiter_5317_1713 / 3UL) % 2UL) * 18432UL) + (((fused_0fused_0_fuseiter_5315___fuseiter_5316_1712___fuseiter_5317_1713 % 3UL) * 6144UL) + ((_fuseiter_5318 * 2048UL) + ((_fuseiter_5319 * 128UL) + ((_fuseiter_5320 * 4UL) + _fuseiter_5321))))))] = __cached_1;
        }
      }
    }
  }
}

static void reorder__4650_closure_266_0wrapper(void* __stream, int8_t* __restrict__ __module_data, uint64_t i, generic_val* __restrict__ args) noexcept{
  reorder__4650_closure_266(i, (int8_t*)(args[0UL].v_ptr), (int8_t*)(args[1UL].v_ptr));
}

static void reorder__4600_closure_267(uint64_t fused_0fused_0fused_0fused_0_fuseiter_5322___fuseiter_5323_1714___fuseiter_5324_1715___fuseiter_5325_1716___fuseiter_5326_1717, int8_t* __restrict__ __ins_0, int8_t* __restrict__ __outs_0) noexcept{
  for (uint64_t _fuseiter_5327 = 0UL; _fuseiter_5327 < 128UL; _fuseiter_5327 += 1UL) {
    for (uint64_t _fuseiter_5328 = 0UL; _fuseiter_5328 < 4UL; _fuseiter_5328 += 1UL) {
      int8_t __cached_0;
      __cached_0 = __ins_0[(((_fuseiter_5327 + ((fused_0fused_0fused_0fused_0_fuseiter_5322___fuseiter_5323_1714___fuseiter_5324_1715___fuseiter_5325_1716___fuseiter_5326_1717 / 288UL) * 128UL)) * 1152UL) + ((((_fuseiter_5328 + ((fused_0fused_0fused_0fused_0_fuseiter_5322___fuseiter_5323_1714___fuseiter_5324_1715___fuseiter_5325_1716___fuseiter_5326_1717 % 16UL) * 4UL)) + (((fused_0fused_0fused_0fused_0_fuseiter_5322___fuseiter_5323_1714___fuseiter_5324_1715___fuseiter_5325_1716___fuseiter_5326_1717 / 144UL) % 2UL) * 64UL)) * 9UL) + ((((fused_0fused_0fused_0fused_0_fuseiter_5322___fuseiter_5323_1714___fuseiter_5324_1715___fuseiter_5325_1716___fuseiter_5326_1717 / 48UL) % 3UL) * 3UL) + ((fused_0fused_0fused_0fused_0_fuseiter_5322___fuseiter_5323_1714___fuseiter_5324_1715___fuseiter_5325_1716___fuseiter_5326_1717 / 16UL) % 3UL))))];
      int8_t __cached_1;
      __cached_1 = __cached_0;
      __outs_0[(((fused_0fused_0fused_0fused_0_fuseiter_5322___fuseiter_5323_1714___fuseiter_5324_1715___fuseiter_5325_1716___fuseiter_5326_1717 / 288UL) * 147456UL) + ((((fused_0fused_0fused_0fused_0_fuseiter_5322___fuseiter_5323_1714___fuseiter_5324_1715___fuseiter_5325_1716___fuseiter_5326_1717 / 144UL) % 2UL) * 73728UL) + ((((fused_0fused_0fused_0fused_0_fuseiter_5322___fuseiter_5323_1714___fuseiter_5324_1715___fuseiter_5325_1716___fuseiter_5326_1717 / 48UL) % 3UL) * 24576UL) + ((((fused_0fused_0fused_0fused_0_fuseiter_5322___fuseiter_5323_1714___fuseiter_5324_1715___fuseiter_5325_1716___fuseiter_5326_1717 / 16UL) % 3UL) * 8192UL) + (((fused_0fused_0fused_0fused_0_fuseiter_5322___fuseiter_5323_1714___fuseiter_5324_1715___fuseiter_5325_1716___fuseiter_5326_1717 % 16UL) * 512UL) + ((_fuseiter_5327 * 4UL) + _fuseiter_5328))))))] = __cached_1;
    }
  }
}

static void reorder__4600_closure_267_0wrapper(void* __stream, int8_t* __restrict__ __module_data, uint64_t i, generic_val* __restrict__ args) noexcept{
  reorder__4600_closure_267(i, (int8_t*)(args[0UL].v_ptr), (int8_t*)(args[1UL].v_ptr));
}

static void reorder__4510_closure_268(uint64_t fused_0fused_0fused_0_fuseiter_5329___fuseiter_5330_1718___fuseiter_5331_1719___fuseiter_5332_1720, int8_t* __restrict__ __ins_0, int8_t* __restrict__ __outs_0) noexcept{
  for (uint64_t _fuseiter_5333 = 0UL; _fuseiter_5333 < 32UL; _fuseiter_5333 += 1UL) {
    for (uint64_t _fuseiter_5334 = 0UL; _fuseiter_5334 < 32UL; _fuseiter_5334 += 1UL) {
      for (uint64_t _fuseiter_5335 = 0UL; _fuseiter_5335 < 4UL; _fuseiter_5335 += 1UL) {
        int8_t __cached_0;
        __cached_0 = __ins_0[(((_fuseiter_5334 + ((fused_0fused_0fused_0_fuseiter_5329___fuseiter_5330_1718___fuseiter_5331_1719___fuseiter_5332_1720 / 9UL) * 32UL)) * 1152UL) + (((_fuseiter_5335 + (_fuseiter_5333 * 4UL)) * 9UL) + ((((fused_0fused_0fused_0_fuseiter_5329___fuseiter_5330_1718___fuseiter_5331_1719___fuseiter_5332_1720 / 3UL) % 3UL) * 3UL) + (fused_0fused_0fused_0_fuseiter_5329___fuseiter_5330_1718___fuseiter_5331_1719___fuseiter_5332_1720 % 3UL))))];
        int8_t __cached_1;
        __cached_1 = __cached_0;
        __outs_0[(((fused_0fused_0fused_0_fuseiter_5329___fuseiter_5330_1718___fuseiter_5331_1719___fuseiter_5332_1720 / 9UL) * 36864UL) + ((((fused_0fused_0fused_0_fuseiter_5329___fuseiter_5330_1718___fuseiter_5331_1719___fuseiter_5332_1720 / 3UL) % 3UL) * 12288UL) + (((fused_0fused_0fused_0_fuseiter_5329___fuseiter_5330_1718___fuseiter_5331_1719___fuseiter_5332_1720 % 3UL) * 4096UL) + ((_fuseiter_5333 * 128UL) + ((_fuseiter_5334 * 4UL) + _fuseiter_5335)))))] = __cached_1;
      }
    }
  }
}

static void reorder__4510_closure_268_0wrapper(void* __stream, int8_t* __restrict__ __module_data, uint64_t i, generic_val* __restrict__ args) noexcept{
  reorder__4510_closure_268(i, (int8_t*)(args[0UL].v_ptr), (int8_t*)(args[1UL].v_ptr));
}

static void reorder__4830_closure_269(uint64_t fused_0fused_0fused_0fused_0_fuseiter_5336___fuseiter_5337_1721___fuseiter_5338_1722___fuseiter_5339_1723___fuseiter_5340_1724, int8_t* __restrict__ __ins_0, int8_t* __restrict__ __outs_0) noexcept{
  for (uint64_t _fuseiter_5341 = 0UL; _fuseiter_5341 < 256UL; _fuseiter_5341 += 1UL) {
    for (uint64_t _fuseiter_5342 = 0UL; _fuseiter_5342 < 4UL; _fuseiter_5342 += 1UL) {
      int8_t __cached_0;
      __cached_0 = __ins_0[(((_fuseiter_5341 + ((fused_0fused_0fused_0fused_0_fuseiter_5336___fuseiter_5337_1721___fuseiter_5338_1722___fuseiter_5339_1723___fuseiter_5340_1724 / 128UL) * 256UL)) * 512UL) + ((_fuseiter_5342 + ((fused_0fused_0fused_0fused_0_fuseiter_5336___fuseiter_5337_1721___fuseiter_5338_1722___fuseiter_5339_1723___fuseiter_5340_1724 % 32UL) * 4UL)) + (((fused_0fused_0fused_0fused_0_fuseiter_5336___fuseiter_5337_1721___fuseiter_5338_1722___fuseiter_5339_1723___fuseiter_5340_1724 / 32UL) % 4UL) * 128UL)))];
      int8_t __cached_1;
      __cached_1 = __cached_0;
      __outs_0[(((fused_0fused_0fused_0fused_0_fuseiter_5336___fuseiter_5337_1721___fuseiter_5338_1722___fuseiter_5339_1723___fuseiter_5340_1724 / 128UL) * 131072UL) + ((((fused_0fused_0fused_0fused_0_fuseiter_5336___fuseiter_5337_1721___fuseiter_5338_1722___fuseiter_5339_1723___fuseiter_5340_1724 / 32UL) % 4UL) * 32768UL) + (((fused_0fused_0fused_0fused_0_fuseiter_5336___fuseiter_5337_1721___fuseiter_5338_1722___fuseiter_5339_1723___fuseiter_5340_1724 % 32UL) * 1024UL) + ((_fuseiter_5341 * 4UL) + _fuseiter_5342))))] = __cached_1;
    }
  }
}

static void reorder__4830_closure_269_0wrapper(void* __stream, int8_t* __restrict__ __module_data, uint64_t i, generic_val* __restrict__ args) noexcept{
  reorder__4830_closure_269(i, (int8_t*)(args[0UL].v_ptr), (int8_t*)(args[1UL].v_ptr));
}

static void reorder__4450_closure_270(uint64_t fused_0_fuseiter_5343___fuseiter_5344_1725, int8_t* __restrict__ __ins_0, int8_t* __restrict__ __outs_0) noexcept{
  for (uint64_t _fuseiter_5347 = 0UL; _fuseiter_5347 < 32UL; _fuseiter_5347 += 1UL) {
    for (uint64_t _fuseiter_5348 = 0UL; _fuseiter_5348 < 32UL; _fuseiter_5348 += 1UL) {
      for (uint64_t _fuseiter_5349 = 0UL; _fuseiter_5349 < 4UL; _fuseiter_5349 += 1UL) {
        int8_t __cached_0;
        __cached_0 = __ins_0[(((_fuseiter_5348 + ((fused_0_fuseiter_5343___fuseiter_5344_1725 / 2UL) * 32UL)) * 256UL) + ((_fuseiter_5349 + (_fuseiter_5347 * 4UL)) + ((fused_0_fuseiter_5343___fuseiter_5344_1725 % 2UL) * 128UL)))];
        int8_t __cached_1;
        __cached_1 = __cached_0;
        __outs_0[(((fused_0_fuseiter_5343___fuseiter_5344_1725 / 2UL) * 8192UL) + (((fused_0_fuseiter_5343___fuseiter_5344_1725 % 2UL) * 4096UL) + ((_fuseiter_5347 * 128UL) + ((_fuseiter_5348 * 4UL) + _fuseiter_5349))))] = __cached_1;
      }
    }
  }
}

static void reorder__4450_closure_270_0wrapper(void* __stream, int8_t* __restrict__ __module_data, uint64_t i, generic_val* __restrict__ args) noexcept{
  reorder__4450_closure_270(i, (int8_t*)(args[0UL].v_ptr), (int8_t*)(args[1UL].v_ptr));
}

static void reorder__4710_closure_271(uint64_t fused_0fused_0fused_0fused_0_fuseiter_5350___fuseiter_5351_1726___fuseiter_5352_1727___fuseiter_5353_1728___fuseiter_5354_1729, int8_t* __restrict__ __ins_0, int8_t* __restrict__ __outs_0) noexcept{
  for (uint64_t _fuseiter_5355 = 0UL; _fuseiter_5355 < 64UL; _fuseiter_5355 += 1UL) {
    for (uint64_t _fuseiter_5356 = 0UL; _fuseiter_5356 < 4UL; _fuseiter_5356 += 1UL) {
      int8_t __cached_0;
      __cached_0 = __ins_0[(((_fuseiter_5355 + ((fused_0fused_0fused_0fused_0_fuseiter_5350___fuseiter_5351_1726___fuseiter_5352_1727___fuseiter_5353_1728___fuseiter_5354_1729 / 128UL) * 64UL)) * 512UL) + ((_fuseiter_5356 + ((fused_0fused_0fused_0fused_0_fuseiter_5350___fuseiter_5351_1726___fuseiter_5352_1727___fuseiter_5353_1728___fuseiter_5354_1729 % 32UL) * 4UL)) + (((fused_0fused_0fused_0fused_0_fuseiter_5350___fuseiter_5351_1726___fuseiter_5352_1727___fuseiter_5353_1728___fuseiter_5354_1729 / 32UL) % 4UL) * 128UL)))];
      int8_t __cached_1;
      __cached_1 = __cached_0;
      __outs_0[(((fused_0fused_0fused_0fused_0_fuseiter_5350___fuseiter_5351_1726___fuseiter_5352_1727___fuseiter_5353_1728___fuseiter_5354_1729 / 128UL) * 32768UL) + ((((fused_0fused_0fused_0fused_0_fuseiter_5350___fuseiter_5351_1726___fuseiter_5352_1727___fuseiter_5353_1728___fuseiter_5354_1729 / 32UL) % 4UL) * 8192UL) + (((fused_0fused_0fused_0fused_0_fuseiter_5350___fuseiter_5351_1726___fuseiter_5352_1727___fuseiter_5353_1728___fuseiter_5354_1729 % 32UL) * 256UL) + ((_fuseiter_5355 * 4UL) + _fuseiter_5356))))] = __cached_1;
    }
  }
}

static void reorder__4710_closure_271_0wrapper(void* __stream, int8_t* __restrict__ __module_data, uint64_t i, generic_val* __restrict__ args) noexcept{
  reorder__4710_closure_271(i, (int8_t*)(args[0UL].v_ptr), (int8_t*)(args[1UL].v_ptr));
}

static void reorder__4640_closure_272(uint64_t fused_0fused_0fused_0fused_0_fuseiter_5357___fuseiter_5358_1730___fuseiter_5359_1731___fuseiter_5360_1732___fuseiter_5361_1733, int8_t* __restrict__ __ins_0, int8_t* __restrict__ __outs_0) noexcept{
  for (uint64_t _fuseiter_5362 = 0UL; _fuseiter_5362 < 128UL; _fuseiter_5362 += 1UL) {
    for (uint64_t _fuseiter_5363 = 0UL; _fuseiter_5363 < 4UL; _fuseiter_5363 += 1UL) {
      int8_t __cached_0;
      __cached_0 = __ins_0[(((_fuseiter_5362 + ((fused_0fused_0fused_0fused_0_fuseiter_5357___fuseiter_5358_1730___fuseiter_5359_1731___fuseiter_5360_1732___fuseiter_5361_1733 / 128UL) * 128UL)) * 512UL) + ((_fuseiter_5363 + ((fused_0fused_0fused_0fused_0_fuseiter_5357___fuseiter_5358_1730___fuseiter_5359_1731___fuseiter_5360_1732___fuseiter_5361_1733 % 16UL) * 4UL)) + (((fused_0fused_0fused_0fused_0_fuseiter_5357___fuseiter_5358_1730___fuseiter_5359_1731___fuseiter_5360_1732___fuseiter_5361_1733 / 16UL) % 8UL) * 64UL)))];
      int8_t __cached_1;
      __cached_1 = __cached_0;
      __outs_0[(((fused_0fused_0fused_0fused_0_fuseiter_5357___fuseiter_5358_1730___fuseiter_5359_1731___fuseiter_5360_1732___fuseiter_5361_1733 / 128UL) * 65536UL) + ((((fused_0fused_0fused_0fused_0_fuseiter_5357___fuseiter_5358_1730___fuseiter_5359_1731___fuseiter_5360_1732___fuseiter_5361_1733 / 16UL) % 8UL) * 8192UL) + (((fused_0fused_0fused_0fused_0_fuseiter_5357___fuseiter_5358_1730___fuseiter_5359_1731___fuseiter_5360_1732___fuseiter_5361_1733 % 16UL) * 512UL) + ((_fuseiter_5362 * 4UL) + _fuseiter_5363))))] = __cached_1;
    }
  }
}

static void reorder__4640_closure_272_0wrapper(void* __stream, int8_t* __restrict__ __module_data, uint64_t i, generic_val* __restrict__ args) noexcept{
  reorder__4640_closure_272(i, (int8_t*)(args[0UL].v_ptr), (int8_t*)(args[1UL].v_ptr));
}

static void reorder__4570_closure_273(uint64_t fused_0fused_0fused_0fused_0_fuseiter_5364___fuseiter_5365_1734___fuseiter_5366_1735___fuseiter_5367_1736___fuseiter_5368_1737, int8_t* __restrict__ __ins_0, int8_t* __restrict__ __outs_0) noexcept{
  for (uint64_t _fuseiter_5369 = 0UL; _fuseiter_5369 < 32UL; _fuseiter_5369 += 1UL) {
    for (uint64_t _fuseiter_5370 = 0UL; _fuseiter_5370 < 4UL; _fuseiter_5370 += 1UL) {
      int8_t __cached_0;
      __cached_0 = __ins_0[(((_fuseiter_5369 + ((fused_0fused_0fused_0fused_0_fuseiter_5364___fuseiter_5365_1734___fuseiter_5366_1735___fuseiter_5367_1736___fuseiter_5368_1737 / 128UL) * 32UL)) * 512UL) + ((_fuseiter_5370 + ((fused_0fused_0fused_0fused_0_fuseiter_5364___fuseiter_5365_1734___fuseiter_5366_1735___fuseiter_5367_1736___fuseiter_5368_1737 % 32UL) * 4UL)) + (((fused_0fused_0fused_0fused_0_fuseiter_5364___fuseiter_5365_1734___fuseiter_5366_1735___fuseiter_5367_1736___fuseiter_5368_1737 / 32UL) % 4UL) * 128UL)))];
      int8_t __cached_1;
      __cached_1 = __cached_0;
      __outs_0[(((fused_0fused_0fused_0fused_0_fuseiter_5364___fuseiter_5365_1734___fuseiter_5366_1735___fuseiter_5367_1736___fuseiter_5368_1737 / 128UL) * 16384UL) + ((((fused_0fused_0fused_0fused_0_fuseiter_5364___fuseiter_5365_1734___fuseiter_5366_1735___fuseiter_5367_1736___fuseiter_5368_1737 / 32UL) % 4UL) * 4096UL) + (((fused_0fused_0fused_0fused_0_fuseiter_5364___fuseiter_5365_1734___fuseiter_5366_1735___fuseiter_5367_1736___fuseiter_5368_1737 % 32UL) * 128UL) + ((_fuseiter_5369 * 4UL) + _fuseiter_5370))))] = __cached_1;
    }
  }
}

static void reorder__4570_closure_273_0wrapper(void* __stream, int8_t* __restrict__ __module_data, uint64_t i, generic_val* __restrict__ args) noexcept{
  reorder__4570_closure_273(i, (int8_t*)(args[0UL].v_ptr), (int8_t*)(args[1UL].v_ptr));
}

static void reorder__4770_closure_274(uint64_t fused_0fused_0fused_0fused_0_fuseiter_5371___fuseiter_5372_1738___fuseiter_5373_1739___fuseiter_5374_1740___fuseiter_5375_1741, int8_t* __restrict__ __ins_0, int8_t* __restrict__ __outs_0) noexcept{
  for (uint64_t _fuseiter_5376 = 0UL; _fuseiter_5376 < 128UL; _fuseiter_5376 += 1UL) {
    for (uint64_t _fuseiter_5377 = 0UL; _fuseiter_5377 < 4UL; _fuseiter_5377 += 1UL) {
      int8_t __cached_0;
      __cached_0 = __ins_0[(((_fuseiter_5376 + ((fused_0fused_0fused_0fused_0_fuseiter_5371___fuseiter_5372_1738___fuseiter_5373_1739___fuseiter_5374_1740___fuseiter_5375_1741 / 32UL) * 128UL)) * 128UL) + ((_fuseiter_5377 + ((fused_0fused_0fused_0fused_0_fuseiter_5371___fuseiter_5372_1738___fuseiter_5373_1739___fuseiter_5374_1740___fuseiter_5375_1741 % 8UL) * 4UL)) + (((fused_0fused_0fused_0fused_0_fuseiter_5371___fuseiter_5372_1738___fuseiter_5373_1739___fuseiter_5374_1740___fuseiter_5375_1741 / 8UL) % 4UL) * 32UL)))];
      int8_t __cached_1;
      __cached_1 = __cached_0;
      __outs_0[(((fused_0fused_0fused_0fused_0_fuseiter_5371___fuseiter_5372_1738___fuseiter_5373_1739___fuseiter_5374_1740___fuseiter_5375_1741 / 32UL) * 16384UL) + ((((fused_0fused_0fused_0fused_0_fuseiter_5371___fuseiter_5372_1738___fuseiter_5373_1739___fuseiter_5374_1740___fuseiter_5375_1741 / 8UL) % 4UL) * 4096UL) + (((fused_0fused_0fused_0fused_0_fuseiter_5371___fuseiter_5372_1738___fuseiter_5373_1739___fuseiter_5374_1740___fuseiter_5375_1741 % 8UL) * 512UL) + ((_fuseiter_5376 * 4UL) + _fuseiter_5377))))] = __cached_1;
    }
  }
}

static void reorder__4770_closure_274_0wrapper(void* __stream, int8_t* __restrict__ __module_data, uint64_t i, generic_val* __restrict__ args) noexcept{
  reorder__4770_closure_274(i, (int8_t*)(args[0UL].v_ptr), (int8_t*)(args[1UL].v_ptr));
}

static void reorder__4680_closure_275(uint64_t fused_0fused_0fused_0fused_0_fuseiter_5378___fuseiter_5379_1742___fuseiter_5380_1743___fuseiter_5381_1744___fuseiter_5382_1745, int8_t* __restrict__ __ins_0, int8_t* __restrict__ __outs_0) noexcept{
  for (uint64_t _fuseiter_5383 = 0UL; _fuseiter_5383 < 64UL; _fuseiter_5383 += 1UL) {
    for (uint64_t _fuseiter_5384 = 0UL; _fuseiter_5384 < 4UL; _fuseiter_5384 += 1UL) {
      int8_t __cached_0;
      __cached_0 = __ins_0[(((_fuseiter_5383 + ((fused_0fused_0fused_0fused_0_fuseiter_5378___fuseiter_5379_1742___fuseiter_5380_1743___fuseiter_5381_1744___fuseiter_5382_1745 / 32UL) * 64UL)) * 128UL) + (_fuseiter_5384 + ((fused_0fused_0fused_0fused_0_fuseiter_5378___fuseiter_5379_1742___fuseiter_5380_1743___fuseiter_5381_1744___fuseiter_5382_1745 % 32UL) * 4UL)))];
      int8_t __cached_1;
      __cached_1 = __cached_0;
      __outs_0[(((fused_0fused_0fused_0fused_0_fuseiter_5378___fuseiter_5379_1742___fuseiter_5380_1743___fuseiter_5381_1744___fuseiter_5382_1745 / 32UL) * 8192UL) + (((fused_0fused_0fused_0fused_0_fuseiter_5378___fuseiter_5379_1742___fuseiter_5380_1743___fuseiter_5381_1744___fuseiter_5382_1745 % 32UL) * 256UL) + ((_fuseiter_5383 * 4UL) + _fuseiter_5384)))] = __cached_1;
    }
  }
}

static void reorder__4680_closure_275_0wrapper(void* __stream, int8_t* __restrict__ __module_data, uint64_t i, generic_val* __restrict__ args) noexcept{
  reorder__4680_closure_275(i, (int8_t*)(args[0UL].v_ptr), (int8_t*)(args[1UL].v_ptr));
}

static void reorder__4610_closure_276(uint64_t fused_0fused_0fused_0fused_0_fuseiter_5385___fuseiter_5386_1746___fuseiter_5387_1747___fuseiter_5388_1748___fuseiter_5389_1749, int8_t* __restrict__ __ins_0, int8_t* __restrict__ __outs_0) noexcept{
  for (uint64_t _fuseiter_5390 = 0UL; _fuseiter_5390 < 128UL; _fuseiter_5390 += 1UL) {
    for (uint64_t _fuseiter_5391 = 0UL; _fuseiter_5391 < 4UL; _fuseiter_5391 += 1UL) {
      int8_t __cached_0;
      __cached_0 = __ins_0[(((_fuseiter_5390 + ((fused_0fused_0fused_0fused_0_fuseiter_5385___fuseiter_5386_1746___fuseiter_5387_1747___fuseiter_5388_1748___fuseiter_5389_1749 / 32UL) * 128UL)) * 128UL) + ((_fuseiter_5391 + ((fused_0fused_0fused_0fused_0_fuseiter_5385___fuseiter_5386_1746___fuseiter_5387_1747___fuseiter_5388_1748___fuseiter_5389_1749 % 16UL) * 4UL)) + (((fused_0fused_0fused_0fused_0_fuseiter_5385___fuseiter_5386_1746___fuseiter_5387_1747___fuseiter_5388_1748___fuseiter_5389_1749 / 16UL) % 2UL) * 64UL)))];
      int8_t __cached_1;
      __cached_1 = __cached_0;
      __outs_0[(((fused_0fused_0fused_0fused_0_fuseiter_5385___fuseiter_5386_1746___fuseiter_5387_1747___fuseiter_5388_1748___fuseiter_5389_1749 / 32UL) * 16384UL) + ((((fused_0fused_0fused_0fused_0_fuseiter_5385___fuseiter_5386_1746___fuseiter_5387_1747___fuseiter_5388_1748___fuseiter_5389_1749 / 16UL) % 2UL) * 8192UL) + (((fused_0fused_0fused_0fused_0_fuseiter_5385___fuseiter_5386_1746___fuseiter_5387_1747___fuseiter_5388_1748___fuseiter_5389_1749 % 16UL) * 512UL) + ((_fuseiter_5390 * 4UL) + _fuseiter_5391))))] = __cached_1;
    }
  }
}

static void reorder__4610_closure_276_0wrapper(void* __stream, int8_t* __restrict__ __module_data, uint64_t i, generic_val* __restrict__ args) noexcept{
  reorder__4610_closure_276(i, (int8_t*)(args[0UL].v_ptr), (int8_t*)(args[1UL].v_ptr));
}

static void reorder__4540_closure_277(uint64_t fused_0_fuseiter_5392___fuseiter_5393_1750, int8_t* __restrict__ __ins_0, int8_t* __restrict__ __outs_0) noexcept{
  for (uint64_t _fuseiter_5396 = 0UL; _fuseiter_5396 < 8UL; _fuseiter_5396 += 1UL) {
    for (uint64_t _fuseiter_5397 = 0UL; _fuseiter_5397 < 32UL; _fuseiter_5397 += 1UL) {
      for (uint64_t _fuseiter_5398 = 0UL; _fuseiter_5398 < 4UL; _fuseiter_5398 += 1UL) {
        int8_t __cached_0;
        __cached_0 = __ins_0[(((_fuseiter_5397 + ((fused_0_fuseiter_5392___fuseiter_5393_1750 / 4UL) * 32UL)) * 128UL) + ((_fuseiter_5398 + (_fuseiter_5396 * 4UL)) + ((fused_0_fuseiter_5392___fuseiter_5393_1750 % 4UL) * 32UL)))];
        int8_t __cached_1;
        __cached_1 = __cached_0;
        __outs_0[(((fused_0_fuseiter_5392___fuseiter_5393_1750 / 4UL) * 4096UL) + (((fused_0_fuseiter_5392___fuseiter_5393_1750 % 4UL) * 1024UL) + ((_fuseiter_5396 * 128UL) + ((_fuseiter_5397 * 4UL) + _fuseiter_5398))))] = __cached_1;
      }
    }
  }
}

static void reorder__4540_closure_277_0wrapper(void* __stream, int8_t* __restrict__ __module_data, uint64_t i, generic_val* __restrict__ args) noexcept{
  reorder__4540_closure_277(i, (int8_t*)(args[0UL].v_ptr), (int8_t*)(args[1UL].v_ptr));
}

static void reorder__4390_closure_278(uint64_t fused_0fused_0fused_0fused_0_fuseiter_5399___fuseiter_5400_1751___fuseiter_5401_1752___fuseiter_5402_1753___fuseiter_5403_1754, int8_t* __restrict__ __ins_0, int8_t* __restrict__ __outs_0) noexcept{
  for (uint64_t _fuseiter_5404 = 0UL; _fuseiter_5404 < 32UL; _fuseiter_5404 += 1UL) {
    for (uint64_t _fuseiter_5405 = 0UL; _fuseiter_5405 < 4UL; _fuseiter_5405 += 1UL) {
      int8_t __cached_0;
      __cached_0 = __ins_0[(((_fuseiter_5404 + ((fused_0fused_0fused_0fused_0_fuseiter_5399___fuseiter_5400_1751___fuseiter_5401_1752___fuseiter_5402_1753___fuseiter_5403_1754 / 144UL) * 32UL)) * 576UL) + (((_fuseiter_5405 + ((fused_0fused_0fused_0fused_0_fuseiter_5399___fuseiter_5400_1751___fuseiter_5401_1752___fuseiter_5402_1753___fuseiter_5403_1754 % 16UL) * 4UL)) * 9UL) + ((((fused_0fused_0fused_0fused_0_fuseiter_5399___fuseiter_5400_1751___fuseiter_5401_1752___fuseiter_5402_1753___fuseiter_5403_1754 / 48UL) % 3UL) * 3UL) + ((fused_0fused_0fused_0fused_0_fuseiter_5399___fuseiter_5400_1751___fuseiter_5401_1752___fuseiter_5402_1753___fuseiter_5403_1754 / 16UL) % 3UL))))];
      int8_t __cached_1;
      __cached_1 = __cached_0;
      __outs_0[(((fused_0fused_0fused_0fused_0_fuseiter_5399___fuseiter_5400_1751___fuseiter_5401_1752___fuseiter_5402_1753___fuseiter_5403_1754 / 144UL) * 18432UL) + ((((fused_0fused_0fused_0fused_0_fuseiter_5399___fuseiter_5400_1751___fuseiter_5401_1752___fuseiter_5402_1753___fuseiter_5403_1754 / 48UL) % 3UL) * 6144UL) + ((((fused_0fused_0fused_0fused_0_fuseiter_5399___fuseiter_5400_1751___fuseiter_5401_1752___fuseiter_5402_1753___fuseiter_5403_1754 / 16UL) % 3UL) * 2048UL) + (((fused_0fused_0fused_0fused_0_fuseiter_5399___fuseiter_5400_1751___fuseiter_5401_1752___fuseiter_5402_1753___fuseiter_5403_1754 % 16UL) * 128UL) + ((_fuseiter_5404 * 4UL) + _fuseiter_5405)))))] = __cached_1;
    }
  }
}

static void reorder__4390_closure_278_0wrapper(void* __stream, int8_t* __restrict__ __module_data, uint64_t i, generic_val* __restrict__ args) noexcept{
  reorder__4390_closure_278(i, (int8_t*)(args[0UL].v_ptr), (int8_t*)(args[1UL].v_ptr));
}

static void reorder__4300_closure_279(uint64_t fused_0fused_0fused_0_fuseiter_5406___fuseiter_5407_1755___fuseiter_5408_1756___fuseiter_5409_1757, int8_t* __restrict__ __ins_0, int8_t* __restrict__ __outs_0) noexcept{
  for (uint64_t _fuseiter_5410 = 0UL; _fuseiter_5410 < 16UL; _fuseiter_5410 += 1UL) {
    for (uint64_t _fuseiter_5411 = 0UL; _fuseiter_5411 < 16UL; _fuseiter_5411 += 1UL) {
      for (uint64_t _fuseiter_5412 = 0UL; _fuseiter_5412 < 4UL; _fuseiter_5412 += 1UL) {
        int8_t __cached_0;
        __cached_0 = __ins_0[(((_fuseiter_5411 + ((fused_0fused_0fused_0_fuseiter_5406___fuseiter_5407_1755___fuseiter_5408_1756___fuseiter_5409_1757 / 9UL) * 16UL)) * 576UL) + (((_fuseiter_5412 + (_fuseiter_5410 * 4UL)) * 9UL) + ((((fused_0fused_0fused_0_fuseiter_5406___fuseiter_5407_1755___fuseiter_5408_1756___fuseiter_5409_1757 / 3UL) % 3UL) * 3UL) + (fused_0fused_0fused_0_fuseiter_5406___fuseiter_5407_1755___fuseiter_5408_1756___fuseiter_5409_1757 % 3UL))))];
        int8_t __cached_1;
        __cached_1 = __cached_0;
        __outs_0[(((fused_0fused_0fused_0_fuseiter_5406___fuseiter_5407_1755___fuseiter_5408_1756___fuseiter_5409_1757 / 9UL) * 9216UL) + ((((fused_0fused_0fused_0_fuseiter_5406___fuseiter_5407_1755___fuseiter_5408_1756___fuseiter_5409_1757 / 3UL) % 3UL) * 3072UL) + (((fused_0fused_0fused_0_fuseiter_5406___fuseiter_5407_1755___fuseiter_5408_1756___fuseiter_5409_1757 % 3UL) * 1024UL) + ((_fuseiter_5410 * 64UL) + ((_fuseiter_5411 * 4UL) + _fuseiter_5412)))))] = __cached_1;
      }
    }
  }
}

static void reorder__4300_closure_279_0wrapper(void* __stream, int8_t* __restrict__ __module_data, uint64_t i, generic_val* __restrict__ args) noexcept{
  reorder__4300_closure_279(i, (int8_t*)(args[0UL].v_ptr), (int8_t*)(args[1UL].v_ptr));
}

static void reorder__4230_closure_280(uint64_t fused_0fused_0fused_0fused_0_fuseiter_5413___fuseiter_5414_1758___fuseiter_5415_1759___fuseiter_5416_1760___fuseiter_5417_1761, int8_t* __restrict__ __ins_0, int8_t* __restrict__ __outs_0) noexcept{
  for (uint64_t _fuseiter_5418 = 0UL; _fuseiter_5418 < 32UL; _fuseiter_5418 += 1UL) {
    for (uint64_t _fuseiter_5419 = 0UL; _fuseiter_5419 < 4UL; _fuseiter_5419 += 1UL) {
      int8_t __cached_0;
      __cached_0 = __ins_0[(((_fuseiter_5418 + ((fused_0fused_0fused_0fused_0_fuseiter_5413___fuseiter_5414_1758___fuseiter_5415_1759___fuseiter_5416_1760___fuseiter_5417_1761 / 144UL) * 32UL)) * 576UL) + (((_fuseiter_5419 + ((fused_0fused_0fused_0fused_0_fuseiter_5413___fuseiter_5414_1758___fuseiter_5415_1759___fuseiter_5416_1760___fuseiter_5417_1761 % 16UL) * 4UL)) * 9UL) + ((((fused_0fused_0fused_0fused_0_fuseiter_5413___fuseiter_5414_1758___fuseiter_5415_1759___fuseiter_5416_1760___fuseiter_5417_1761 / 48UL) % 3UL) * 3UL) + ((fused_0fused_0fused_0fused_0_fuseiter_5413___fuseiter_5414_1758___fuseiter_5415_1759___fuseiter_5416_1760___fuseiter_5417_1761 / 16UL) % 3UL))))];
      int8_t __cached_1;
      __cached_1 = __cached_0;
      __outs_0[(((fused_0fused_0fused_0fused_0_fuseiter_5413___fuseiter_5414_1758___fuseiter_5415_1759___fuseiter_5416_1760___fuseiter_5417_1761 / 144UL) * 18432UL) + ((((fused_0fused_0fused_0fused_0_fuseiter_5413___fuseiter_5414_1758___fuseiter_5415_1759___fuseiter_5416_1760___fuseiter_5417_1761 / 48UL) % 3UL) * 6144UL) + ((((fused_0fused_0fused_0fused_0_fuseiter_5413___fuseiter_5414_1758___fuseiter_5415_1759___fuseiter_5416_1760___fuseiter_5417_1761 / 16UL) % 3UL) * 2048UL) + (((fused_0fused_0fused_0fused_0_fuseiter_5413___fuseiter_5414_1758___fuseiter_5415_1759___fuseiter_5416_1760___fuseiter_5417_1761 % 16UL) * 128UL) + ((_fuseiter_5418 * 4UL) + _fuseiter_5419)))))] = __cached_1;
    }
  }
}

static void reorder__4230_closure_280_0wrapper(void* __stream, int8_t* __restrict__ __module_data, uint64_t i, generic_val* __restrict__ args) noexcept{
  reorder__4230_closure_280(i, (int8_t*)(args[0UL].v_ptr), (int8_t*)(args[1UL].v_ptr));
}

static void reorder__4480_closure_281(uint64_t fused_0fused_0fused_0fused_0_fuseiter_5420___fuseiter_5421_1762___fuseiter_5422_1763___fuseiter_5423_1764___fuseiter_5424_1765, int8_t* __restrict__ __ins_0, int8_t* __restrict__ __outs_0) noexcept{
  for (uint64_t _fuseiter_5425 = 0UL; _fuseiter_5425 < 64UL; _fuseiter_5425 += 1UL) {
    for (uint64_t _fuseiter_5426 = 0UL; _fuseiter_5426 < 4UL; _fuseiter_5426 += 1UL) {
      int8_t __cached_0;
      __cached_0 = __ins_0[(((_fuseiter_5425 + ((fused_0fused_0fused_0fused_0_fuseiter_5420___fuseiter_5421_1762___fuseiter_5422_1763___fuseiter_5423_1764___fuseiter_5424_1765 / 64UL) * 64UL)) * 256UL) + ((_fuseiter_5426 + ((fused_0fused_0fused_0fused_0_fuseiter_5420___fuseiter_5421_1762___fuseiter_5422_1763___fuseiter_5423_1764___fuseiter_5424_1765 % 32UL) * 4UL)) + (((fused_0fused_0fused_0fused_0_fuseiter_5420___fuseiter_5421_1762___fuseiter_5422_1763___fuseiter_5423_1764___fuseiter_5424_1765 / 32UL) % 2UL) * 128UL)))];
      int8_t __cached_1;
      __cached_1 = __cached_0;
      __outs_0[(((fused_0fused_0fused_0fused_0_fuseiter_5420___fuseiter_5421_1762___fuseiter_5422_1763___fuseiter_5423_1764___fuseiter_5424_1765 / 64UL) * 16384UL) + ((((fused_0fused_0fused_0fused_0_fuseiter_5420___fuseiter_5421_1762___fuseiter_5422_1763___fuseiter_5423_1764___fuseiter_5424_1765 / 32UL) % 2UL) * 8192UL) + (((fused_0fused_0fused_0fused_0_fuseiter_5420___fuseiter_5421_1762___fuseiter_5422_1763___fuseiter_5423_1764___fuseiter_5424_1765 % 32UL) * 256UL) + ((_fuseiter_5425 * 4UL) + _fuseiter_5426))))] = __cached_1;
    }
  }
}

static void reorder__4480_closure_281_0wrapper(void* __stream, int8_t* __restrict__ __module_data, uint64_t i, generic_val* __restrict__ args) noexcept{
  reorder__4480_closure_281(i, (int8_t*)(args[0UL].v_ptr), (int8_t*)(args[1UL].v_ptr));
}

static void reorder__4360_closure_282(uint64_t fused_0fused_0fused_0fused_0_fuseiter_5427___fuseiter_5428_1766___fuseiter_5429_1767___fuseiter_5430_1768___fuseiter_5431_1769, int8_t* __restrict__ __ins_0, int8_t* __restrict__ __outs_0) noexcept{
  for (uint64_t _fuseiter_5432 = 0UL; _fuseiter_5432 < 32UL; _fuseiter_5432 += 1UL) {
    for (uint64_t _fuseiter_5433 = 0UL; _fuseiter_5433 < 4UL; _fuseiter_5433 += 1UL) {
      int8_t __cached_0;
      __cached_0 = __ins_0[(((_fuseiter_5432 + ((fused_0fused_0fused_0fused_0_fuseiter_5427___fuseiter_5428_1766___fuseiter_5429_1767___fuseiter_5430_1768___fuseiter_5431_1769 / 64UL) * 32UL)) * 256UL) + ((_fuseiter_5433 + ((fused_0fused_0fused_0fused_0_fuseiter_5427___fuseiter_5428_1766___fuseiter_5429_1767___fuseiter_5430_1768___fuseiter_5431_1769 % 16UL) * 4UL)) + (((fused_0fused_0fused_0fused_0_fuseiter_5427___fuseiter_5428_1766___fuseiter_5429_1767___fuseiter_5430_1768___fuseiter_5431_1769 / 16UL) % 4UL) * 64UL)))];
      int8_t __cached_1;
      __cached_1 = __cached_0;
      __outs_0[(((fused_0fused_0fused_0fused_0_fuseiter_5427___fuseiter_5428_1766___fuseiter_5429_1767___fuseiter_5430_1768___fuseiter_5431_1769 / 64UL) * 8192UL) + ((((fused_0fused_0fused_0fused_0_fuseiter_5427___fuseiter_5428_1766___fuseiter_5429_1767___fuseiter_5430_1768___fuseiter_5431_1769 / 16UL) % 4UL) * 2048UL) + (((fused_0fused_0fused_0fused_0_fuseiter_5427___fuseiter_5428_1766___fuseiter_5429_1767___fuseiter_5430_1768___fuseiter_5431_1769 % 16UL) * 128UL) + ((_fuseiter_5432 * 4UL) + _fuseiter_5433))))] = __cached_1;
    }
  }
}

static void reorder__4360_closure_282_0wrapper(void* __stream, int8_t* __restrict__ __module_data, uint64_t i, generic_val* __restrict__ args) noexcept{
  reorder__4360_closure_282(i, (int8_t*)(args[0UL].v_ptr), (int8_t*)(args[1UL].v_ptr));
}

static void reorder__4290_closure_283(uint64_t fused_0fused_0fused_0fused_0_fuseiter_5434___fuseiter_5435_1770___fuseiter_5436_1771___fuseiter_5437_1772___fuseiter_5438_1773, int8_t* __restrict__ __ins_0, int8_t* __restrict__ __outs_0) noexcept{
  for (uint64_t _fuseiter_5439 = 0UL; _fuseiter_5439 < 64UL; _fuseiter_5439 += 1UL) {
    for (uint64_t _fuseiter_5440 = 0UL; _fuseiter_5440 < 4UL; _fuseiter_5440 += 1UL) {
      int8_t __cached_0;
      __cached_0 = __ins_0[(((_fuseiter_5439 + ((fused_0fused_0fused_0fused_0_fuseiter_5434___fuseiter_5435_1770___fuseiter_5436_1771___fuseiter_5437_1772___fuseiter_5438_1773 / 64UL) * 64UL)) * 256UL) + ((_fuseiter_5440 + ((fused_0fused_0fused_0fused_0_fuseiter_5434___fuseiter_5435_1770___fuseiter_5436_1771___fuseiter_5437_1772___fuseiter_5438_1773 % 8UL) * 4UL)) + (((fused_0fused_0fused_0fused_0_fuseiter_5434___fuseiter_5435_1770___fuseiter_5436_1771___fuseiter_5437_1772___fuseiter_5438_1773 / 8UL) % 8UL) * 32UL)))];
      int8_t __cached_1;
      __cached_1 = __cached_0;
      __outs_0[(((fused_0fused_0fused_0fused_0_fuseiter_5434___fuseiter_5435_1770___fuseiter_5436_1771___fuseiter_5437_1772___fuseiter_5438_1773 / 64UL) * 16384UL) + ((((fused_0fused_0fused_0fused_0_fuseiter_5434___fuseiter_5435_1770___fuseiter_5436_1771___fuseiter_5437_1772___fuseiter_5438_1773 / 8UL) % 8UL) * 2048UL) + (((fused_0fused_0fused_0fused_0_fuseiter_5434___fuseiter_5435_1770___fuseiter_5436_1771___fuseiter_5437_1772___fuseiter_5438_1773 % 8UL) * 256UL) + ((_fuseiter_5439 * 4UL) + _fuseiter_5440))))] = __cached_1;
    }
  }
}

static void reorder__4290_closure_283_0wrapper(void* __stream, int8_t* __restrict__ __module_data, uint64_t i, generic_val* __restrict__ args) noexcept{
  reorder__4290_closure_283(i, (int8_t*)(args[0UL].v_ptr), (int8_t*)(args[1UL].v_ptr));
}

static void reorder__4420_closure_284(uint64_t fused_0fused_0fused_0fused_0_fuseiter_5441___fuseiter_5442_1774___fuseiter_5443_1775___fuseiter_5444_1776___fuseiter_5445_1777, int8_t* __restrict__ __ins_0, int8_t* __restrict__ __outs_0) noexcept{
  for (uint64_t _fuseiter_5446 = 0UL; _fuseiter_5446 < 64UL; _fuseiter_5446 += 1UL) {
    for (uint64_t _fuseiter_5447 = 0UL; _fuseiter_5447 < 4UL; _fuseiter_5447 += 1UL) {
      int8_t __cached_0;
      __cached_0 = __ins_0[(((_fuseiter_5446 + ((fused_0fused_0fused_0fused_0_fuseiter_5441___fuseiter_5442_1774___fuseiter_5443_1775___fuseiter_5444_1776___fuseiter_5445_1777 / 16UL) * 64UL)) * 64UL) + (_fuseiter_5447 + ((fused_0fused_0fused_0fused_0_fuseiter_5441___fuseiter_5442_1774___fuseiter_5443_1775___fuseiter_5444_1776___fuseiter_5445_1777 % 16UL) * 4UL)))];
      int8_t __cached_1;
      __cached_1 = __cached_0;
      __outs_0[(((fused_0fused_0fused_0fused_0_fuseiter_5441___fuseiter_5442_1774___fuseiter_5443_1775___fuseiter_5444_1776___fuseiter_5445_1777 / 16UL) * 4096UL) + (((fused_0fused_0fused_0fused_0_fuseiter_5441___fuseiter_5442_1774___fuseiter_5443_1775___fuseiter_5444_1776___fuseiter_5445_1777 % 16UL) * 256UL) + ((_fuseiter_5446 * 4UL) + _fuseiter_5447)))] = __cached_1;
    }
  }
}

static void reorder__4420_closure_284_0wrapper(void* __stream, int8_t* __restrict__ __module_data, uint64_t i, generic_val* __restrict__ args) noexcept{
  reorder__4420_closure_284(i, (int8_t*)(args[0UL].v_ptr), (int8_t*)(args[1UL].v_ptr));
}

static void reorder__4330_closure_285(uint64_t fused_0fused_0fused_0fused_0_fuseiter_5448___fuseiter_5449_1778___fuseiter_5450_1779___fuseiter_5451_1780___fuseiter_5452_1781, int8_t* __restrict__ __ins_0, int8_t* __restrict__ __outs_0) noexcept{
  for (uint64_t _fuseiter_5453 = 0UL; _fuseiter_5453 < 32UL; _fuseiter_5453 += 1UL) {
    for (uint64_t _fuseiter_5454 = 0UL; _fuseiter_5454 < 4UL; _fuseiter_5454 += 1UL) {
      int8_t __cached_0;
      __cached_0 = __ins_0[(((_fuseiter_5453 + ((fused_0fused_0fused_0fused_0_fuseiter_5448___fuseiter_5449_1778___fuseiter_5450_1779___fuseiter_5451_1780___fuseiter_5452_1781 / 16UL) * 32UL)) * 64UL) + (_fuseiter_5454 + ((fused_0fused_0fused_0fused_0_fuseiter_5448___fuseiter_5449_1778___fuseiter_5450_1779___fuseiter_5451_1780___fuseiter_5452_1781 % 16UL) * 4UL)))];
      int8_t __cached_1;
      __cached_1 = __cached_0;
      __outs_0[(((fused_0fused_0fused_0fused_0_fuseiter_5448___fuseiter_5449_1778___fuseiter_5450_1779___fuseiter_5451_1780___fuseiter_5452_1781 / 16UL) * 2048UL) + (((fused_0fused_0fused_0fused_0_fuseiter_5448___fuseiter_5449_1778___fuseiter_5450_1779___fuseiter_5451_1780___fuseiter_5452_1781 % 16UL) * 128UL) + ((_fuseiter_5453 * 4UL) + _fuseiter_5454)))] = __cached_1;
    }
  }
}

static void reorder__4330_closure_285_0wrapper(void* __stream, int8_t* __restrict__ __module_data, uint64_t i, generic_val* __restrict__ args) noexcept{
  reorder__4330_closure_285(i, (int8_t*)(args[0UL].v_ptr), (int8_t*)(args[1UL].v_ptr));
}

static void reorder__4260_closure_286(uint64_t fused_0fused_0fused_0fused_0_fuseiter_5455___fuseiter_5456_1782___fuseiter_5457_1783___fuseiter_5458_1784___fuseiter_5459_1785, int8_t* __restrict__ __ins_0, int8_t* __restrict__ __outs_0) noexcept{
  for (uint64_t _fuseiter_5460 = 0UL; _fuseiter_5460 < 64UL; _fuseiter_5460 += 1UL) {
    for (uint64_t _fuseiter_5461 = 0UL; _fuseiter_5461 < 4UL; _fuseiter_5461 += 1UL) {
      int8_t __cached_0;
      __cached_0 = __ins_0[(((_fuseiter_5460 + ((fused_0fused_0fused_0fused_0_fuseiter_5455___fuseiter_5456_1782___fuseiter_5457_1783___fuseiter_5458_1784___fuseiter_5459_1785 / 16UL) * 64UL)) * 64UL) + ((_fuseiter_5461 + ((fused_0fused_0fused_0fused_0_fuseiter_5455___fuseiter_5456_1782___fuseiter_5457_1783___fuseiter_5458_1784___fuseiter_5459_1785 % 8UL) * 4UL)) + (((fused_0fused_0fused_0fused_0_fuseiter_5455___fuseiter_5456_1782___fuseiter_5457_1783___fuseiter_5458_1784___fuseiter_5459_1785 / 8UL) % 2UL) * 32UL)))];
      int8_t __cached_1;
      __cached_1 = __cached_0;
      __outs_0[(((fused_0fused_0fused_0fused_0_fuseiter_5455___fuseiter_5456_1782___fuseiter_5457_1783___fuseiter_5458_1784___fuseiter_5459_1785 / 16UL) * 4096UL) + ((((fused_0fused_0fused_0fused_0_fuseiter_5455___fuseiter_5456_1782___fuseiter_5457_1783___fuseiter_5458_1784___fuseiter_5459_1785 / 8UL) % 2UL) * 2048UL) + (((fused_0fused_0fused_0fused_0_fuseiter_5455___fuseiter_5456_1782___fuseiter_5457_1783___fuseiter_5458_1784___fuseiter_5459_1785 % 8UL) * 256UL) + ((_fuseiter_5460 * 4UL) + _fuseiter_5461))))] = __cached_1;
    }
  }
}

static void reorder__4260_closure_286_0wrapper(void* __stream, int8_t* __restrict__ __module_data, uint64_t i, generic_val* __restrict__ args) noexcept{
  reorder__4260_closure_286(i, (int8_t*)(args[0UL].v_ptr), (int8_t*)(args[1UL].v_ptr));
}

static void reorder__4190_closure_287(uint64_t fused_0fused_0fused_0fused_0_fuseiter_5462___fuseiter_5463_1786___fuseiter_5464_1787___fuseiter_5465_1788___fuseiter_5466_1789, int8_t* __restrict__ __ins_0, int8_t* __restrict__ __outs_0) noexcept{
  for (uint64_t _fuseiter_5467 = 0UL; _fuseiter_5467 < 16UL; _fuseiter_5467 += 1UL) {
    for (uint64_t _fuseiter_5468 = 0UL; _fuseiter_5468 < 4UL; _fuseiter_5468 += 1UL) {
      int8_t __cached_0;
      __cached_0 = __ins_0[(((_fuseiter_5467 + ((fused_0fused_0fused_0fused_0_fuseiter_5462___fuseiter_5463_1786___fuseiter_5464_1787___fuseiter_5465_1788___fuseiter_5466_1789 / 16UL) * 16UL)) * 64UL) + (_fuseiter_5468 + ((fused_0fused_0fused_0fused_0_fuseiter_5462___fuseiter_5463_1786___fuseiter_5464_1787___fuseiter_5465_1788___fuseiter_5466_1789 % 16UL) * 4UL)))];
      int8_t __cached_1;
      __cached_1 = __cached_0;
      __outs_0[(((fused_0fused_0fused_0fused_0_fuseiter_5462___fuseiter_5463_1786___fuseiter_5464_1787___fuseiter_5465_1788___fuseiter_5466_1789 / 16UL) * 1024UL) + (((fused_0fused_0fused_0fused_0_fuseiter_5462___fuseiter_5463_1786___fuseiter_5464_1787___fuseiter_5465_1788___fuseiter_5466_1789 % 16UL) * 64UL) + ((_fuseiter_5467 * 4UL) + _fuseiter_5468)))] = __cached_1;
    }
  }
}

static void reorder__4190_closure_287_0wrapper(void* __stream, int8_t* __restrict__ __module_data, uint64_t i, generic_val* __restrict__ args) noexcept{
  reorder__4190_closure_287(i, (int8_t*)(args[0UL].v_ptr), (int8_t*)(args[1UL].v_ptr));
}

static void mul__6560_closure_288(uint64_t fused_0fused_0fused_0__itr_0____itr_1_1790____itr_2_1791____itr_3_1792, float* __restrict__ __ins_0, float* __restrict__ __ins_1, float* __restrict__ __outs_0) noexcept{
  for (uint64_t _fuseiter_5473 = 0UL; _fuseiter_5473 < 64UL; _fuseiter_5473 += 16UL) {
    vec_f32x16 __cached_0;
    __cached_0 = vec_f32x16::load(&__ins_0[((((fused_0fused_0fused_0__itr_0____itr_1_1790____itr_2_1791____itr_3_1792 / 8UL) * 512UL) + ((fused_0fused_0fused_0__itr_0____itr_1_1790____itr_2_1791____itr_3_1792 % 8UL) * 64UL)) + _fuseiter_5473)]);
    float __cached_1;
    __cached_1 = __ins_1[0];
    vec_f32x16 __cached_2;
    __cached_2 = (__cached_0 * vec_f32x16(__cached_1));
    vec_f32x16::store(__cached_2, &__outs_0[((((fused_0fused_0fused_0__itr_0____itr_1_1790____itr_2_1791____itr_3_1792 / 8UL) * 512UL) + ((fused_0fused_0fused_0__itr_0____itr_1_1790____itr_2_1791____itr_3_1792 % 8UL) * 64UL)) + _fuseiter_5473)]);
  }
}

static void mul__6560_closure_288_0wrapper(void* __stream, int8_t* __restrict__ __module_data, uint64_t i, generic_val* __restrict__ args) noexcept{
  mul__6560_closure_288(i, (float*)(args[0UL].v_ptr), (float*)(args[1UL].v_ptr), (float*)(args[2UL].v_ptr));
}

static void mul__6550_closure_289(uint64_t fused_0fused_0fused_0__itr_0____itr_1_1793____itr_2_1794____itr_3_1795, float* __restrict__ __ins_0, float* __restrict__ __ins_1, float* __restrict__ __outs_0) noexcept{
  for (uint64_t _fuseiter_5479 = 0UL; _fuseiter_5479 < 64UL; _fuseiter_5479 += 16UL) {
    vec_f32x16 __cached_0;
    __cached_0 = vec_f32x16::load(&__ins_0[((((fused_0fused_0fused_0__itr_0____itr_1_1793____itr_2_1794____itr_3_1795 / 8UL) * 512UL) + ((fused_0fused_0fused_0__itr_0____itr_1_1793____itr_2_1794____itr_3_1795 % 8UL) * 64UL)) + _fuseiter_5479)]);
    float __cached_1;
    __cached_1 = __ins_1[0];
    vec_f32x16 __cached_2;
    __cached_2 = (__cached_0 * vec_f32x16(__cached_1));
    vec_f32x16::store(__cached_2, &__outs_0[((((fused_0fused_0fused_0__itr_0____itr_1_1793____itr_2_1794____itr_3_1795 / 8UL) * 512UL) + ((fused_0fused_0fused_0__itr_0____itr_1_1793____itr_2_1794____itr_3_1795 % 8UL) * 64UL)) + _fuseiter_5479)]);
  }
}

static void mul__6550_closure_289_0wrapper(void* __stream, int8_t* __restrict__ __module_data, uint64_t i, generic_val* __restrict__ args) noexcept{
  mul__6550_closure_289(i, (float*)(args[0UL].v_ptr), (float*)(args[1UL].v_ptr), (float*)(args[2UL].v_ptr));
}

static void reorder__5340_closure_290(uint64_t fused_0_fuseiter_5481___fuseiter_5482_1796, int8_t* __restrict__ __ins_0, int8_t* __restrict__ __outs_0) noexcept{
  for (uint64_t _fuseiter_5485 = 0UL; _fuseiter_5485 < 64UL; _fuseiter_5485 += 1UL) {
    for (uint64_t _fuseiter_5486 = 0UL; _fuseiter_5486 < 64UL; _fuseiter_5486 += 1UL) {
      for (uint64_t _fuseiter_5487 = 0UL; _fuseiter_5487 < 4UL; _fuseiter_5487 += 1UL) {
        int8_t __cached_0;
        __cached_0 = __ins_0[(((_fuseiter_5486 + ((fused_0_fuseiter_5481___fuseiter_5482_1796 / 4UL) * 64UL)) * 1024UL) + ((_fuseiter_5487 + (_fuseiter_5485 * 4UL)) + ((fused_0_fuseiter_5481___fuseiter_5482_1796 % 4UL) * 256UL)))];
        int8_t __cached_1;
        __cached_1 = __cached_0;
        __outs_0[(((fused_0_fuseiter_5481___fuseiter_5482_1796 / 4UL) * 65536UL) + (((fused_0_fuseiter_5481___fuseiter_5482_1796 % 4UL) * 16384UL) + ((_fuseiter_5485 * 256UL) + ((_fuseiter_5486 * 4UL) + _fuseiter_5487))))] = __cached_1;
      }
    }
  }
}

static void reorder__5340_closure_290_0wrapper(void* __stream, int8_t* __restrict__ __module_data, uint64_t i, generic_val* __restrict__ args) noexcept{
  reorder__5340_closure_290(i, (int8_t*)(args[0UL].v_ptr), (int8_t*)(args[1UL].v_ptr));
}

static void mul__2430_closure_291(uint64_t fused_0fused_0__itr_0____itr_1_1797____itr_2_1798, float* __restrict__ __ins_0, float* __restrict__ __ins_1, float* __restrict__ __outs_0) noexcept{
  float __cached_0;
  __cached_0 = __ins_0[(((fused_0fused_0__itr_0____itr_1_1797____itr_2_1798 / 512UL) * 512UL) + (fused_0fused_0__itr_0____itr_1_1797____itr_2_1798 % 512UL))];
  float __cached_1;
  __cached_1 = __ins_1[(fused_0fused_0__itr_0____itr_1_1797____itr_2_1798 / 512UL)];
  float __cached_2;
  __cached_2 = (__cached_0 * __cached_1);
  __outs_0[(((fused_0fused_0__itr_0____itr_1_1797____itr_2_1798 / 512UL) * 512UL) + (fused_0fused_0__itr_0____itr_1_1797____itr_2_1798 % 512UL))] = __cached_2;
}

static void mul__2430_closure_291_0wrapper(void* __stream, int8_t* __restrict__ __module_data, uint64_t i, generic_val* __restrict__ args) noexcept{
  mul__2430_closure_291(i, (float*)(args[0UL].v_ptr), (float*)(args[1UL].v_ptr), (float*)(args[2UL].v_ptr));
}

static void cast__2440_closure_292(uint64_t fused_0fused_0__itr_0____itr_1_1799____itr_2_1800, float* __restrict__ __ins_0, int8_t* __restrict__ __outs_0) noexcept{
  float __cached_0;
  __cached_0 = __ins_0[(((fused_0fused_0__itr_0____itr_1_1799____itr_2_1800 / 512UL) * 512UL) + (fused_0fused_0__itr_0____itr_1_1799____itr_2_1800 % 512UL))];
  int8_t __cached_1;
  __cached_1 = (int8_t)sc_max(sc_min(sc_round_and_cast<int32_t>(__cached_0), 127), -128);
  __outs_0[(((fused_0fused_0__itr_0____itr_1_1799____itr_2_1800 / 512UL) * 512UL) + (fused_0fused_0__itr_0____itr_1_1799____itr_2_1800 % 512UL))] = __cached_1;
}

static void cast__2440_closure_292_0wrapper(void* __stream, int8_t* __restrict__ __module_data, uint64_t i, generic_val* __restrict__ args) noexcept{
  cast__2440_closure_292(i, (float*)(args[0UL].v_ptr), (int8_t*)(args[1UL].v_ptr));
}

static void mul__2520_closure_293(uint64_t fused_0fused_0__itr_0____itr_1_1801____itr_2_1802, float* __restrict__ __ins_0, float* __restrict__ __ins_1, float* __restrict__ __outs_0) noexcept{
  float __cached_0;
  __cached_0 = __ins_0[(((fused_0fused_0__itr_0____itr_1_1801____itr_2_1802 / 512UL) * 512UL) + (fused_0fused_0__itr_0____itr_1_1801____itr_2_1802 % 512UL))];
  float __cached_1;
  __cached_1 = __ins_1[(fused_0fused_0__itr_0____itr_1_1801____itr_2_1802 / 512UL)];
  float __cached_2;
  __cached_2 = (__cached_0 * __cached_1);
  __outs_0[(((fused_0fused_0__itr_0____itr_1_1801____itr_2_1802 / 512UL) * 512UL) + (fused_0fused_0__itr_0____itr_1_1801____itr_2_1802 % 512UL))] = __cached_2;
}

static void mul__2520_closure_293_0wrapper(void* __stream, int8_t* __restrict__ __module_data, uint64_t i, generic_val* __restrict__ args) noexcept{
  mul__2520_closure_293(i, (float*)(args[0UL].v_ptr), (float*)(args[1UL].v_ptr), (float*)(args[2UL].v_ptr));
}

static void cast__2530_closure_294(uint64_t fused_0fused_0__itr_0____itr_1_1803____itr_2_1804, float* __restrict__ __ins_0, int8_t* __restrict__ __outs_0) noexcept{
  float __cached_0;
  __cached_0 = __ins_0[(((fused_0fused_0__itr_0____itr_1_1803____itr_2_1804 / 512UL) * 512UL) + (fused_0fused_0__itr_0____itr_1_1803____itr_2_1804 % 512UL))];
  int8_t __cached_1;
  __cached_1 = (int8_t)sc_max(sc_min(sc_round_and_cast<int32_t>(__cached_0), 127), -128);
  __outs_0[(((fused_0fused_0__itr_0____itr_1_1803____itr_2_1804 / 512UL) * 512UL) + (fused_0fused_0__itr_0____itr_1_1803____itr_2_1804 % 512UL))] = __cached_1;
}

static void cast__2530_closure_294_0wrapper(void* __stream, int8_t* __restrict__ __module_data, uint64_t i, generic_val* __restrict__ args) noexcept{
  cast__2530_closure_294(i, (float*)(args[0UL].v_ptr), (int8_t*)(args[1UL].v_ptr));
}

static void mul__2610_closure_295(uint64_t fused_0fused_0__itr_0____itr_1_1805____itr_2_1806, float* __restrict__ __ins_0, float* __restrict__ __ins_1, float* __restrict__ __outs_0) noexcept{
  float __cached_0;
  __cached_0 = __ins_0[(((fused_0fused_0__itr_0____itr_1_1805____itr_2_1806 / 512UL) * 512UL) + (fused_0fused_0__itr_0____itr_1_1805____itr_2_1806 % 512UL))];
  float __cached_1;
  __cached_1 = __ins_1[(fused_0fused_0__itr_0____itr_1_1805____itr_2_1806 / 512UL)];
  float __cached_2;
  __cached_2 = (__cached_0 * __cached_1);
  __outs_0[(((fused_0fused_0__itr_0____itr_1_1805____itr_2_1806 / 512UL) * 512UL) + (fused_0fused_0__itr_0____itr_1_1805____itr_2_1806 % 512UL))] = __cached_2;
}

static void mul__2610_closure_295_0wrapper(void* __stream, int8_t* __restrict__ __module_data, uint64_t i, generic_val* __restrict__ args) noexcept{
  mul__2610_closure_295(i, (float*)(args[0UL].v_ptr), (float*)(args[1UL].v_ptr), (float*)(args[2UL].v_ptr));
}

static void cast__2620_closure_296(uint64_t fused_0fused_0__itr_0____itr_1_1807____itr_2_1808, float* __restrict__ __ins_0, int8_t* __restrict__ __outs_0) noexcept{
  float __cached_0;
  __cached_0 = __ins_0[(((fused_0fused_0__itr_0____itr_1_1807____itr_2_1808 / 512UL) * 512UL) + (fused_0fused_0__itr_0____itr_1_1807____itr_2_1808 % 512UL))];
  int8_t __cached_1;
  __cached_1 = (int8_t)sc_max(sc_min(sc_round_and_cast<int32_t>(__cached_0), 127), -128);
  __outs_0[(((fused_0fused_0__itr_0____itr_1_1807____itr_2_1808 / 512UL) * 512UL) + (fused_0fused_0__itr_0____itr_1_1807____itr_2_1808 % 512UL))] = __cached_1;
}

static void cast__2620_closure_296_0wrapper(void* __stream, int8_t* __restrict__ __module_data, uint64_t i, generic_val* __restrict__ args) noexcept{
  cast__2620_closure_296(i, (float*)(args[0UL].v_ptr), (int8_t*)(args[1UL].v_ptr));
}

static void mul__2460_closure_297(uint64_t fused_0fused_0__itr_0____itr_1_1809____itr_2_1810, float* __restrict__ __ins_0, float* __restrict__ __ins_1, float* __restrict__ __outs_0) noexcept{
  float __cached_0;
  __cached_0 = __ins_0[(((fused_0fused_0__itr_0____itr_1_1809____itr_2_1810 / 2048UL) * 2048UL) + (fused_0fused_0__itr_0____itr_1_1809____itr_2_1810 % 2048UL))];
  float __cached_1;
  __cached_1 = __ins_1[(fused_0fused_0__itr_0____itr_1_1809____itr_2_1810 / 2048UL)];
  float __cached_2;
  __cached_2 = (__cached_0 * __cached_1);
  __outs_0[(((fused_0fused_0__itr_0____itr_1_1809____itr_2_1810 / 2048UL) * 2048UL) + (fused_0fused_0__itr_0____itr_1_1809____itr_2_1810 % 2048UL))] = __cached_2;
}

static void mul__2460_closure_297_0wrapper(void* __stream, int8_t* __restrict__ __module_data, uint64_t i, generic_val* __restrict__ args) noexcept{
  mul__2460_closure_297(i, (float*)(args[0UL].v_ptr), (float*)(args[1UL].v_ptr), (float*)(args[2UL].v_ptr));
}

static void cast__2470_closure_298(uint64_t fused_0fused_0__itr_0____itr_1_1811____itr_2_1812, float* __restrict__ __ins_0, int8_t* __restrict__ __outs_0) noexcept{
  float __cached_0;
  __cached_0 = __ins_0[(((fused_0fused_0__itr_0____itr_1_1811____itr_2_1812 / 2048UL) * 2048UL) + (fused_0fused_0__itr_0____itr_1_1811____itr_2_1812 % 2048UL))];
  int8_t __cached_1;
  __cached_1 = (int8_t)sc_max(sc_min(sc_round_and_cast<int32_t>(__cached_0), 127), -128);
  __outs_0[(((fused_0fused_0__itr_0____itr_1_1811____itr_2_1812 / 2048UL) * 2048UL) + (fused_0fused_0__itr_0____itr_1_1811____itr_2_1812 % 2048UL))] = __cached_1;
}

static void cast__2470_closure_298_0wrapper(void* __stream, int8_t* __restrict__ __module_data, uint64_t i, generic_val* __restrict__ args) noexcept{
  cast__2470_closure_298(i, (float*)(args[0UL].v_ptr), (int8_t*)(args[1UL].v_ptr));
}

static void mul__2550_closure_299(uint64_t fused_0fused_0__itr_0____itr_1_1813____itr_2_1814, float* __restrict__ __ins_0, float* __restrict__ __ins_1, float* __restrict__ __outs_0) noexcept{
  float __cached_0;
  __cached_0 = __ins_0[(((fused_0fused_0__itr_0____itr_1_1813____itr_2_1814 / 2048UL) * 2048UL) + (fused_0fused_0__itr_0____itr_1_1813____itr_2_1814 % 2048UL))];
  float __cached_1;
  __cached_1 = __ins_1[(fused_0fused_0__itr_0____itr_1_1813____itr_2_1814 / 2048UL)];
  float __cached_2;
  __cached_2 = (__cached_0 * __cached_1);
  __outs_0[(((fused_0fused_0__itr_0____itr_1_1813____itr_2_1814 / 2048UL) * 2048UL) + (fused_0fused_0__itr_0____itr_1_1813____itr_2_1814 % 2048UL))] = __cached_2;
}

static void mul__2550_closure_299_0wrapper(void* __stream, int8_t* __restrict__ __module_data, uint64_t i, generic_val* __restrict__ args) noexcept{
  mul__2550_closure_299(i, (float*)(args[0UL].v_ptr), (float*)(args[1UL].v_ptr), (float*)(args[2UL].v_ptr));
}

static void cast__2560_closure_300(uint64_t fused_0fused_0__itr_0____itr_1_1815____itr_2_1816, float* __restrict__ __ins_0, int8_t* __restrict__ __outs_0) noexcept{
  float __cached_0;
  __cached_0 = __ins_0[(((fused_0fused_0__itr_0____itr_1_1815____itr_2_1816 / 2048UL) * 2048UL) + (fused_0fused_0__itr_0____itr_1_1815____itr_2_1816 % 2048UL))];
  int8_t __cached_1;
  __cached_1 = (int8_t)sc_max(sc_min(sc_round_and_cast<int32_t>(__cached_0), 127), -128);
  __outs_0[(((fused_0fused_0__itr_0____itr_1_1815____itr_2_1816 / 2048UL) * 2048UL) + (fused_0fused_0__itr_0____itr_1_1815____itr_2_1816 % 2048UL))] = __cached_1;
}

static void cast__2560_closure_300_0wrapper(void* __stream, int8_t* __restrict__ __module_data, uint64_t i, generic_val* __restrict__ args) noexcept{
  cast__2560_closure_300(i, (float*)(args[0UL].v_ptr), (int8_t*)(args[1UL].v_ptr));
}

static void mul__2340_closure_301(uint64_t fused_0fused_0__itr_0____itr_1_1817____itr_2_1818, float* __restrict__ __ins_0, float* __restrict__ __ins_1, float* __restrict__ __outs_0) noexcept{
  float __cached_0;
  __cached_0 = __ins_0[(((fused_0fused_0__itr_0____itr_1_1817____itr_2_1818 / 1024UL) * 1024UL) + (fused_0fused_0__itr_0____itr_1_1817____itr_2_1818 % 1024UL))];
  float __cached_1;
  __cached_1 = __ins_1[(fused_0fused_0__itr_0____itr_1_1817____itr_2_1818 / 1024UL)];
  float __cached_2;
  __cached_2 = (__cached_0 * __cached_1);
  __outs_0[(((fused_0fused_0__itr_0____itr_1_1817____itr_2_1818 / 1024UL) * 1024UL) + (fused_0fused_0__itr_0____itr_1_1817____itr_2_1818 % 1024UL))] = __cached_2;
}

static void mul__2340_closure_301_0wrapper(void* __stream, int8_t* __restrict__ __module_data, uint64_t i, generic_val* __restrict__ args) noexcept{
  mul__2340_closure_301(i, (float*)(args[0UL].v_ptr), (float*)(args[1UL].v_ptr), (float*)(args[2UL].v_ptr));
}

static void cast__2350_closure_302(uint64_t fused_0fused_0__itr_0____itr_1_1819____itr_2_1820, float* __restrict__ __ins_0, int8_t* __restrict__ __outs_0) noexcept{
  float __cached_0;
  __cached_0 = __ins_0[(((fused_0fused_0__itr_0____itr_1_1819____itr_2_1820 / 1024UL) * 1024UL) + (fused_0fused_0__itr_0____itr_1_1819____itr_2_1820 % 1024UL))];
  int8_t __cached_1;
  __cached_1 = (int8_t)sc_max(sc_min(sc_round_and_cast<int32_t>(__cached_0), 127), -128);
  __outs_0[(((fused_0fused_0__itr_0____itr_1_1819____itr_2_1820 / 1024UL) * 1024UL) + (fused_0fused_0__itr_0____itr_1_1819____itr_2_1820 % 1024UL))] = __cached_1;
}

static void cast__2350_closure_302_0wrapper(void* __stream, int8_t* __restrict__ __module_data, uint64_t i, generic_val* __restrict__ args) noexcept{
  cast__2350_closure_302(i, (float*)(args[0UL].v_ptr), (int8_t*)(args[1UL].v_ptr));
}

static void batchwise_4_fused_res2a_conv_b_cast_mul_add_cast_reorder_res2a_conv_0_cast_mul_add_relu_cast_res2a_conv_1_cast_mul_add_relu_cast_res2a_conv_2_cast_mul_add_cast_add_cast_reorder_res2b_conv_0_cast_mul_add_relu_cast_res2b_conv_1_cast_mul_add_relu_cast_reorder_res2b_conv_2_cast_mul_add_cast_add_cast_reorder_res2c_conv_0_cast_mul_add_relu_cast_reorder_res2c_conv_1_cast_mul_add_relu_cast_reorder_res2c_conv_2_cast_mul_add_cast_add_cast_reorder_res3a_conv_b_cast_mul_add_cast_res3a_conv_0_cast_mul_add_relu_cast_reorder_res3a_conv_1_cast_mul_add_relu_cast_res3a_conv_2_cast_mul_add_cast_add_cast_reorder_res3b_conv_0_cast_mul_add_relu_cast_reorder_res3b_conv_1_cast_mul_add_relu_cast_reorder_res3b_conv_2_cast_mul_add_cast_add_cast_reorder_res3c_conv_0_cast_mul_add_relu_cast_reorder_res3c_conv_1_cast_mul_add_relu_cast_reorder_res3c_conv_2_cast_mul_add_cast_add_cast_reorder_res3d_conv_0_cast_mul_add_relu_cast_reorder_res3d_conv_1_cast_mul_add_relu_cast_res3d_conv_2_cast_mul_add_cast_add_cast_res4a_conv_b_cast_mul_add_cast_reorder_res4a_conv_0_cast_mul_add_relu_cast_res4a_conv_1_cast_mul_add_relu_cast_reorder_res4a_conv_2_cast_mul_add_cast_add_cast_reorder_res4b_conv_0_cast_mul_add_relu_cast_reorder_res4b_conv_1_cast_mul_add_relu_cast_reorder_res4b_conv_2_cast_mul_add_cast_add_cast_reorder_res4c_conv_0_cast_mul_add_relu_cast_reorder_res4c_conv_1_cast_mul_add_relu_cast_reorder_res4c_conv_2_cast_mul_add_cast_add_cast_res4d_conv_0_cast_mul_add_relu_cast_res4d_conv_1_cast_mul_add_relu_cast_reorder_res4d_conv_2_cast_mul_add_cast_add_cast_res4e_conv_0_cast_mul_add_relu_cast_reorder_res4e_conv_1_cast_mul_add_relu_cast_reorder_res4e_conv_2_cast_mul_add_cast_add_cast_reorder_reorder_res4f_conv_0_cast_mul_add_relu_cast_reorder_res4f_conv_1_cast_mul_add_relu_cast_reorder_res4f_conv_2_cast_mul_add_cast_add_cast__6830_closure_303(uint64_t __batchwise_iter_0, int8_t* __restrict__ __ins_0, int8_t* __restrict__ __ins_4, float* __restrict__ __ins_5, float* __restrict__ __ins_6, int8_t* __restrict__ __ins_7, float* __restrict__ __ins_8, float* __restrict__ __ins_9, int8_t* __restrict__ __ins_1, float* __restrict__ __ins_2, float* __restrict__ __ins_3, int8_t* __restrict__ __ins_10, float* __restrict__ __ins_11, float* __restrict__ __ins_12, int8_t* __restrict__ __ins_13, float* __restrict__ __ins_14, float* __restrict__ __ins_15, int8_t* __restrict__ __ins_16, float* __restrict__ __ins_17, float* __restrict__ __ins_18, int8_t* __restrict__ __ins_19, float* __restrict__ __ins_20, float* __restrict__ __ins_21, int8_t* __restrict__ __ins_22, float* __restrict__ __ins_23, float* __restrict__ __ins_24, int8_t* __restrict__ __ins_25, float* __restrict__ __ins_26, float* __restrict__ __ins_27, int8_t* __restrict__ __ins_28, float* __restrict__ __ins_29, float* __restrict__ __ins_30, int8_t* __restrict__ __ins_34, float* __restrict__ __ins_35, float* __restrict__ __ins_36, int8_t* __restrict__ __ins_37, float* __restrict__ __ins_38, float* __restrict__ __ins_39, int8_t* __restrict__ __ins_31, float* __restrict__ __ins_32, float* __restrict__ __ins_33, int8_t* __restrict__ __ins_40, float* __restrict__ __ins_41, float* __restrict__ __ins_42, int8_t* __restrict__ __ins_43, float* __restrict__ __ins_44, float* __restrict__ __ins_45, int8_t* __restrict__ __ins_46, float* __restrict__ __ins_47, float* __restrict__ __ins_48, int8_t* __restrict__ __ins_49, float* __restrict__ __ins_50, float* __restrict__ __ins_51, int8_t* __restrict__ __ins_52, float* __restrict__ __ins_53, float* __restrict__ __ins_54, int8_t* __restrict__ __ins_55, float* __restrict__ __ins_56, float* __restrict__ __ins_57, int8_t* __restrict__ __ins_58, float* __restrict__ __ins_59, float* __restrict__ __ins_60, int8_t* __restrict__ __ins_61, float* __restrict__ __ins_62, float* __restrict__ __ins_63, int8_t* __restrict__ __ins_64, float* __restrict__ __ins_65, float* __restrict__ __ins_66, int8_t* __restrict__ __ins_67, float* __restrict__ __ins_68, float* __restrict__ __ins_69, int8_t* __restrict__ __ins_73, float* __restrict__ __ins_74, float* __restrict__ __ins_75, int8_t* __restrict__ __ins_76, float* __restrict__ __ins_77, float* __restrict__ __ins_78, int8_t* __restrict__ __ins_70, float* __restrict__ __ins_71, float* __restrict__ __ins_72, int8_t* __restrict__ __ins_79, float* __restrict__ __ins_80, float* __restrict__ __ins_81, int8_t* __restrict__ __ins_82, float* __restrict__ __ins_83, float* __restrict__ __ins_84, int8_t* __restrict__ __ins_85, float* __restrict__ __ins_86, float* __restrict__ __ins_87, int8_t* __restrict__ __ins_88, float* __restrict__ __ins_89, float* __restrict__ __ins_90, int8_t* __restrict__ __ins_91, float* __restrict__ __ins_92, float* __restrict__ __ins_93, int8_t* __restrict__ __ins_94, float* __restrict__ __ins_95, float* __restrict__ __ins_96, int8_t* __restrict__ __ins_97, float* __restrict__ __ins_98, float* __restrict__ __ins_99, int8_t* __restrict__ __ins_100, float* __restrict__ __ins_101, float* __restrict__ __ins_102, int8_t* __restrict__ __ins_103, float* __restrict__ __ins_104, float* __restrict__ __ins_105, int8_t* __restrict__ __ins_106, float* __restrict__ __ins_107, float* __restrict__ __ins_108, int8_t* __restrict__ __ins_109, float* __restrict__ __ins_110, float* __restrict__ __ins_111, int8_t* __restrict__ __ins_112, float* __restrict__ __ins_113, float* __restrict__ __ins_114, int8_t* __restrict__ __ins_115, float* __restrict__ __ins_116, float* __restrict__ __ins_117, int8_t* __restrict__ __ins_118, float* __restrict__ __ins_119, float* __restrict__ __ins_120, int8_t* __restrict__ __ins_121, float* __restrict__ __ins_122, float* __restrict__ __ins_123, uint8_t* __restrict__ __outs_0, int8_t* __restrict__ __ins_124, float* __restrict__ __ins_125, float* __restrict__ __ins_126) noexcept{
  int8_t* __rescheduled_1 = (int8_t*)sc_thread_aligned_malloc(__stream, 2021632UL);
  // [s8 [1, 1, 58, 58, 64] @ ABCD64b]
  int8_t* buffer_127 = (int8_t*)&__rescheduled_1[0UL];
  res2a_conv_0_cast_mul_add_relu_cast__8(buffer_127, &__ins_0[(__batchwise_iter_0 * 200704UL)], &__ins_4[0UL], &__ins_5[0UL], &__ins_6[0UL]);
  // [s8 [1, 2, 56, 56, 32] @ ABCD32b]
  int8_t* buffer_128 = (int8_t*)&__rescheduled_1[802816UL];
  res2a_conv_1_cast_mul_add_relu_cast__12(buffer_128, buffer_127, &__ins_7[0UL], &__ins_8[0UL], &__ins_9[0UL]);
  // [s8 [1, 4, 56, 56, 64] @ ABCD64b]
  int8_t* buffer_129 = (int8_t*)&__rescheduled_1[0UL];
  res2a_conv_b_cast_mul_add_cast_reorder__4(buffer_129, &__ins_0[(__batchwise_iter_0 * 200704UL)], &__ins_1[0UL], &__ins_2[0UL], &__ins_3[0UL]);
  // [u8 [1, 8, 56, 56, 32] @ ABCD32b]
  uint8_t* buffer_130 = (uint8_t*)&__rescheduled_1[1218816UL];
  res2a_conv_2_cast_mul_add_cast_add_cast_reorder__16(buffer_130, buffer_128, &__ins_10[0UL], &__ins_11[0UL], &__ins_12[0UL], buffer_129);
  // [s8 [1, 1, 58, 58, 64] @ ABCD64b]
  int8_t* buffer_131 = (int8_t*)&__rescheduled_1[0UL];
  res2b_conv_0_cast_mul_add_relu_cast__20(buffer_131, buffer_130, &__ins_13[0UL], &__ins_14[0UL], &__ins_15[0UL]);
  // [s8 [1, 1, 56, 56, 64] @ ABCD64b]
  int8_t* buffer_132 = (int8_t*)&__rescheduled_1[215296UL];
  res2b_conv_1_cast_mul_add_relu_cast_reorder__24(buffer_132, buffer_131, &__ins_16[0UL], &__ins_17[0UL], &__ins_18[0UL]);
  // [u8 [1, 4, 56, 56, 64] @ ABCD64b]
  uint8_t* buffer_133 = (uint8_t*)&__rescheduled_1[416000UL];
  res2b_conv_2_cast_mul_add_cast_add_cast_reorder__28(buffer_133, buffer_132, &__ins_19[0UL], &__ins_20[0UL], &__ins_21[0UL], buffer_130);
  // [s8 [1, 1, 58, 58, 64] @ ABCD64b]
  int8_t* buffer_134 = (int8_t*)&__rescheduled_1[0UL];
  res2c_conv_0_cast_mul_add_relu_cast_reorder__32(buffer_134, buffer_133, &__ins_22[0UL], &__ins_23[0UL], &__ins_24[0UL]);
  // [s8 [1, 1, 56, 56, 64] @ ABCD64b]
  int8_t* buffer_135 = (int8_t*)&__rescheduled_1[215296UL];
  res2c_conv_1_cast_mul_add_relu_cast_reorder__36(buffer_135, buffer_134, &__ins_25[0UL], &__ins_26[0UL], &__ins_27[0UL]);
  // [u8 [1, 2, 56, 56, 128] @ ABCD128b]
  uint8_t* buffer_136 = (uint8_t*)&__rescheduled_1[1218816UL];
  res2c_conv_2_cast_mul_add_cast_add_cast_reorder__40(buffer_136, buffer_135, &__ins_28[0UL], &__ins_29[0UL], &__ins_30[0UL], buffer_133);
  // [s8 [1, 1, 58, 58, 128] @ ABCD128b]
  int8_t* buffer_137 = (int8_t*)&__rescheduled_1[0UL];
  res3a_conv_0_cast_mul_add_relu_cast_reorder__48(buffer_137, buffer_136, &__ins_34[0UL], &__ins_35[0UL], &__ins_36[0UL]);
  // [s8 [1, 4, 28, 28, 32] @ ABCD32b]
  int8_t* buffer_138 = (int8_t*)&__rescheduled_1[430592UL];
  res3a_conv_1_cast_mul_add_relu_cast__52(buffer_138, buffer_137, &__ins_37[0UL], &__ins_38[0UL], &__ins_39[0UL]);
  // [s8 [1, 16, 28, 28, 32] @ ABCD32b]
  int8_t* buffer_139 = (int8_t*)&__rescheduled_1[0UL];
  res3a_conv_b_cast_mul_add_cast__44(buffer_139, buffer_136, &__ins_31[0UL], &__ins_32[0UL], &__ins_33[0UL]);
  // [u8 [1, 4, 28, 28, 128] @ ABCD128b]
  uint8_t* buffer_140 = (uint8_t*)&__rescheduled_1[530944UL];
  res3a_conv_2_cast_mul_add_cast_add_cast_reorder__56(buffer_140, buffer_138, &__ins_40[0UL], &__ins_41[0UL], &__ins_42[0UL], buffer_139);
  // [s8 [1, 2, 30, 30, 64] @ ABCD64b]
  int8_t* buffer_141 = (int8_t*)&__rescheduled_1[0UL];
  res3b_conv_0_cast_mul_add_relu_cast_reorder__60(buffer_141, buffer_140, &__ins_43[0UL], &__ins_44[0UL], &__ins_45[0UL]);
  // [s8 [1, 2, 28, 28, 64] @ ABCD64b]
  int8_t* buffer_142 = (int8_t*)&__rescheduled_1[115200UL];
  res3b_conv_1_cast_mul_add_relu_cast_reorder__64(buffer_142, buffer_141, &__ins_46[0UL], &__ins_47[0UL], &__ins_48[0UL]);
  // [u8 [1, 8, 28, 28, 64] @ ABCD64b]
  uint8_t* buffer_143 = (uint8_t*)&__rescheduled_1[932352UL];
  res3b_conv_2_cast_mul_add_cast_add_cast_reorder__68(buffer_143, buffer_142, &__ins_49[0UL], &__ins_50[0UL], &__ins_51[0UL], buffer_140);
  // [s8 [1, 2, 30, 30, 64] @ ABCD64b]
  int8_t* buffer_144 = (int8_t*)&__rescheduled_1[1333760UL];
  res3c_conv_0_cast_mul_add_relu_cast_reorder__72(buffer_144, buffer_143, &__ins_52[0UL], &__ins_53[0UL], &__ins_54[0UL]);
  // [s8 [1, 1, 28, 28, 128] @ ABCD128b]
  int8_t* buffer_145 = (int8_t*)&__rescheduled_1[1448960UL];
  res3c_conv_1_cast_mul_add_relu_cast_reorder__76(buffer_145, buffer_144, &__ins_55[0UL], &__ins_56[0UL], &__ins_57[0UL]);
  // [u8 [1, 4, 28, 28, 128] @ ABCD128b]
  uint8_t* buffer_146 = (uint8_t*)&__rescheduled_1[1549312UL];
  res3c_conv_2_cast_mul_add_cast_add_cast_reorder__80(buffer_146, buffer_145, &__ins_58[0UL], &__ins_59[0UL], &__ins_60[0UL], buffer_143);
  // [s8 [1, 1, 30, 30, 128] @ ABCD128b]
  int8_t* buffer_147 = (int8_t*)&__rescheduled_1[0UL];
  res3d_conv_0_cast_mul_add_relu_cast_reorder__84(buffer_147, buffer_146, &__ins_61[0UL], &__ins_62[0UL], &__ins_63[0UL]);
  // [s8 [1, 4, 28, 28, 32] @ ABCD32b]
  int8_t* buffer_148 = (int8_t*)&__rescheduled_1[115200UL];
  res3d_conv_1_cast_mul_add_relu_cast__88(buffer_148, buffer_147, &__ins_64[0UL], &__ins_65[0UL], &__ins_66[0UL]);
  // [u8 [1, 4, 28, 28, 128] @ ABCD128b]
  uint8_t* buffer_149 = (uint8_t*)&__rescheduled_1[215552UL];
  res3d_conv_2_cast_mul_add_cast_add_cast__92(buffer_149, buffer_148, &__ins_67[0UL], &__ins_68[0UL], &__ins_69[0UL], buffer_146);
  // [s8 [1, 1, 30, 30, 256] @ ABCD256b]
  int8_t* buffer_150 = (int8_t*)&__rescheduled_1[616960UL];
  res4a_conv_0_cast_mul_add_relu_cast__100(buffer_150, buffer_149, &__ins_73[0UL], &__ins_74[0UL], &__ins_75[0UL]);
  // [s8 [1, 1, 14, 14, 256] @ ABCD256b]
  int8_t* buffer_151 = (int8_t*)&__rescheduled_1[0UL];
  res4a_conv_1_cast_mul_add_relu_cast_reorder__104(buffer_151, buffer_150, &__ins_76[0UL], &__ins_77[0UL], &__ins_78[0UL]);
  // [s8 [1, 8, 14, 14, 128] @ ABCD128b]
  int8_t* buffer_152 = (int8_t*)&__rescheduled_1[616960UL];
  res4a_conv_b_cast_mul_add_cast_reorder__96(buffer_152, buffer_149, &__ins_70[0UL], &__ins_71[0UL], &__ins_72[0UL]);
  // [u8 [1, 1, 14, 14, 1024] @ ABCD1024b]
  uint8_t* buffer_153 = (uint8_t*)&__rescheduled_1[50176UL];
  res4a_conv_2_cast_mul_add_cast_add_cast_reorder__108(buffer_153, buffer_151, &__ins_79[0UL], &__ins_80[0UL], &__ins_81[0UL], buffer_152);
  // [s8 [1, 1, 16, 16, 256] @ ABCD256b]
  int8_t* buffer_154 = (int8_t*)&__rescheduled_1[250880UL];
  res4b_conv_0_cast_mul_add_relu_cast_reorder__112(buffer_154, buffer_153, &__ins_82[0UL], &__ins_83[0UL], &__ins_84[0UL]);
  // [s8 [1, 4, 14, 14, 64] @ ABCD64b]
  int8_t* buffer_155 = (int8_t*)&__rescheduled_1[0UL];
  res4b_conv_1_cast_mul_add_relu_cast_reorder__116(buffer_155, buffer_154, &__ins_85[0UL], &__ins_86[0UL], &__ins_87[0UL]);
  // [u8 [1, 8, 14, 14, 128] @ ABCD128b]
  uint8_t* buffer_156 = (uint8_t*)&__rescheduled_1[250880UL];
  res4b_conv_2_cast_mul_add_cast_add_cast_reorder__120(buffer_156, buffer_155, &__ins_88[0UL], &__ins_89[0UL], &__ins_90[0UL], buffer_153);
  // [s8 [1, 2, 16, 16, 128] @ ABCD128b]
  int8_t* buffer_157 = (int8_t*)&__rescheduled_1[0UL];
  res4c_conv_0_cast_mul_add_relu_cast_reorder__124(buffer_157, buffer_156, &__ins_91[0UL], &__ins_92[0UL], &__ins_93[0UL]);
  // [s8 [1, 1, 14, 14, 256] @ ABCD256b]
  int8_t* buffer_158 = (int8_t*)&__rescheduled_1[65536UL];
  res4c_conv_1_cast_mul_add_relu_cast_reorder__128(buffer_158, buffer_157, &__ins_94[0UL], &__ins_95[0UL], &__ins_96[0UL]);
  // [u8 [1, 8, 14, 14, 128] @ ABCD128b]
  uint8_t* buffer_159 = (uint8_t*)&__rescheduled_1[451584UL];
  res4c_conv_2_cast_mul_add_cast_add_cast__132(buffer_159, buffer_158, &__ins_97[0UL], &__ins_98[0UL], &__ins_99[0UL], buffer_156);
  // [s8 [1, 4, 16, 16, 64] @ ABCD64b]
  int8_t* buffer_160 = (int8_t*)&__rescheduled_1[0UL];
  res4d_conv_0_cast_mul_add_relu_cast__136(buffer_160, buffer_159, &__ins_100[0UL], &__ins_101[0UL], &__ins_102[0UL]);
  // [s8 [1, 2, 14, 14, 128] @ ABCD128b]
  int8_t* buffer_161 = (int8_t*)&__rescheduled_1[65536UL];
  res4d_conv_1_cast_mul_add_relu_cast_reorder__140(buffer_161, buffer_160, &__ins_103[0UL], &__ins_104[0UL], &__ins_105[0UL]);
  // [u8 [1, 8, 14, 14, 128] @ ABCD128b]
  uint8_t* buffer_162 = (uint8_t*)&__rescheduled_1[115712UL];
  res4d_conv_2_cast_mul_add_cast_add_cast__144(buffer_162, buffer_161, &__ins_106[0UL], &__ins_107[0UL], &__ins_108[0UL], buffer_159);
  // [s8 [1, 2, 16, 16, 128] @ ABCD128b]
  int8_t* buffer_163 = (int8_t*)&__rescheduled_1[0UL];
  res4e_conv_0_cast_mul_add_relu_cast_reorder__148(buffer_163, buffer_162, &__ins_109[0UL], &__ins_110[0UL], &__ins_111[0UL]);
  // [s8 [1, 1, 14, 14, 256] @ ABCD256b]
  int8_t* buffer_164 = (int8_t*)&__rescheduled_1[65536UL];
  res4e_conv_1_cast_mul_add_relu_cast_reorder__152(buffer_164, buffer_163, &__ins_112[0UL], &__ins_113[0UL], &__ins_114[0UL]);
  // [u8 [1, 8, 14, 14, 128] @ ABCD128b]
  uint8_t* buffer_165 = (uint8_t*)&__rescheduled_1[316416UL];
  // [u8 [1, 4, 14, 14, 256] @ ABCD256b]
  uint8_t* buffer_166 = (uint8_t*)&__rescheduled_1[517120UL];
  res4e_conv_2_cast_mul_add_cast_add_cast_reorder__156(buffer_165, buffer_166, buffer_164, &__ins_115[0UL], &__ins_116[0UL], &__ins_117[0UL], buffer_162);
  // [u8 [1, 2, 14, 14, 512] @ ABCD512b]
  uint8_t* buffer_167 = (uint8_t*)&__rescheduled_1[0UL];
  reorder__157(buffer_167, buffer_165);
  // [s8 [1, 4, 16, 16, 64] @ ABCD64b]
  int8_t* buffer_168 = (int8_t*)&__rescheduled_1[200704UL];
  res4f_conv_0_cast_mul_add_relu_cast_reorder__161(buffer_168, buffer_167, &__ins_118[0UL], &__ins_119[0UL], &__ins_120[0UL]);
  // [s8 [1, 2, 14, 14, 128] @ ABCD128b]
  int8_t* buffer_169 = (int8_t*)&__rescheduled_1[0UL];
  res4f_conv_1_cast_mul_add_relu_cast_reorder__165(buffer_169, buffer_168, &__ins_121[0UL], &__ins_122[0UL], &__ins_123[0UL]);
  res4f_conv_2_cast_mul_add_cast_add_cast__170(&__outs_0[(__batchwise_iter_0 * 200704UL)], buffer_169, &__ins_124[0UL], &__ins_125[0UL], &__ins_126[0UL], buffer_166);
  sc_thread_aligned_free(__stream, __rescheduled_1);
}

static void batchwise_4_fused_res2a_conv_b_cast_mul_add_cast_reorder_res2a_conv_0_cast_mul_add_relu_cast_res2a_conv_1_cast_mul_add_relu_cast_res2a_conv_2_cast_mul_add_cast_add_cast_reorder_res2b_conv_0_cast_mul_add_relu_cast_res2b_conv_1_cast_mul_add_relu_cast_reorder_res2b_conv_2_cast_mul_add_cast_add_cast_reorder_res2c_conv_0_cast_mul_add_relu_cast_reorder_res2c_conv_1_cast_mul_add_relu_cast_reorder_res2c_conv_2_cast_mul_add_cast_add_cast_reorder_res3a_conv_b_cast_mul_add_cast_res3a_conv_0_cast_mul_add_relu_cast_reorder_res3a_conv_1_cast_mul_add_relu_cast_res3a_conv_2_cast_mul_add_cast_add_cast_reorder_res3b_conv_0_cast_mul_add_relu_cast_reorder_res3b_conv_1_cast_mul_add_relu_cast_reorder_res3b_conv_2_cast_mul_add_cast_add_cast_reorder_res3c_conv_0_cast_mul_add_relu_cast_reorder_res3c_conv_1_cast_mul_add_relu_cast_reorder_res3c_conv_2_cast_mul_add_cast_add_cast_reorder_res3d_conv_0_cast_mul_add_relu_cast_reorder_res3d_conv_1_cast_mul_add_relu_cast_res3d_conv_2_cast_mul_add_cast_add_cast_res4a_conv_b_cast_mul_add_cast_reorder_res4a_conv_0_cast_mul_add_relu_cast_res4a_conv_1_cast_mul_add_relu_cast_reorder_res4a_conv_2_cast_mul_add_cast_add_cast_reorder_res4b_conv_0_cast_mul_add_relu_cast_reorder_res4b_conv_1_cast_mul_add_relu_cast_reorder_res4b_conv_2_cast_mul_add_cast_add_cast_reorder_res4c_conv_0_cast_mul_add_relu_cast_reorder_res4c_conv_1_cast_mul_add_relu_cast_reorder_res4c_conv_2_cast_mul_add_cast_add_cast_res4d_conv_0_cast_mul_add_relu_cast_res4d_conv_1_cast_mul_add_relu_cast_reorder_res4d_conv_2_cast_mul_add_cast_add_cast_res4e_conv_0_cast_mul_add_relu_cast_reorder_res4e_conv_1_cast_mul_add_relu_cast_reorder_res4e_conv_2_cast_mul_add_cast_add_cast_reorder_reorder_res4f_conv_0_cast_mul_add_relu_cast_reorder_res4f_conv_1_cast_mul_add_relu_cast_reorder_res4f_conv_2_cast_mul_add_cast_add_cast__6830_closure_303_0wrapper(void* __stream, int8_t* __restrict__ __module_data, uint64_t i, generic_val* __restrict__ args) noexcept{
  batchwise_4_fused_res2a_conv_b_cast_mul_add_cast_reorder_res2a_conv_0_cast_mul_add_relu_cast_res2a_conv_1_cast_mul_add_relu_cast_res2a_conv_2_cast_mul_add_cast_add_cast_reorder_res2b_conv_0_cast_mul_add_relu_cast_res2b_conv_1_cast_mul_add_relu_cast_reorder_res2b_conv_2_cast_mul_add_cast_add_cast_reorder_res2c_conv_0_cast_mul_add_relu_cast_reorder_res2c_conv_1_cast_mul_add_relu_cast_reorder_res2c_conv_2_cast_mul_add_cast_add_cast_reorder_res3a_conv_b_cast_mul_add_cast_res3a_conv_0_cast_mul_add_relu_cast_reorder_res3a_conv_1_cast_mul_add_relu_cast_res3a_conv_2_cast_mul_add_cast_add_cast_reorder_res3b_conv_0_cast_mul_add_relu_cast_reorder_res3b_conv_1_cast_mul_add_relu_cast_reorder_res3b_conv_2_cast_mul_add_cast_add_cast_reorder_res3c_conv_0_cast_mul_add_relu_cast_reorder_res3c_conv_1_cast_mul_add_relu_cast_reorder_res3c_conv_2_cast_mul_add_cast_add_cast_reorder_res3d_conv_0_cast_mul_add_relu_cast_reorder_res3d_conv_1_cast_mul_add_relu_cast_res3d_conv_2_cast_mul_add_cast_add_cast_res4a_conv_b_cast_mul_add_cast_reorder_res4a_conv_0_cast_mul_add_relu_cast_res4a_conv_1_cast_mul_add_relu_cast_reorder_res4a_conv_2_cast_mul_add_cast_add_cast_reorder_res4b_conv_0_cast_mul_add_relu_cast_reorder_res4b_conv_1_cast_mul_add_relu_cast_reorder_res4b_conv_2_cast_mul_add_cast_add_cast_reorder_res4c_conv_0_cast_mul_add_relu_cast_reorder_res4c_conv_1_cast_mul_add_relu_cast_reorder_res4c_conv_2_cast_mul_add_cast_add_cast_res4d_conv_0_cast_mul_add_relu_cast_res4d_conv_1_cast_mul_add_relu_cast_reorder_res4d_conv_2_cast_mul_add_cast_add_cast_res4e_conv_0_cast_mul_add_relu_cast_reorder_res4e_conv_1_cast_mul_add_relu_cast_reorder_res4e_conv_2_cast_mul_add_cast_add_cast_reorder_reorder_res4f_conv_0_cast_mul_add_relu_cast_reorder_res4f_conv_1_cast_mul_add_relu_cast_reorder_res4f_conv_2_cast_mul_add_cast_add_cast__6830_closure_303(i, (int8_t*)(args[0UL].v_ptr), (int8_t*)(args[1UL].v_ptr), (float*)(args[2UL].v_ptr), (float*)(args[3UL].v_ptr), (int8_t*)(args[4UL].v_ptr), (float*)(args[5UL].v_ptr), (float*)(args[6UL].v_ptr), (int8_t*)(args[7UL].v_ptr), (float*)(args[8UL].v_ptr), (float*)(args[9UL].v_ptr), (int8_t*)(args[10UL].v_ptr), (float*)(args[11UL].v_ptr), (float*)(args[12UL].v_ptr), (int8_t*)(args[13UL].v_ptr), (float*)(args[14UL].v_ptr), (float*)(args[15UL].v_ptr), (int8_t*)(args[16UL].v_ptr), (float*)(args[17UL].v_ptr), (float*)(args[18UL].v_ptr), (int8_t*)(args[19UL].v_ptr), (float*)(args[20UL].v_ptr), (float*)(args[21UL].v_ptr), (int8_t*)(args[22UL].v_ptr), (float*)(args[23UL].v_ptr), (float*)(args[24UL].v_ptr), (int8_t*)(args[25UL].v_ptr), (float*)(args[26UL].v_ptr), (float*)(args[27UL].v_ptr), (int8_t*)(args[28UL].v_ptr), (float*)(args[29UL].v_ptr), (float*)(args[30UL].v_ptr), (int8_t*)(args[31UL].v_ptr), (float*)(args[32UL].v_ptr), (float*)(args[33UL].v_ptr), (int8_t*)(args[34UL].v_ptr), (float*)(args[35UL].v_ptr), (float*)(args[36UL].v_ptr), (int8_t*)(args[37UL].v_ptr), (float*)(args[38UL].v_ptr), (float*)(args[39UL].v_ptr), (int8_t*)(args[40UL].v_ptr), (float*)(args[41UL].v_ptr), (float*)(args[42UL].v_ptr), (int8_t*)(args[43UL].v_ptr), (float*)(args[44UL].v_ptr), (float*)(args[45UL].v_ptr), (int8_t*)(args[46UL].v_ptr), (float*)(args[47UL].v_ptr), (float*)(args[48UL].v_ptr), (int8_t*)(args[49UL].v_ptr), (float*)(args[50UL].v_ptr), (float*)(args[51UL].v_ptr), (int8_t*)(args[52UL].v_ptr), (float*)(args[53UL].v_ptr), (float*)(args[54UL].v_ptr), (int8_t*)(args[55UL].v_ptr), (float*)(args[56UL].v_ptr), (float*)(args[57UL].v_ptr), (int8_t*)(args[58UL].v_ptr), (float*)(args[59UL].v_ptr), (float*)(args[60UL].v_ptr), (int8_t*)(args[61UL].v_ptr), (float*)(args[62UL].v_ptr), (float*)(args[63UL].v_ptr), (int8_t*)(args[64UL].v_ptr), (float*)(args[65UL].v_ptr), (float*)(args[66UL].v_ptr), (int8_t*)(args[67UL].v_ptr), (float*)(args[68UL].v_ptr), (float*)(args[69UL].v_ptr), (int8_t*)(args[70UL].v_ptr), (float*)(args[71UL].v_ptr), (float*)(args[72UL].v_ptr), (int8_t*)(args[73UL].v_ptr), (float*)(args[74UL].v_ptr), (float*)(args[75UL].v_ptr), (int8_t*)(args[76UL].v_ptr), (float*)(args[77UL].v_ptr), (float*)(args[78UL].v_ptr), (int8_t*)(args[79UL].v_ptr), (float*)(args[80UL].v_ptr), (float*)(args[81UL].v_ptr), (int8_t*)(args[82UL].v_ptr), (float*)(args[83UL].v_ptr), (float*)(args[84UL].v_ptr), (int8_t*)(args[85UL].v_ptr), (float*)(args[86UL].v_ptr), (float*)(args[87UL].v_ptr), (int8_t*)(args[88UL].v_ptr), (float*)(args[89UL].v_ptr), (float*)(args[90UL].v_ptr), (int8_t*)(args[91UL].v_ptr), (float*)(args[92UL].v_ptr), (float*)(args[93UL].v_ptr), (int8_t*)(args[94UL].v_ptr), (float*)(args[95UL].v_ptr), (float*)(args[96UL].v_ptr), (int8_t*)(args[97UL].v_ptr), (float*)(args[98UL].v_ptr), (float*)(args[99UL].v_ptr), (int8_t*)(args[100UL].v_ptr), (float*)(args[101UL].v_ptr), (float*)(args[102UL].v_ptr), (int8_t*)(args[103UL].v_ptr), (float*)(args[104UL].v_ptr), (float*)(args[105UL].v_ptr), (int8_t*)(args[106UL].v_ptr), (float*)(args[107UL].v_ptr), (float*)(args[108UL].v_ptr), (int8_t*)(args[109UL].v_ptr), (float*)(args[110UL].v_ptr), (float*)(args[111UL].v_ptr), (int8_t*)(args[112UL].v_ptr), (float*)(args[113UL].v_ptr), (float*)(args[114UL].v_ptr), (int8_t*)(args[115UL].v_ptr), (float*)(args[116UL].v_ptr), (float*)(args[117UL].v_ptr), (int8_t*)(args[118UL].v_ptr), (float*)(args[119UL].v_ptr), (float*)(args[120UL].v_ptr), (int8_t*)(args[121UL].v_ptr), (float*)(args[122UL].v_ptr), (float*)(args[123UL].v_ptr), (uint8_t*)(args[124UL].v_ptr), (int8_t*)(args[125UL].v_ptr), (float*)(args[126UL].v_ptr), (float*)(args[127UL].v_ptr));
}

static void reorder__5310_closure_304(uint64_t _fuseiter_7023, int8_t* __restrict__ __ins_0, int8_t* __restrict__ __outs_0) noexcept{
  for (uint64_t _fuseiter_7024 = 0UL; _fuseiter_7024 < 4UL; _fuseiter_7024 += 1UL) {
    for (uint64_t _fuseiter_7027 = 0UL; _fuseiter_7027 < 64UL; _fuseiter_7027 += 1UL) {
      for (uint64_t _fuseiter_7028 = 0UL; _fuseiter_7028 < 16UL; _fuseiter_7028 += 1UL) {
        for (uint64_t _fuseiter_7029 = 0UL; _fuseiter_7029 < 4UL; _fuseiter_7029 += 1UL) {
          int8_t __cached_0;
          __cached_0 = __ins_0[(((_fuseiter_7028 + (_fuseiter_7023 * 16UL)) * 1024UL) + ((_fuseiter_7029 + (_fuseiter_7027 * 4UL)) + (_fuseiter_7024 * 256UL)))];
          int8_t __cached_1;
          __cached_1 = __cached_0;
          __outs_0[((_fuseiter_7023 * 16384UL) + ((_fuseiter_7024 * 4096UL) + ((_fuseiter_7027 * 64UL) + ((_fuseiter_7028 * 4UL) + _fuseiter_7029))))] = __cached_1;
        }
      }
    }
  }
}

static void reorder__5310_closure_304_0wrapper(void* __stream, int8_t* __restrict__ __module_data, uint64_t i, generic_val* __restrict__ args) noexcept{
  reorder__5310_closure_304(i, (int8_t*)(args[0UL].v_ptr), (int8_t*)(args[1UL].v_ptr));
}

static void mul__6540_closure_305(uint64_t fused_0fused_0fused_0__itr_0____itr_1_1881____itr_2_1882____itr_3_1883, float* __restrict__ __ins_0, float* __restrict__ __ins_1, float* __restrict__ __outs_0) noexcept{
  vec_f32x16 __cached_0;
  __cached_0 = vec_f32x16::load(&__ins_0[(((fused_0fused_0fused_0__itr_0____itr_1_1881____itr_2_1882____itr_3_1883 / 128UL) * 2048UL) + ((fused_0fused_0fused_0__itr_0____itr_1_1881____itr_2_1882____itr_3_1883 % 128UL) * 16UL))]);
  float __cached_1;
  __cached_1 = __ins_1[0];
  vec_f32x16 __cached_2;
  __cached_2 = (__cached_0 * vec_f32x16(__cached_1));
  vec_f32x16::store(__cached_2, &__outs_0[(((fused_0fused_0fused_0__itr_0____itr_1_1881____itr_2_1882____itr_3_1883 / 128UL) * 2048UL) + ((fused_0fused_0fused_0__itr_0____itr_1_1881____itr_2_1882____itr_3_1883 % 128UL) * 16UL))]);
}

static void mul__6540_closure_305_0wrapper(void* __stream, int8_t* __restrict__ __module_data, uint64_t i, generic_val* __restrict__ args) noexcept{
  mul__6540_closure_305(i, (float*)(args[0UL].v_ptr), (float*)(args[1UL].v_ptr), (float*)(args[2UL].v_ptr));
}

static void mul__6530_closure_306(uint64_t fused_0fused_0fused_0__itr_0____itr_1_1884____itr_2_1885____itr_3_1886, float* __restrict__ __ins_0, float* __restrict__ __ins_1, float* __restrict__ __outs_0) noexcept{
  vec_f32x16 __cached_0;
  __cached_0 = vec_f32x16::load(&__ins_0[(((fused_0fused_0fused_0__itr_0____itr_1_1884____itr_2_1885____itr_3_1886 / 128UL) * 2048UL) + ((fused_0fused_0fused_0__itr_0____itr_1_1884____itr_2_1885____itr_3_1886 % 128UL) * 16UL))]);
  float __cached_1;
  __cached_1 = __ins_1[0];
  vec_f32x16 __cached_2;
  __cached_2 = (__cached_0 * vec_f32x16(__cached_1));
  vec_f32x16::store(__cached_2, &__outs_0[(((fused_0fused_0fused_0__itr_0____itr_1_1884____itr_2_1885____itr_3_1886 / 128UL) * 2048UL) + ((fused_0fused_0fused_0__itr_0____itr_1_1884____itr_2_1885____itr_3_1886 % 128UL) * 16UL))]);
}

static void mul__6530_closure_306_0wrapper(void* __stream, int8_t* __restrict__ __module_data, uint64_t i, generic_val* __restrict__ args) noexcept{
  mul__6530_closure_306(i, (float*)(args[0UL].v_ptr), (float*)(args[1UL].v_ptr), (float*)(args[2UL].v_ptr));
}

static void mul__2400_closure_307(uint64_t fused_0fused_0__itr_0____itr_1_1887____itr_2_1888, float* __restrict__ __ins_0, float* __restrict__ __ins_1, float* __restrict__ __outs_0) noexcept{
  for (uint64_t _fuseiter_7046 = 0UL; _fuseiter_7046 < 3UL; _fuseiter_7046 += 1UL) {
    float __cached_0;
    __cached_0 = __ins_0[((((fused_0fused_0__itr_0____itr_1_1887____itr_2_1888 / 1536UL) * 4608UL) + ((((fused_0fused_0__itr_0____itr_1_1887____itr_2_1888 / 3UL) % 512UL) * 9UL) + ((fused_0fused_0__itr_0____itr_1_1887____itr_2_1888 % 3UL) * 3UL))) + _fuseiter_7046)];
    float __cached_1;
    __cached_1 = __ins_1[(fused_0fused_0__itr_0____itr_1_1887____itr_2_1888 / 1536UL)];
    float __cached_2;
    __cached_2 = (__cached_0 * __cached_1);
    __outs_0[((((fused_0fused_0__itr_0____itr_1_1887____itr_2_1888 / 1536UL) * 4608UL) + ((((fused_0fused_0__itr_0____itr_1_1887____itr_2_1888 / 3UL) % 512UL) * 9UL) + ((fused_0fused_0__itr_0____itr_1_1887____itr_2_1888 % 3UL) * 3UL))) + _fuseiter_7046)] = __cached_2;
  }
}

static void mul__2400_closure_307_0wrapper(void* __stream, int8_t* __restrict__ __module_data, uint64_t i, generic_val* __restrict__ args) noexcept{
  mul__2400_closure_307(i, (float*)(args[0UL].v_ptr), (float*)(args[1UL].v_ptr), (float*)(args[2UL].v_ptr));
}

static void cast__2410_closure_308(uint64_t fused_0fused_0__itr_0____itr_1_1889____itr_2_1890, float* __restrict__ __ins_0, int8_t* __restrict__ __outs_0) noexcept{
  for (uint64_t _fuseiter7051 = 0UL; _fuseiter7051 < 3UL; _fuseiter7051 += 1UL) {
    float __cached_0;
    __cached_0 = __ins_0[((((fused_0fused_0__itr_0____itr_1_1889____itr_2_1890 / 1536UL) * 4608UL) + ((((fused_0fused_0__itr_0____itr_1_1889____itr_2_1890 / 3UL) % 512UL) * 9UL) + ((fused_0fused_0__itr_0____itr_1_1889____itr_2_1890 % 3UL) * 3UL))) + _fuseiter7051)];
    int8_t __cached_1;
    __cached_1 = (int8_t)sc_max(sc_min(sc_round_and_cast<int32_t>(__cached_0), 127), -128);
    __outs_0[((((fused_0fused_0__itr_0____itr_1_1889____itr_2_1890 / 1536UL) * 4608UL) + ((((fused_0fused_0__itr_0____itr_1_1889____itr_2_1890 / 3UL) % 512UL) * 9UL) + ((fused_0fused_0__itr_0____itr_1_1889____itr_2_1890 % 3UL) * 3UL))) + _fuseiter7051)] = __cached_1;
  }
}

static void cast__2410_closure_308_0wrapper(void* __stream, int8_t* __restrict__ __module_data, uint64_t i, generic_val* __restrict__ args) noexcept{
  cast__2410_closure_308(i, (float*)(args[0UL].v_ptr), (int8_t*)(args[1UL].v_ptr));
}

static void reorder__5370_closure_309(uint64_t fused_0fused_0_fuseiter_7052___fuseiter_7053_1891___fuseiter_7054_1892, int8_t* __restrict__ __ins_0, int8_t* __restrict__ __outs_0) noexcept{
  for (uint64_t _fuseiter_7055 = 0UL; _fuseiter_7055 < 3UL; _fuseiter_7055 += 1UL) {
    for (uint64_t _fuseiter_7056 = 0UL; _fuseiter_7056 < 64UL; _fuseiter_7056 += 1UL) {
      for (uint64_t _fuseiter_7057 = 0UL; _fuseiter_7057 < 128UL; _fuseiter_7057 += 1UL) {
        for (uint64_t _fuseiter_7058 = 0UL; _fuseiter_7058 < 4UL; _fuseiter_7058 += 1UL) {
          int8_t __cached_0;
          __cached_0 = __ins_0[(((_fuseiter_7057 + ((fused_0fused_0_fuseiter_7052___fuseiter_7053_1891___fuseiter_7054_1892 / 6UL) * 128UL)) * 4608UL) + ((((_fuseiter_7058 + (_fuseiter_7056 * 4UL)) + (((fused_0fused_0_fuseiter_7052___fuseiter_7053_1891___fuseiter_7054_1892 / 3UL) % 2UL) * 256UL)) * 9UL) + (((fused_0fused_0_fuseiter_7052___fuseiter_7053_1891___fuseiter_7054_1892 % 3UL) * 3UL) + _fuseiter_7055)))];
          int8_t __cached_1;
          __cached_1 = __cached_0;
          __outs_0[(((fused_0fused_0_fuseiter_7052___fuseiter_7053_1891___fuseiter_7054_1892 / 6UL) * 589824UL) + ((((fused_0fused_0_fuseiter_7052___fuseiter_7053_1891___fuseiter_7054_1892 / 3UL) % 2UL) * 294912UL) + (((fused_0fused_0_fuseiter_7052___fuseiter_7053_1891___fuseiter_7054_1892 % 3UL) * 98304UL) + ((_fuseiter_7055 * 32768UL) + ((_fuseiter_7056 * 512UL) + ((_fuseiter_7057 * 4UL) + _fuseiter_7058))))))] = __cached_1;
        }
      }
    }
  }
}

static void reorder__5370_closure_309_0wrapper(void* __stream, int8_t* __restrict__ __module_data, uint64_t i, generic_val* __restrict__ args) noexcept{
  reorder__5370_closure_309(i, (int8_t*)(args[0UL].v_ptr), (int8_t*)(args[1UL].v_ptr));
}

static void res5a_conv_0_cast_mul_add_relu_cast_reorder__6810_closure_310(uint64_t fused_0n__k_1893, int8_t* __restrict__ __outs_0) noexcept{
  memset(&__outs_0[(((fused_0n__k_1893 / 2UL) * 131072UL) + ((fused_0n__k_1893 % 2UL) * 65536UL))], 0, 4096UL);
  for (uint64_t p1 = 0UL; p1 < 14UL; p1 += 1UL) {
    memset(&__outs_0[(((fused_0n__k_1893 / 2UL) * 131072UL) + (((fused_0n__k_1893 % 2UL) * 65536UL) + ((p1 + 1UL) * 4096UL)))], 0, 256UL);
    memset(&__outs_0[((((fused_0n__k_1893 / 2UL) * 131072UL) + (((fused_0n__k_1893 % 2UL) * 65536UL) + ((p1 + 1UL) * 4096UL))) + 3840UL)], 0, 256UL);
  }
  memset(&__outs_0[((((fused_0n__k_1893 / 2UL) * 131072UL) + ((fused_0n__k_1893 % 2UL) * 65536UL)) + 61440UL)], 0, 4096UL);
}

static void res5a_conv_0_cast_mul_add_relu_cast_reorder__6810_closure_310_0wrapper(void* __stream, int8_t* __restrict__ __module_data, uint64_t i, generic_val* __restrict__ args) noexcept{
  res5a_conv_0_cast_mul_add_relu_cast_reorder__6810_closure_310(i, (int8_t*)(args[0UL].v_ptr));
}

static void res5a_conv_0_cast_mul_add_relu_cast_reorder__6810_closure_311(uint64_t fused_0n__k_1894, uint8_t* __restrict__ __ins_0, int8_t* __restrict__ __ins_1, float* __restrict__ __ins_2, float* __restrict__ __ins_3, int8_t* __restrict__ __outs_0) noexcept{
  void*& __sc_kernel_cache_182 = *(void**)(__module_data + 280);
  alignas(64) int8_t __rescheduled_2[128UL];
  int32_t* __origouts_1980_shr = (int32_t*)sc_thread_aligned_malloc(__stream, 50176UL);
  void** A_list = (void**)&__rescheduled_2[0UL];
  void** B_list = (void**)&__rescheduled_2[64UL];
  for (uint64_t c = 0UL; c < 4UL; c += 1UL) {
    void* __cached_0;
    __cached_0 = &__ins_0[(((fused_0n__k_1894 / 8UL) * 200704UL) + (c * 50176UL))];
    A_list[c] = __cached_0;
    void* __cached_1;
    __cached_1 = &__ins_1[(((fused_0n__k_1894 % 8UL) * 65536UL) + (c * 16384UL))];
    B_list[c] = __cached_1;
  }
  void* _arg_cache_42 = &__origouts_1980_shr[0UL];
  dnnl_brgemm_list_call(__sc_kernel_cache_182, A_list, B_list, &__origouts_1980_shr[0UL], 1, 1, 1, 4, 8, 7, __stream);
  for (uint64_t _fuseiter7061 = 0UL; _fuseiter7061 < 14UL; _fuseiter7061 += 1UL) {
    for (uint64_t _fuseiter7062 = 0UL; _fuseiter7062 < 14UL; _fuseiter7062 += 1UL) {
      for (uint64_t _fuseiter7063 = 0UL; _fuseiter7063 < 64UL; _fuseiter7063 += 16UL) {
        vec_s32x16 __cached_2;
        __cached_2 = vec_s32x16::load(&__origouts_1980_shr[((_fuseiter7061 * 896UL) + ((_fuseiter7062 * 64UL) + _fuseiter7063))]);
        vec_f32x16 __cached_3;
        __cached_3 = (vec_f32x16)(__cached_2);
        vec_f32x16 __cached_4;
        __cached_4 = vec_f32x16::load(&__ins_2[(((fused_0n__k_1894 % 8UL) * 64UL) + _fuseiter7063)]);
        __cached_3 = (__cached_3 * __cached_4);
        vec_f32x16 __cached_5;
        __cached_5 = vec_f32x16::load(&__ins_3[(((fused_0n__k_1894 % 8UL) * 64UL) + _fuseiter7063)]);
        __cached_3 = (__cached_3 + __cached_5);
        __cached_3 = sc_max(__cached_3, vec_f32x16(0.f));
        vec_s8x16 __cached_6;
        __cached_6 = sc_saturated_cast<vec_s8x16>(sc_round_and_cast<vec_s32x16>(__cached_3));
        vec_s8x16 __cached_7;
        __cached_7 = __cached_6;
        vec_s8x16::store(__cached_7, &__outs_0[(((fused_0n__k_1894 / 8UL) * 131072UL) + ((((_fuseiter7063 + ((fused_0n__k_1894 % 8UL) * 64UL)) / 256UL) * 65536UL) + (((_fuseiter7061 + 1UL) * 4096UL) + (((_fuseiter7062 + 1UL) * 256UL) + ((_fuseiter7063 + ((fused_0n__k_1894 % 8UL) * 64UL)) % 256UL)))))]);
      }
    }
  }
  sc_thread_aligned_free(__stream, __origouts_1980_shr);
}

static void res5a_conv_0_cast_mul_add_relu_cast_reorder__6810_closure_311_0wrapper(void* __stream, int8_t* __restrict__ __module_data, uint64_t i, generic_val* __restrict__ args) noexcept{
  res5a_conv_0_cast_mul_add_relu_cast_reorder__6810_closure_311(i, (uint8_t*)(args[0UL].v_ptr), (int8_t*)(args[1UL].v_ptr), (float*)(args[2UL].v_ptr), (float*)(args[3UL].v_ptr), (int8_t*)(args[4UL].v_ptr));
}

static void mul__6580_closure_312(uint64_t fused_0fused_0fused_0__itr_0____itr_1_1895____itr_2_1896____itr_3_1897, float* __restrict__ __ins_0, float* __restrict__ __ins_1, float* __restrict__ __outs_0) noexcept{
  for (uint64_t _fuseiter_7098 = 0UL; _fuseiter_7098 < 128UL; _fuseiter_7098 += 16UL) {
    vec_f32x16 __cached_0;
    __cached_0 = vec_f32x16::load(&__ins_0[((((fused_0fused_0fused_0__itr_0____itr_1_1895____itr_2_1896____itr_3_1897 / 4UL) * 512UL) + ((fused_0fused_0fused_0__itr_0____itr_1_1895____itr_2_1896____itr_3_1897 % 4UL) * 128UL)) + _fuseiter_7098)]);
    float __cached_1;
    __cached_1 = __ins_1[0];
    vec_f32x16 __cached_2;
    __cached_2 = (__cached_0 * vec_f32x16(__cached_1));
    vec_f32x16::store(__cached_2, &__outs_0[((((fused_0fused_0fused_0__itr_0____itr_1_1895____itr_2_1896____itr_3_1897 / 4UL) * 512UL) + ((fused_0fused_0fused_0__itr_0____itr_1_1895____itr_2_1896____itr_3_1897 % 4UL) * 128UL)) + _fuseiter_7098)]);
  }
}

static void mul__6580_closure_312_0wrapper(void* __stream, int8_t* __restrict__ __module_data, uint64_t i, generic_val* __restrict__ args) noexcept{
  mul__6580_closure_312(i, (float*)(args[0UL].v_ptr), (float*)(args[1UL].v_ptr), (float*)(args[2UL].v_ptr));
}

static void mul__6570_closure_313(uint64_t fused_0fused_0fused_0__itr_0____itr_1_1898____itr_2_1899____itr_3_1900, float* __restrict__ __ins_0, float* __restrict__ __ins_1, float* __restrict__ __outs_0) noexcept{
  for (uint64_t _fuseiter_7104 = 0UL; _fuseiter_7104 < 128UL; _fuseiter_7104 += 16UL) {
    vec_f32x16 __cached_0;
    __cached_0 = vec_f32x16::load(&__ins_0[((((fused_0fused_0fused_0__itr_0____itr_1_1898____itr_2_1899____itr_3_1900 / 4UL) * 512UL) + ((fused_0fused_0fused_0__itr_0____itr_1_1898____itr_2_1899____itr_3_1900 % 4UL) * 128UL)) + _fuseiter_7104)]);
    float __cached_1;
    __cached_1 = __ins_1[0];
    vec_f32x16 __cached_2;
    __cached_2 = (__cached_0 * vec_f32x16(__cached_1));
    vec_f32x16::store(__cached_2, &__outs_0[((((fused_0fused_0fused_0__itr_0____itr_1_1898____itr_2_1899____itr_3_1900 / 4UL) * 512UL) + ((fused_0fused_0fused_0__itr_0____itr_1_1898____itr_2_1899____itr_3_1900 % 4UL) * 128UL)) + _fuseiter_7104)]);
  }
}

static void mul__6570_closure_313_0wrapper(void* __stream, int8_t* __restrict__ __module_data, uint64_t i, generic_val* __restrict__ args) noexcept{
  mul__6570_closure_313(i, (float*)(args[0UL].v_ptr), (float*)(args[1UL].v_ptr), (float*)(args[2UL].v_ptr));
}

static void res5a_conv_1_cast_mul_add_relu_cast_reorder__6800_closure_314(uint64_t fused_0k_o__n_1901, int32_t* __restrict__ conv_os_acc_size, int32_t* __restrict__ conv_os_blk_size, int8_t* __restrict__ __ins_0, int8_t* __restrict__ __ins_1, float* __restrict__ __ins_2, float* __restrict__ __ins_3, int8_t* __restrict__ __outs_0) noexcept{
  void** __sc_kernel_cache_arr_186 = (void**)&__uninitialized_data[23657560UL];
  int8_t* __rescheduled_1 = (int8_t*)sc_thread_aligned_malloc(__stream, 384UL);
  int32_t* __origouts_1990_shr = (int32_t*)sc_thread_aligned_malloc(__stream, 25088UL);
  void** A_list = (void**)&__rescheduled_1[0UL];
  void** B_list = (void**)&__rescheduled_1[192UL];
  int32_t __cached_2;
  __cached_2 = conv_os_acc_size[0UL];
  int32_t __cached_3;
  __cached_3 = conv_os_blk_size[0UL];
  memset(&__origouts_1990_shr[(uint64_t)(((__cached_2 / 7) * 896) + ((__cached_2 % 7) * 128))], 0, ((uint64_t)(__cached_3 * 128) * 4UL));
  for (uint64_t c_o = 0UL; c_o < 2UL; c_o += 1UL) {
    for (uint64_t r = 0UL; r < 3UL; r += 1UL) {
      for (uint64_t s = 0UL; s < 3UL; s += 1UL) {
        void* __cached_4;
        __cached_4 = &__ins_0[(((fused_0k_o__n_1901 % 4UL) * 131072UL) + ((c_o * 65536UL) + ((r * 4096UL) + (s * 256UL))))];
        A_list[(((c_o * 9UL) + (r * 3UL)) + s)] = __cached_4;
        void* __cached_5;
        __cached_5 = &__ins_1[(((fused_0k_o__n_1901 / 4UL) * 589824UL) + ((c_o * 294912UL) + ((r * 98304UL) + (s * 32768UL))))];
        B_list[(((c_o * 9UL) + (r * 3UL)) + s)] = __cached_5;
      }
    }
  }
  void* _arg_cache_43 = &__origouts_1990_shr[(uint64_t)(((__cached_2 / 7) * 896) + ((__cached_2 % 7) * 128))];
  dnnl_brgemm_list_call(__sc_kernel_cache_arr_186[0UL], A_list, B_list, &__origouts_1990_shr[(uint64_t)(((__cached_2 / 7) * 896) + ((__cached_2 % 7) * 128))], 1, 256, 32768, 18, 7, 7, __stream);
  for (uint64_t _fuseiter7108 = 0UL; _fuseiter7108 < 7UL; _fuseiter7108 += 1UL) {
    for (uint64_t _fuseiter7109 = 0UL; _fuseiter7109 < 7UL; _fuseiter7109 += 1UL) {
      for (uint64_t _fuseiter7110 = 0UL; _fuseiter7110 < 128UL; _fuseiter7110 += 16UL) {
        vec_s32x16 __cached_6;
        __cached_6 = vec_s32x16::load(&__origouts_1990_shr[((_fuseiter7108 * 896UL) + ((_fuseiter7109 * 128UL) + _fuseiter7110))]);
        vec_f32x16 __cached_7;
        __cached_7 = (vec_f32x16)(__cached_6);
        vec_f32x16 __cached_8;
        __cached_8 = vec_f32x16::load(&__ins_2[(((fused_0k_o__n_1901 / 4UL) * 128UL) + _fuseiter7110)]);
        __cached_7 = (__cached_7 * __cached_8);
        vec_f32x16 __cached_9;
        __cached_9 = vec_f32x16::load(&__ins_3[(((fused_0k_o__n_1901 / 4UL) * 128UL) + _fuseiter7110)]);
        __cached_7 = (__cached_7 + __cached_9);
        __cached_7 = sc_max(__cached_7, vec_f32x16(0.f));
        vec_s8x16 __cached_10;
        __cached_10 = sc_saturated_cast<vec_s8x16>(sc_round_and_cast<vec_s32x16>(__cached_7));
        vec_s8x16 __cached_11;
        __cached_11 = __cached_10;
        vec_s8x16::store(__cached_11, &__outs_0[(((fused_0k_o__n_1901 % 4UL) * 25088UL) + ((((_fuseiter7110 + ((fused_0k_o__n_1901 / 4UL) * 128UL)) / 256UL) * 12544UL) + ((_fuseiter7108 * 1792UL) + ((_fuseiter7109 * 256UL) + ((_fuseiter7110 + ((fused_0k_o__n_1901 / 4UL) * 128UL)) % 256UL)))))]);
      }
    }
  }
  sc_thread_aligned_free(__stream, __origouts_1990_shr);
  sc_thread_aligned_free(__stream, __rescheduled_1);
}

static void res5a_conv_1_cast_mul_add_relu_cast_reorder__6800_closure_314_0wrapper(void* __stream, int8_t* __restrict__ __module_data, uint64_t i, generic_val* __restrict__ args) noexcept{
  res5a_conv_1_cast_mul_add_relu_cast_reorder__6800_closure_314(i, (int32_t*)(args[0UL].v_ptr), (int32_t*)(args[1UL].v_ptr), (int8_t*)(args[2UL].v_ptr), (int8_t*)(args[3UL].v_ptr), (float*)(args[4UL].v_ptr), (float*)(args[5UL].v_ptr), (int8_t*)(args[6UL].v_ptr));
}

static void res5a_conv_b_cast_mul_add_cast_reorder__6820_closure_315(uint64_t fused_0n__c_o_1902, uint8_t* __restrict__ __ins_0, uint8_t* __restrict__ input_tmp) noexcept{
  for (uint64_t p = 0UL; p < 7UL; p += 1UL) {
    for (uint64_t q = 0UL; q < 7UL; q += 1UL) {
      for (uint64_t c_i = 0UL; c_i < 256UL; c_i += 64UL) {
        vec_u8x64 __cached_0;
        __cached_0 = vec_u8x64::load(&__ins_0[(((fused_0n__c_o_1902 / 4UL) * 200704UL) + (((fused_0n__c_o_1902 % 4UL) * 50176UL) + ((p * 7168UL) + ((q * 512UL) + c_i))))]);
        vec_u8x64 __cached_1;
        __cached_1 = __cached_0;
        vec_u8x64::store(__cached_1, &input_tmp[(((fused_0n__c_o_1902 / 4UL) * 50176UL) + (((fused_0n__c_o_1902 % 4UL) * 12544UL) + ((p * 1792UL) + ((q * 256UL) + c_i))))]);
      }
    }
  }
}

static void res5a_conv_b_cast_mul_add_cast_reorder__6820_closure_315_0wrapper(void* __stream, int8_t* __restrict__ __module_data, uint64_t i, generic_val* __restrict__ args) noexcept{
  res5a_conv_b_cast_mul_add_cast_reorder__6820_closure_315(i, (uint8_t*)(args[0UL].v_ptr), (uint8_t*)(args[1UL].v_ptr));
}

static void res5a_conv_b_cast_mul_add_cast_reorder__6820_closure_316(uint64_t fused_0k__n_1903, uint8_t* __restrict__ input_tmp, int8_t* __restrict__ __ins_1, float* __restrict__ __ins_2, float* __restrict__ __ins_3, int8_t* __restrict__ __outs_0) noexcept{
  void*& __sc_kernel_cache_188 = *(void**)(__module_data + 288);
  alignas(64) int8_t __rescheduled_2[128UL];
  int32_t* __origouts_2000_shr = (int32_t*)sc_thread_aligned_malloc(__stream, 3136UL);
  void** A_list = (void**)&__rescheduled_2[0UL];
  void** B_list = (void**)&__rescheduled_2[64UL];
  for (uint64_t c = 0UL; c < 4UL; c += 1UL) {
    void* __cached_2;
    __cached_2 = &input_tmp[(((fused_0k__n_1903 % 4UL) * 50176UL) + (c * 12544UL))];
    A_list[c] = __cached_2;
    void* __cached_3;
    __cached_3 = &__ins_1[(((fused_0k__n_1903 / 4UL) * 16384UL) + (c * 4096UL))];
    B_list[c] = __cached_3;
  }
  void* _arg_cache_44 = &__origouts_2000_shr[0UL];
  dnnl_brgemm_list_call(__sc_kernel_cache_188, A_list, B_list, &__origouts_2000_shr[0UL], 1, 1, 1, 4, 8, 7, __stream);
  for (uint64_t _fuseiter7143 = 0UL; _fuseiter7143 < 7UL; _fuseiter7143 += 1UL) {
    for (uint64_t _fuseiter7144 = 0UL; _fuseiter7144 < 7UL; _fuseiter7144 += 1UL) {
      vec_s32x16 __cached_4;
      __cached_4 = vec_s32x16::load(&__origouts_2000_shr[((_fuseiter7143 * 112UL) + (_fuseiter7144 * 16UL))]);
      vec_f32x16 __cached_5;
      __cached_5 = (vec_f32x16)(__cached_4);
      vec_f32x16 __cached_6;
      __cached_6 = vec_f32x16::load(&__ins_2[((fused_0k__n_1903 / 4UL) * 16UL)]);
      __cached_5 = (__cached_5 * __cached_6);
      vec_f32x16 __cached_7;
      __cached_7 = vec_f32x16::load(&__ins_3[((fused_0k__n_1903 / 4UL) * 16UL)]);
      __cached_5 = (__cached_5 + __cached_7);
      vec_s8x16 __cached_8;
      __cached_8 = sc_saturated_cast<vec_s8x16>(sc_round_and_cast<vec_s32x16>(__cached_5));
      vec_s8x16 __cached_9;
      __cached_9 = __cached_8;
      vec_s8x16::store(__cached_9, &__outs_0[(((fused_0k__n_1903 % 4UL) * 100352UL) + (((((fused_0k__n_1903 / 4UL) * 16UL) / 64UL) * 3136UL) + ((_fuseiter7143 * 448UL) + ((_fuseiter7144 * 64UL) + (((fused_0k__n_1903 / 4UL) * 16UL) % 64UL)))))]);
    }
  }
  sc_thread_aligned_free(__stream, __origouts_2000_shr);
}

static void res5a_conv_b_cast_mul_add_cast_reorder__6820_closure_316_0wrapper(void* __stream, int8_t* __restrict__ __module_data, uint64_t i, generic_val* __restrict__ args) noexcept{
  res5a_conv_b_cast_mul_add_cast_reorder__6820_closure_316(i, (uint8_t*)(args[0UL].v_ptr), (int8_t*)(args[1UL].v_ptr), (float*)(args[2UL].v_ptr), (float*)(args[3UL].v_ptr), (int8_t*)(args[4UL].v_ptr));
}

static void mul__6600_closure_317(uint64_t fused_0fused_0fused_0__itr_0____itr_1_1904____itr_2_1905____itr_3_1906, float* __restrict__ __ins_0, float* __restrict__ __ins_1, float* __restrict__ __outs_0) noexcept{
  for (uint64_t _fuseiter_7174 = 0UL; _fuseiter_7174 < 64UL; _fuseiter_7174 += 16UL) {
    vec_f32x16 __cached_0;
    __cached_0 = vec_f32x16::load(&__ins_0[((((fused_0fused_0fused_0__itr_0____itr_1_1904____itr_2_1905____itr_3_1906 / 32UL) * 2048UL) + ((fused_0fused_0fused_0__itr_0____itr_1_1904____itr_2_1905____itr_3_1906 % 32UL) * 64UL)) + _fuseiter_7174)]);
    float __cached_1;
    __cached_1 = __ins_1[0];
    vec_f32x16 __cached_2;
    __cached_2 = (__cached_0 * vec_f32x16(__cached_1));
    vec_f32x16::store(__cached_2, &__outs_0[((((fused_0fused_0fused_0__itr_0____itr_1_1904____itr_2_1905____itr_3_1906 / 32UL) * 2048UL) + ((fused_0fused_0fused_0__itr_0____itr_1_1904____itr_2_1905____itr_3_1906 % 32UL) * 64UL)) + _fuseiter_7174)]);
  }
}

static void mul__6600_closure_317_0wrapper(void* __stream, int8_t* __restrict__ __module_data, uint64_t i, generic_val* __restrict__ args) noexcept{
  mul__6600_closure_317(i, (float*)(args[0UL].v_ptr), (float*)(args[1UL].v_ptr), (float*)(args[2UL].v_ptr));
}

static void mul__6590_closure_318(uint64_t fused_0fused_0fused_0__itr_0____itr_1_1907____itr_2_1908____itr_3_1909, float* __restrict__ __ins_0, float* __restrict__ __ins_1, float* __restrict__ __outs_0) noexcept{
  for (uint64_t _fuseiter_7180 = 0UL; _fuseiter_7180 < 64UL; _fuseiter_7180 += 16UL) {
    vec_f32x16 __cached_0;
    __cached_0 = vec_f32x16::load(&__ins_0[((((fused_0fused_0fused_0__itr_0____itr_1_1907____itr_2_1908____itr_3_1909 / 32UL) * 2048UL) + ((fused_0fused_0fused_0__itr_0____itr_1_1907____itr_2_1908____itr_3_1909 % 32UL) * 64UL)) + _fuseiter_7180)]);
    float __cached_1;
    __cached_1 = __ins_1[0];
    vec_f32x16 __cached_2;
    __cached_2 = (__cached_0 * vec_f32x16(__cached_1));
    vec_f32x16::store(__cached_2, &__outs_0[((((fused_0fused_0fused_0__itr_0____itr_1_1907____itr_2_1908____itr_3_1909 / 32UL) * 2048UL) + ((fused_0fused_0fused_0__itr_0____itr_1_1907____itr_2_1908____itr_3_1909 % 32UL) * 64UL)) + _fuseiter_7180)]);
  }
}

static void mul__6590_closure_318_0wrapper(void* __stream, int8_t* __restrict__ __module_data, uint64_t i, generic_val* __restrict__ args) noexcept{
  mul__6590_closure_318(i, (float*)(args[0UL].v_ptr), (float*)(args[1UL].v_ptr), (float*)(args[2UL].v_ptr));
}

static void reorder__5400_closure_319(uint64_t _fuseiter_7182, int8_t* __restrict__ __ins_0, int8_t* __restrict__ __outs_0) noexcept{
  for (uint64_t _fuseiter_7183 = 0UL; _fuseiter_7183 < 2UL; _fuseiter_7183 += 1UL) {
    for (uint64_t _fuseiter_7186 = 0UL; _fuseiter_7186 < 64UL; _fuseiter_7186 += 1UL) {
      for (uint64_t _fuseiter_7187 = 0UL; _fuseiter_7187 < 64UL; _fuseiter_7187 += 1UL) {
        for (uint64_t _fuseiter_7188 = 0UL; _fuseiter_7188 < 4UL; _fuseiter_7188 += 1UL) {
          int8_t __cached_0;
          __cached_0 = __ins_0[(((_fuseiter_7187 + (_fuseiter_7182 * 64UL)) * 512UL) + ((_fuseiter_7188 + (_fuseiter_7186 * 4UL)) + (_fuseiter_7183 * 256UL)))];
          int8_t __cached_1;
          __cached_1 = __cached_0;
          __outs_0[((_fuseiter_7182 * 32768UL) + ((_fuseiter_7183 * 16384UL) + ((_fuseiter_7186 * 256UL) + ((_fuseiter_7187 * 4UL) + _fuseiter_7188))))] = __cached_1;
        }
      }
    }
  }
}

static void reorder__5400_closure_319_0wrapper(void* __stream, int8_t* __restrict__ __module_data, uint64_t i, generic_val* __restrict__ args) noexcept{
  reorder__5400_closure_319(i, (int8_t*)(args[0UL].v_ptr), (int8_t*)(args[1UL].v_ptr));
}

static void mul__6620_closure_320(uint64_t fused_0fused_0fused_0__itr_0____itr_1_1910____itr_2_1911____itr_3_1912, float* __restrict__ __ins_0, float* __restrict__ __ins_1, float* __restrict__ __outs_0) noexcept{
  for (uint64_t _fuseiter_7193 = 0UL; _fuseiter_7193 < 128UL; _fuseiter_7193 += 16UL) {
    vec_f32x16 __cached_0;
    __cached_0 = vec_f32x16::load(&__ins_0[((((fused_0fused_0fused_0__itr_0____itr_1_1910____itr_2_1911____itr_3_1912 / 4UL) * 512UL) + ((fused_0fused_0fused_0__itr_0____itr_1_1910____itr_2_1911____itr_3_1912 % 4UL) * 128UL)) + _fuseiter_7193)]);
    float __cached_1;
    __cached_1 = __ins_1[0];
    vec_f32x16 __cached_2;
    __cached_2 = (__cached_0 * vec_f32x16(__cached_1));
    vec_f32x16::store(__cached_2, &__outs_0[((((fused_0fused_0fused_0__itr_0____itr_1_1910____itr_2_1911____itr_3_1912 / 4UL) * 512UL) + ((fused_0fused_0fused_0__itr_0____itr_1_1910____itr_2_1911____itr_3_1912 % 4UL) * 128UL)) + _fuseiter_7193)]);
  }
}

static void mul__6620_closure_320_0wrapper(void* __stream, int8_t* __restrict__ __module_data, uint64_t i, generic_val* __restrict__ args) noexcept{
  mul__6620_closure_320(i, (float*)(args[0UL].v_ptr), (float*)(args[1UL].v_ptr), (float*)(args[2UL].v_ptr));
}

static void mul__6610_closure_321(uint64_t fused_0fused_0fused_0__itr_0____itr_1_1913____itr_2_1914____itr_3_1915, float* __restrict__ __ins_0, float* __restrict__ __ins_1, float* __restrict__ __outs_0) noexcept{
  for (uint64_t _fuseiter_7199 = 0UL; _fuseiter_7199 < 128UL; _fuseiter_7199 += 16UL) {
    vec_f32x16 __cached_0;
    __cached_0 = vec_f32x16::load(&__ins_0[((((fused_0fused_0fused_0__itr_0____itr_1_1913____itr_2_1914____itr_3_1915 / 4UL) * 512UL) + ((fused_0fused_0fused_0__itr_0____itr_1_1913____itr_2_1914____itr_3_1915 % 4UL) * 128UL)) + _fuseiter_7199)]);
    float __cached_1;
    __cached_1 = __ins_1[0];
    vec_f32x16 __cached_2;
    __cached_2 = (__cached_0 * vec_f32x16(__cached_1));
    vec_f32x16::store(__cached_2, &__outs_0[((((fused_0fused_0fused_0__itr_0____itr_1_1913____itr_2_1914____itr_3_1915 / 4UL) * 512UL) + ((fused_0fused_0fused_0__itr_0____itr_1_1913____itr_2_1914____itr_3_1915 % 4UL) * 128UL)) + _fuseiter_7199)]);
  }
}

static void mul__6610_closure_321_0wrapper(void* __stream, int8_t* __restrict__ __module_data, uint64_t i, generic_val* __restrict__ args) noexcept{
  mul__6610_closure_321(i, (float*)(args[0UL].v_ptr), (float*)(args[1UL].v_ptr), (float*)(args[2UL].v_ptr));
}

static void reorder__5430_closure_322(uint64_t fused_0_fuseiter_7201___fuseiter_7202_1916, int8_t* __restrict__ __ins_0, int8_t* __restrict__ __outs_0) noexcept{
  for (uint64_t _fuseiter_7205 = 0UL; _fuseiter_7205 < 16UL; _fuseiter_7205 += 1UL) {
    for (uint64_t _fuseiter_7206 = 0UL; _fuseiter_7206 < 128UL; _fuseiter_7206 += 1UL) {
      for (uint64_t _fuseiter_7207 = 0UL; _fuseiter_7207 < 4UL; _fuseiter_7207 += 1UL) {
        int8_t __cached_0;
        __cached_0 = __ins_0[(((_fuseiter_7206 + ((fused_0_fuseiter_7201___fuseiter_7202_1916 / 32UL) * 128UL)) * 2048UL) + ((_fuseiter_7207 + (_fuseiter_7205 * 4UL)) + ((fused_0_fuseiter_7201___fuseiter_7202_1916 % 32UL) * 64UL)))];
        int8_t __cached_1;
        __cached_1 = __cached_0;
        __outs_0[(((fused_0_fuseiter_7201___fuseiter_7202_1916 / 32UL) * 262144UL) + (((fused_0_fuseiter_7201___fuseiter_7202_1916 % 32UL) * 8192UL) + ((_fuseiter_7205 * 512UL) + ((_fuseiter_7206 * 4UL) + _fuseiter_7207))))] = __cached_1;
      }
    }
  }
}

static void reorder__5430_closure_322_0wrapper(void* __stream, int8_t* __restrict__ __module_data, uint64_t i, generic_val* __restrict__ args) noexcept{
  reorder__5430_closure_322(i, (int8_t*)(args[0UL].v_ptr), (int8_t*)(args[1UL].v_ptr));
}

static void mul__2490_closure_323(uint64_t fused_0fused_0__itr_0____itr_1_1917____itr_2_1918, float* __restrict__ __ins_0, float* __restrict__ __ins_1, float* __restrict__ __outs_0) noexcept{
  for (uint64_t _fuseiter_7212 = 0UL; _fuseiter_7212 < 3UL; _fuseiter_7212 += 1UL) {
    float __cached_0;
    __cached_0 = __ins_0[((((fused_0fused_0__itr_0____itr_1_1917____itr_2_1918 / 1536UL) * 4608UL) + ((((fused_0fused_0__itr_0____itr_1_1917____itr_2_1918 / 3UL) % 512UL) * 9UL) + ((fused_0fused_0__itr_0____itr_1_1917____itr_2_1918 % 3UL) * 3UL))) + _fuseiter_7212)];
    float __cached_1;
    __cached_1 = __ins_1[(fused_0fused_0__itr_0____itr_1_1917____itr_2_1918 / 1536UL)];
    float __cached_2;
    __cached_2 = (__cached_0 * __cached_1);
    __outs_0[((((fused_0fused_0__itr_0____itr_1_1917____itr_2_1918 / 1536UL) * 4608UL) + ((((fused_0fused_0__itr_0____itr_1_1917____itr_2_1918 / 3UL) % 512UL) * 9UL) + ((fused_0fused_0__itr_0____itr_1_1917____itr_2_1918 % 3UL) * 3UL))) + _fuseiter_7212)] = __cached_2;
  }
}

static void mul__2490_closure_323_0wrapper(void* __stream, int8_t* __restrict__ __module_data, uint64_t i, generic_val* __restrict__ args) noexcept{
  mul__2490_closure_323(i, (float*)(args[0UL].v_ptr), (float*)(args[1UL].v_ptr), (float*)(args[2UL].v_ptr));
}

static void cast__2500_closure_324(uint64_t fused_0fused_0__itr_0____itr_1_1919____itr_2_1920, float* __restrict__ __ins_0, int8_t* __restrict__ __outs_0) noexcept{
  for (uint64_t _fuseiter7217 = 0UL; _fuseiter7217 < 3UL; _fuseiter7217 += 1UL) {
    float __cached_0;
    __cached_0 = __ins_0[((((fused_0fused_0__itr_0____itr_1_1919____itr_2_1920 / 1536UL) * 4608UL) + ((((fused_0fused_0__itr_0____itr_1_1919____itr_2_1920 / 3UL) % 512UL) * 9UL) + ((fused_0fused_0__itr_0____itr_1_1919____itr_2_1920 % 3UL) * 3UL))) + _fuseiter7217)];
    int8_t __cached_1;
    __cached_1 = (int8_t)sc_max(sc_min(sc_round_and_cast<int32_t>(__cached_0), 127), -128);
    __outs_0[((((fused_0fused_0__itr_0____itr_1_1919____itr_2_1920 / 1536UL) * 4608UL) + ((((fused_0fused_0__itr_0____itr_1_1919____itr_2_1920 / 3UL) % 512UL) * 9UL) + ((fused_0fused_0__itr_0____itr_1_1919____itr_2_1920 % 3UL) * 3UL))) + _fuseiter7217)] = __cached_1;
  }
}

static void cast__2500_closure_324_0wrapper(void* __stream, int8_t* __restrict__ __module_data, uint64_t i, generic_val* __restrict__ args) noexcept{
  cast__2500_closure_324(i, (float*)(args[0UL].v_ptr), (int8_t*)(args[1UL].v_ptr));
}

static void mul__2580_closure_325(uint64_t fused_0fused_0__itr_0____itr_1_1921____itr_2_1922, float* __restrict__ __ins_0, float* __restrict__ __ins_1, float* __restrict__ __outs_0) noexcept{
  for (uint64_t _fuseiter_7222 = 0UL; _fuseiter_7222 < 3UL; _fuseiter_7222 += 1UL) {
    float __cached_0;
    __cached_0 = __ins_0[((((fused_0fused_0__itr_0____itr_1_1921____itr_2_1922 / 1536UL) * 4608UL) + ((((fused_0fused_0__itr_0____itr_1_1921____itr_2_1922 / 3UL) % 512UL) * 9UL) + ((fused_0fused_0__itr_0____itr_1_1921____itr_2_1922 % 3UL) * 3UL))) + _fuseiter_7222)];
    float __cached_1;
    __cached_1 = __ins_1[(fused_0fused_0__itr_0____itr_1_1921____itr_2_1922 / 1536UL)];
    float __cached_2;
    __cached_2 = (__cached_0 * __cached_1);
    __outs_0[((((fused_0fused_0__itr_0____itr_1_1921____itr_2_1922 / 1536UL) * 4608UL) + ((((fused_0fused_0__itr_0____itr_1_1921____itr_2_1922 / 3UL) % 512UL) * 9UL) + ((fused_0fused_0__itr_0____itr_1_1921____itr_2_1922 % 3UL) * 3UL))) + _fuseiter_7222)] = __cached_2;
  }
}

static void mul__2580_closure_325_0wrapper(void* __stream, int8_t* __restrict__ __module_data, uint64_t i, generic_val* __restrict__ args) noexcept{
  mul__2580_closure_325(i, (float*)(args[0UL].v_ptr), (float*)(args[1UL].v_ptr), (float*)(args[2UL].v_ptr));
}

static void cast__2590_closure_326(uint64_t fused_0fused_0__itr_0____itr_1_1923____itr_2_1924, float* __restrict__ __ins_0, int8_t* __restrict__ __outs_0) noexcept{
  for (uint64_t _fuseiter7227 = 0UL; _fuseiter7227 < 3UL; _fuseiter7227 += 1UL) {
    float __cached_0;
    __cached_0 = __ins_0[((((fused_0fused_0__itr_0____itr_1_1923____itr_2_1924 / 1536UL) * 4608UL) + ((((fused_0fused_0__itr_0____itr_1_1923____itr_2_1924 / 3UL) % 512UL) * 9UL) + ((fused_0fused_0__itr_0____itr_1_1923____itr_2_1924 % 3UL) * 3UL))) + _fuseiter7227)];
    int8_t __cached_1;
    __cached_1 = (int8_t)sc_max(sc_min(sc_round_and_cast<int32_t>(__cached_0), 127), -128);
    __outs_0[((((fused_0fused_0__itr_0____itr_1_1923____itr_2_1924 / 1536UL) * 4608UL) + ((((fused_0fused_0__itr_0____itr_1_1923____itr_2_1924 / 3UL) % 512UL) * 9UL) + ((fused_0fused_0__itr_0____itr_1_1923____itr_2_1924 % 3UL) * 3UL))) + _fuseiter7227)] = __cached_1;
  }
}

static void cast__2590_closure_326_0wrapper(void* __stream, int8_t* __restrict__ __module_data, uint64_t i, generic_val* __restrict__ args) noexcept{
  cast__2590_closure_326(i, (float*)(args[0UL].v_ptr), (int8_t*)(args[1UL].v_ptr));
}

static void mul__6640_closure_327(uint64_t fused_0fused_0fused_0__itr_0____itr_1_1925____itr_2_1926____itr_3_1927, float* __restrict__ __ins_0, float* __restrict__ __ins_1, float* __restrict__ __outs_0) noexcept{
  vec_f32x16 __cached_0;
  __cached_0 = vec_f32x16::load(&__ins_0[(((fused_0fused_0fused_0__itr_0____itr_1_1925____itr_2_1926____itr_3_1927 / 32UL) * 512UL) + ((fused_0fused_0fused_0__itr_0____itr_1_1925____itr_2_1926____itr_3_1927 % 32UL) * 16UL))]);
  float __cached_1;
  __cached_1 = __ins_1[0];
  vec_f32x16 __cached_2;
  __cached_2 = (__cached_0 * vec_f32x16(__cached_1));
  vec_f32x16::store(__cached_2, &__outs_0[(((fused_0fused_0fused_0__itr_0____itr_1_1925____itr_2_1926____itr_3_1927 / 32UL) * 512UL) + ((fused_0fused_0fused_0__itr_0____itr_1_1925____itr_2_1926____itr_3_1927 % 32UL) * 16UL))]);
}

static void mul__6640_closure_327_0wrapper(void* __stream, int8_t* __restrict__ __module_data, uint64_t i, generic_val* __restrict__ args) noexcept{
  mul__6640_closure_327(i, (float*)(args[0UL].v_ptr), (float*)(args[1UL].v_ptr), (float*)(args[2UL].v_ptr));
}

static void mul__6630_closure_328(uint64_t fused_0fused_0fused_0__itr_0____itr_1_1928____itr_2_1929____itr_3_1930, float* __restrict__ __ins_0, float* __restrict__ __ins_1, float* __restrict__ __outs_0) noexcept{
  vec_f32x16 __cached_0;
  __cached_0 = vec_f32x16::load(&__ins_0[(((fused_0fused_0fused_0__itr_0____itr_1_1928____itr_2_1929____itr_3_1930 / 32UL) * 512UL) + ((fused_0fused_0fused_0__itr_0____itr_1_1928____itr_2_1929____itr_3_1930 % 32UL) * 16UL))]);
  float __cached_1;
  __cached_1 = __ins_1[0];
  vec_f32x16 __cached_2;
  __cached_2 = (__cached_0 * vec_f32x16(__cached_1));
  vec_f32x16::store(__cached_2, &__outs_0[(((fused_0fused_0fused_0__itr_0____itr_1_1928____itr_2_1929____itr_3_1930 / 32UL) * 512UL) + ((fused_0fused_0fused_0__itr_0____itr_1_1928____itr_2_1929____itr_3_1930 % 32UL) * 16UL))]);
}

static void mul__6630_closure_328_0wrapper(void* __stream, int8_t* __restrict__ __module_data, uint64_t i, generic_val* __restrict__ args) noexcept{
  mul__6630_closure_328(i, (float*)(args[0UL].v_ptr), (float*)(args[1UL].v_ptr), (float*)(args[2UL].v_ptr));
}

static void reorder__5460_closure_329(uint64_t _fuseiter_7240, int8_t* __restrict__ __ins_0, int8_t* __restrict__ __outs_0) noexcept{
  for (uint64_t _fuseiter_7242 = 0UL; _fuseiter_7242 < 3UL; _fuseiter_7242 += 1UL) {
    for (uint64_t _fuseiter_7243 = 0UL; _fuseiter_7243 < 3UL; _fuseiter_7243 += 1UL) {
      for (uint64_t _fuseiter_7244 = 0UL; _fuseiter_7244 < 128UL; _fuseiter_7244 += 1UL) {
        for (uint64_t _fuseiter_7245 = 0UL; _fuseiter_7245 < 16UL; _fuseiter_7245 += 1UL) {
          for (uint64_t _fuseiter_7246 = 0UL; _fuseiter_7246 < 4UL; _fuseiter_7246 += 1UL) {
            int8_t __cached_0;
            __cached_0 = __ins_0[(((_fuseiter_7245 + (_fuseiter_7240 * 16UL)) * 4608UL) + (((_fuseiter_7246 + (_fuseiter_7244 * 4UL)) * 9UL) + ((_fuseiter_7242 * 3UL) + _fuseiter_7243)))];
            int8_t __cached_1;
            __cached_1 = __cached_0;
            __outs_0[((_fuseiter_7240 * 73728UL) + ((_fuseiter_7242 * 24576UL) + ((_fuseiter_7243 * 8192UL) + ((_fuseiter_7244 * 64UL) + ((_fuseiter_7245 * 4UL) + _fuseiter_7246)))))] = __cached_1;
          }
        }
      }
    }
  }
}

static void reorder__5460_closure_329_0wrapper(void* __stream, int8_t* __restrict__ __module_data, uint64_t i, generic_val* __restrict__ args) noexcept{
  reorder__5460_closure_329(i, (int8_t*)(args[0UL].v_ptr), (int8_t*)(args[1UL].v_ptr));
}

static void res5a_conv_2_cast_mul_add_cast_add_cast__6790_closure_330(uint64_t fused_0k__n_1931, int8_t* __restrict__ __ins_0, int8_t* __restrict__ __ins_1, float* __restrict__ __ins_2, float* __restrict__ __ins_3, int8_t* __restrict__ __ins_4, uint8_t* __restrict__ __outs_0) noexcept{
  void*& __sc_kernel_cache_190 = *(void**)(__module_data + 296);
  alignas(64) int8_t __rescheduled_1[128UL];
  int32_t* __origouts_2010_shr = (int32_t*)sc_thread_aligned_malloc(__stream, 12544UL);
  void** A_list = (void**)&__rescheduled_1[0UL];
  void** B_list = (void**)&__rescheduled_1[64UL];
  for (uint64_t c = 0UL; c < 2UL; c += 1UL) {
    void* __cached_0;
    __cached_0 = &__ins_0[(((fused_0k__n_1931 % 4UL) * 25088UL) + (c * 12544UL))];
    A_list[c] = __cached_0;
    void* __cached_1;
    __cached_1 = &__ins_1[(((fused_0k__n_1931 / 4UL) * 32768UL) + (c * 16384UL))];
    B_list[c] = __cached_1;
  }
  void* _arg_cache_45 = &__origouts_2010_shr[0UL];
  dnnl_brgemm_list_call(__sc_kernel_cache_190, A_list, B_list, &__origouts_2010_shr[0UL], 1, 1, 1, 2, 7, 7, __stream);
  for (uint64_t _fuseiter7249 = 0UL; _fuseiter7249 < 7UL; _fuseiter7249 += 1UL) {
    for (uint64_t _fuseiter7250 = 0UL; _fuseiter7250 < 7UL; _fuseiter7250 += 1UL) {
      for (uint64_t _fuseiter7251 = 0UL; _fuseiter7251 < 64UL; _fuseiter7251 += 16UL) {
        vec_s32x16 __cached_2;
        __cached_2 = vec_s32x16::load(&__origouts_2010_shr[((_fuseiter7249 * 448UL) + ((_fuseiter7250 * 64UL) + _fuseiter7251))]);
        vec_f32x16 __cached_3;
        __cached_3 = (vec_f32x16)(__cached_2);
        vec_f32x16 __cached_4;
        __cached_4 = vec_f32x16::load(&__ins_2[(((fused_0k__n_1931 / 4UL) * 64UL) + _fuseiter7251)]);
        __cached_3 = (__cached_3 * __cached_4);
        vec_f32x16 __cached_5;
        __cached_5 = vec_f32x16::load(&__ins_3[(((fused_0k__n_1931 / 4UL) * 64UL) + _fuseiter7251)]);
        __cached_3 = (__cached_3 + __cached_5);
        vec_s8x16 __cached_6;
        __cached_6 = sc_saturated_cast<vec_s8x16>(sc_round_and_cast<vec_s32x16>(__cached_3));
        vec_s8x16 __cached_7;
        __cached_7 = vec_s8x16::load(&__ins_4[((((fused_0k__n_1931 % 4UL) * 100352UL) + ((fused_0k__n_1931 / 4UL) * 3136UL)) + ((_fuseiter7249 * 448UL) + ((_fuseiter7250 * 64UL) + _fuseiter7251)))]);
        __cached_6 = (__cached_6 + __cached_7);
        vec_u8x16 __cached_8;
        __cached_8 = (vec_u8x16)(sc_max(__cached_6, vec_s8x16(0)));
        vec_u8x16::store(__cached_8, &__outs_0[((((fused_0k__n_1931 % 4UL) * 100352UL) + ((fused_0k__n_1931 / 4UL) * 3136UL)) + ((_fuseiter7249 * 448UL) + ((_fuseiter7250 * 64UL) + _fuseiter7251)))]);
      }
    }
  }
  sc_thread_aligned_free(__stream, __origouts_2010_shr);
}

static void res5a_conv_2_cast_mul_add_cast_add_cast__6790_closure_330_0wrapper(void* __stream, int8_t* __restrict__ __module_data, uint64_t i, generic_val* __restrict__ args) noexcept{
  res5a_conv_2_cast_mul_add_cast_add_cast__6790_closure_330(i, (int8_t*)(args[0UL].v_ptr), (int8_t*)(args[1UL].v_ptr), (float*)(args[2UL].v_ptr), (float*)(args[3UL].v_ptr), (int8_t*)(args[4UL].v_ptr), (uint8_t*)(args[5UL].v_ptr));
}

static void res5b_conv_0_cast_mul_add_relu_cast_reorder__6780_closure_331(uint64_t fused_0n__k_1932, int8_t* __restrict__ __outs_0) noexcept{
  memset(&__outs_0[(fused_0n__k_1932 * 41472UL)], 0, 4608UL);
  for (uint64_t p1 = 0UL; p1 < 7UL; p1 += 1UL) {
    memset(&__outs_0[((fused_0n__k_1932 * 41472UL) + ((p1 + 1UL) * 4608UL))], 0, 512UL);
    memset(&__outs_0[(((fused_0n__k_1932 * 41472UL) + ((p1 + 1UL) * 4608UL)) + 4096UL)], 0, 512UL);
  }
  memset(&__outs_0[((fused_0n__k_1932 * 41472UL) + 36864UL)], 0, 4608UL);
}

static void res5b_conv_0_cast_mul_add_relu_cast_reorder__6780_closure_331_0wrapper(void* __stream, int8_t* __restrict__ __module_data, uint64_t i, generic_val* __restrict__ args) noexcept{
  res5b_conv_0_cast_mul_add_relu_cast_reorder__6780_closure_331(i, (int8_t*)(args[0UL].v_ptr));
}

static void res5b_conv_0_cast_mul_add_relu_cast_reorder__6780_closure_332(uint64_t fused_0n__k_1933, uint8_t* __restrict__ __ins_0, int8_t* __restrict__ __ins_1, float* __restrict__ __ins_2, float* __restrict__ __ins_3, int8_t* __restrict__ __outs_0) noexcept{
  void*& __sc_kernel_cache_192 = *(void**)(__module_data + 304);
  int8_t* __rescheduled_2 = (int8_t*)sc_thread_aligned_malloc(__stream, 512UL);
  int32_t* __origouts_2020_shr = (int32_t*)sc_thread_aligned_malloc(__stream, 25088UL);
  void** A_list = (void**)&__rescheduled_2[0UL];
  void** B_list = (void**)&__rescheduled_2[256UL];
  for (uint64_t c = 0UL; c < 32UL; c += 1UL) {
    void* __cached_0;
    __cached_0 = &__ins_0[(((fused_0n__k_1933 / 4UL) * 100352UL) + (c * 3136UL))];
    A_list[c] = __cached_0;
    void* __cached_1;
    __cached_1 = &__ins_1[(((fused_0n__k_1933 % 4UL) * 262144UL) + (c * 8192UL))];
    B_list[c] = __cached_1;
  }
  void* _arg_cache_46 = &__origouts_2020_shr[0UL];
  dnnl_brgemm_list_call(__sc_kernel_cache_192, A_list, B_list, &__origouts_2020_shr[0UL], 1, 1, 1, 32, 8, 7, __stream);
  for (uint64_t _fuseiter7285 = 0UL; _fuseiter7285 < 7UL; _fuseiter7285 += 1UL) {
    for (uint64_t _fuseiter7286 = 0UL; _fuseiter7286 < 7UL; _fuseiter7286 += 1UL) {
      for (uint64_t _fuseiter7287 = 0UL; _fuseiter7287 < 128UL; _fuseiter7287 += 16UL) {
        vec_s32x16 __cached_2;
        __cached_2 = vec_s32x16::load(&__origouts_2020_shr[((_fuseiter7285 * 896UL) + ((_fuseiter7286 * 128UL) + _fuseiter7287))]);
        vec_f32x16 __cached_3;
        __cached_3 = (vec_f32x16)(__cached_2);
        vec_f32x16 __cached_4;
        __cached_4 = vec_f32x16::load(&__ins_2[(((fused_0n__k_1933 % 4UL) * 128UL) + _fuseiter7287)]);
        __cached_3 = (__cached_3 * __cached_4);
        vec_f32x16 __cached_5;
        __cached_5 = vec_f32x16::load(&__ins_3[(((fused_0n__k_1933 % 4UL) * 128UL) + _fuseiter7287)]);
        __cached_3 = (__cached_3 + __cached_5);
        __cached_3 = sc_max(__cached_3, vec_f32x16(0.f));
        vec_s8x16 __cached_6;
        __cached_6 = sc_saturated_cast<vec_s8x16>(sc_round_and_cast<vec_s32x16>(__cached_3));
        vec_s8x16 __cached_7;
        __cached_7 = __cached_6;
        vec_s8x16::store(__cached_7, &__outs_0[(((fused_0n__k_1933 / 4UL) * 41472UL) + ((((_fuseiter7287 + ((fused_0n__k_1933 % 4UL) * 128UL)) / 512UL) * 41472UL) + (((_fuseiter7285 + 1UL) * 4608UL) + (((_fuseiter7286 + 1UL) * 512UL) + ((_fuseiter7287 + ((fused_0n__k_1933 % 4UL) * 128UL)) % 512UL)))))]);
      }
    }
  }
  sc_thread_aligned_free(__stream, __origouts_2020_shr);
  sc_thread_aligned_free(__stream, __rescheduled_2);
}

static void res5b_conv_0_cast_mul_add_relu_cast_reorder__6780_closure_332_0wrapper(void* __stream, int8_t* __restrict__ __module_data, uint64_t i, generic_val* __restrict__ args) noexcept{
  res5b_conv_0_cast_mul_add_relu_cast_reorder__6780_closure_332(i, (uint8_t*)(args[0UL].v_ptr), (int8_t*)(args[1UL].v_ptr), (float*)(args[2UL].v_ptr), (float*)(args[3UL].v_ptr), (int8_t*)(args[4UL].v_ptr));
}

static void res5b_conv_1_cast_mul_add_relu_cast_reorder__6770_closure_333(uint64_t fused_0k_o__n_1934, int32_t* __restrict__ conv_os_acc_size, int32_t* __restrict__ conv_os_blk_size, int8_t* __restrict__ __ins_0, int8_t* __restrict__ __ins_1, float* __restrict__ __ins_2, float* __restrict__ __ins_3, int8_t* __restrict__ __outs_0) noexcept{
  void** __sc_kernel_cache_arr_196 = (void**)&__uninitialized_data[23657576UL];
  int8_t* __rescheduled_1 = (int8_t*)sc_thread_aligned_malloc(__stream, 256UL);
  int32_t* __origouts_2030_shr = (int32_t*)sc_thread_aligned_malloc(__stream, 3136UL);
  void** A_list = (void**)&__rescheduled_1[0UL];
  void** B_list = (void**)&__rescheduled_1[128UL];
  int32_t __cached_2;
  __cached_2 = conv_os_acc_size[0UL];
  int32_t __cached_3;
  __cached_3 = conv_os_blk_size[0UL];
  memset(&__origouts_2030_shr[(uint64_t)(((__cached_2 / 7) * 112) + ((__cached_2 % 7) * 16))], 0, ((uint64_t)(__cached_3 * 16) * 4UL));
  for (uint64_t r = 0UL; r < 3UL; r += 1UL) {
    for (uint64_t s = 0UL; s < 3UL; s += 1UL) {
      void* __cached_4;
      __cached_4 = &__ins_0[(((fused_0k_o__n_1934 % 4UL) * 41472UL) + ((r * 4608UL) + (s * 512UL)))];
      A_list[((r * 3UL) + s)] = __cached_4;
      void* __cached_5;
      __cached_5 = &__ins_1[(((fused_0k_o__n_1934 / 4UL) * 73728UL) + ((r * 24576UL) + (s * 8192UL)))];
      B_list[((r * 3UL) + s)] = __cached_5;
    }
  }
  void* _arg_cache_47 = &__origouts_2030_shr[(uint64_t)(((__cached_2 / 7) * 112) + ((__cached_2 % 7) * 16))];
  dnnl_brgemm_list_call(__sc_kernel_cache_arr_196[0UL], A_list, B_list, &__origouts_2030_shr[(uint64_t)(((__cached_2 / 7) * 112) + ((__cached_2 % 7) * 16))], 1, 512, 8192, 9, 7, 7, __stream);
  for (uint64_t _fuseiter7320 = 0UL; _fuseiter7320 < 7UL; _fuseiter7320 += 1UL) {
    for (uint64_t _fuseiter7321 = 0UL; _fuseiter7321 < 7UL; _fuseiter7321 += 1UL) {
      vec_s32x16 __cached_6;
      __cached_6 = vec_s32x16::load(&__origouts_2030_shr[((_fuseiter7320 * 112UL) + (_fuseiter7321 * 16UL))]);
      vec_f32x16 __cached_7;
      __cached_7 = (vec_f32x16)(__cached_6);
      vec_f32x16 __cached_8;
      __cached_8 = vec_f32x16::load(&__ins_2[((fused_0k_o__n_1934 / 4UL) * 16UL)]);
      __cached_7 = (__cached_7 * __cached_8);
      vec_f32x16 __cached_9;
      __cached_9 = vec_f32x16::load(&__ins_3[((fused_0k_o__n_1934 / 4UL) * 16UL)]);
      __cached_7 = (__cached_7 + __cached_9);
      __cached_7 = sc_max(__cached_7, vec_f32x16(0.f));
      vec_s8x16 __cached_10;
      __cached_10 = sc_saturated_cast<vec_s8x16>(sc_round_and_cast<vec_s32x16>(__cached_7));
      vec_s8x16 __cached_11;
      __cached_11 = __cached_10;
      vec_s8x16::store(__cached_11, &__outs_0[(((fused_0k_o__n_1934 % 4UL) * 25088UL) + (((((fused_0k_o__n_1934 / 4UL) * 16UL) / 256UL) * 12544UL) + ((_fuseiter7320 * 1792UL) + ((_fuseiter7321 * 256UL) + (((fused_0k_o__n_1934 / 4UL) * 16UL) % 256UL)))))]);
    }
  }
  sc_thread_aligned_free(__stream, __origouts_2030_shr);
  sc_thread_aligned_free(__stream, __rescheduled_1);
}

static void res5b_conv_1_cast_mul_add_relu_cast_reorder__6770_closure_333_0wrapper(void* __stream, int8_t* __restrict__ __module_data, uint64_t i, generic_val* __restrict__ args) noexcept{
  res5b_conv_1_cast_mul_add_relu_cast_reorder__6770_closure_333(i, (int32_t*)(args[0UL].v_ptr), (int32_t*)(args[1UL].v_ptr), (int8_t*)(args[2UL].v_ptr), (int8_t*)(args[3UL].v_ptr), (float*)(args[4UL].v_ptr), (float*)(args[5UL].v_ptr), (int8_t*)(args[6UL].v_ptr));
}

static void mul__6660_closure_334(uint64_t fused_0fused_0fused_0__itr_0____itr_1_1935____itr_2_1936____itr_3_1937, float* __restrict__ __ins_0, float* __restrict__ __ins_1, float* __restrict__ __outs_0) noexcept{
  for (uint64_t _fuseiter_7357 = 0UL; _fuseiter_7357 < 64UL; _fuseiter_7357 += 16UL) {
    vec_f32x16 __cached_0;
    __cached_0 = vec_f32x16::load(&__ins_0[((((fused_0fused_0fused_0__itr_0____itr_1_1935____itr_2_1936____itr_3_1937 / 32UL) * 2048UL) + ((fused_0fused_0fused_0__itr_0____itr_1_1935____itr_2_1936____itr_3_1937 % 32UL) * 64UL)) + _fuseiter_7357)]);
    float __cached_1;
    __cached_1 = __ins_1[0];
    vec_f32x16 __cached_2;
    __cached_2 = (__cached_0 * vec_f32x16(__cached_1));
    vec_f32x16::store(__cached_2, &__outs_0[((((fused_0fused_0fused_0__itr_0____itr_1_1935____itr_2_1936____itr_3_1937 / 32UL) * 2048UL) + ((fused_0fused_0fused_0__itr_0____itr_1_1935____itr_2_1936____itr_3_1937 % 32UL) * 64UL)) + _fuseiter_7357)]);
  }
}

static void mul__6660_closure_334_0wrapper(void* __stream, int8_t* __restrict__ __module_data, uint64_t i, generic_val* __restrict__ args) noexcept{
  mul__6660_closure_334(i, (float*)(args[0UL].v_ptr), (float*)(args[1UL].v_ptr), (float*)(args[2UL].v_ptr));
}

static void mul__6650_closure_335(uint64_t fused_0fused_0fused_0__itr_0____itr_1_1938____itr_2_1939____itr_3_1940, float* __restrict__ __ins_0, float* __restrict__ __ins_1, float* __restrict__ __outs_0) noexcept{
  for (uint64_t _fuseiter_7363 = 0UL; _fuseiter_7363 < 64UL; _fuseiter_7363 += 16UL) {
    vec_f32x16 __cached_0;
    __cached_0 = vec_f32x16::load(&__ins_0[((((fused_0fused_0fused_0__itr_0____itr_1_1938____itr_2_1939____itr_3_1940 / 32UL) * 2048UL) + ((fused_0fused_0fused_0__itr_0____itr_1_1938____itr_2_1939____itr_3_1940 % 32UL) * 64UL)) + _fuseiter_7363)]);
    float __cached_1;
    __cached_1 = __ins_1[0];
    vec_f32x16 __cached_2;
    __cached_2 = (__cached_0 * vec_f32x16(__cached_1));
    vec_f32x16::store(__cached_2, &__outs_0[((((fused_0fused_0fused_0__itr_0____itr_1_1938____itr_2_1939____itr_3_1940 / 32UL) * 2048UL) + ((fused_0fused_0fused_0__itr_0____itr_1_1938____itr_2_1939____itr_3_1940 % 32UL) * 64UL)) + _fuseiter_7363)]);
  }
}

static void mul__6650_closure_335_0wrapper(void* __stream, int8_t* __restrict__ __module_data, uint64_t i, generic_val* __restrict__ args) noexcept{
  mul__6650_closure_335(i, (float*)(args[0UL].v_ptr), (float*)(args[1UL].v_ptr), (float*)(args[2UL].v_ptr));
}

static void reorder__5490_closure_336(uint64_t _fuseiter_7365, int8_t* __restrict__ __ins_0, int8_t* __restrict__ __outs_0) noexcept{
  for (uint64_t _fuseiter_7366 = 0UL; _fuseiter_7366 < 2UL; _fuseiter_7366 += 1UL) {
    for (uint64_t _fuseiter_7369 = 0UL; _fuseiter_7369 < 64UL; _fuseiter_7369 += 1UL) {
      for (uint64_t _fuseiter_7370 = 0UL; _fuseiter_7370 < 64UL; _fuseiter_7370 += 1UL) {
        for (uint64_t _fuseiter_7371 = 0UL; _fuseiter_7371 < 4UL; _fuseiter_7371 += 1UL) {
          int8_t __cached_0;
          __cached_0 = __ins_0[(((_fuseiter_7370 + (_fuseiter_7365 * 64UL)) * 512UL) + ((_fuseiter_7371 + (_fuseiter_7369 * 4UL)) + (_fuseiter_7366 * 256UL)))];
          int8_t __cached_1;
          __cached_1 = __cached_0;
          __outs_0[((_fuseiter_7365 * 32768UL) + ((_fuseiter_7366 * 16384UL) + ((_fuseiter_7369 * 256UL) + ((_fuseiter_7370 * 4UL) + _fuseiter_7371))))] = __cached_1;
        }
      }
    }
  }
}

static void reorder__5490_closure_336_0wrapper(void* __stream, int8_t* __restrict__ __module_data, uint64_t i, generic_val* __restrict__ args) noexcept{
  reorder__5490_closure_336(i, (int8_t*)(args[0UL].v_ptr), (int8_t*)(args[1UL].v_ptr));
}

static void mul__6680_closure_337(uint64_t fused_0fused_0fused_0__itr_0____itr_1_1941____itr_2_1942____itr_3_1943, float* __restrict__ __ins_0, float* __restrict__ __ins_1, float* __restrict__ __outs_0) noexcept{
  for (uint64_t _fuseiter_7376 = 0UL; _fuseiter_7376 < 32UL; _fuseiter_7376 += 16UL) {
    vec_f32x16 __cached_0;
    __cached_0 = vec_f32x16::load(&__ins_0[((((fused_0fused_0fused_0__itr_0____itr_1_1941____itr_2_1942____itr_3_1943 / 16UL) * 512UL) + ((fused_0fused_0fused_0__itr_0____itr_1_1941____itr_2_1942____itr_3_1943 % 16UL) * 32UL)) + _fuseiter_7376)]);
    float __cached_1;
    __cached_1 = __ins_1[0];
    vec_f32x16 __cached_2;
    __cached_2 = (__cached_0 * vec_f32x16(__cached_1));
    vec_f32x16::store(__cached_2, &__outs_0[((((fused_0fused_0fused_0__itr_0____itr_1_1941____itr_2_1942____itr_3_1943 / 16UL) * 512UL) + ((fused_0fused_0fused_0__itr_0____itr_1_1941____itr_2_1942____itr_3_1943 % 16UL) * 32UL)) + _fuseiter_7376)]);
  }
}

static void mul__6680_closure_337_0wrapper(void* __stream, int8_t* __restrict__ __module_data, uint64_t i, generic_val* __restrict__ args) noexcept{
  mul__6680_closure_337(i, (float*)(args[0UL].v_ptr), (float*)(args[1UL].v_ptr), (float*)(args[2UL].v_ptr));
}

static void mul__6670_closure_338(uint64_t fused_0fused_0fused_0__itr_0____itr_1_1944____itr_2_1945____itr_3_1946, float* __restrict__ __ins_0, float* __restrict__ __ins_1, float* __restrict__ __outs_0) noexcept{
  for (uint64_t _fuseiter_7382 = 0UL; _fuseiter_7382 < 32UL; _fuseiter_7382 += 16UL) {
    vec_f32x16 __cached_0;
    __cached_0 = vec_f32x16::load(&__ins_0[((((fused_0fused_0fused_0__itr_0____itr_1_1944____itr_2_1945____itr_3_1946 / 16UL) * 512UL) + ((fused_0fused_0fused_0__itr_0____itr_1_1944____itr_2_1945____itr_3_1946 % 16UL) * 32UL)) + _fuseiter_7382)]);
    float __cached_1;
    __cached_1 = __ins_1[0];
    vec_f32x16 __cached_2;
    __cached_2 = (__cached_0 * vec_f32x16(__cached_1));
    vec_f32x16::store(__cached_2, &__outs_0[((((fused_0fused_0fused_0__itr_0____itr_1_1944____itr_2_1945____itr_3_1946 / 16UL) * 512UL) + ((fused_0fused_0fused_0__itr_0____itr_1_1944____itr_2_1945____itr_3_1946 % 16UL) * 32UL)) + _fuseiter_7382)]);
  }
}

static void mul__6670_closure_338_0wrapper(void* __stream, int8_t* __restrict__ __module_data, uint64_t i, generic_val* __restrict__ args) noexcept{
  mul__6670_closure_338(i, (float*)(args[0UL].v_ptr), (float*)(args[1UL].v_ptr), (float*)(args[2UL].v_ptr));
}

static void reorder__5520_closure_339(uint64_t fused_0_fuseiter_7384___fuseiter_7385_1947, int8_t* __restrict__ __ins_0, int8_t* __restrict__ __outs_0) noexcept{
  for (uint64_t _fuseiter_7388 = 0UL; _fuseiter_7388 < 128UL; _fuseiter_7388 += 1UL) {
    for (uint64_t _fuseiter_7389 = 0UL; _fuseiter_7389 < 32UL; _fuseiter_7389 += 1UL) {
      for (uint64_t _fuseiter_7390 = 0UL; _fuseiter_7390 < 4UL; _fuseiter_7390 += 1UL) {
        int8_t __cached_0;
        __cached_0 = __ins_0[(((_fuseiter_7389 + ((fused_0_fuseiter_7384___fuseiter_7385_1947 / 4UL) * 32UL)) * 2048UL) + ((_fuseiter_7390 + (_fuseiter_7388 * 4UL)) + ((fused_0_fuseiter_7384___fuseiter_7385_1947 % 4UL) * 512UL)))];
        int8_t __cached_1;
        __cached_1 = __cached_0;
        __outs_0[(((fused_0_fuseiter_7384___fuseiter_7385_1947 / 4UL) * 65536UL) + (((fused_0_fuseiter_7384___fuseiter_7385_1947 % 4UL) * 16384UL) + ((_fuseiter_7388 * 128UL) + ((_fuseiter_7389 * 4UL) + _fuseiter_7390))))] = __cached_1;
      }
    }
  }
}

static void reorder__5520_closure_339_0wrapper(void* __stream, int8_t* __restrict__ __module_data, uint64_t i, generic_val* __restrict__ args) noexcept{
  reorder__5520_closure_339(i, (int8_t*)(args[0UL].v_ptr), (int8_t*)(args[1UL].v_ptr));
}

static void mul__6700_closure_340(uint64_t fused_0fused_0fused_0__itr_0____itr_1_1948____itr_2_1949____itr_3_1950, float* __restrict__ __ins_0, float* __restrict__ __ins_1, float* __restrict__ __outs_0) noexcept{
  for (uint64_t _fuseiter_7395 = 0UL; _fuseiter_7395 < 128UL; _fuseiter_7395 += 16UL) {
    vec_f32x16 __cached_0;
    __cached_0 = vec_f32x16::load(&__ins_0[((((fused_0fused_0fused_0__itr_0____itr_1_1948____itr_2_1949____itr_3_1950 / 4UL) * 512UL) + ((fused_0fused_0fused_0__itr_0____itr_1_1948____itr_2_1949____itr_3_1950 % 4UL) * 128UL)) + _fuseiter_7395)]);
    float __cached_1;
    __cached_1 = __ins_1[0];
    vec_f32x16 __cached_2;
    __cached_2 = (__cached_0 * vec_f32x16(__cached_1));
    vec_f32x16::store(__cached_2, &__outs_0[((((fused_0fused_0fused_0__itr_0____itr_1_1948____itr_2_1949____itr_3_1950 / 4UL) * 512UL) + ((fused_0fused_0fused_0__itr_0____itr_1_1948____itr_2_1949____itr_3_1950 % 4UL) * 128UL)) + _fuseiter_7395)]);
  }
}

static void mul__6700_closure_340_0wrapper(void* __stream, int8_t* __restrict__ __module_data, uint64_t i, generic_val* __restrict__ args) noexcept{
  mul__6700_closure_340(i, (float*)(args[0UL].v_ptr), (float*)(args[1UL].v_ptr), (float*)(args[2UL].v_ptr));
}

static void mul__6690_closure_341(uint64_t fused_0fused_0fused_0__itr_0____itr_1_1951____itr_2_1952____itr_3_1953, float* __restrict__ __ins_0, float* __restrict__ __ins_1, float* __restrict__ __outs_0) noexcept{
  for (uint64_t _fuseiter_7401 = 0UL; _fuseiter_7401 < 128UL; _fuseiter_7401 += 16UL) {
    vec_f32x16 __cached_0;
    __cached_0 = vec_f32x16::load(&__ins_0[((((fused_0fused_0fused_0__itr_0____itr_1_1951____itr_2_1952____itr_3_1953 / 4UL) * 512UL) + ((fused_0fused_0fused_0__itr_0____itr_1_1951____itr_2_1952____itr_3_1953 % 4UL) * 128UL)) + _fuseiter_7401)]);
    float __cached_1;
    __cached_1 = __ins_1[0];
    vec_f32x16 __cached_2;
    __cached_2 = (__cached_0 * vec_f32x16(__cached_1));
    vec_f32x16::store(__cached_2, &__outs_0[((((fused_0fused_0fused_0__itr_0____itr_1_1951____itr_2_1952____itr_3_1953 / 4UL) * 512UL) + ((fused_0fused_0fused_0__itr_0____itr_1_1951____itr_2_1952____itr_3_1953 % 4UL) * 128UL)) + _fuseiter_7401)]);
  }
}

static void mul__6690_closure_341_0wrapper(void* __stream, int8_t* __restrict__ __module_data, uint64_t i, generic_val* __restrict__ args) noexcept{
  mul__6690_closure_341(i, (float*)(args[0UL].v_ptr), (float*)(args[1UL].v_ptr), (float*)(args[2UL].v_ptr));
}

static void reorder__5550_closure_342(uint64_t fused_0fused_0_fuseiter_7403___fuseiter_7404_1954___fuseiter_7405_1955, int8_t* __restrict__ __ins_0, int8_t* __restrict__ __outs_0) noexcept{
  for (uint64_t _fuseiter_7406 = 0UL; _fuseiter_7406 < 3UL; _fuseiter_7406 += 1UL) {
    for (uint64_t _fuseiter_7407 = 0UL; _fuseiter_7407 < 64UL; _fuseiter_7407 += 1UL) {
      for (uint64_t _fuseiter_7408 = 0UL; _fuseiter_7408 < 128UL; _fuseiter_7408 += 1UL) {
        for (uint64_t _fuseiter_7409 = 0UL; _fuseiter_7409 < 4UL; _fuseiter_7409 += 1UL) {
          int8_t __cached_0;
          __cached_0 = __ins_0[(((_fuseiter_7408 + ((fused_0fused_0_fuseiter_7403___fuseiter_7404_1954___fuseiter_7405_1955 / 6UL) * 128UL)) * 4608UL) + ((((_fuseiter_7409 + (_fuseiter_7407 * 4UL)) + (((fused_0fused_0_fuseiter_7403___fuseiter_7404_1954___fuseiter_7405_1955 / 3UL) % 2UL) * 256UL)) * 9UL) + (((fused_0fused_0_fuseiter_7403___fuseiter_7404_1954___fuseiter_7405_1955 % 3UL) * 3UL) + _fuseiter_7406)))];
          int8_t __cached_1;
          __cached_1 = __cached_0;
          __outs_0[(((fused_0fused_0_fuseiter_7403___fuseiter_7404_1954___fuseiter_7405_1955 / 6UL) * 589824UL) + ((((fused_0fused_0_fuseiter_7403___fuseiter_7404_1954___fuseiter_7405_1955 / 3UL) % 2UL) * 294912UL) + (((fused_0fused_0_fuseiter_7403___fuseiter_7404_1954___fuseiter_7405_1955 % 3UL) * 98304UL) + ((_fuseiter_7406 * 32768UL) + ((_fuseiter_7407 * 512UL) + ((_fuseiter_7408 * 4UL) + _fuseiter_7409))))))] = __cached_1;
        }
      }
    }
  }
}

static void reorder__5550_closure_342_0wrapper(void* __stream, int8_t* __restrict__ __module_data, uint64_t i, generic_val* __restrict__ args) noexcept{
  reorder__5550_closure_342(i, (int8_t*)(args[0UL].v_ptr), (int8_t*)(args[1UL].v_ptr));
}

static void res5b_conv_2_cast_mul_add_cast_add_cast_reorder__6760_closure_343(uint64_t fused_0n__k_1956, int8_t* __restrict__ __ins_0, int8_t* __restrict__ __ins_1, float* __restrict__ __ins_2, float* __restrict__ __ins_3, uint8_t* __restrict__ __ins_4, uint8_t* __restrict__ __outs_0) noexcept{
  void*& __sc_kernel_cache_198 = *(void**)(__module_data + 312);
  alignas(64) int8_t __rescheduled_1[128UL];
  for (uint64_t p_o = 0UL; p_o < 7UL; p_o += 1UL) {
    int32_t* __origouts_2040_shr = (int32_t*)sc_thread_aligned_malloc(__stream, 1792UL);
    void** A_list = (void**)&__rescheduled_1[0UL];
    void** B_list = (void**)&__rescheduled_1[64UL];
    for (uint64_t c = 0UL; c < 2UL; c += 1UL) {
      void* __cached_0;
      __cached_0 = &__ins_0[(((fused_0n__k_1956 / 32UL) * 25088UL) + ((c * 12544UL) + (p_o * 1792UL)))];
      A_list[c] = __cached_0;
      void* __cached_1;
      __cached_1 = &__ins_1[(((fused_0n__k_1956 % 32UL) * 32768UL) + (c * 16384UL))];
      B_list[c] = __cached_1;
    }
    void* _arg_cache_48 = &__origouts_2040_shr[0UL];
    dnnl_brgemm_list_call(__sc_kernel_cache_198, A_list, B_list, &__origouts_2040_shr[0UL], 1, 1, 1, 2, 7, 7, __stream);
    for (uint64_t _fuseiter7413 = 0UL; _fuseiter7413 < 7UL; _fuseiter7413 += 1UL) {
      for (uint64_t _fuseiter7414 = 0UL; _fuseiter7414 < 64UL; _fuseiter7414 += 16UL) {
        vec_s32x16 __cached_2;
        __cached_2 = vec_s32x16::load(&__origouts_2040_shr[((_fuseiter7413 * 64UL) + _fuseiter7414)]);
        vec_f32x16 __cached_3;
        __cached_3 = (vec_f32x16)(__cached_2);
        vec_f32x16 __cached_4;
        __cached_4 = vec_f32x16::load(&__ins_2[(((fused_0n__k_1956 % 32UL) * 64UL) + _fuseiter7414)]);
        __cached_3 = (__cached_3 * __cached_4);
        vec_f32x16 __cached_5;
        __cached_5 = vec_f32x16::load(&__ins_3[(((fused_0n__k_1956 % 32UL) * 64UL) + _fuseiter7414)]);
        __cached_3 = (__cached_3 + __cached_5);
        vec_s8x16 __cached_6;
        __cached_6 = sc_saturated_cast<vec_s8x16>(sc_round_and_cast<vec_s32x16>(__cached_3));
        vec_u8x16 __cached_7;
        __cached_7 = vec_u8x16::load(&__ins_4[((((fused_0n__k_1956 / 32UL) * 100352UL) + (((fused_0n__k_1956 % 32UL) * 3136UL) + (p_o * 448UL))) + ((_fuseiter7413 * 64UL) + _fuseiter7414))]);
        __cached_6 = (__cached_6 + (vec_s8x16)(__cached_7));
        vec_u8x16 __cached_8;
        __cached_8 = (vec_u8x16)(sc_max(__cached_6, vec_s8x16(0)));
        vec_u8x16 __cached_9;
        __cached_9 = __cached_8;
        vec_u8x16::store(__cached_9, &__outs_0[(((fused_0n__k_1956 / 32UL) * 100352UL) + ((((_fuseiter7414 + ((fused_0n__k_1956 % 32UL) * 64UL)) / 512UL) * 25088UL) + ((p_o * 3584UL) + ((_fuseiter7413 * 512UL) + ((_fuseiter7414 + ((fused_0n__k_1956 % 32UL) * 64UL)) % 512UL)))))]);
      }
    }
    sc_thread_aligned_free(__stream, __origouts_2040_shr);
  }
}

static void res5b_conv_2_cast_mul_add_cast_add_cast_reorder__6760_closure_343_0wrapper(void* __stream, int8_t* __restrict__ __module_data, uint64_t i, generic_val* __restrict__ args) noexcept{
  res5b_conv_2_cast_mul_add_cast_add_cast_reorder__6760_closure_343(i, (int8_t*)(args[0UL].v_ptr), (int8_t*)(args[1UL].v_ptr), (float*)(args[2UL].v_ptr), (float*)(args[3UL].v_ptr), (uint8_t*)(args[4UL].v_ptr), (uint8_t*)(args[5UL].v_ptr));
}

static void res5c_conv_0_cast_mul_add_relu_cast_reorder__6750_closure_344(uint64_t fused_0n__k_1957, int8_t* __restrict__ __outs_0) noexcept{
  memset(&__outs_0[(((fused_0n__k_1957 / 2UL) * 41472UL) + ((fused_0n__k_1957 % 2UL) * 20736UL))], 0, 2304UL);
  for (uint64_t p1 = 0UL; p1 < 7UL; p1 += 1UL) {
    memset(&__outs_0[(((fused_0n__k_1957 / 2UL) * 41472UL) + (((fused_0n__k_1957 % 2UL) * 20736UL) + ((p1 + 1UL) * 2304UL)))], 0, 256UL);
    memset(&__outs_0[((((fused_0n__k_1957 / 2UL) * 41472UL) + (((fused_0n__k_1957 % 2UL) * 20736UL) + ((p1 + 1UL) * 2304UL))) + 2048UL)], 0, 256UL);
  }
  memset(&__outs_0[((((fused_0n__k_1957 / 2UL) * 41472UL) + ((fused_0n__k_1957 % 2UL) * 20736UL)) + 18432UL)], 0, 2304UL);
}

static void res5c_conv_0_cast_mul_add_relu_cast_reorder__6750_closure_344_0wrapper(void* __stream, int8_t* __restrict__ __module_data, uint64_t i, generic_val* __restrict__ args) noexcept{
  res5c_conv_0_cast_mul_add_relu_cast_reorder__6750_closure_344(i, (int8_t*)(args[0UL].v_ptr));
}

static void res5c_conv_0_cast_mul_add_relu_cast_reorder__6750_closure_345(uint64_t fused_0n__k_1958, uint8_t* __restrict__ __ins_0, int8_t* __restrict__ __ins_1, float* __restrict__ __ins_2, float* __restrict__ __ins_3, int8_t* __restrict__ __outs_0) noexcept{
  void*& __sc_kernel_cache_200 = *(void**)(__module_data + 320);
  alignas(64) int8_t __rescheduled_2[128UL];
  int32_t* __origouts_2050_shr = (int32_t*)sc_thread_aligned_malloc(__stream, 6272UL);
  void** A_list = (void**)&__rescheduled_2[0UL];
  void** B_list = (void**)&__rescheduled_2[64UL];
  for (uint64_t c = 0UL; c < 4UL; c += 1UL) {
    void* __cached_0;
    __cached_0 = &__ins_0[(((fused_0n__k_1958 / 16UL) * 100352UL) + (c * 25088UL))];
    A_list[c] = __cached_0;
    void* __cached_1;
    __cached_1 = &__ins_1[(((fused_0n__k_1958 % 16UL) * 65536UL) + (c * 16384UL))];
    B_list[c] = __cached_1;
  }
  void* _arg_cache_49 = &__origouts_2050_shr[0UL];
  dnnl_brgemm_list_call(__sc_kernel_cache_200, A_list, B_list, &__origouts_2050_shr[0UL], 1, 1, 1, 4, 8, 7, __stream);
  for (uint64_t _fuseiter7453 = 0UL; _fuseiter7453 < 7UL; _fuseiter7453 += 1UL) {
    for (uint64_t _fuseiter7454 = 0UL; _fuseiter7454 < 7UL; _fuseiter7454 += 1UL) {
      for (uint64_t _fuseiter7455 = 0UL; _fuseiter7455 < 32UL; _fuseiter7455 += 16UL) {
        vec_s32x16 __cached_2;
        __cached_2 = vec_s32x16::load(&__origouts_2050_shr[((_fuseiter7453 * 224UL) + ((_fuseiter7454 * 32UL) + _fuseiter7455))]);
        vec_f32x16 __cached_3;
        __cached_3 = (vec_f32x16)(__cached_2);
        vec_f32x16 __cached_4;
        __cached_4 = vec_f32x16::load(&__ins_2[(((fused_0n__k_1958 % 16UL) * 32UL) + _fuseiter7455)]);
        __cached_3 = (__cached_3 * __cached_4);
        vec_f32x16 __cached_5;
        __cached_5 = vec_f32x16::load(&__ins_3[(((fused_0n__k_1958 % 16UL) * 32UL) + _fuseiter7455)]);
        __cached_3 = (__cached_3 + __cached_5);
        __cached_3 = sc_max(__cached_3, vec_f32x16(0.f));
        vec_s8x16 __cached_6;
        __cached_6 = sc_saturated_cast<vec_s8x16>(sc_round_and_cast<vec_s32x16>(__cached_3));
        vec_s8x16 __cached_7;
        __cached_7 = __cached_6;
        vec_s8x16::store(__cached_7, &__outs_0[(((fused_0n__k_1958 / 16UL) * 41472UL) + ((((_fuseiter7455 + ((fused_0n__k_1958 % 16UL) * 32UL)) / 256UL) * 20736UL) + (((_fuseiter7453 + 1UL) * 2304UL) + (((_fuseiter7454 + 1UL) * 256UL) + ((_fuseiter7455 + ((fused_0n__k_1958 % 16UL) * 32UL)) % 256UL)))))]);
      }
    }
  }
  sc_thread_aligned_free(__stream, __origouts_2050_shr);
}

static void res5c_conv_0_cast_mul_add_relu_cast_reorder__6750_closure_345_0wrapper(void* __stream, int8_t* __restrict__ __module_data, uint64_t i, generic_val* __restrict__ args) noexcept{
  res5c_conv_0_cast_mul_add_relu_cast_reorder__6750_closure_345(i, (uint8_t*)(args[0UL].v_ptr), (int8_t*)(args[1UL].v_ptr), (float*)(args[2UL].v_ptr), (float*)(args[3UL].v_ptr), (int8_t*)(args[4UL].v_ptr));
}

static void res5c_conv_1_cast_mul_add_relu_cast_reorder__6740_closure_346(uint64_t fused_0k_o__n_1959, int32_t* __restrict__ conv_os_acc_size, int32_t* __restrict__ conv_os_blk_size, int8_t* __restrict__ __ins_0, int8_t* __restrict__ __ins_1, float* __restrict__ __ins_2, float* __restrict__ __ins_3, int8_t* __restrict__ __outs_0) noexcept{
  void** __sc_kernel_cache_arr_202 = (void**)&__uninitialized_data[23657584UL];
  int8_t* __rescheduled_1 = (int8_t*)sc_thread_aligned_malloc(__stream, 384UL);
  int32_t* __origouts_2060_shr = (int32_t*)sc_thread_aligned_malloc(__stream, 25088UL);
  void** A_list = (void**)&__rescheduled_1[0UL];
  void** B_list = (void**)&__rescheduled_1[192UL];
  int32_t __cached_2;
  __cached_2 = conv_os_acc_size[0UL];
  int32_t __cached_3;
  __cached_3 = conv_os_blk_size[0UL];
  memset(&__origouts_2060_shr[(uint64_t)(((__cached_2 / 7) * 896) + ((__cached_2 % 7) * 128))], 0, ((uint64_t)(__cached_3 * 128) * 4UL));
  for (uint64_t c_o = 0UL; c_o < 2UL; c_o += 1UL) {
    for (uint64_t r = 0UL; r < 3UL; r += 1UL) {
      for (uint64_t s = 0UL; s < 3UL; s += 1UL) {
        void* __cached_4;
        __cached_4 = &__ins_0[(((fused_0k_o__n_1959 % 4UL) * 41472UL) + ((c_o * 20736UL) + ((r * 2304UL) + (s * 256UL))))];
        A_list[(((c_o * 9UL) + (r * 3UL)) + s)] = __cached_4;
        void* __cached_5;
        __cached_5 = &__ins_1[(((fused_0k_o__n_1959 / 4UL) * 589824UL) + ((c_o * 294912UL) + ((r * 98304UL) + (s * 32768UL))))];
        B_list[(((c_o * 9UL) + (r * 3UL)) + s)] = __cached_5;
      }
    }
  }
  void* _arg_cache_50 = &__origouts_2060_shr[(uint64_t)(((__cached_2 / 7) * 896) + ((__cached_2 % 7) * 128))];
  dnnl_brgemm_list_call(__sc_kernel_cache_arr_202[0UL], A_list, B_list, &__origouts_2060_shr[(uint64_t)(((__cached_2 / 7) * 896) + ((__cached_2 % 7) * 128))], 1, 256, 32768, 18, 7, 7, __stream);
  for (uint64_t _fuseiter7488 = 0UL; _fuseiter7488 < 7UL; _fuseiter7488 += 1UL) {
    for (uint64_t _fuseiter7489 = 0UL; _fuseiter7489 < 7UL; _fuseiter7489 += 1UL) {
      for (uint64_t _fuseiter7490 = 0UL; _fuseiter7490 < 128UL; _fuseiter7490 += 16UL) {
        vec_s32x16 __cached_6;
        __cached_6 = vec_s32x16::load(&__origouts_2060_shr[((_fuseiter7488 * 896UL) + ((_fuseiter7489 * 128UL) + _fuseiter7490))]);
        vec_f32x16 __cached_7;
        __cached_7 = (vec_f32x16)(__cached_6);
        vec_f32x16 __cached_8;
        __cached_8 = vec_f32x16::load(&__ins_2[(((fused_0k_o__n_1959 / 4UL) * 128UL) + _fuseiter7490)]);
        __cached_7 = (__cached_7 * __cached_8);
        vec_f32x16 __cached_9;
        __cached_9 = vec_f32x16::load(&__ins_3[(((fused_0k_o__n_1959 / 4UL) * 128UL) + _fuseiter7490)]);
        __cached_7 = (__cached_7 + __cached_9);
        __cached_7 = sc_max(__cached_7, vec_f32x16(0.f));
        vec_s8x16 __cached_10;
        __cached_10 = sc_saturated_cast<vec_s8x16>(sc_round_and_cast<vec_s32x16>(__cached_7));
        vec_s8x16 __cached_11;
        __cached_11 = __cached_10;
        vec_s8x16::store(__cached_11, &__outs_0[(((fused_0k_o__n_1959 % 4UL) * 25088UL) + ((((_fuseiter7490 + ((fused_0k_o__n_1959 / 4UL) * 128UL)) / 512UL) * 25088UL) + ((_fuseiter7488 * 3584UL) + ((_fuseiter7489 * 512UL) + ((_fuseiter7490 + ((fused_0k_o__n_1959 / 4UL) * 128UL)) % 512UL)))))]);
      }
    }
  }
  sc_thread_aligned_free(__stream, __origouts_2060_shr);
  sc_thread_aligned_free(__stream, __rescheduled_1);
}

static void res5c_conv_1_cast_mul_add_relu_cast_reorder__6740_closure_346_0wrapper(void* __stream, int8_t* __restrict__ __module_data, uint64_t i, generic_val* __restrict__ args) noexcept{
  res5c_conv_1_cast_mul_add_relu_cast_reorder__6740_closure_346(i, (int32_t*)(args[0UL].v_ptr), (int32_t*)(args[1UL].v_ptr), (int8_t*)(args[2UL].v_ptr), (int8_t*)(args[3UL].v_ptr), (float*)(args[4UL].v_ptr), (float*)(args[5UL].v_ptr), (int8_t*)(args[6UL].v_ptr));
}

static void mul__6720_closure_347(uint64_t fused_0fused_0fused_0__itr_0____itr_1_1960____itr_2_1961____itr_3_1962, float* __restrict__ __ins_0, float* __restrict__ __ins_1, float* __restrict__ __outs_0) noexcept{
  for (uint64_t _fuseiter_7525 = 0UL; _fuseiter_7525 < 512UL; _fuseiter_7525 += 16UL) {
    vec_f32x16 __cached_0;
    __cached_0 = vec_f32x16::load(&__ins_0[((((fused_0fused_0fused_0__itr_0____itr_1_1960____itr_2_1961____itr_3_1962 / 4UL) * 2048UL) + ((fused_0fused_0fused_0__itr_0____itr_1_1960____itr_2_1961____itr_3_1962 % 4UL) * 512UL)) + _fuseiter_7525)]);
    float __cached_1;
    __cached_1 = __ins_1[0];
    vec_f32x16 __cached_2;
    __cached_2 = (__cached_0 * vec_f32x16(__cached_1));
    vec_f32x16::store(__cached_2, &__outs_0[((((fused_0fused_0fused_0__itr_0____itr_1_1960____itr_2_1961____itr_3_1962 / 4UL) * 2048UL) + ((fused_0fused_0fused_0__itr_0____itr_1_1960____itr_2_1961____itr_3_1962 % 4UL) * 512UL)) + _fuseiter_7525)]);
  }
}

static void mul__6720_closure_347_0wrapper(void* __stream, int8_t* __restrict__ __module_data, uint64_t i, generic_val* __restrict__ args) noexcept{
  mul__6720_closure_347(i, (float*)(args[0UL].v_ptr), (float*)(args[1UL].v_ptr), (float*)(args[2UL].v_ptr));
}

static void mul__6710_closure_348(uint64_t fused_0fused_0fused_0__itr_0____itr_1_1963____itr_2_1964____itr_3_1965, float* __restrict__ __ins_0, float* __restrict__ __ins_1, float* __restrict__ __outs_0) noexcept{
  for (uint64_t _fuseiter_7531 = 0UL; _fuseiter_7531 < 512UL; _fuseiter_7531 += 16UL) {
    vec_f32x16 __cached_0;
    __cached_0 = vec_f32x16::load(&__ins_0[((((fused_0fused_0fused_0__itr_0____itr_1_1963____itr_2_1964____itr_3_1965 / 4UL) * 2048UL) + ((fused_0fused_0fused_0__itr_0____itr_1_1963____itr_2_1964____itr_3_1965 % 4UL) * 512UL)) + _fuseiter_7531)]);
    float __cached_1;
    __cached_1 = __ins_1[0];
    vec_f32x16 __cached_2;
    __cached_2 = (__cached_0 * vec_f32x16(__cached_1));
    vec_f32x16::store(__cached_2, &__outs_0[((((fused_0fused_0fused_0__itr_0____itr_1_1963____itr_2_1964____itr_3_1965 / 4UL) * 2048UL) + ((fused_0fused_0fused_0__itr_0____itr_1_1963____itr_2_1964____itr_3_1965 % 4UL) * 512UL)) + _fuseiter_7531)]);
  }
}

static void mul__6710_closure_348_0wrapper(void* __stream, int8_t* __restrict__ __module_data, uint64_t i, generic_val* __restrict__ args) noexcept{
  mul__6710_closure_348(i, (float*)(args[0UL].v_ptr), (float*)(args[1UL].v_ptr), (float*)(args[2UL].v_ptr));
}

static void reorder__5580_closure_349(uint64_t fused_0fused_0fused_0fused_0_fuseiter_7533___fuseiter_7534_1966___fuseiter_7535_1967___fuseiter_7536_1968___fuseiter_7537_1969, int8_t* __restrict__ __ins_0, int8_t* __restrict__ __outs_0) noexcept{
  for (uint64_t _fuseiter_7538 = 0UL; _fuseiter_7538 < 512UL; _fuseiter_7538 += 1UL) {
    for (uint64_t _fuseiter_7539 = 0UL; _fuseiter_7539 < 4UL; _fuseiter_7539 += 1UL) {
      int8_t __cached_0;
      __cached_0 = __ins_0[(((_fuseiter_7538 + ((fused_0fused_0fused_0fused_0_fuseiter_7533___fuseiter_7534_1966___fuseiter_7535_1967___fuseiter_7536_1968___fuseiter_7537_1969 / 128UL) * 512UL)) * 512UL) + (_fuseiter_7539 + ((fused_0fused_0fused_0fused_0_fuseiter_7533___fuseiter_7534_1966___fuseiter_7535_1967___fuseiter_7536_1968___fuseiter_7537_1969 % 128UL) * 4UL)))];
      int8_t __cached_1;
      __cached_1 = __cached_0;
      __outs_0[(((fused_0fused_0fused_0fused_0_fuseiter_7533___fuseiter_7534_1966___fuseiter_7535_1967___fuseiter_7536_1968___fuseiter_7537_1969 / 128UL) * 262144UL) + (((fused_0fused_0fused_0fused_0_fuseiter_7533___fuseiter_7534_1966___fuseiter_7535_1967___fuseiter_7536_1968___fuseiter_7537_1969 % 128UL) * 2048UL) + ((_fuseiter_7538 * 4UL) + _fuseiter_7539)))] = __cached_1;
    }
  }
}

static void reorder__5580_closure_349_0wrapper(void* __stream, int8_t* __restrict__ __module_data, uint64_t i, generic_val* __restrict__ args) noexcept{
  reorder__5580_closure_349(i, (int8_t*)(args[0UL].v_ptr), (int8_t*)(args[1UL].v_ptr));
}

static void res5c_conv_2_cast_mul_add_cast_add_cast_cast__6730_closure_350(uint64_t fused_0k__n_1970, int8_t* __restrict__ __ins_0, int8_t* __restrict__ __ins_1, float* __restrict__ __ins_2, float* __restrict__ __ins_3, uint8_t* __restrict__ __ins_4, int8_t* __restrict__ __outs_0) noexcept{
  void*& __sc_kernel_cache_204 = *(void**)(__module_data + 328);
  alignas(64) int8_t __rescheduled_1[128UL];
  int32_t* __origouts_2070_shr = (int32_t*)sc_thread_aligned_malloc(__stream, 100352UL);
  void** A_list = (void**)&__rescheduled_1[0UL];
  void** B_list = (void**)&__rescheduled_1[64UL];
  void* __cached_0;
  __cached_0 = &__ins_0[((fused_0k__n_1970 % 4UL) * 25088UL)];
  A_list[0UL] = __cached_0;
  void* __cached_1;
  __cached_1 = &__ins_1[((fused_0k__n_1970 / 4UL) * 262144UL)];
  B_list[0UL] = __cached_1;
  void* _arg_cache_51 = &__origouts_2070_shr[0UL];
  dnnl_brgemm_list_call(__sc_kernel_cache_204, A_list, B_list, &__origouts_2070_shr[0UL], 1, 1, 1, 1, 7, 7, __stream);
  for (uint64_t _fuseiter7542 = 0UL; _fuseiter7542 < 7UL; _fuseiter7542 += 1UL) {
    for (uint64_t _fuseiter7543 = 0UL; _fuseiter7543 < 7UL; _fuseiter7543 += 1UL) {
      for (uint64_t _fuseiter7544 = 0UL; _fuseiter7544 < 512UL; _fuseiter7544 += 16UL) {
        vec_s32x16 __cached_2;
        __cached_2 = vec_s32x16::load(&__origouts_2070_shr[((_fuseiter7542 * 3584UL) + ((_fuseiter7543 * 512UL) + _fuseiter7544))]);
        vec_f32x16 __cached_3;
        __cached_3 = (vec_f32x16)(__cached_2);
        vec_f32x16 __cached_4;
        __cached_4 = vec_f32x16::load(&__ins_2[(((fused_0k__n_1970 / 4UL) * 512UL) + _fuseiter7544)]);
        __cached_3 = (__cached_3 * __cached_4);
        vec_f32x16 __cached_5;
        __cached_5 = vec_f32x16::load(&__ins_3[(((fused_0k__n_1970 / 4UL) * 512UL) + _fuseiter7544)]);
        __cached_3 = (__cached_3 + __cached_5);
        vec_s8x16 __cached_6;
        __cached_6 = sc_saturated_cast<vec_s8x16>(sc_round_and_cast<vec_s32x16>(__cached_3));
        vec_u8x16 __cached_7;
        __cached_7 = vec_u8x16::load(&__ins_4[((((fused_0k__n_1970 % 4UL) * 100352UL) + ((fused_0k__n_1970 / 4UL) * 25088UL)) + ((_fuseiter7542 * 3584UL) + ((_fuseiter7543 * 512UL) + _fuseiter7544)))]);
        __cached_6 = (__cached_6 + (vec_s8x16)(__cached_7));
        vec_u8x16 __cached_8;
        __cached_8 = (vec_u8x16)(sc_max(__cached_6, vec_s8x16(0)));
        vec_s8x16 __cached_9;
        __cached_9 = (vec_s8x16)(__cached_8);
        vec_s8x16::store(__cached_9, &__outs_0[((((fused_0k__n_1970 % 4UL) * 100352UL) + ((fused_0k__n_1970 / 4UL) * 25088UL)) + ((_fuseiter7542 * 3584UL) + ((_fuseiter7543 * 512UL) + _fuseiter7544)))]);
      }
    }
  }
  sc_thread_aligned_free(__stream, __origouts_2070_shr);
}

static void res5c_conv_2_cast_mul_add_cast_add_cast_cast__6730_closure_350_0wrapper(void* __stream, int8_t* __restrict__ __module_data, uint64_t i, generic_val* __restrict__ args) noexcept{
  res5c_conv_2_cast_mul_add_cast_add_cast_cast__6730_closure_350(i, (int8_t*)(args[0UL].v_ptr), (int8_t*)(args[1UL].v_ptr), (float*)(args[2UL].v_ptr), (float*)(args[3UL].v_ptr), (uint8_t*)(args[4UL].v_ptr), (int8_t*)(args[5UL].v_ptr));
}

static void reorder__1050_closure_351(uint64_t fused_0n__h_1971, int8_t* __restrict__ __ins_0, int8_t* __restrict__ __outs_0) noexcept{
  for (uint64_t w = 0UL; w < 7UL; w += 1UL) {
    for (uint64_t c = 0UL; c < 2048UL; c += 64UL) {
      vec_s8x64 zmm0;
      vec_s8x64 __cached_0;
      __cached_0 = vec_s8x64::load(&__ins_0[(((fused_0n__h_1971 / 7UL) * 100352UL) + (((c / 512UL) * 25088UL) + (((fused_0n__h_1971 % 7UL) * 3584UL) + ((w * 512UL) + (c % 512UL)))))]);
      zmm0 = __cached_0;
      vec_s8x64 __cached_1;
      __cached_1 = zmm0;
      vec_s8x64::store(__cached_1, &__outs_0[(((fused_0n__h_1971 / 7UL) * 100352UL) + (((fused_0n__h_1971 % 7UL) * 14336UL) + ((w * 2048UL) + c)))]);
    }
  }
}

static void reorder__1050_closure_351_0wrapper(void* __stream, int8_t* __restrict__ __module_data, uint64_t i, generic_val* __restrict__ args) noexcept{
  reorder__1050_closure_351(i, (int8_t*)(args[0UL].v_ptr), (int8_t*)(args[1UL].v_ptr));
}

