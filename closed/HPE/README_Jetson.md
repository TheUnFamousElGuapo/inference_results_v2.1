# MLPerf Inference v2.1 NVIDIA-Optimized Inference on Jetson Systems
This is a repository of NVIDIA-optimized implementations for the [MLPerf](https://mlcommons.org/en/) Inference Benchmark.
This README is a quickstart tutorial on how to use our code for Jetson systems as a public / external user.
It is recommended to also read README.md for general instructions

**NOTE**: This document is autogenerated from internal documentation. If something is wrong or confusing, please contact NVIDIA.

---

NVIDIA Jetson is a platform for AI at the edge. Its high-performance, low-power computing for deep learning and computer vision makes it the ideal platform for compute-intensive projects. The Jetson platform includes a variety of Jetson modules together with NVIDIA JetPackâ„¢ SDK.
Each Jetson module is a computing system packaged as a plug-in unit (a System on Module (SOM)). NVIDIA offers a variety of Jetson modules with different capabilities.
JetPack bundles all of the Jetson platform software, starting with NVIDIA Jetson Linux. Jetson Linux provides the Linux kernel, bootloader, NVIDIA drivers, flashing utilities, sample file system, and more for the Jetson platform.

### NVIDIA Submissions

The Jetson/Orin submission supports:

- ResNet50 (Offline, Single Stream, and Multistream), at 99% of FP32 accuracy target
- 3D-unet (Offline, Single Stream), at 99% of FP32 accuracy target
- bert (Offline, Single Stream), at 99% of FP32 accuracy target
- rnn-t (Offline, Single Stream), at 99% of FP32 accuracy target

To generate the preprocessed datasets, follow the benchmark-specific instructions described in the `[README.md](http://README.md)` files stored in `code/[benchmark]/openvino` for each benchmark.

### Orin Jetson Setup

Note that for ML workload it is benefitial to enable large page size (>=64kb) for best performance. This step is necessary to reproduce our MLperf results. Without the optmization we observe 3-6% perf regression across supported benchmarks. For result reproduction, please follow Jetson [kernel customization](https://docs.nvidia.com/jetson/archives/r34.1/DeveloperGuide/text/SD/Kernel/KernelCustomization.html#building-the-kernel) and enable 64kb in knernel config and rebuild kernel.

```
CONFIG_ARM64_64K_PAGES=y
```

### Orin Jetson Perf/W Optmization

NVIDIA Orin SoC enjoy various high-end peripheral features such as GbE, Wi-Fi 6, and 4k HDMI. In MLperf benchmarks we disabled few of the more demanding features to minimize idle power and consolidated communications through single usb-c port.

#### Disabling Wi-Fi
```
echo 14100000.pcie > /sys/bus/platform/drivers/tegra194-pcie/unbind
```
#### Limit display clock
```
echo 1 > /sys/kernel/debug/bpmp/debug/clk/nafll_dce/mrq_rate_locked
echo 115200000 > /sys/kernel/debug/bpmp/debug/clk/nafll_dce/rate
```
#### Limit DRAM clock
```
echo 1 > /sys/kernel/debug/bpmp/debug/clk/emc/mrq_rate_locked
echo 2133000000 > /sys/kernel/debug/bpmp/debug/clk/emc/rate
```


### Running a Benchmark

As noted in README.md Jetson benchmark run natively without docker container. We provided preconfigured flag "--config_ver=[maxq]" that captures our maxQ submission system setup. Follow the steps in the main README.md for instructions on compliance tests and making an actual submission.

