<div class="intro-container">
  <h1 class="intro">Intel® Neural Compressor</h1>
  <p class="intro">
    <br>
    <br>
    Intel® Neural Compressor supports automatic quantization tuning flow by converting
    quantizable layers to INT8 and allowing users to control model accuracy and performance tradeoffs
    and implements the latest quantization algorithms from the research community.
    <br>
    <br>
    Visit the Intel® Neural Compressor online document website at:
    <a href="https://intel.github.io/neural-compressor/">intel.github.io/neural-compressor</a>.
  </p>

  <div class="steps intro">
    <p>Run experiments with quantized models in 4 steps:</p>
    <p><a class="big-number">1</a> <a>Create project and choose input model</a></p>
    <p><a class="big-number">2</a> <a>Add optimization to get optimized models</a></p>
    <p><a class="big-number">3</a> <a>Benchmark and profile optimized models</a></p>
  </div>
</div>

<mat-card (click)="createNewProject()" queryParamsHandling="preserve" class="big-button">
  <img class="big-icon" src="./../../assets/create-new.png">
  <div>
    <h1 class="margin-top"> Create new project</h1>
    <h3 class="margin-top margin-left">Click to create new project and start adding optimizations.</h3>
  </div>
</mat-card>

<mat-spinner style="margin:0 auto;" mode="indeterminate" class="big-spinner-center"
  [style.display]="showSpinner ? 'block' : 'none'">
</mat-spinner>