# MLPerf Inference v2.1 NVIDIA-Optimized Inference on Jetson Systems
This is a repository of NVIDIA-optimized implementations for the [MLPerf](https://mlcommons.org/en/) Inference Benchmark.
This README is a quickstart tutorial on how to use our code for Jetson systems as a public / external user.
It is recommended to also read README.md for general instructions

**NOTE**: This document is autogenerated from internal documentation. If something is wrong or confusing, please contact NVIDIA.

---

NVIDIA Jetson is a platform for AI at the edge. Its high-performance, low-power computing for deep learning and computer vision makes it the ideal platform for compute-intensive projects. The Jetson platform includes a variety of Jetson modules together with NVIDIA JetPackâ„¢ SDK.
Each Jetson module is a computing system packaged as a plug-in unit (a System on Module (SOM)). NVIDIA offers a variety of Jetson modules with different capabilities.
JetPack bundles all of the Jetson platform software, starting with NVIDIA Jetson Linux. Jetson Linux provides the Linux kernel, bootloader, NVIDIA drivers, flashing utilities, sample file system, and more for the Jetson platform.

### NVIDIA Submissions

The Jetson/Orin submission supports:

- ResNet50 (Offline, Single Stream, and Multistream), at 99% of FP32 accuracy target
- RetinaNet (Offline, Single Stream, and Multistream), at 99% of FP32 accuracy target
- 3D-unet (Offline and Single Stream), at 99% and 99.9% of FP32 accuracy target
- bert (Offline and Single Stream), at 99% of FP32 accuracy target
- rnn-t (Offline and Single Stream), at 99% of FP32 accuracy target

To generate the preprocessed datasets, follow the benchmark-specific instructions described in the [README.md](http://README.md) files for each benchmark.

### Orin Jetson Setup for AI Developers

For developer who wishes to replicate NVIDIA MLperf inference results, NVIDIA released [22.08 Jetson CUDA-X AI Developer Preview](https://developer.nvidia.com/embedded/22.08-jetson-cuda-x-ai-developer-preview) with official support.

#### What is in the AI Developer Preview?

The Orin Jetson CUDA components used for MLperf inference benchmark were tabulated below. Using the recommended tool alignment improves stability and performance reproducability.

| JetPack Component | MLperf Inference v2.1 CUDA Components |
|-------------------|---------------------------------------|
| L4T               | 35.1.1                                |
| CUDA              | 11.4.14 RC11                          |
| cuDNN             | 8.5.0                                 |
| TensorRT          | 8.5.0                                 |

Because it is benefitial to enable large page size (>=64kb) for Mlperf inference workload, for the best MLperf inference performance, the Jetson CUDA-X Linux kernel came with 64kb page size. For developers with their own custom kernel images, please follow Jetson [kernel customization](https://docs.nvidia.com/jetson/archives/r34.1/DeveloperGuide/text/SD/Kernel/KernelCustomization.html#building-the-kernel) and enable/disable 64kb page size in the knernel config and rebuild the kernel.

```
CONFIG_ARM64_64K_PAGES=y
```

#### Trouble Shooting

- System Non-operational After Flashing the L4T Image`

Make sure the following dependencies are installed on your host system

```
sudo apt update && sudo apt install -y libxml2-utils && sudo apt install -y qemu-user-static
```

### Orin Jetson Perf Optmization

The best ML performance can be achieved with highest power settings. In NVIDIA published Orin benchmarks, the board were configured to use MAXN power profile. The MAXN mode unlock GPU/DLA/CPU/EMC TDP restrictions and is available through flash config switch. More board flash related guide may be obtained on the Jetson [Flash Script Usage](https://docs.nvidia.com/jetson/archives/l4t-archived/l4t-3271/index.html#page/Tegra%20Linux%20Driver%20Package%20Development%20Guide/flashing.html#wwpID0E0KO0HA).

```
sudo ./flash.sh jetson-agx-orin-devkit-maxn mmcblk0p1
```

To revert to a non-MAXN config, flash with the Orin board's default config. For example, flashing a Concord board:

```
sudo ./flash.sh concord mmcblk0p1
```

To check if a board supports MAXN. First use nvpmodel to select MAXN mode. Then use jetson_clocks reset to the highest clocks config:
```
sudo /usr/sbin/nvpmodel -f /etc/nvpmodel.conf -m 0
sudo jetson_clocks & sudo jetson_clocks --show
```
GPU/DLA should be maxed out at 1.3GHz/1.6Ghz respectively. EMC frequency should also maxed out at 3.2Ghz (note that default EMC clock max out at 3.199Ghz which bottlenecks GPU/DLA).
### Orin Jetson Perf/W Optmization

NVIDIA Orin SoC packed with various high-end peripheral features such as GbE, Wi-Fi 6, and 4k HDMI. In MLperf inference submission for MaxQ, power demanding features were disabled to minimize idle power. Physical I/O were disconnected and communications consolidated through a single usb-c port.

#### Disabling Wi-Fi
```
echo 14100000.pcie > /sys/bus/platform/drivers/tegra194-pcie/unbind
```
#### Limit display clock
```
echo 1 > /sys/kernel/debug/bpmp/debug/clk/nafll_dce/mrq_rate_locked
echo 115200000 > /sys/kernel/debug/bpmp/debug/clk/nafll_dce/rate
```

The MaxQ measurements were collected with the display disabled completely.

```
sudo systemctl disable gdm3

```

#### Limit DRAM clock
```
echo 1 > /sys/kernel/debug/bpmp/debug/clk/emc/mrq_rate_locked
echo 2133000000 > /sys/kernel/debug/bpmp/debug/clk/emc/rate
```

### Fan profile
The default Orin fan profiles can be tuned to optmize chip leakage and fan power tradeoff. A custom default fan profile 'mlperf' can be created in /etc/nvpower/nvfancontrol/nvfancontrol_p3701_0000.conf to minimize overall system power for MaxQ submission. Note that the ambient temperature strongly influences the operating temperature, the rule of thumb is to maintain between 55-75C for the best result.

```
...
	FAN_PROFILE mlperf {
		#TEMP 	HYST	PWM	RPM
		0	0 	255 	2900
		10	0 	255	2900
		11	0	171	1940
		23	0	171	1940
		60	0	0	0
		105	0	0	0
	}
...
FAN_DEFAULT_CONTROL open_loop
FAN_DEFAULT_PROFILE mlperf
FAN_DEFAULT_GOVERNOR pid
```

### USB-C Power Adapters
Taking advantage of the USB-C PD features, Orin results were collected on high efficiency third party USB-C adapters. For MaxP systems, Dell 130.0W Adapter (HA130PM170) were used. For MaxQ systems, Anker 715 Charger (Nano II 65W) were used.


### Running a Benchmark

As noted in README.md Jetson benchmark run natively without docker container. We provided preconfigured flag "--config_ver=maxq" that captures our maxQ submission system config. Follow the steps in the main README.md for instructions on compliance tests and making an actual submission.
