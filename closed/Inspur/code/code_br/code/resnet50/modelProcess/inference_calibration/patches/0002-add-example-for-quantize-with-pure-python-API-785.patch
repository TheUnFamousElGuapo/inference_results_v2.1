From 15477100cef756e430c8ef8ef79729f0c80c8ce6 Mon Sep 17 00:00:00 2001
From: "chen, suyue" <suyue.chen@intel.com>
Date: Fri, 18 Mar 2022 17:35:36 +0800
Subject: [PATCH 2/4] add example for quantize with pure python API (#785)

(cherry picked from commit ce50a69d10ec3a3fa3da5e4901b721cf1c36fdde)
---
 README.md                                     |  91 -----------
 docs/full_model_list.md                       | 152 ++++++++++++++++++
 examples/helloworld/tf_example8/README.md     |  28 ++++
 .../helloworld/tf_example8/requirements.txt   |   1 +
 examples/helloworld/tf_example8/test.py       |  17 ++
 neural_compressor/conf/config.py              |   3 +-
 6 files changed, 200 insertions(+), 92 deletions(-)
 create mode 100644 examples/helloworld/tf_example8/README.md
 create mode 100644 examples/helloworld/tf_example8/requirements.txt
 create mode 100644 examples/helloworld/tf_example8/test.py

diff --git a/README.md b/README.md
index 8d7f948e..61ba9e6d 100644
--- a/README.md
+++ b/README.md
@@ -923,7 +923,6 @@ Intel® Neural Compressor provides numerous examples to show the performance gai
     <th rowspan="2">model</th>
     <th colspan="3">Accuracy</th>
     <th colspan="3">Performance<br>1s4c10ins1bs/throughput<br>(samples/sec)<br></th>
-    <th colspan="3">Performance<br>2s4c20ins64bs/throughput<br>(samples/sec)<br></th>
   </tr>
   <tr>
     <th>INT8</th>
@@ -932,9 +931,6 @@ Intel® Neural Compressor provides numerous examples to show the performance gai
     <th>INT8</th>
     <th>FP32</th>
     <th>Preformance<br>Ratio[INT8/FP32]</th>
-    <th>INT8</th>
-    <th>FP32</th>
-    <th>Preformance<br>Ratio[INT8/FP32]</th>
   </tr>
 </thead>
 <tbody>
@@ -946,9 +942,6 @@ Intel® Neural Compressor provides numerous examples to show the performance gai
     <td>45.32</td>
     <td>12.53</td>
     <td>3.62x</td>
-    <td>362.21</td>
-    <td>88.38</td>
-    <td>4.10x</td>
   </tr>
   <tr>
     <td>distilbert_base_uncased_sst2</td>
@@ -958,9 +951,6 @@ Intel® Neural Compressor provides numerous examples to show the performance gai
     <td>999.98</td>
     <td>283.96</td>
     <td>3.52x</td>
-    <td>2104.26</td>
-    <td>606.58</td>
-    <td>3.47x</td>
   </tr>
   <tr>
     <td>minilm_l6_h384_uncased_sst2</td>
@@ -970,9 +960,6 @@ Intel® Neural Compressor provides numerous examples to show the performance gai
     <td>2690.5</td>
     <td>1002.7</td>
     <td>2.68x</td>
-    <td>5389.98</td>
-    <td>2333.14</td>
-    <td>2.31x</td>
   </tr>
   <tr>
     <td>roberta_base_mrpc</td>
@@ -982,9 +969,6 @@ Intel® Neural Compressor provides numerous examples to show the performance gai
     <td>508.18</td>
     <td>142.48</td>
     <td>3.57x</td>
-    <td>1167.09</td>
-    <td>311.5</td>
-    <td>3.75x</td>
   </tr>
   <tr>
     <td>bert_base_nli_mean_tokens_stsb</td>
@@ -994,81 +978,6 @@ Intel® Neural Compressor provides numerous examples to show the performance gai
     <td>504.15</td>
     <td>141.5</td>
     <td>3.56x</td>
-    <td>1096.46</td>
-    <td>332.54</td>
-    <td>3.30x</td>
-  </tr>
-  <tr>
-    <td>bert_base_sparse_mrpc</td>
-    <td>70.34%</td>
-    <td>70.59%</td>
-    <td>-0.35%</td>
-    <td>507.59</td>
-    <td>142.88</td>
-    <td>3.55x</td>
-    <td>1133.04</td>
-    <td>339.96</td>
-    <td>3.33x</td>
-  </tr>
-  <tr>
-    <td>distilroberta_base_wnli</td>
-    <td>56.34%</td>
-    <td>56.34%</td>
-    <td>0.00%</td>
-    <td>1032.04</td>
-    <td>291.78</td>
-    <td>3.54x</td>
-    <td>2309.9</td>
-    <td>620.81</td>
-    <td>3.72x</td>
-  </tr>
-  <tr>
-    <td>paraphrase_xlm_r_multilingual_v1_stsb</td>
-    <td>86.71%</td>
-    <td>87.23%</td>
-    <td>-0.60%</td>
-    <td>511.92</td>
-    <td>142.85</td>
-    <td>3.58x</td>
-    <td>1169.45</td>
-    <td>311.59</td>
-    <td>3.75x</td>
-  </tr>
-  <tr>
-    <td>distilbert_base_uncased_mrpc</td>
-    <td>84.07%</td>
-    <td>84.07%</td>
-    <td>0.00%</td>
-    <td>996.79</td>
-    <td>280.88</td>
-    <td>3.55x</td>
-    <td>2107.96</td>
-    <td>606.95</td>
-    <td>3.47x</td>
-  </tr>
-  <tr>
-    <td>finbert_financial_phrasebank</td>
-    <td>82.68%</td>
-    <td>82.80%</td>
-    <td>-0.14%</td>
-    <td>922.88</td>
-    <td>272.75</td>
-    <td>3.38x</td>
-    <td>1101.13</td>
-    <td>331.88</td>
-    <td>3.32x</td>
-  </tr>
-  <tr>
-    <td>distilbert_base_uncased_emotion</td>
-    <td>93.85%</td>
-    <td>94.20%</td>
-    <td>-0.37%</td>
-    <td>999.97</td>
-    <td>283.98</td>
-    <td>3.52x</td>
-    <td>2103.22</td>
-    <td>607.08</td>
-    <td>3.46x</td>
   </tr>
 </tbody>
 </table>
diff --git a/docs/full_model_list.md b/docs/full_model_list.md
index 7bd1214c..7a0dd199 100644
--- a/docs/full_model_list.md
+++ b/docs/full_model_list.md
@@ -1340,6 +1340,158 @@ Intel technologies may require enabled hardware, software or service activation.
 </tbody>
 </table>
 
+### INC-ENGINE Models
+<table>
+<thead>
+  <tr>
+    <th rowspan="2">Backend</th>
+    <th rowspan="2">model</th>
+    <th colspan="3">Accuracy</th>
+    <th colspan="3">Performance<br>1s4c10ins1bs/throughput<br>(samples/sec)<br></th>
+  </tr>
+  <tr>
+    <th>INT8</th>
+    <th>FP32</th>
+    <th>Acc Ratio[(INT8-FP32)/FP32]</th>
+    <th>INT8</th>
+    <th>FP32</th>
+    <th>Performance Ratio[INT8/FP32]</th>
+  </tr>
+</thead>
+<tbody>
+  <tr>
+    <td>INC-ENGINE</td>
+    <td>bert_base_mrpc</td>
+    <td>82.35%</td>
+    <td>83.09%</td>
+    <td>-0.89%</td>
+    <td>487.41</td>
+    <td>140.978</td>
+    <td>3.46x</td>
+  </tr>
+  <tr>
+    <td>INC-ENGINE</td>
+    <td>bert_base_nli_mean_tokens_stsb</td>
+    <td>89.26%</td>
+    <td>89.55%</td>
+    <td>-0.32%</td>
+    <td>504.147</td>
+    <td>141.504</td>
+    <td>3.56x</td>
+  </tr>
+  <tr>
+    <td>INC-ENGINE</td>
+    <td>bert_base_sparse_mrpc</td>
+    <td>70.34%</td>
+    <td>70.59%</td>
+    <td>-0.35%</td>
+    <td>507.585</td>
+    <td>142.876</td>
+    <td>3.55x</td>
+  </tr>
+  <tr>
+    <td>INC-ENGINE</td>
+    <td>bert_large_squad</td>
+    <td>90.70</td>
+    <td>90.87</td>
+    <td>-0.19%</td>
+    <td>45.32</td>
+    <td>12.531</td>
+    <td>3.62x</td>
+  </tr>
+  <tr>
+    <td>INC-ENGINE</td>
+    <td>distilbert_base_uncased_emotion</td>
+    <td>93.85%</td>
+    <td>94.20%</td>
+    <td>-0.37%</td>
+    <td>999.973</td>
+    <td>283.975</td>
+    <td>3.52x</td>
+  </tr>
+  <tr>
+    <td>INC-ENGINE</td>
+    <td>distilbert_base_uncased_mrpc</td>
+    <td>84.07%</td>
+    <td>84.07%</td>
+    <td>0.00%</td>
+    <td>996.79</td>
+    <td>280.88</td>
+    <td>3.55x</td>
+  </tr>
+  <tr>
+    <td>INC-ENGINE</td>
+    <td>distilbert_base_uncased_sst2</td>
+    <td>90.14%</td>
+    <td>90.25%</td>
+    <td>-0.12%</td>
+    <td>999.98</td>
+    <td>283.96</td>
+    <td>3.52x</td>
+  </tr>
+  <tr>
+    <td>INC-ENGINE</td>
+    <td>distilroberta_base_wnli</td>
+    <td>56.34%</td>
+    <td>56.34%</td>
+    <td>0.00%</td>
+    <td>1032.043</td>
+    <td>291.782</td>
+    <td>3.54x</td>
+  </tr>
+  <tr>
+    <td>INC-ENGINE</td>
+    <td>dlrm</td>
+    <td>78.07%</td>
+    <td>78.10%</td>
+    <td>-0.04%</td>
+    <td>54898.34</td>
+    <td>48331.14</td>
+    <td>1.14x</td>
+  </tr>
+  <tr>
+    <td>INC-ENGINE</td>
+    <td>finbert_financial_phrasebank</td>
+    <td>82.68%</td>
+    <td>82.80%</td>
+    <td>-0.14%</td>
+    <td>922.877</td>
+    <td>272.751</td>
+    <td>3.38x</td>
+  </tr>
+  <tr>
+    <td>INC-ENGINE</td>
+    <td>minilm_l6_h384_uncased_sst2</td>
+    <td>89.33%</td>
+    <td>90.14%</td>
+    <td>-0.90%</td>
+    <td>2690.501</td>
+    <td>1002.695</td>
+    <td>2.68x</td>
+  </tr>
+  <tr>
+    <td>INC-ENGINE</td>
+    <td>paraphrase_xlm_r_multilingual_v1_stsb</td>
+    <td>86.71%</td>
+    <td>87.23%</td>
+    <td>-0.60%</td>
+    <td>511.919</td>
+    <td>142.851</td>
+    <td>3.58x</td>
+  </tr>
+  <tr>
+    <td>INC-ENGINE</td>
+    <td>roberta_base_mrpc</td>
+    <td>89.71%</td>
+    <td>88.97%</td>
+    <td>0.83%</td>
+    <td>508.184</td>
+    <td>142.483</td>
+    <td>3.57x</td>
+  </tr>
+</tbody>
+</table>
+
 ### BACKUP
 <table>
 <tr><th>System Configuration</th><th>Intel Xeon Platinum 8380 Scalable processor</th></tr>
diff --git a/examples/helloworld/tf_example8/README.md b/examples/helloworld/tf_example8/README.md
new file mode 100644
index 00000000..0240c94c
--- /dev/null
+++ b/examples/helloworld/tf_example8/README.md
@@ -0,0 +1,28 @@
+tf_example8 example
+=====================
+This example is used to demonstrate how to quantize a TensorFlow model with pure python API.  
+
+### 1. Installation
+```shell
+pip install -r requirements.txt
+```
+
+### 2. Download the FP32 model
+```shell
+wget https://storage.googleapis.com/intel-optimized-tensorflow/models/v1_6/mobilenet_v1_1.0_224_frozen.pb
+```
+
+### 3. Run Command
+```shell
+python test.py
+``` 
+
+### 4. Introduction
+We can create a quantizer without config yaml, only need to set the dataloader with dummy dataset to generate an int8 model.   
+```python
+    quantizer = Quantization()
+    quantizer.model = './mobilenet_v1_1.0_224_frozen.pb'
+    dataset = quantizer.dataset('dummy', shape=(20, 224, 224, 3))
+    quantizer.calib_dataloader = common.DataLoader(dataset)
+    quantized_model = quantizer.fit()
+```
\ No newline at end of file
diff --git a/examples/helloworld/tf_example8/requirements.txt b/examples/helloworld/tf_example8/requirements.txt
new file mode 100644
index 00000000..34523e93
--- /dev/null
+++ b/examples/helloworld/tf_example8/requirements.txt
@@ -0,0 +1 @@
+intel-tensorflow==2.7.0
\ No newline at end of file
diff --git a/examples/helloworld/tf_example8/test.py b/examples/helloworld/tf_example8/test.py
new file mode 100644
index 00000000..2c097f37
--- /dev/null
+++ b/examples/helloworld/tf_example8/test.py
@@ -0,0 +1,17 @@
+import tensorflow as tf
+from neural_compressor.experimental import Quantization,  common
+
+tf.compat.v1.disable_eager_execution()
+
+def main():
+
+    quantizer = Quantization()
+    quantizer.model = './mobilenet_v1_1.0_224_frozen.pb'
+    dataset = quantizer.dataset('dummy', shape=(20, 224, 224, 3))
+    quantizer.calib_dataloader = common.DataLoader(dataset)
+    quantized_model = quantizer.fit()
+
+
+if __name__ == "__main__":
+
+    main()
\ No newline at end of file
diff --git a/neural_compressor/conf/config.py b/neural_compressor/conf/config.py
index 95b9541c..822f5f05 100644
--- a/neural_compressor/conf/config.py
+++ b/neural_compressor/conf/config.py
@@ -841,7 +841,8 @@ quantization_default_schema = Schema({
                                                       'scale_propagation_concat': True,
                                                       'first_conv_or_matmul_quantization': True},
                                       'model_wise': {'weight': {'bit': [7.0]},
-                                                     'activation': {}}}): dict,
+                                                     'activation': {}},
+                                        'dtype':'int8'}): dict,
 
     Optional('tuning', default={
         'strategy': {'name': 'basic'},
-- 
2.17.1

